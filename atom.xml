<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mia&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/2d5354ebc5a8c2413323ef55a6c6d252</icon>
  <subtitle>Je marche lentement, mais je ne recule jamais.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-02-07T13:46:46.379Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Mia Feng</name>
    <email>skaudreymia@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Paper--Graph Level Anomaly Detection</title>
    <link href="http://yoursite.com/posts/notes/2021-02-07-notes-paper-fall-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-02-07-notes-paper-fall-survey.html</id>
    <published>2021-02-07T13:46:39.000Z</published>
    <updated>2021-02-07T13:46:46.379Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full">Elderly Fall Detection Systems: A Literature Survey</a></p><a id="more"></a><h2 id="Why">Why?</h2><h2 id="Goals">Goals</h2><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><h3 id="Implementation">Implementation</h3><h2 id="Experiments">Experiments</h2><h2 id="Conclusion">Conclusion</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full&quot;&gt;Elderly Fall Detection Systems: A Literature Survey&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="fall" scheme="http://yoursite.com/tags/fall/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
  </entry>
  
  <entry>
    <title>What Can Artificial Intelligence Do in Data Assimilation? Dec. 9th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2021-02-04-talk-mlutility.html"/>
    <id>http://yoursite.com/posts/talks/2021-02-04-talk-mlutility.html</id>
    <published>2021-02-05T00:50:10.390Z</published>
    <updated>2018-12-10T02:06:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my lab, and most of them haven’t learn much about AI. The talk would like to show them what can AI do these days, and help them figure out what else can AI do in data assimilation.</p><p>Slides are avaliable <a href="/assets/slides/mlDo/mlUtility.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my l
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="Data assimilation" scheme="http://yoursite.com/tags/Data-assimilation/"/>
    
  </entry>
  
  <entry>
    <title>The Introduction of Infrared Hyperspectral Data and Kernel PCA, June 5th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2021-02-04-talk-hyp.html"/>
    <id>http://yoursite.com/posts/talks/2021-02-04-talk-hyp.html</id>
    <published>2021-02-05T00:50:10.320Z</published>
    <updated>2018-12-10T08:01:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:</p><ul><li>high spectral correlation,</li><li>high spatial correlation,</li><li>and sparsity,</li></ul><p>and they casue a trouble during processing.</p><a id="more"></a><p>This talk explained why they are highly correlated but also sparse. Kernel PCA for compressing was also tested.</p><p>Check <a href="/assets/slides/hyp/hypCompression.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;high spectral correlation,&lt;/li&gt;
&lt;li&gt;high spatial correlation,&lt;/li&gt;
&lt;li&gt;and sparsity,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and they casue a trouble during processing.&lt;/p&gt;
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="compression" scheme="http://yoursite.com/tags/compression/"/>
    
      <category term="reconstruction" scheme="http://yoursite.com/tags/reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Multivariate Interpolation of Wind Fields Based on Gaussian Process Regression, Jan. 24th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2021-02-04-talk-gpr.html"/>
    <id>http://yoursite.com/posts/talks/2021-02-04-talk-gpr.html</id>
    <published>2021-02-05T00:50:10.260Z</published>
    <updated>2018-12-10T02:45:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check the <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html">projects’ introduction </a> and <a href="https://github.com/skaudrey/gpml/">github</a> for more details.</p><p>Slides are avaliable <a href="/assets/slides/gpr/windInterpolation.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check th
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
      <category term="interpolation" scheme="http://yoursite.com/tags/interpolation/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks and Remote Sensing, July 26th, 2019.</title>
    <link href="http://yoursite.com/posts/talks/2021-02-04-talk-gan-rs.html"/>
    <id>http://yoursite.com/posts/talks/2021-02-04-talk-gan-rs.html</id>
    <published>2021-02-05T00:50:10.220Z</published>
    <updated>2019-08-18T14:16:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>Some works using GANs handle the problems in remote sensing.</p><p>Check <a href="/assets/slides/GAN/GANRS.pdf">slide</a> for more details.``</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Some works using GANs handle the problems in remote sensing.&lt;/p&gt;
&lt;p&gt;Check &lt;a href=&quot;/assets/slides/GAN/GANRS.pdf&quot;&gt;slide&lt;/a&gt; for more detai
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="remote sensing images" scheme="http://yoursite.com/tags/remote-sensing-images/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2021-02-04-talk-da.html"/>
    <id>http://yoursite.com/posts/talks/2021-02-04-talk-da.html</id>
    <published>2021-02-05T00:50:10.160Z</published>
    <updated>2018-11-17T08:19:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it from other forms of machine learning, image analysis, and statistical methods. This talk discussed the basic ideas of machine leaning, and compared it with machine learning. It is given after I came back from Harbin’s summer school in Agust, 2017. After this talk, I began to throw myself into studying machine learning.</p><p>Check <a href="/assets/slides/D.A/pres.pdf">slide</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2021-02-04-notes-fourier-GCN.html"/>
    <id>http://yoursite.com/posts/talks/2021-02-04-notes-fourier-GCN.html</id>
    <published>2021-02-05T00:50:07.490Z</published>
    <updated>2021-01-12T20:59:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.</p><p>Check <a href="/assets/slides/notes/GCN/fourier-GCN.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.&lt;/p&gt;
&lt;p&gt;Check &lt;a href
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>The naive implementation of some popular machine learning algorithms.</title>
    <link href="http://yoursite.com/posts/projects/2021-02-04-ml-implement.html"/>
    <id>http://yoursite.com/posts/projects/2021-02-04-ml-implement.html</id>
    <published>2021-02-05T00:50:07.380Z</published>
    <updated>2018-12-10T12:57:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>Naive implementations of some M.L. algorithms, which are updated continuously.</p><p>The algorithms that have been implemented are listed as follows:</p><ul><li>Logistic Regression,</li><li>SVM solved by SMO,</li><li>K-Means，</li><li>GMM solved by EM,</li><li>Perceptron，</li><li>Naive Bayes,</li><li>LeNet-Keras，</li><li>MLP-Numpy solved with BP,</li><li>MCMC sampling.</li></ul><p>Codes are available <a href="https://github.com/skaudrey/ml_algorithm">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Naive implementations of some M.L. algorithms, which are updated continuously.&lt;/p&gt;
&lt;p&gt;The algorithms that have been implemented are liste
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="naive" scheme="http://yoursite.com/tags/naive/"/>
    
  </entry>
  
  <entry>
    <title>Computing in the 21th Century &amp; Asia Faculty Summit held by Microsoft, Nov., 2018.</title>
    <link href="http://yoursite.com/posts/meetings/2021-02-04-microsoft.html"/>
    <id>http://yoursite.com/posts/meetings/2021-02-04-microsoft.html</id>
    <published>2021-02-05T00:50:07.350Z</published>
    <updated>2018-12-10T08:53:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is <a href="https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda">here</a>.</p><p>Babysitting AI and computational neuroscience impressed me a lot.</p><a id="more"></a><h1>computing neuroscience: robots with feelings.</h1><p>Prof. Lenore Blue’s keynote is about computational neuroscience. They try to let robots feel pain, and to simulate the long and short term encoding happening in our brains.</p><p>What she talked reminds me of something I had read before.</p><p>According to the book <em>Psychology</em> written by Daniel Schacter, our brains do encode information into long and short codes, and the short form is possibly trasformed into a long one. Even though we don’t mean to encode or memorize something sometimes, encoding still occurs unconsciously.</p><p>Reviewing is one useful way for recalling these information. Moreover, if you are in the similar environment in which you encoded the codes before, you will have higher possibility to recall it. However, the <strong>encoding error</strong> happend during reviewing is more, and that’s why a detective should try to get full information while inquiring evidences from witnesses at the 1st time.</p><p>Also, our brains are more sensible to pictorial information compared with text information. So, if you try to make each thing you want to keep code as a photo, you can boost the capability of memorizing.</p><h1>Yoshu Bengio: Beyond i.i.d. and babysitting AI</h1><p>One of the basic assumption that makes generation possible is independent identically distributed assumption. However, influenced by observing equipments, imbalanced samples and others, the distributions of training data and test data are not always the same. Hence, Prof. Bengio’s team proposed that all data are sampled from the same system rather than same distribution. As for weather of two different seasons, data desciping them are sampled from the same atmospheric circulation system, but they don’t distribute identically. Bengio said they tend to initialize this system with diversed initial conditions, and the result of this distribution will be taken as what the data set follows. It makes sense.</p><p><em>One thing that troubles me is, how can I model the system and figure out the initial conditions? For things with obvious physical rules, it is easy, and even the model’s codes are open-acssessed online. What if the one I don’t know? How to make this idea works in common situations?</em></p><p>CNN is renowned as its power in representation learning, which encodes a variety of information into vectors. Our brains also work like this. Nontheless, what they learn are supervised, and the utility of binary network, the simplification of network structure all demostrate that the captured information are redundant. The model learned is fragile, too. After adding some noises into an image, even though the image dosen’t change visually for us humans, model can not tell what it is as before. It all comes from the uncontrolling unsupervision. Babysitting AI aims at modeling with environmental information and other information, so that leading AI models.</p><h1>Andrew C. Yao: The Advent of Quantum Computing</h1><p>The quantum computing will offer exponential speedup for crypto-code breaking, simulation of quantum physical systems, simulation of materials, chemistry, and biology, nonlinear optimization, ML and AI. It will break through the bottleneck of computing.</p><p>Its implementation is like crystallography. In terms of crystallography, you take an X-ray photo for a crystal and then compute its structure. For quantum computing, instead of taking a real photo, you just need to collect a polynomial number of sample points. By wave-particle duality, this single photo can recreate the raw image probabilistically.</p><p>According to Andrew, dimond qubits are in the highest possibility to be used in our laptops.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is &lt;a href=&quot;https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Babysitting AI and computational neuroscience impressed me a lot.&lt;/p&gt;
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="applications" scheme="http://yoursite.com/tags/applications/"/>
    
      <category term="tendency" scheme="http://yoursite.com/tags/tendency/"/>
    
  </entry>
  
  <entry>
    <title>Clouds detection of infrared hyperspectral data based on logistic.</title>
    <link href="http://yoursite.com/posts/projects/2021-02-04-lr.html"/>
    <id>http://yoursite.com/posts/projects/2021-02-04-lr.html</id>
    <published>2021-02-05T00:50:07.310Z</published>
    <updated>2018-12-10T13:17:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed with what clouds release, and they exist in more than 90% IFOVs, cloudy IFOVs have to be kicked off in order to get clean data.</p><p>Therefore, a new feature construction method is proposed for infrared hyperspectral data, such as what IASI releases. Concretely, four channels of IASI are picked, namely channel 921, channel 386, channel 306 and channel 241. They are picked because of physical characteristics. And then, cloudy IFOVs are detected by logistic regression.</p><p>The recall, auc and accuracy of this new method carried on IASI data was more than 0.95 when detecting IFOVs of sea, while the result of land’s IFOVs was less than it. After adding surface emissivity features, the auc of it increased by aroud 5%, and recall of it grew by 10% approximately.</p><p>Codes are available <a href="https://github.com/skaudrey/cloud">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed 
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="logistic" scheme="http://yoursite.com/tags/logistic/"/>
    
  </entry>
  
  <entry>
    <title>Build up personal blog</title>
    <link href="http://yoursite.com/posts/notes/2021-02-04-hexo+killy+github%20pages=blog.html"/>
    <id>http://yoursite.com/posts/notes/2021-02-04-hexo+killy+github pages=blog.html</id>
    <published>2021-02-05T00:50:07.260Z</published>
    <updated>2021-01-12T21:00:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github.</p><a id="more"></a><h2 id="Preliminaries">Preliminaries</h2><p>Before starting, you need:</p><ul><li><p>node.js+npm,</p><p>Get node.js from <a href="https://pan.baidu.com/s/1kU5OCOB#list/path=%2Fpub%2Fnodejs">here</a>. Check <a href="https://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000/00143450141843488beddae2a1044cab5acb5125baf0882000">here</a> for more info about node.</p></li><li><p>hexo,<br>Install by npm:</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li><li><p>git,</p></li><li><p>an account of Github,</p></li></ul><p>and configure the ssh-key on your device.</p><h2 id="Build-blog">Build blog</h2><h3 id="Initialize-hexo-with-hexo">Initialize hexo with hexo</h3><p>Create a local folder as your root directory, such as “blog”, and go to the directory in your terminal and initialize it by hexo.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br><span class="line">$ <span class="built_in">cd</span> blog</span><br></pre></td></tr></table></figure><p>Then initialize this directory with npm.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="Link-hexo-with-Github">Link hexo with Github</h3><p>Set deployment tool,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>and initialize the remote repository for your blog on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br><span class="line">$ git add *</span><br><span class="line">$ git commit -m <span class="string">&quot;init commit&quot;</span></span><br></pre></td></tr></table></figure><p>Change the deployment in file “_config.yml” like:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line">deploy: </span><br><span class="line">    <span class="built_in">type</span>: git </span><br><span class="line">    repo: git@github.com:jack/jack.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure><p>Tips: The name of your hosting repository should be “[githubname].github.io”, such as “<a href="http://jack.github.io">jack.github.io</a>”. And mind the blankspaces while rewriting file “_config.yml”.</p><h3 id="Generate-static-files">Generate static files</h3><p>Do it before you push it on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><h1>see your pages</h1><p>Click https://[githubname].github.io/, such as <a href="https://jack.github.io/">https://jack.github.io/</a>.</p><h2 id="Customization">Customization</h2><h3 id="change-theme">change theme</h3><p>I picked theme yilia. Configuration should be done as bellow:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ git <span class="built_in">clone</span> https://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br></pre></td></tr></table></figure><p>Change the default theme defined in “_config.yml” under root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: yilia</span><br></pre></td></tr></table></figure><h3 id="upload-your-avatar">upload your avatar</h3><p>New a folder under the “source” directory, I named it assets. I also new the “img” folder for pictures. Put you avatar picture here. Then reconfigure the _config.yml file beneath theme “yilia”'s folder, which is:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avatar: /assets/img/avatar.jpg</span><br></pre></td></tr></table></figure><h3 id="classify-your-posts-by-categories-rather-than-tags-in-default">classify your posts by categories rather than tags in default</h3><p>Now take your eye away from file “_config.yml” under theme yilia, open the file “_config.yml” under the root directory of your blog. You need to configure category_map, for instance,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  about: about</span><br><span class="line">  notes: notes</span><br><span class="line">  projects: projects</span><br><span class="line">  papers: papers</span><br><span class="line">  talks: talks</span><br><span class="line">  meetings: meetings</span><br></pre></td></tr></table></figure><p>Each pair of it can be different, it is just a mapping, such as:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  parole: talks</span><br><span class="line">  关于我: about</span><br></pre></td></tr></table></figure><h3 id="change-the-naming-rule-of-a-new-post">change the naming rule of a new post</h3><p>The default naming rule of hexo is YYYY/MM/DD/[post name], which leads to a hyper-link without html suffix. I change it as html. It can be accomplished by configure the _config.yml in root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permalink: posts/:category/:year-:month-:day-:title.html</span><br></pre></td></tr></table></figure><h3 id="Truncate-the-post-in-home-list-when-it-is-too-long">Truncate the post in home list when it is too long.</h3><p>You need to add</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure><p>after where you want to trucate in a post. And confiure the _<em>config.yml</em> under themes’ folder.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The truncate signal while post is too long.</span></span><br><span class="line">excerpt_link: <span class="string">&quot;more&quot;</span></span><br></pre></td></tr></table></figure><h3 id="Support-Latex">Support Latex</h3><p>Check <a href="https://www.jianshu.com/p/5623c5e35c93">here</a> for details.</p><h1>Tips</h1><p>You can debug pages locally by</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>, which is convenient before deployment.</p><h1>Acknowledgement</h1><p>Thank <a href="https://www.cnblogs.com/wumz/p/8030244.html">Mauger</a>, and <a href="https://github.com/litten/hexo-theme-yilia">litten</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>HCR--Compress and Resonstruct Hyperspectral Data.</title>
    <link href="http://yoursite.com/posts/projects/2021-02-04-hcr.html"/>
    <id>http://yoursite.com/posts/projects/2021-02-04-hcr.html</id>
    <published>2021-02-05T00:50:07.230Z</published>
    <updated>2018-12-10T12:45:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression and reconstruction. The numerous infrared hyperspectral data are overloaden for computing resources currently. Taking IASI, an atmosphere detector on satellite Metop launched by European Organization for the Exploitation of Meteorological Satellites (EUMETSAT), as an example, it has 8461 channels, which can detect atmosphere vertically in details. To process these data more efficiently, compressing them and then reconstructing is required.</p><p>Considering their high correlation in spectral and spatial dimension, a new compressing and reconstructing network HCR is proposed. Concretely, the radiation brightness values are gridded so that one value at specific location is recongnized as a color value at this pixel. After normalizing by batch normalization, HCR compresses by convolution and reconstructs by deconvlution.</p><p>Carrying on IASI data, the RMSE of this new method was decreased by 5% at least compared with the result of principle component analysis (PCA) in the same compression ratio. The compression kernels encode tempetature information and reconstruct it. In reconstruction, the kernels’ weights for likewise data are similar.</p><p>Codes are available <a href="https://github.com/skaudrey/hyp">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression an
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="cnn" scheme="http://yoursite.com/tags/cnn/"/>
    
      <category term="compress" scheme="http://yoursite.com/tags/compress/"/>
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
  </entry>
  
  <entry>
    <title>The summer school held in Harbin, Aug. 2017.</title>
    <link href="http://yoursite.com/posts/meetings/2021-02-04-harbin.html"/>
    <id>http://yoursite.com/posts/meetings/2021-02-04-harbin.html</id>
    <published>2021-02-05T00:50:07.190Z</published>
    <updated>2018-12-10T07:55:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check <a href="http://mss2017.hit.edu.cn/showSubjectDominWebSite.do">here</a> for more info. I gave a talk to the students in my lab after returning, <a href="https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html">here</a> are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation.</p><a id="more"></a><p>The themes given by those professors are listed below.</p><ul><li><p>Prof. Francois</p><p>Research Area：Variational data assimilation (VAR), especially 4DVAR.</p><p>Keynotes：The direvation of adjoint models, sensitivity analysis and the introduction of image assimilation. See the [minutes file](/assets/notes/harbin/François meeting minutes.pdf) for details.</p></li><li><p>Prof. Jordan</p><p>Research Area：Statistical Learning</p><p>Keynotes: Summarize popular machine learning algorithms, and prove the convergence etc. See the [minutes file](/assets/notes/harbin/Jordan meeting minutes.pdf) for details.</p></li><li><p>Prof. Jurgen</p><p>Research Area：AI, DL</p><p>Keynotes: The introduction of utilizing AI. Check more from his <a href="http://people.idsia.ch/~juergen/">home page</a>.</p></li><li><p>Prof. Ma</p><p>Research Area：Compression sensing.</p><p>Keynotes: The introduction of compression sensing and its applications.</p></li><li><p>Prof. Cai</p><p>Research Area：Statistical inference.</p><p>Keynotes: Statistical inference in high-dimensions. No slides, the minutes file is [here](/assets/notes/harbin/Tony meeting minutes.pdf).</p></li></ul><p>Slides and records are available in the <a href="https://pan.baidu.com/s/1jGj07koiMIV-MOf17N_jeg">baidu network disk</a> with password 6o2x. Enjoy yourself.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check &lt;a href=&quot;http://mss2017.hit.edu.cn/showSubjectDominWebSite.do&quot;&gt;here&lt;/a&gt; for more info. I gave a talk to the students in my lab after returning, &lt;a href=&quot;https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html&quot;&gt;here&lt;/a&gt; are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation.&lt;/p&gt;
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="summer school" scheme="http://yoursite.com/tags/summer-school/"/>
    
      <category term="mathematics" scheme="http://yoursite.com/tags/mathematics/"/>
    
  </entry>
  
  <entry>
    <title>Weather processes interpolation based on GPR</title>
    <link href="http://yoursite.com/posts/projects/2021-02-04-gpr.html"/>
    <id>http://yoursite.com/posts/projects/2021-02-04-gpr.html</id>
    <published>2021-02-05T00:50:07.140Z</published>
    <updated>2021-01-12T19:35:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale dependencies of weather processes. Weather processes with and without cyclones are discussed, and two interpolation methods are proposed. Check <a href="http://www.mdpi.com/2073-4433/9/5/194/pdf">paper</a> for more information. Codes are available <a href="https://github.com/skaudrey/gpml">here</a>.</p><h1>Reference</h1><pre><code>Carl Edward Rasmussen. Gaussian process for Machine Learning.</code></pre><h1>Acknowledgement</h1><pre><code>Thanks for the opening source toolbox GAUSSIAN PROCESS REGRESSION AND CLASSIFICATION Toolbox version 4.0, programmed by Carl et al.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale depen
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Graph Level Anomaly Detection</title>
    <link href="http://yoursite.com/posts/notes/2021-01-28-notes-paper-anomal-graphlevel-SSL.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-28-notes-paper-anomal-graphlevel-SSL.html</id>
    <published>2021-01-28T18:33:39.000Z</published>
    <updated>2021-01-28T18:35:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="http://snap.stanford.edu/class/cs224w-2019/project/26424135.pdf">Graph Level Anomaly Detection</a></p><a id="more"></a><h2 id="Why">Why?</h2><h2 id="Goals">Goals</h2><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><h3 id="Implementation">Implementation</h3><h2 id="Experiments">Experiments</h2><h2 id="Conclusion">Conclusion</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;http://snap.stanford.edu/class/cs224w-2019/project/26424135.pdf&quot;&gt;Graph Level Anomaly Detection&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Graph Embedded Pose Clustering for Anomaly Detection</title>
    <link href="http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html</id>
    <published>2021-01-15T20:17:13.000Z</published>
    <updated>2021-01-21T22:51:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1912.11850.pdf">Graph Embedded Pose Clustering for Anomaly Detection</a></p><p>Code <a href="https://github.com/amirmk89/gepc">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li><p>Anomaly detection task</p><ul><li>Fine-grained anomaly detection: Detecting abnormal variations of an action: e.g. an abnormal type of walking</li><li>Coarse-grained anomaly detection: Defining normal actions and regard other action as abnormal. Aka there are multiple poses regarded as normal actions, rather than a single normal action.</li></ul></li><li><p>Video anomaly detection</p><ul><li>Reconstructive models:  learn a feature representation for each sample and attempt to reconstruct a sample based on that embedding, often using <strong>Autoencoder</strong>. Samples poorly reconstructed are considered anomalous.</li><li>Predictive models: model the current frame based on a set of previous frames, often relying on recurrent neural networks or 3D convolutions. Samples poorly predicted are considered anomalous.</li><li>Reconstructive + predictive models</li><li>Generative models: used to reconstruct, predict or model the distribution of the data, often using Variational Autoencoders (VAEs) or GANs. E.g. the differences in gradient-based features and optical flow.</li></ul></li><li><p>GNNs</p><p>The point is the weighted adjacency matrix.</p><ul><li>Temporal and multiple adjacency extensions. (ST-GCN)</li><li>Graph attention networks. (2s-AGCN)</li></ul></li><li><p>Deep clustering models</p><p>Provide useful cluster assignments by optimizing a deep model under a cluster inducing objective.</p></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Skeleton-based methods make the analysis independent of nuisance parameters such as viewpoint or illumination.</li></ul></li><li>Limitations<ul><li>Traditional RGB-based anomaly detection methods have to consider many trivial information (viewing direction, illumination, background clutter etc.), and those data are sparse in human pose.</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Generating action words from skeleton-based graphs and then classify actions into normal and abnormal (anomaly detection). With an aim at it can work both on fine-grained and coarse-grained task.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><p>Map graphs into representation space and then cluster them so as to get action words. At last, Dirichlet process based mixture is used for classifying normal and abnormal.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115162120.png" alt="image-20210115162120660" style="zoom:50%;" /><h3 id="Data-Preparation">Data Preparation</h3><ul><li>Similar skeleton graph as what used in ST-GCN.</li></ul><h3 id="Implementation">Implementation</h3><ul><li><p>Backbone: ST-GCN</p></li><li><p>ST-GCAE network</p><ul><li><p>GCN block</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115185527.png" alt="image-20210115185526918" style="zoom:50%;" /><p>The block will be used in SAGC</p></li><li><p><strong>SAGC</strong> block</p><p>Each adjacency type is applied with its own GCN, using separate weights.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115190041.png" alt="image-20210115190041541" style="zoom:50%;" /><ul><li><p>Adjacency matrices</p><table><thead><tr><th>matrix</th><th>sharing</th><th>level</th><th>Dimension</th></tr></thead><tbody><tr><td>$\mathrm{A}$</td><td>fixed and shared by all layers</td><td>body-part connectivity over node relations</td><td>$[V,V]$, $V$ is the number of nodes</td></tr><tr><td>$\mathrm{B}$</td><td>individual at each layer, applied equally to all samples</td><td>dataset level keypoint relations</td><td>$[V,V]$</td></tr><tr><td>$\mathrm{C}$</td><td>is different for different sample</td><td>sample specific relations</td><td>$[N,V,V]$, $N$ is the batch size</td></tr></tbody></table></li></ul></li><li><p>ST-GCAE</p><p>The encoder uses large temporal strides with an increasing channel number to compress an input sequence to a latent vector. The decoder uses temporal up-sampling layers and additional graph convolutional blocks.</p></li></ul></li><li><p><strong>Deep embedded cluster</strong></p><ul><li><p>The input is the embedding from ST-GCAE, denoted as $\mathrm{z}_i$ for sample $i$</p></li><li><p>Soft-assignment --clustering layer</p><p>The probability $p_{ik}$ for the $i$-th sample to be assigned to the $k$-th cluster is:</p><p>$p_{ik}=Pr(y_i=k|\mathrm{z}<em>i,\Theta)=\frac{exp(\theta^T_k\mathrm{z}<em>i)}{\sum\limits</em>{k’=1}^{K}exp(\theta^T</em>{k’}\mathrm{z}_i)}$, where $\Theta$ is the clustering layer’s parameters. (Simple softmax)</p></li><li><p><strong>Optimize clustering layer</strong></p><ul><li>Objective: Minimize the KL-divergence between the current model probability clustering prediction $P$ and a target distribution $Q$. The target distribution aims to strengthen current cluster assignments by normalizing and pushing each value closer to a value of either 0 or 1.</li><li><strong>EM</strong> style. In expectation step, the entire model is fixed and the target distribution $Q$ is updated. In maximization stage, the model is optimized to minimize the clustering loss $L_{cluster}$.</li></ul></li></ul></li><li><p>Anomaly classifier–<strong>normality scoring??</strong></p><ul><li>Two types of multimodal distributions. One is at the cluster assignment level; the other is at the soft-assignment vector level.</li><li>DPMM based. Classifier is fitted by soft-assignment vector (e.g., for class $i$ the softmax result) and then it can do inference.</li></ul></li><li><p>Model</p><p>Feeding the embedding from ST-GCAE to clustering layer, then fixing decoder, fine-tune the encoder in ST-GCAE and clustering layer by combined loss. After fine tuning, using DPMM-based classifier for final inference.</p><ul><li><p>Loss function</p><ul><li><p><strong>Reconstruction loss</strong> $L_{rec}$: $\ell_2$ loss between the original temporal pose graphs and those reconstructed by ST-GCAE, <strong>used in pre-training stage</strong>, for training the whole ST-GCAE.</p></li><li><p><strong>Clustering loss</strong> $L_{cluster}$, combined with reconstruction loss and used for fine-tuning encoder of ST-GCAE+clustering layer</p><p>$L_{cluster}=KL(Q||P)=\sum\limits_i\sum\limits_kq_{ik}\log\frac{q_{ik}}{p_{ik}},\ q_{ik}=\frac{p_{ik}/(\sum_{i’}p_{i’k})^{\frac{1}{2}}}{\sum_{k’}p_{ik’}/(\sum_{i’}p_{i’k’})^{\frac{1}{2}}}$</p></li><li><p>Combined loss</p><p>$L_{combined}=L_{rec}+\lambda\cdot L_{cluster}$, this loss is for training encoder and clustering layer, which means the decoder is fixed while using it.</p></li></ul></li><li><p>Optimization</p><ul><li>encoder: reconstruction loss + cluster loss</li><li>decoder: reconstruction loss</li><li>clustering layer: cluster loss</li></ul></li></ul></li></ul><h2 id="Experiments">Experiments</h2><ul><li><p>Dataset</p><ul><li>ShanghaiTech: 130 abnormal events captured in 13 different scenes with complex lighting conditions and camera angles.<ul><li>training set contains only normal examples</li><li>test set contains both normal and abnormal examples</li><li>2D pose</li></ul></li><li>Kinetics-based: Kinetics-250 and NTU-RGBD. Actions in each set are sampled randomly or meaningfully. In Kinetics dataset, remove actions that focus only on slightly part joints’ movements, like hair braiding.<ul><li><em>Few vs. Many</em>: few normal actions ($3\sim5$) in the training set and many abnormal  ($10\sim 11$ hundreds) actions in the test set</li><li><em>Many vs. Few</em>: switch the training set and test set in experiment above.</li></ul></li></ul></li><li><p>Preprocessing</p><ul><li>Pre-extracting 2D pose from ShanghaiTech Campus</li></ul></li><li><p>Input features</p><ul><li>The coordinates of joints</li><li>For ShanghaiTech: The embeddings of the patch around each joint (from one of the pose estimation model’s hidden layers)</li></ul></li><li><p>Test Algorithms on coarse-grained (Kinetics and NTU-RGBD)</p><ul><li><p>Autoencoder reconstruction loss: ST-GCAE reached convergence prior to the deep clustering fine-tuning stage.</p></li><li><p>Autoencoder based one-class SVM: fit a one-class SVM using the encoded pose sequence representation</p></li><li><p>Video anomaly detection methods: Train Future frame prediction model and the skeleton trajectory model. Anomaly scores for each video are obtained by averaging the per-frame scores.</p></li><li><p>Classifier softmax scores: supervised baseline. Anomaly score is by either using the softmax vector’s max value or by using the Dirichlet normality score</p></li><li><p>Test video in fixed size but with sliding-window if the test video with unknown frames</p></li></ul></li><li><p>Evaluation metrics</p><ul><li>Frame-level score: the maximal score over all the people in the frame</li><li>AUC as the combined score over all frames of one test</li></ul></li><li><p>Summary</p><ul><li><p>On ShanghaiTech (fine-grained): Patches ST-GCAE outstands.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121170448.png" alt="image-20210121170448502" style="zoom:40%;" /></li><li><p>On coarse grained dataset, ST-GCAE outperforms, but better on meaningful actions. <em>A good skeleton help ST-GCAE</em> (NTU-RGBD has better detection on skeletons cause the depth data is known.)</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121171357.png" alt="image-20210121171357539" style="zoom:30%;" /></li><li><p><em>Failed cases</em>: occlusions, high-speed action like cycling, non-person related abnormal like bursting into a vehicle.</p></li><li><p><strong>Ablation study:</strong> adding some abnormal actions into normal actions</p><ul><li>ST-GACE on NTU-RGBD (<em><strong>only dropping, touching and Rand8 dataset are tested</strong></em>): ST-GCAE loses on average less than $10%$ of performance when trained with $5%$ abnormal actions added as noises.</li></ul></li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>The use of embedded pose graphs and a Dirichlet process mixture for video anomaly detection;</li><li>A new coarse-grained setting for exploring broader aspects of video anomaly detection;</li><li>State-of-the-art AUC of 0.761 for the ShanghaiTech Campus anomaly detection benchmark.</li></ul><h2 id="font-color-blue-Remarks-font"><font color='blue'>Remarks</font></h2><ul><li>The reconstruction (learning representations of graph) is mixed with clustering in the final loss, will this be good? Won’t the trivial information of clustering influence the reconstruction?</li><li>The training set for clustering layer is initialized by the K-Means centroids, won’t the initialization methods matter?</li><li><strong>The embeddings of patches around each joint outperforms the simple joint coordinates</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1912.11850.pdf&quot;&gt;Graph Embedded Pose Clustering for Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/amirmk89/gepc&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html</id>
    <published>2021-01-15T01:51:32.000Z</published>
    <updated>2021-01-15T20:20:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/lshiwjx/2s-AGCN">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li>Convolutional DL based methods  manually structure the skeleton as a sequence of joint-coordinate vectors or as a pseudo-image, which is fed into RNNs or CNNs to generate the prediction.</li><li>Skeleton-based action recognition<ul><li>Design handcrafted features to model human body, but they are barely satisfactory.</li><li>DL-based: CNN-based methods are generally more popular than RNN-based methods. But both fail to fully represent the structure of the skeleton data.</li><li>GCN-based: ST-GCN, eliminates the meed for designing handcrafted part assignment or traversal rules.</li></ul></li><li>GNNs<ul><li><strong>Spatial perspective</strong>: directly perform the convolution filters on the graph vertexes and their neighbors, which are extracted and normalized based on manually designed rules.</li><li>Spectral perspective: use the eigenvalues and eigenvectors of the graph Laplace matrices. They perform the graph convolution in the frequency domain with the help of graph Fourier transform.</li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>The second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods.</li></ul></li><li>Limitations:<ul><li>Representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependency between correlated joints</li><li>In GCN-based skeleton action recognition, the topology of the graph is set manually and thus may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks.</li><li>ST-GCN: 1)  The skeleton graph is heuristically predefined and represents only the physical structure of the human body. 2) The fixed topology of graph limiting the flexibility and capacity to model the multilevel semantic information. 3)  One fixed graph structure may not be optimal for all the samples of different action classes. Like hands-related actions and legs-related actions</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Propose a improved ST-GCN (graph convolutional based model), so as to use 2nd order information and improve the accuracy of action recognition based on skeletons.</p><p>Make the graph is unique for different layers and samples.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><p>Modification on ST-GCN</p><ul><li>Two types of graphs:<ul><li>Global graph: represents the common pattern for all the data</li><li>Individual graph: represents the unique pattern for each data</li></ul></li><li>Second-order information: the length and directions of bones are formulated as a vector pointing from its source joint to its target joint.</li></ul><h3 id="Data-Preparation">Data Preparation</h3><ul><li>The structure of the graph follows the work of ST-GCN.</li></ul><h3 id="A-look-at-ST-GCN">A look at ST-GCN</h3><p>Graph convolution in ST-GCN: $f_{out}(v_{ti})=\sum\limits_{v_{tj}\in \mathcal{B}<em>i}\frac{1}{Z</em>{ti}(v_{tj})}f_{in}(v_{j})\cdot \mathrm{w}(l_{ti}(v_{tj}))$, follows spatial configuration partitioning.</p><ul><li>Graph convolution in spatial dimension</li></ul><p>$f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}<em>k(f</em>{in}\mathrm{A}_k)\odot\mathrm{M}_k$, where $\mathrm{M}_k$ is an $N\times N$ attention map that indicates the importance of each vertex. $\mathrm{A}_k$ <strong>determines whether there are connections between two vertexes and $\mathrm{M}_k$ determines the strength of the connections.</strong></p><ul><li>Graph convolution in temporal dimension: $K_t\times 1$convolution on the output feature map</li></ul><p><em>The model is calculated based on a predefined graph, which may not be a good choice.</em></p><h3 id="Implementation">Implementation</h3><ul><li><p>Adaptive graph convolutional network (<strong>AGCN</strong>)</p><p><strong>BN+9 of adaptive graph convolutional blocks + global average pooling + softmax classifier</strong></p><ul><li><p>Adaptive graph convolutional layer:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142649.png" alt="image-20210115142649202" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115133332.png" alt="image-20210115133332722" style="zoom:50%;" /></p><p>$f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}<em>kf</em>{in}(\mathrm{A}_k+\mathrm{B}_k+\mathrm{C}_k)$. The adjacency matrix is now divided into three parts</p><ul><li><p>$\mathrm{A}_k$: same as $N\times N$ adjacency matrix $\mathrm{A}_k$ in ST-GCN, it <em>represents the physical structure of the human body</em>.</p></li><li><p>$\mathrm{B}_k$: An $N\times N$ adjacency matrix. It’s trainable. It acts as $\mathrm{M}_k$ (attention mechanism) in ST-GCN, influenced by the connections between two joints and also the strength of the connections.</p></li><li><p>$\mathrm{C}_k$: a similarity matrix calculated by the normalized embedded Gaussian function with vectors embedded by $1\times 1$ convolutional layer.</p><p>$\mathrm{C}<em>k=softmax(\mathrm{f}</em>{in}^T\mathrm{W}<em>{\theta k}^{T}\mathrm{W}</em>{\phi k}\mathrm{f}<em>{in})$, where $\mathrm{W}</em>\theta,\mathrm{W}_\phi$ are the parameters of the embedding functions $\theta,\phi$, respectively.</p></li></ul></li><li><p>Adaptive graph convolutional block</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115132928.png" alt="image-20210115132928144" style="zoom:33%;" /></li></ul></li><li><p>Model: two stream networks</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142748.png" alt="image-20210115142748418" style="zoom:50%;" /><ul><li>J-stream: the input data are joints, as what’s depicted in AGCN.</li><li>B-stream: the input data are bones.<ul><li>A bone is the vector pointing from its source joint to its target joint. This vector will contain both length and direction of a bone. An empty bone is added so as to make sure the B-stream has similar quantity of  input as J-stream.</li></ul></li></ul><p>Finally, the <em>softmax</em> scores of the two streams are added to obtain the fused score and do prediction.</p></li><li><p>Loss function</p><p>Cross-entropy</p></li><li><p>Why does it work?</p><ul><li>Considering bones (2nd information)</li><li>Offers trainable attention matrix $\mathrm{B}_k$ and the similarity evaluation of $\mathrm{C}_k$ to estimate the strength of connection. Both of them offer more possible connections and provide more flexibility.</li></ul></li></ul><h2 id="Experiments">Experiments</h2><ul><li>Dataset: Kinetics and NTU-RGBD<ul><li>NTU-RGBD: If the number of bodies in the sample is less than 2, the second body is padded with 0.</li><li>Kinetics: Same data augmentation as done in ST-GCN.</li></ul></li><li>Training: SGD with Nesterov momentum (0.9), batch size is 64. The weight decay is set to 0.0001.<ul><li>NTU-RGBD: learning rate is set as 0.1 and is divided by 10 at the 30th epoch and 40th epoch. The training process is ended at the 50th epoch.</li><li>Kinetics: The learning rate is set as 0.1 and is divided by 10 at the 45th epoch and 55th epoch. Training ends at the 65th epoch.</li></ul></li><li>Evaluation metrics</li><li>NTU-RGBD: top-1 accuracy<ul><li>Kinetics : top-1 and top-5 accuracy</li></ul></li></ul><h3 id="Ablation-study">Ablation study</h3><ul><li><p>Adaptive graph convolutional block</p><p>Manually  delete one of the graphs and estimate.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115150909.png" alt="image-20210115150909820" style="zoom:33%;" /><ul><li>Given each connection, a weight parameter is important, which also proves the importance of the adaptive graph structure</li></ul></li><li><p>Visualization of the learned graphs</p><p>Denote the strength of joints by dot size, the bigger the stronger connection.</p><ul><li><p>A higher layer in AGCN contains higher-level information, comparing the dot size in different layers</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115151421.png" alt="image-20210115151421543" style="zoom:50%;" /></li><li><p>The diversity for different sample in the same layer is proved.</p></li></ul></li><li><p>Two-stream framework</p><p>The two-stream method outperforms the one-stream-based methods either the J-stream or the B-stream.</p></li></ul><h3 id="Compared-with-SOTA">Compared with SOTA</h3><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115144443.png" alt="image-20210115144443887" style="zoom:50%;" /><ul><li>Question 1: <font color='red'> ResNet helps?</font></li><li>Question 2: <font color='red'>How about compared with methods based on RGB or optical flow ?</font> In paper ST-GCN their model fails to those models.</li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>An adaptive graph convolutional network is proposed.</li><li>The second-order information of the skeleton data is explicitly formulated and combined with the first-order information using a two-stream framework, which brings notable improvement for the recognition performance.</li><li>On two large-scale datasets for skeleton-based action recognition, the proposed 2s-AGCN exceeds the SOTA by a significant margin.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf&quot;&gt;Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/lshiwjx/2s-AGCN&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html</id>
    <published>2021-01-13T02:58:39.000Z</published>
    <updated>2021-01-15T18:26:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/yysijie/st-gcn">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li>Human action recognition: can be solved by appearance, depth, optical flows, and body skeletons.</li><li>Models for graph<ul><li>Recurrent neural networks</li><li>GNNs:<ul><li>Spectral perspective: the locality of the graph convolution is considered in the form of spectral analysis.</li><li><strong>Spatial perspective</strong>: the convolution filters are applied directly on the graph nodes and their neighbors.</li></ul></li></ul></li><li>Skeleton Based Action Recognition<ul><li>Handcrafted feature based methods: design several handcrafted features to capture the dynamics of joint motion. E.g., covariance matrices of joint trajectories, relative positions of joints, rotations and translations between body parts.</li><li>Deep learning methods: recurrent neural networks and temporal CNNs. Many emphasize <strong>the importance of modeling the joints within parts of human bodies.</strong></li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Earlier methods of using skeletons for action recognition simply employ the joint coordinates at individual time steps to form feature vectors, and apply temporal analysis thereon.  <strong>They do not explicitly exploit the spatial relationships among the joint</strong>.</li><li>Most existing methods which explore spatial relationship rely on hand-crafted parts or rules to analyze the spatial patterns.</li><li>Traditional CNNs are not suitable for 2D or 3D skeletons (graphs rather than data in grids).</li></ul></li><li>Limitations:<ul><li>Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization.</li><li>Models using hand-crafted parts are difficult to be generalized to others</li><li>These parts used in DL based methods are usually explicitly assigned using domain knowledge, which is not automatic and practical.</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Build a better model for dynamics of human body skeletons. Specifically, a new method that can automatically capture the patterns embedded in the <strong>spatial configuration of the joints as well as the  temporal dynamics</strong> is required.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><p>Skeletons in frames are connected as natural spatial-temporal graph, then using GCN.</p><h3 id="Data-Preparation">Data Preparation</h3><ul><li>The feature vector on a node $F(v_{ti})$ consists of coordinate vectors, as well as estimation confidence, of the i-th joint on frame t.</li><li>Construct the spatial temporal graph on the skeleton sequences in two steps. First, the joints within one frame are connected with edges according to the connectivity of human body structure. Then each joint will be connected to the same joint in the consecutive frame.</li><li>Both 18 joints skeleton model or 25 joints skeleton model work fine.</li></ul><h3 id="Implementation">Implementation</h3><ul><li><p>Model</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113115442.png" alt="image-20210113115429152" style="zoom:50%;" /><p>ResNet mechanism is applied on each ST-GCN unit.</p><table><thead><tr><th>Layer name</th><th>configuration</th></tr></thead><tbody><tr><td>$1\sim3$</td><td>64 channels</td></tr><tr><td>$4\sim6$</td><td>128 channels</td></tr><tr><td>$7\sim9$</td><td>256 channels, 9 temporal kernel size</td></tr><tr><td>Global pooling+softmax</td><td></td></tr></tbody></table></li><li><p>Loss function</p><ul><li><p><strong>Sampling function</strong> $\mathbb{p}$ enumerates the neighbors of location $x$.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113155452.png" alt="image-20210113155452481" style="zoom:40%;" /><p>In this paper, <strong>the 1-neighbor set</strong> of joint nodes are used.</p></li><li><p><strong>The filter weights</strong> $\mathrm{w} (v_{ti},v_{tj})$ are shared everywhere on the input image.</p><p>Build a mapping $l_{ti}:B(v_{ti})\rightarrow{0,\cdots,K−1}$ which maps a node in the neighborhood to its subset label. The weight function $\mathrm{w}(v_{ti}, v_{tj}):B(v_{ti})\rightarrow R^c$ can be implemented by indexing a tensor of $(c,K)$ dimension or</p><p>$\mathrm{w}(v_{ti},v_{tj})=\mathrm{w}'(l_{ti}(v_{tj})$</p><ul><li><p>Labeling strategies (the definition of $l_{ti}$)</p><img src="C:%5CUsers%5C10457%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210113195834069.png" alt="image-20210113195834069" style="zoom:50%;" /><ul><li><p>Uni-labeling</p><p>Make the whole neighbor set itself as subset. Then feature vectors on every neighboring node will have a inner product with the same weight vector. Formally, $K=1,l_{ti}(v_{tj})=0,\forall i,j\in V$</p></li><li><p>Distance partitioning</p><p>$d=0$ refers to the root node itself and remaining neighbor nodes are in the $d =1$ subset. Formally , $K=2,l_{ti}(v_{tj})=d(v_{tj},v_{ti})$</p></li><li><p>Spatial configuration partitioning</p><p>Three subsets: 1) the root node itself; 2)centripetal group and 3)  centrifugal group.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113200600.png" alt="image-20210113200600480" style="zoom:33%;" />, where $r_i$ is the average distance from gravity center to joint $i$ over all frames in the training set.</p></li></ul></li><li><p>Learnable edge importance weighting</p><p>One joint appears in multiple body parts should have different importance in modeling the dynamics of these parts. A  learnable mask M is added on every layer of spatial temporal graph convolution.</p></li></ul></li><li><p>The <strong>spatial graph convolution</strong></p><p>$f_{out}(v_{ti})=\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(\mathbb{p}(v_{ti},v_{tj}))\cdot \mathrm{w}(v_{ti},v_{tj})=\\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(v_{tj})\cdot \mathrm{w}(l_{ti}(v_{tj}))$, $Z_{ti}(v_{tj})=|{v_{tk}|l_{ti}(v_{tk})=l_{ti}(v_{tj})}|$ is the cardinality of the corresponding subset. It’s for balancing the contributions of different subsets to the output.</p></li><li><p><strong>Spatial temporal modeling</strong></p><p>Extend neighbors so as to include temporally connected joints</p><p>$B(v_{ti})={v_{qj}|d(v_{tj},v_{ti})\le K,|q-t|\le \lfloor\Gamma/2\rfloor}$, where $\Gamma$ is temporal kernel size (controls the temporal range to be included in the neighbor graph). Then , the sampling function is $l_{ST}(v_{qj})=l_{ti}(v_{tj})+(q-t+\lfloor\Sigma/2\rfloor)\times K$.</p></li><li><p>The final convolution formula:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113222927.png" alt="image-20210113222926988" style="zoom:40%;" />, where $\Lambda_j^{ii}=\sum_k(A_j^{ik})+\alpha$, $\alpha=0.001$ is to avoid empty rows in $A_j$. To add learnable  mask $M$, displace $A_j$ as $A_j\otimes M$.</p></li></ul></li><li><p>Why does it work?</p><ul><li>Parts restrict the modeling of joints trajectories within “local regions” compared with the whole skeleton, thus forming a hierarchical representation of the skeleton sequences.</li></ul></li></ul><h2 id="Experiments">Experiments</h2><ul><li><p>Dataset:</p><ul><li><p>Kinetics (unconstrained action recognition dataset), provides only raw video clips without skeleton data.</p><ul><li><p>Augmentation: To avoid overfitting,two kinds of augmentation are used to replace dropout layers when training on the Kinetics dataset. 1) affine transformations, 2) sampling part of the frames from the whole frame and testing by a whole frame. <font color=Blue>May that’s a way for avoiding the two consecutive frame are too similar?</font></p></li><li><p>Videos to skeletons</p><p>To work on skeletons, openpose is used for extracting. Concretely, resize all videos to the resolution of 340 × 256 and convert the frame rate to 30 FPS. Then OpenPose toolbox is used to estimate the location of 18 joints on every frame of the clips.</p></li><li><p>Final features:</p><p>Finally <strong>the clips are represented by a tensor in shape</strong> $(3,T,18,2)$, where 18 is the number of joints, 2 is the number of people and 3 is the number of features (X,Y,C), C is the confidence.</p></li></ul></li><li><p>NTU-RGBD ( in-house captured action recognition dataset)</p><ul><li>Already annotated with 25 3D joints</li><li>Each clip is guaranteed to have at most 2 subjects</li></ul></li></ul></li><li><p>Training: SGD with a learning rate of 0.01. $lr$ decay by 0.1 after every 10 epochs.</p></li><li><p>Evaluation metrics</p><ul><li><p>Kinetics</p><p>Test on validation set. Using <strong>top-1 and top-5 classification accuracy</strong></p></li><li><p>NTU-RGBD</p><p>Report top-1 recognition accuracy.</p><ul><li><strong>Cross-subject</strong>: Train on one subset of actors and test on the remaining actors.</li><li><strong>Cross-view</strong>: Train on skeletons from camera views 2 and 3, and test on those from camera view 1.</li></ul></li></ul></li></ul><h3 id="Ablation-study">Ablation study</h3><p>Applied on Kinetics dataset.</p><ul><li><p>Spatial temporal graph convolution</p><table><thead><tr><th>Model</th><th>configuration</th></tr></thead><tbody><tr><td><em>baseline-TCN</em></td><td>squeeze Spatial dimension, concatenate all input joint locations to form the input features at each frame $t$.</td></tr><tr><td><em>local convolution</em></td><td>The input data are the same format, but with unshared convolution filters.</td></tr></tbody></table></li><li><p>Partition strategies: same as what described before. <em>Distance partitioning*</em> is as intermediate between the distance partitioning and uni-labeling. The filters in this setting only differs with a scaling factor -1, or to say $\mathrm{w}_0=-\mathrm{w}_1$.</p></li><li><p>Learnable edge importance weighting</p><p>This setting is named as <em>ST-GCN+Imp</em>.</p></li><li><p>Results</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114194501.png" alt="image-20210114194452831" style="zoom:50%;" /><ul><li>Better performance of ST-GCN based models could justify the power of the spatial temporal graph convolution in skeleton based action recognition</li><li>Distance partitioning* achieves better performance than uni-labeling, which again demonstrate <strong>the importance of the partitioning with multiple subsets</strong>.</li><li>ST-GCN model with learnable edge importance weights can learn to express the joint importance.</li></ul></li></ul><h3 id="Compared-with-SOTA">Compared with SOTA</h3><p><strong>Model setting: ST-GCN+Learnable weights+Spatial configuration partitioning</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204800.png" alt="image-20210114204800283" style="zoom:30%;" />     <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204722.png" alt="image-20210114204722008" style="zoom:30%;" /></p><ul><li>Kinetics: ST-GCN is able to outperform previous representative approaches, but under-perform methods in RGB or optical flow.</li><li>NTU-RGBD: No data augmentation before training. It outperforms all other selected candidates.</li><li>The skeleton based model ST-GCN can provide complementary information to RGB and optical flow models.</li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Propose ST-GCN, a generic graph-based formulation for modeling dynamic skeletons, which is the first that applies graph-based neural networks for action recognition.</li><li>Propose several principles in designing convolution kernels in ST-GCN to meet the specific demands in skeleton modeling.</li><li>On two large scale datasets for skeleton-based action recognition, the proposed model achieves superior performance as compared to previous methods using hand-crafted parts or traversal rules, with considerably less effort in manual design.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1801.07455.pdf&quot;&gt;Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/yysijie/st-gcn&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Self-supervised Learning on Graphs, Deep Insights and New Directions</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html</id>
    <published>2021-01-12T03:00:00.000Z</published>
    <updated>2021-01-24T16:54:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2006.10141.pdf">Self-supervised Learning on Graphs: Deep Insights and New Directions</a></p><p><a href="https://github.com/ChandlerBang/SelfTask-GNN">Codes</a></p><a id="more"></a><h2 id="Why">Why?</h2><ul><li>Nodes in graphs present unique structure information and they are inherently linked indicating not independent and identically distributed (or i.i.d.).</li><li>(SSL) has been introduced in both the image and text domains to alleviate the need of large labeled data by deriving labels for the significantly more unlabeled data.</li><li>To fully exploit the unlabeled nodes for GNNs, SSL can be naturally harnessed for providing additional supervision.</li><li>The challenges of graph to use SSL:<ul><li>graphs are not restricted to these rigid structures.</li><li>each node in a graph is an individual instance and has its own associated attributes and topological structures</li><li>instances (or nodes) are inherently linked and dependent of each other.</li></ul></li></ul><h2 id="Goals">Goals</h2><ul><li><p>Focus on advancing GNNs for node classification where GNNs leverage both labeled and unlabeled nodes on a graph to jointly learn node representations and a classifier that can predict the labels of unlabeled nodes on the graph. Aims at gain insights on when and why SSL works for GNNs and which strategy can better integrate SSL for GNNs.</p></li><li><p><em><strong>Focus on semi-supervised node classification task</strong></em></p><p>$\min\limits_{\theta}\mathcal{L}<em>{task}(\theta,\mathrm{A,X},\mathcal{D}<em>L)=\sum\limits</em>{(v_i,y_i)\in\mathcal{D}<em>L}\ell(f</em>{\theta}(\mathcal{G})</em>{v_i},y_i)$</p></li></ul><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121192249.png" alt="image-20210121192249653" style="zoom:40%;" /><h2 id="Previous-work">Previous work</h2><h3 id="Examples">Examples</h3><ul><li><a href="https://arxiv.org/abs/1902.11038">Multi-stage self-supervised learning for graph convolutional networks on graphs with few labels</a> utilize the clustering assignments of node embeddings as guidance to update the graph neural networks.</li><li><a href="https://arxiv.org/abs/2003.01604">Self-supervised graph representation learning via global context prediction</a> proposed to use the global context of nodes as the supervisory signals to learn node embeddings.</li></ul><h3 id="Basic-pretext-task-on-graphs">Basic pretext task on graphs</h3><h4 id="Structure-information-Adjacency-matrix-mathrm-A">Structure information  (Adjacency matrix $\mathrm{A}$)</h4><p>Construct self-supervision information for the unlabeled nodes based on their local structure information, or how they relate to the rest of the graph</p><h5 id="Local-structure-information"><strong>Local structure information</strong></h5><ul><li><p>Node property</p><ul><li><p>use node degree as a representative local node property for self-supervision while leaving other node properties (or the combination) as one future work</p></li><li><p>Formally, let $d_i=\sum\limits_{j=1}^{N}\mathrm{A}_{ij}$ denote the degree of $v_i$ and construct the associated loss of the SSL pretext task as</p><p>$\mathcal{L}_{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}(f</em>{\theta’}(\mathcal{G})</em>{v_i}-d_i)^2$, where $\mathcal{D}_U$ denote the set of unlabeled nodes and associated pretext task labels in the graph.</p></li><li><p>Assumption: The node property information is related to the specific task of interest.</p></li></ul></li><li><p><strong>EdgeMask</strong></p><ul><li><p>Build pretext task based on the connections between two nodes in the graph. Specifically, <strong>one can first randomly mask some edges and then the model is asked to reconstruct the masked edges</strong>.</p></li><li><p>Formally, first mask $m_e$ edges denotes as the set $\mathcal{M}_e\subset\varepsilon$ and also sample the set $\bar{\mathcal{M}_e}={(v_i,v_j)|v_i,v_j\in\mathcal{V},(v_i,v_j)\notin\varepsilon}$, $|\bar{\mathcal{M}_e}|=|\mathcal{M}_e|=m_e$ . Then the SSL pretext task is to predict whether there exist a link between a given node pair.</p><p>$\mathcal{L}<em>{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\\frac{1}{|\mathcal{M}<em>e|}\sum\limits</em>{(v_i,v_j)\in\mathcal{M}<em>e}\ell(f_w(|f</em>{\theta’}(\mathcal{G})</em>{v_i}-f</em>{\theta’}(\mathcal{G})<em>{v_j}|),1)+\frac{1}{|\bar{\mathcal{M}<em>e}|}\sum\limits</em>{(v_i,v_j)\in\bar{\mathcal{M}<em>e}}\ell(f_w(|f</em>{\theta’}(\mathcal{G})</em>{v_i}-f_{\theta’}(\mathcal{G})_{v_j}|),0)$, where $\ell(\cdot,\cdot)$ is the cross entropy loss, $f_w$ linearly maps to 1-dimension.</p></li><li><p>Expecting to help GNN learn information about local connectivity.</p></li></ul></li></ul><h5 id="Global-structure-information"><strong>Global structure information</strong></h5><p>Not only based on the node itself or limited to its immediate local neighborhood, but also considering the position of the node in the graph.</p><ul><li>PairwiseDistance<ul><li>Maintain global topology information through a pairwise comparison. Or to say, pretext task will be able to distinguish/predict the distance between different node pairs.</li><li>The measurements of distance vary.</li><li>If use the shortest path length $p_{ij}$ as a measure of the distance, then for all node pairs ${(v_i,v_j)|v_i,v_j\in\mathcal{V}}$, they are grouped into four categories: $p_{ij}={1,2,3,\ge4}$, $4$ is because of the computing price and accuracy (the more neighbors , the more unrelated noises are included.) Practically, randomly sample a certain amount of node pairs $S$ used for SSL during epoch. Then the SSL loss is $\mathcal{L}<em>{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|S|}\sum\limits</em>{(v_i,v_j)\in S}\ell(f_w(|f</em>{\theta’}(\mathcal{G})<em>{v_i}-f</em>{\theta’}(\mathcal{G})<em>{v_j}|),C</em>{p_{ij}})$, where $C_{p_{ij}}$ is the corresponding distance category of $p_{ij}$.</li></ul></li><li><em><strong>Distance2Clusters</strong></em><ul><li>Predicting the distance from the unlabeled nodes to predefined graph clusters. Thus enforce the representations to learn a global positioning vector of each of the nodes.</li><li>First partitioning the graph to get $k$ clusters ${C_1,C_2,\cdots,C_k}$ by METIS graph partitioning methods. Denote the node with highest degree as center node $c_j$ in each cluster.</li><li>Formally, the SSL will optimize $\mathcal{L}_{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-d_i|^2$, where $\mathrm{d}_i$ is the distance vector between node $v_i$ and each center.</li></ul></li></ul><h4 id="Attribute-information-Nodes-matrix-mathrm-X">Attribute information (Nodes matrix $\mathrm{X}$)</h4><p>Guide the GNN to ensure certain aspects of node/neighborhood attribute information is encoded in the node embeddings after a SSL attribute-based pretext.</p><ul><li><p>AttributeMask</p><ul><li>Let GNN learn attribute information via pretext</li><li>Randomly mask (e.g. set to zero ) the features of $m_a$ nodes $\mathcal{M}_a\subset\mathcal{V}, |\mathcal{M}<em>a|=m_a$, then SSL will try to construct these features. Formally , $\mathcal{L}</em>{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{M}<em>a|}\sum\limits</em>{v_i\in\mathcal{M}<em>a}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-\mathrm{x}_i|^2$, where $\mathrm{x}_i$ is the dense features after PCA.</li></ul></li><li><p>PairewiseAttrSim</p><ul><li><p>The similarity two nodes have in the input feature space is not guaranteed in the learned representations due to the GNN aggregating features from the two nodes local neighborhoods.</p></li><li><p>Specifically,</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210122233255.png" alt="image-20210122233252928" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210122233404.png" alt="image-20210122233403875" style="zoom:50%;" /></li><li><p>Only constrain the intra-class distance</p></li></ul></li></ul><h3 id="Merge-pretext-task-on-Graphs">Merge pretext task on Graphs</h3><ul><li><p>Joint Training</p><ul><li><p>Optimize the SSL loss (i.e., $\mathcal{L}<em>{self}$) and supervised loss (i.e., $\mathcal{L}</em>{task}$)   simultaneously.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123174253.png" alt="image-20210123174253225" style="zoom:50%;" /></li><li><p>The overall objective is $\min\limits_{\theta,\theta’}\mathcal{L}_{task}(\theta,\mathrm{A,X},\mathcal{D}_L)+\lambda(\theta’,\mathrm{A,X},\mathcal{D}_U)$, where $\lambda$ is the hyperparameter to control the distribution of self-supervision.</p></li></ul></li><li><p>Two-stage training</p><p>Fine tuning the model which is pretrained on pretext task on downstream dataset.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123181120.png" alt="image-20210123181120517" style="zoom:50%;" /></li></ul><h2 id="Analysis">Analysis</h2><ul><li><p>Targets: Understand what SSL information works for GNNs, which strategies can better integrate SSL for GNNs, and further analyze why SSL is able to improve GNNs</p></li><li><p>Datasets: Cora, Citeseer, Pubmed</p></li><li><p>Training: Adam, learning rate $0.01$, $L_2$ regularization $5e-4$, dropout rate $0.5$, $128$ hidden units across all self-supervised information and GCN, top-K=bottom-K=$3$. $\lambda$ in range ${0,0.001,0.01,0.1,1,5,10,50,100,500,1000}$, $m_e,m_a$ in ${10%,20%}$ the size of $|V|$.</p></li><li><p><strong>Two-stage training</strong></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123200058.png" alt="image-20210123200058736" style="zoom:50%;" /><ul><li>the configuration of one graph convolutional layer for feature extraction, one graph convolutional layer for the adaptation of node classification and one linear layer for the adaptation of pretext task works very well for all three strategies</li><li>In most cases, the strategy of “Tune all&quot; achieves the best performance–&gt; fine tune for downstream task is necessary.</li></ul></li><li><p><strong>SSL for GNNs</strong></p><ul><li><p><em>Joint training vs. Two-stage Training</em></p><p><em><strong>Joint training outperforms the Two-stage training in most settings.</strong></em></p></li><li><p><em>What SSL works for GNNs</em></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123202214.png" alt="image-20210123202214101" style="zoom:28%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123202231.png" alt="image-20210123202231858" style="zoom:30%;" /></p><ul><li>SSL for GNNs will improve the accuracy for downstream task</li><li>Across all datasets , the best performing method is a pretext task developed from <em>global structure information.</em></li><li>self-supervised information from both the structure and attributes have potentials</li><li>For the structure information, the global pretext tasks are likely to provide much more significant improvements compared to the local ones.</li></ul></li><li><p><em>Why SSL Works for GNNs</em></p><ul><li>GCN for node classification is naturally semi-supervised that has explored the unlabeled nodes, those (SSL pretext) failed to improve GCNs is argued resulted in GCN has already learned that information.</li><li>GCN is unable to naturally learn the global structure information and employing pairwise node distance prediction as the SSL task can help boost its performance for the downstream task.</li></ul></li><li><p>The capability of pretext representations maintaining similarity</p><p>The most popular similarity for graph is structural equivalence and regular equivalence (规则的等效节点是那些不一定具有相同邻居但具有自身相似的邻居的节点).</p><ul><li>Authors argue pretext task can maintain these two similarities by changing the definition of task (like nodes attribute task or distance between a pair of nodes can maintain structure similarity and regular equivalence.</li></ul></li></ul></li></ul><h2 id="Advanced-pretext-task-on-graphs">Advanced pretext task on graphs</h2><p>Pretext tasks are built with the intuition of adapting the notion of regular equivalence to having neighbors with similar node labels (or regular task equivalence). Specifically, <strong>if every node constructs a pretext vector based on information in regards to the labels from their neighborhood, then two nodes having similar (or dissimilar) vectors will be encouraged to be similar (or dissimilar) in the embedding space.</strong></p><h3 id="Proposed-Tasks">Proposed Tasks</h3><h4 id="Distance2Labeled">Distance2Labeled</h4><ul><li>Modify Distance2Cluster. Propose to predict the distance vector from each node to the labeled nodes (i.e., $\mathcal{V}_L$) as the pretext task. <em>For class $c_j\in{1，\cdots,K}$ and unlabeled node $v_i\in\mathcal{V}_U$, the distance vector $\mathrm{d}_i$ for node  $v_i$ is defined as  three shortest path length (average, minimum, maximum) from $v_i$ to all labeled nodes in class $c_i$.</em></li><li>Formally, the objective is $\mathcal{L}_{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-d_i|^2$.</li></ul><h4 id="ContextLabel">ContextLabel</h4><ul><li>Considering the sparsity of labels, use similarity based function which utilize structure , attributes , and the current labeled nodes to <strong>construct a neighbor label distribution context vector $\bar{\mathrm{y}}_i$</strong> for each nodes as follows: $f_s({\mathrm{A,X},\mathcal{D}_L,\mathcal{V}_U})\rightarrow{\bar{\mathrm{y}}<em>i|v_i\in\mathcal{V}<em>U}$. Specifically , the $c$-th item of $\bar{\mathrm{y}}$ is: $\bar{\mathrm{y}}</em>{ic}=\frac{|\mathcal{N}</em>{\mathcal{V}<em>L}(v_i,c)|+|\mathcal{N}</em>{\mathcal{V}<em>U}(v_i,c)|}{|\mathcal{N}</em>{\mathcal{V}<em>L}(v_i)|+|\mathcal{N}</em>{\mathcal{V}_U}(v_i)|},c=1,\cdots,K$. (For the neighbors of node $v_i$ (including unlabeled and labeled neighbors), the ratio of neighbors in class $c$)</li><li>Formally, the objective is $\mathcal{L}_{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-\mathrm{y}_i|^2$</li><li>The labels of nodes (aka $f_s$) can be generated by LP (Label propagation ) or ICA (Iterative Classification Algorithm). But these  will import weak  labels that are too noisy.</li></ul><h4 id="EnsembleLabel">EnsembleLabel</h4><ul><li>Ensemble various functions $f_s$. $\bar{y}<em>i=\arg\max_c\sigma</em>{LP}(v_i)+\sigma_{ICA}(v_i),c=1,\cdots,K$</li><li>The objective is the same as ContextLabel method.</li></ul><h4 id="CorrectedLabel">CorrectedLabel</h4><ul><li>Enhance ContextLabel by iteratively improving the context vectors. GNN $f_{\theta}$ is trained on both the original (e.g., $\bar{\mathrm{y}}_i$) and corrected (e.g., $\hat{\mathrm{y}}_i$) context distributions.</li><li>Formally, the loss is $\mathcal{L}<em>{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U,\hat{\mathcal{D}}<em>U)=\\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-\bar{\mathrm{y}}<em>i|^2+\alpha(\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-\hat{\mathrm{y}}<em>i|^2)$, where the 1st and second terms are to fit the original and corrected context distributions respectively, and $\alpha$ controls the contribution from the corrected context distribution. $\hat{y}<em>i=\arg\max_c\frac{1}{p}\sum\limits</em>{l=1}^p\cos(f</em>{\theta’}(\mathcal{G})</em>{v_i},\mathrm{z}</em>{cl}),c=1,\cdots,K$. Where $p$ indicates the prototype nodes in top-$p$ largest $\rho$ values, indicating <strong>the nodes if the measurements ($\rho$) of their neighbors’ label similarity is in top-$p$.</strong>. Concretely, the similarity of labels’ similarity is defined as $\rho_i=\sum\limits_{j=1}^m\mathrm{sign}(\mathrm{S}<em>{ij}-S_c)$, where $\mathrm{S}</em>{ij}$ is the  cosine similarity between two nodes based on their embeddings, $S_c$ indicating a constant value (which is selected as the value rank in top $40%$ in $\mathrm{S}$).</li><li>In other words, the average similarity between $v_i$ and $p$ prototypes is used to represent the similarity between $v_i$ and the corresponding class, and then assign the class $c$ having the largest similarity to $v_i$.</li></ul><h3 id="Experiments-for-evaluating">Experiments for evaluating</h3><ul><li><p>Experiment settings</p><ul><li>Datasets: Cora, Citeseer, Pubmed and Reddit</li><li>Model:  2-layer GCN as the backbone, with hidden units of 128, $L_2$ regularization $5e−4$, dropout rate $0.5$ and learning rate $0.01$. For the SSL loss, the hidden representations from the first layer of GCN are fed through a linear layer to solve SSL pretext task. Jointly train SSL and GCNs. $\lambda$ ranges in ${1, 5, 10, 50, 100, 500}$. $\alpha$ ranges in ${0.5, 0.8, 1, 1.2, 1.5}$.</li><li>Measurements: the average accuracy with standard deviation.</li></ul></li><li><p>Analysis</p><ul><li><p>Performance comparison: Though they argue the performance exist, but seems not that significant. They summarize: <strong>label correction can better</strong> extend label information to unlabeled nodes than ensemble, but it’s much less inefficient. A tradeoff must be taken in.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124113911.png" alt="image-20210124113911642" style="zoom:50%;" /></li><li><p>Fewer Labeled Samples</p><ul><li><p>By randomly sampling 5 or 10 nodes per class for training and the same number of nodes for validation.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124114147.png" alt="image-20210124114147761" style="zoom:50%;" /></li><li><p><strong>SelfTask achieves even greater improvement when the labeled samples are fewer and consistently outperforms the state-of-the-art baselines.</strong></p></li><li><p><font color='red'>Why with fewer samples per class SelfTask can be even better?</font></p></li></ul></li><li><p>Parameter Analysis</p><ul><li><p>Only the sensitivity of the best model <em>SelfTaskCorrectedLabel-ICA</em> is evaluated. Vary $\lambda$ in the range of ${0, 0.1, 0.5, 1, 5, 10, 50, 100}$ and $\alpha$ from 0 to 2.5 with an interval of 0.25.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124114646.png" alt="image-20210124114646842" style="zoom:50%;" /></li><li><p>The performance of this model first increases with the increase of $\lambda$, which controls the contribution of SSL pretext task.</p></li><li><p>The using of correction is confirmed.</p></li><li><p>They don’t report sensitivity on other datasets, which should have been in supplementary.</p></li></ul></li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Present detailed empirical study to understand when and why SSL works for GNNs and which strategy can better work with GNNs.</li><li>Propose a new direction SelfTask to build advanced pretext tasks which further exploit task-specific self-supervised information, and demonstrate that our advanced method achieves state-of-the-art performance.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2006.10141.pdf&quot;&gt;Self-supervised Learning on Graphs: Deep Insights and New Directions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ChandlerBang/SelfTask-GNN&quot;&gt;Codes&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Predicting What You Already Know Helps, Provable Self-Supervised Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html</id>
    <published>2021-01-11T20:00:00.000Z</published>
    <updated>2021-01-12T19:38:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2008.01064.pdf">Predicting What You Already Know Helps: Provable Self-Supervised Learning</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li><p>Pretext tasks</p><ul><li><strong>Reconstruct images from corrupted versions or just part it</strong>: including denoising auto-encoders, image inpainting, and split-brain autoencoder</li><li><strong>Using visual common sense</strong>, including predicting rotation angle, relative patch position, recovering color channels, solving jigsaw puzzle games, and discriminating images created from distortion.</li><li><strong>Contrastive learning</strong>:  learn representations that <strong>bring similar data points closer</strong> while pushing randomly selected points further away or <strong>maximize a contrastive-based mutual information lower bound</strong> between different views</li><li><strong>Create auxiliary tasks</strong>: The natural ordering or topology of data is also exploited in video-based, graph-based or map-based self-supervised learning. For instance, the pretext task is to determine the correct temporal order for video frames.</li></ul></li><li><p>Theory for self-supervised learning: contrastive learning</p><ul><li>Contrastive learning may not work when conditional independence holds only with additional latent variables</li></ul><table><thead><tr><th style="text-align:left">Theory</th><th style="text-align:left">Limitations</th></tr></thead><tbody><tr><td style="text-align:left">Shows shows guarantees for contrastive learning representations on linear classification tasks using a class conditional independence assumption</td><td style="text-align:left">Not handle approximate conditional independence</td></tr><tr><td style="text-align:left">Contrastive learning representations can linearly recover any continuous functions of the underlying topic posterior under a topic modeling assumption for text</td><td style="text-align:left">The <strong>assumption of independent sampling of words</strong> that they exploit is <strong>strong</strong> and <strong>not generalizable to other domains</strong> like images</td></tr><tr><td style="text-align:left">Studies contrastive learning on the hypersphere through intuitive properties like alignment and uniformity of representations</td><td style="text-align:left">No connection made to downstream tasks</td></tr><tr><td style="text-align:left">A mutual information maximization view of contrastive learning</td><td style="text-align:left">Some issues point by paper [45]</td></tr><tr><td style="text-align:left">Explain negative sampling based methods use the theory of noise contrastive estimation</td><td style="text-align:left"><strong>guarantees are only asymptotic and not for downstream tasks.</strong></td></tr><tr><td style="text-align:left">Conditional independence assumptions and redundancy assumptions on multiple views are used to analyze co-training</td><td style="text-align:left">not for downstream task</td></tr></tbody></table></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Forming the pretext tasks:<ul><li>Colorization: can be interpreted as $p(X_1,X_2|Y)=p(X_1|Y)\times p(X_2|Y)$, aka $X_1,X_2$ are independently conditioned on $Y$</li><li>Inpainting: $p(X_1,X_2|Y,Z)=p(X_1|Y,Z)\times p(X_2|Y,Z)$,aka the inpainted $X_2$ is conditionally independent of $X_2$ (the remainder) given $Y,Z$.</li></ul></li><li>The only way to solve the pretext task is to first implicitly predict $Y$ and then predict $X_2$ from $Y$</li></ul></li><li>Limitations:<ul><li>The underlying principles of self-supervised learning are still mysterious since it is a-priori unclear why predicting what we already know should help.</li></ul></li></ul><h2 id="Goals">Goals</h2><p><em><strong>What conceptual connection between pretext and downstream tasks ensures good representations?</strong></em></p><p><em><strong>What is a good way to quantify this?</strong></em></p><h2 id="How">How?</h2><h3 id="Notations">Notations</h3><table><thead><tr><th style="text-align:left">Symbol</th><th style="text-align:left">Meaning</th></tr></thead><tbody><tr><td style="text-align:left">$\mathbb{E}^L[Y</td><td style="text-align:left">X]$</td></tr><tr><td style="text-align:left">$\Sigma_{XY</td><td style="text-align:left">Z}$</td></tr><tr><td style="text-align:left">$X_1,X_2$</td><td style="text-align:left">the input variable and the target random variable for the pretext tasks</td></tr><tr><td style="text-align:left">$Y$</td><td style="text-align:left">label for the downstream task</td></tr><tr><td style="text-align:left">$P_{X_1X_2Y}$</td><td style="text-align:left">the joint distribution over $\mathcal{X}_1 \times \mathcal{X}_2 \times \mathcal{Y}$</td></tr></tbody></table><h3 id="Idea">Idea</h3><ul><li>Under approximate condition independence (CI) (quantified by the norm of a certain partial covariance matrix), show similar sample complexity improvements.</li><li>Testify pretext task helps when CI is approximately satisfied in text domain.</li><li>Demonstrate on a real-world image dataset that a pretext task-based linear model outperforms or is comparable to many baselines.</li></ul><h3 id="Formalize-SSL-with-pretext-task">Formalize SSL with pretext task</h3><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111171426.png" alt="image-20210111171415185" style="zoom:50%;" /><p>It will be estimated by:</p><ul><li><strong>approximation erro</strong>r:<img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173318.png" alt="image-20210111173318840" style="zoom:33%;" />, where $f^*=\mathbb{E}[Y|X_1]$ is the optimal predictor for the task</li><li><strong>estimation error</strong>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173520.png" alt="image-20210111173520930" style="zoom:25%;" />, it’s the difference between Predicting $Y$ directly by $X_1$ and Predicting by the representations from pretext task</li></ul><h2 id="Experiments">Experiments</h2><ul><li></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks</li><li>Quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) <strong>allows us to learn representations that can solve the downstream task with drastically reduced sample complexity</strong> by just training a linear layer on top of the learned representation.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2008.01064.pdf&quot;&gt;Predicting What You Already Know Helps: Provable Self-Supervised Learning&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Colorful Image Colorization</title>
    <link href="http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html</id>
    <published>2021-01-10T22:15:39.000Z</published>
    <updated>2021-01-12T21:19:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1603.08511.pdf">Colorful Image Colorization</a></p><p>Code <a href="http://richzhang.github.io/colorization/">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><p>Predicting colors in free way: taking the image’s $L$ channel as input and its $ab$ channels as the supervisory signal–&gt; but  tend to look desaturated, one explanation is using loss functions that encourage conservative predictions</p><ul><li><p>Non-parametric methods:  given an input grayscale image, first define one or more color reference images. Then, transfer colors onto the input image from analogous regions of the reference image(s).</p></li><li><p>Parametric methods: learn prediction functions from large datasets of color images at training time, posing the problem as either regression onto continuous color space or classification of quantized color values. --&gt; Work in this paper is also classification task.</p></li><li><p>Concurrent work on colorization</p><table><thead><tr><th>Paper</th><th style="text-align:center">loss</th><th>CNNs</th><th>Dataset</th></tr></thead><tbody><tr><td>Larsson et al.</td><td style="text-align:center">un-rebalanced classification loss</td><td>hypercolumns on a VGG</td><td>ImageNet</td></tr><tr><td>Iizuka et al.</td><td style="text-align:center">regression loss</td><td>two-stream architecture in which fuse global and local features</td><td>Places</td></tr><tr><td>This paper</td><td style="text-align:center">classification loss, with rebalanced rare classes,</td><td>a single-stream, VGG-styled network with added depth and dilated convolutions</td><td>ImageNet</td></tr></tbody></table></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Even in gray images, the semantics of the scene and its surface texture provide ample cues for many regions in each image</li><li>Color prediction is inherently multimodal --&gt; sparks for a loss tailored to their work</li></ul></li><li>Limitations:<ul><li>Loss only cares Euclidean distance: If an object can take on a set of distinct ab values, the optimal solution to the Euclidean loss will be the mean of the set. In color prediction, this averaging effect favors grayish, desaturated results. Additionally, if the set of plausible colorizations is non-convex, the solution will in fact be out of the set, giving implausible results.</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Design colorization based pretext task to get a good image semantic representations: <strong>produce a plausible colorization that could potentially fool a human observer</strong></p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li><strong>Produce vibrant colorization</strong>: Predict a distribution of possible colors for each pixel. Then, re-weight the loss at training time to emphasize rare colors. This encourages the model to exploit the full diversity of the large-scale data on which it is trained. Lastly, produce a final colorization by taking the annealed mean of the distribution.</li><li>Evaluate synthesized images:  set up a “colorization Turing test”.</li></ul><h3 id="Data-Preparation">Data Preparation</h3><ul><li>Quantize the $ab$ output space into bins with grid size $10$ and keep the $Q = 313$ values which are in-gamut. Then this is the label $Z$ of each pixel. Formally, denote the raw label as $Y$, then $Z = H^{−1}_{gt} (Y)$, which converts ground truth color $Y$ to vector $Z$.</li></ul><h3 id="Implementation">Implementation</h3><ul><li><p>Model</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110175555.png" alt="image-20210110175554683" style="zoom:30%;" /></li><li><p>Loss function</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210112160157.png" alt="image-20210110180438146" style="zoom:33%;" /><p>where $v(·)$ is a weighting term that can be used to re-balance the loss based on color-class rarity.</p><ul><li><p>Re-balancing</p><p>The distribution of $ab$ values in natural images is strongly biased towards values with low $ab$ values.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110233506.png" alt="image-20210110233506301" style="zoom:33%;" />, where $\tilde{p}$ the  empirical probability of colors in the quantized ab space $p\in \Delta Q$ from the full ImageNet training set and smooth the distribution with a Gaussian kernel $G_{\sigma}$. $\lambda=\frac{1}{2}, \sigma=5$ worked well.</p></li></ul></li><li><p>Inferring point estimates</p><p>map probability distribution $\hat{Z}$ to color values $\hat{Y}$ with function $\hat{Y} = H(\hat{Z})$</p></li></ul><p>They interpolate by re-adjusting the temperature $T$ of the softmax distribution, and taking the mean of the result. Lowering the temperature $T$ produces a more strongly peaked distribution, and setting $T\rightarrow 0$ results in a 1-hot encoding at the distribution mode. They find that $T=0.38$ captures the vibrancy of the mode while maintaining the spatial coherence of the mean.</p><h2 id="Experiments">Experiments</h2><ul><li>Dataset: ImageNet</li><li>Base models</li></ul><table><thead><tr><th>Model Name</th><th style="text-align:center">Loss</th><th>Train</th></tr></thead><tbody><tr><td>Ours(full)</td><td style="text-align:center">classification loss</td><td>from scratch with kmeans initialization, ADAM solver for about 450K iterations. $\beta_1 = .9, \beta_2 = .99$, and weight decay = $10^{−3}$ . Initial learning rate was $3 × 10^{−5}$ and dropped to $10^{−5}$ and $3 × 10^{−6}$ when loss plateaued, at 200k and 375k iterations, respectively.</td></tr><tr><td>Ours(class)</td><td style="text-align:center">classification loss withou rebalancing ($\lambda=1$)</td><td>similar training protocol as Ours(full)</td></tr><tr><td>Ours(L2)</td><td style="text-align:center">L2 regression loss</td><td>same training protocol</td></tr><tr><td>Ours(L2,ft)</td><td style="text-align:center">L2 regression loss</td><td>fine tuned from our full classification with rebalancing network</td></tr><tr><td>Larsson et al.</td><td style="text-align:center"></td><td>CNN method</td></tr><tr><td>Dahl</td><td style="text-align:center">L2 regression loss</td><td>a Laplacian pyramid on VGG features</td></tr><tr><td>Gray</td><td style="text-align:center">–</td><td>every pixel is gray, with $(a, b) = 0$</td></tr><tr><td>Random</td><td style="text-align:center">–</td><td>Copies the colors from a random image from the training set</td></tr></tbody></table><h3 id="Colorization-quality">Colorization quality</h3><ul><li>AMT: participants confirm their results. They argue that their work produce a more prototypical appearance for those are poorly white balanced</li><li>Semantic interpretability (VGG classification): Are the results  realistic enough colorizations to be interpretable to an off-the-shelf object classifier? They check it by  by feeding their fake colorized images to a VGG network  that was trained to predict ImageNet classes from real color photos.<ul><li>The result is $3.4%$ lower than Larsson’s.</li><li>Without any additional training or fine-tuning, one can improve performance on grayscale image classification, simply by colorizing images with our algorithm and passing them to an off-the-shelf classifier.</li></ul></li><li>Raw accuracy (AuC):<ul><li>L2 metric can achieve accurate colorizations, but has difficulty in optimization from scratch</li><li>class-rebalancing in the training objective achieved its desired effect</li></ul></li><li>Compared with others<ul><li>LEARCH: On SUN dataset, authors have $17.2%$ on AMT task while LEARCH has $9.8%$</li></ul></li></ul><h3 id="Cross-channel-encoding-as-SSL-Feature-learning">Cross-channel encoding as SSL Feature learning</h3><ul><li><p>Datasets: ImageNet, PASCAL (fine tuned after training on ImageNet)</p></li><li><p>Backbone: AlexNet</p></li><li><p>Settings</p><ul><li>ImageNet: fixing the extractor and retrain the classifier (softmax layer) by labels</li><li>PASCAL: : (1) keeping the input grayscale by disregarding color information (Ours (gray)) and (2) modifying conv1 to receive a full 3-channel $Lab$ input, initializing the weights on the $ab$ channels to be zero (Ours (color)).</li></ul></li><li><p>Summary</p><ul><li>For ImageNet, there is a $6%$ performance gap between color and grayscale inputs. Except for the 1st layer, representations from other deeper layers catch and outperform most methods, indicating that <strong>solving the colorization task encourages representations that linearly separate semantic classes in the trained data distribution</strong></li><li>On PASCAL, when conv1 is frozen, the network is effectively only able to interpret grayscale images.</li></ul></li></ul><h3 id="The-properties-of-network">The properties of network</h3><ul><li><p>Is it exploiting low-level cues?</p><p>Given a grayscale Macbeth color chart as input, it was unable to recover its colors. On the other hand, given two recognizable vegetables that are roughly <strong>isoluminant</strong>, <strong>the system is able to recover their color</strong>.</p></li><li><p>Does it learn multimodal color distributions ?</p><p>Take effective dilation ( the spacing at which consecutive elements of the convolutional kernel are evaluated, relative to the input pixels, and is computed by the product of the accumulated stride and the layer dilation) as the measurement.  Through each convolutional block from conv1 to conv5, the effective dilation of the convolutional kernel is increased. From conv6 to conv8, the effective dilation is decreased</p></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Designing an appropriate objective function that handles the multimodal uncertainty of the colorization problem and captures a wide diversity of colors</li><li>Introducing a novel framework for testing colorization algorithms, potentially applicable to other image synthesis tasks</li><li>Setting a new high-water mark on the task by training on a million color photos.</li><li>Introduce the colorization task as a competitive and straightforward method for self-supervised representation learning, achieving state-of-the-art results on several benchmarks.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1603.08511.pdf&quot;&gt;Colorful Image Colorization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;http://richzhang.github.io/colorization/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Revisiting Self-Supervised Visual Representation Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html</id>
    <published>2021-01-09T21:21:00.000Z</published>
    <updated>2021-01-12T19:33:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf">Revisiting Self-Supervised Visual Representation Learning</a></p><p>Code <a href="https://github.com/google/revisiting-self-supervised">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li>robotics: the result of interacting with the world, and the fact that multiple perception modalities simultaneously get sensory inputs are strong signals for pretext</li><li>videos: the synchronized cross-modality stream of audio, video, and potentially subtitles, or of the consistency in the temporal dimension</li><li>image  datasets:<ul><li>Patch-based methods: E.g.: predicting the relative location of image patches; “jigsaw puzzle”</li><li>Image-level classification tasks:<ul><li>RotNet, create class labels by clustering images, image inpaiting, image colorization, split-brain and motion segmentation prediction;</li><li>Enforce structural constraints on the representation space: an equivariance relation to match the sum of multiple tiled representations to a single scaled representation;  predict future patches in via autoregressive predictive coding</li><li>Combining multiple pretext task: E.g. extend the “jigsaw puzzle” task by combining it with colorization and inpainting; Combining the jigsaw puzzle task with clustering-based pseudo labels ( Jigsaw++) ;  make one single neural network learn all of four different SSL methdos in a multi-task setting; combined the selfsupervised loss GANs objective</li></ul></li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Expensive labeled data for supervised task</li></ul></li><li>Limitations:<ul><li>Previous works mostly concentrate on pretext task, but didn’t pay much attention to the choice of backbones etc.</li></ul></li></ul><h2 id="Goals">Goals</h2><p><strong>An optimal CNN architecture for pretext task</strong>,  investigating the influence of architecture design on the representation quality.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li>a comparison of different self-supervision methods using a unified neural network architecture, but with the goal of combining all these tasks into a single self-supervision task</li></ul><h3 id="Implementation">Implementation</h3><h4 id="Family-of-CNNs">Family of CNNs</h4><ul><li><p>variants of ResNet:</p></li><li><p><strong>ResNet50</strong>, the output before task-specific logits layer is named as $pre-logits$. explore $k \in {4, 8, 12, 16}$, resulting in pre-logits of size $2048, 4096, 6144$ and $8192$ respectively. $k$ is the widening factor.</p></li><li><p><strong>ResNet v1</strong>: ???batch normalization (BN) right after each convolution and before activation???</p></li><li><p><strong>ResNet v2</strong>: ?</p></li><li><p><strong>ResNet (-)</strong>: without ReLU preceding the global average pooling</p></li><li><p>a batch-normalized <strong>VGG</strong> architecture since VGG is structurally close to AlexNet. BN between CNN and activation, VGG19.</p></li><li><p><strong>RevNets</strong>: stronger invertibility guarantees so as to compare with ResNets. The residual unit used here is equivalent to double application of the residual unit.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110133429.png" alt="image-20210110133429332" style="zoom:50%;" />, check <a href="https://papers.nips.cc/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf">here</a> for details. Apart from this slightly complex residual unit, others are the same as ResNet.</p></li></ul><h4 id="Family-of-pretext-tasks">Family of pretext tasks</h4><ul><li><strong>Rotation</strong>: same as RotNet, ${0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}}$</li><li><strong>Exemplar</strong>: triplet loss</li><li><strong>Jigsaw</strong>: recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches was performed. Patches are sampled with a random gap between them. Each patch is then independently converted to grayscale with probability $\frac{2}{3}$ and normalized to zero mean and unit standard deviation. Extract final image representations by averaging representations of 9 cropped patches.</li><li><strong>Relative patch location</strong>: predicting the relative location of two given patches of an image. Extract final image representations by averaging representations of 9 cropped patches.</li></ul><h4 id="Evaluation-of-the-quality-of-learned-representations">Evaluation of the quality of learned representations</h4><ul><li>Idea: <strong>Using learned representations for training a linear logistic regression model to solve multiclass image classification tasks</strong> (downstream tasks). All representations come from pre-logits level.</li><li>Details: the linear logistic regression model is trained by L-BFGS. But for comparison, using SGD with momentum and use data augmentation during training.</li></ul><h2 id="Experiments">Experiments</h2><ul><li>Datasets</li></ul><table><thead><tr><th>Datasets</th><th style="text-align:center">Train</th><th>Test</th></tr></thead><tbody><tr><td>ImageNet</td><td style="text-align:center">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr><tr><td>Places 205</td><td style="text-align:center">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr></tbody></table><h3 id="Pretext-CNNs-Downstream">Pretext? CNNs? Downstream?</h3><ul><li><p>Pretext and its preferred CNN architecture: <strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110151545.png" alt="image-20210110151545316" style="zoom:30%;" /></li><li><p>The generalization of representations from pretext tasks: each pretext task can be generalized to other dataset. Check the trendings in figure 2.</p></li><li><p>Optimal CNNs for Pretext and downstream tasks: not consistent. But after selecting the right architecture for each self-supervision and increasing the widening factor, models significantly outperform previously reported results.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155158.png" alt="image-20210110155157874" style="zoom:45%;" /></li><li><p><strong>Better performance on the pretext task does not always translate to better representations</strong>: Performance on pretext cannot be used to reliably select the model architecture.</p></li></ul><h3 id="CNNs-architecture">CNNs architecture</h3><ul><li>Skip-connection: For VGG, representations deteriorate towards the end of the network cause models specialize to the pretext task in the later layers. ResNet prevent this deterioration. They argue that this is because <strong>ResNet’s residual units being invertible under some conditions</strong> and confirm this by RevNet.</li><li>Depth of CNNs: For residual architectures, the pre-logits are always best.</li><li>Model-width and representation size:<ul><li>whether the increase in performance is due to increased network capacity or to the use of higher-dimensional representations, or to the interplay of both? To answer it, authors disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer.</li><li><strong>Model-width and representation size both matter independently, and larger is always better.</strong></li><li>SSL techniques are likely to <strong>benefit from using CNNs with increased number of channels</strong> across wide range of scenarios, even under low-data regime.</li></ul></li></ul><h3 id="Evaluate-the-quality-of-representations">Evaluate the quality of representations</h3><ul><li><strong>A linear model is adequate</strong>:  MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged</li></ul>  <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155830.png" alt="image-20210110155829820" style="zoom:50%;" /><ul><li>To train <strong>linear model, SGD optimization hyperparameters:</strong>  very long training (≈ 500 epochs) results in higher accuracy</li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Most affect performance in the fully labeled setting, may significantly affect performance in the selfsupervised setting.</li><li>the quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</li><li>Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations</li><li>The evaluation procedure, where a linear model is trained on a fixed visual representation using stochastic gradient descent, is sensitive to the learning rate schedule and may take many epochs to converge</li><li><strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong>–&gt;<strong>pretext tasks for self-supervised learning</strong> should not <strong>be considered</strong> in isolation, but <strong>in conjunction with underlying architectures</strong>.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&quot;&gt;Revisiting Self-Supervised Visual Representation Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/google/revisiting-self-supervised&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Representation Learning by Predicting Image Rotations</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html</id>
    <published>2021-01-09T04:11:12.000Z</published>
    <updated>2021-01-12T20:56:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1803.07728.pdf">Unsupervised Representation Learning by Predicting Image Rotations</a></p><p>Codes <a href="https://github.com/gidariss/FeatureLearningRotNet">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><p>How to get <strong>a high-level image semantic representation using unlabeled data</strong></p><ul><li>SSL:  defines an annotation free pretext task, has been proved as good alternatives for transferring on other vision tasks. E.g.: colorize gray scale images, predict the relative position of image patches, predict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames.</li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li><strong>the attention maps are equivariant w.r.t. the image rotations, check appendix A.</strong></li></ul></li><li>Limitations<ul><li>supervised feature learning has the main limitation of requiring intensive manual labeling effort</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Provide a “self-supervised” formulation for image data, a <strong>self defined supervised task involving predicting the transformations used for image.</strong> The model won’t have access to the initial image.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li>Concretely, define the geometric transformations as the image rotations by 0, 90, 180, and 270 degrees. Thus, the ConvNet model is trained on the 4-way image classification task of recognizing one of the four image rotations.</li></ul><h3 id="Implementation">Implementation</h3><h4 id="Data-preparation">Data preparation</h4><ul><li><p>2D image Rotation</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109124726.png" alt="image-20210109124726420" style="zoom:40%;" /><table><thead><tr><th>Operations</th><th style="text-align:center">Implementation</th></tr></thead><tbody><tr><td>+90</td><td style="text-align:center">transpose then flip vertically</td></tr><tr><td>+180</td><td style="text-align:center">flip  vertically then flip horizontally</td></tr><tr><td>+270</td><td style="text-align:center">flip vertically then transpose</td></tr></tbody></table></li></ul><h4 id="Learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109123815.png" alt="image-20210109123815880" style="zoom:45%;" />, where $F(\cdot)$ is the predicted probability of the geometric transformation with label $y$ and $\theta$ are the learnable parameters of model $F(\cdot)$, and $g(X_i|y)$ is the transformed image with transformation $y$.</p></li><li><p><strong>Why does it works?</strong></p><ul><li>To work for this pretext task, extractor has to  <strong>understand the concept of the objects depicted in the image</strong>. Models must learn to localize salient objects in the image, recognize their orientation and object type, and then relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images.</li><li>Easy to be implemented by flipping and transpose, no chance for importing low-level visual artifacts so as to avoid trivial features (which have no practical value)</li><li>Operations are easy to be recognized manually.</li></ul></li></ul><h2 id="Experiments">Experiments</h2><h3 id="CIFAR-object-recognition">CIFAR: object recognition</h3><ul><li>Dataset:</li></ul><table><thead><tr><th>Datasets</th><th>Preprocess</th></tr></thead><tbody><tr><td>CIFAR-10</td><td>Rotations</td></tr></tbody></table><ul><li>Training:  SGD with batch size 128, momentum 0.9, weight decay $5e−4$ and $lr$ of 0.1. We drop the learning rates by a factor of 5 after epochs 30, 60, and 80. 100 epochs. Each time feeding with all 4 images.</li><li>Summary<ul><li><em>The learned feature hierarchies</em>: convnet with different number of layers. <strong>Representations from the 2nd block</strong> are good, and <strong>increasing the total depth</strong> of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers.</li><li><em>The quality of the learned features w.r.t. the number of recognized rotations</em>: 4 discrete rotations outperform.</li><li><em>Compared with previous work</em> : almost the same as the NIN  supervised model. Fine-tuned the unsupervised learned features further improves the classification performance.</li><li><em>Correlation between object classification task and rotation prediction task</em>: The representations from pretext make classifier converge faster compared with the classifier trained from scratch.</li><li><em>Semi-supervised setting</em>: pretrained on the whole dataset without labels, then fine-tuned on a small labeled subset. It exceeds the supervised model when the number of examples per category drops below 1000.</li></ul></li></ul><h3 id="Others-classification-object-detection-segmentation">Others: classification, object detection , segmentation</h3><ul><li><p>Dataset: ImageNet, Places, and PASCAL VOC.</p><table><thead><tr><th>Task</th><th>Datasets</th></tr></thead><tbody><tr><td>Classification</td><td>Pretrained on ImageNet, then test on ImageNet, Places, and PASCAL VOC.</td></tr><tr><td>Object detection</td><td>PASCAL VOC</td></tr><tr><td>Object segmentation</td><td>PASCAL VOC</td></tr></tbody></table></li><li><p>Backbones: AlexNet without local response normalization units, dropout units, or groups in the  colvolutional layers while it includes batch normalization units after each linear layer</p></li><li><p>Pretrained: on ImageNet, SGD with batch size 192, momentum 0.9, weight decay $5e − 4$ and $lr$ of 0.01. Learning rates are dropped by a factor of 10 after epochs 10, and 20 epochs. Trained in total for 30 epochs.</p></li><li><p>Summary:</p><ul><li><p>ImageNet classification task: surpasses all the other unsupervised methods by a significant margin, narrows the performance gap between unsupervised features and supervised features.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154626.png" alt="image-20210109154626599" style="zoom:50%;" /></li><li><p>Transfer learning evaluation on PASCAL VOC: fine tuning, used weight rescaling proposed by Krahenbuhl et al.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154831.png" alt="image-20210109154831162" style="zoom:60%;" /></li><li><p>Places classification task: the learnt features are evaluated w.r.t. their generalization on classes that were “unseen” during the unsupervised training phase</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154953.png" alt="image-20210109154952935" style="zoom:50%;" /></li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Propose a new self-supervised task that is very simple and at the same time.</li><li>Rotationsod under various settings (e.g. semi-supervised or transfer learning settings) and in various vision tasks (i.e., CIFAR-10, ImageNet, Places, and PASCAL classification, detection, or segmentation tasks).</li><li>They argue this self-supervised formulation demonstrates state-of-the-art results with dramatic improvements w.r.t. prior unsupervised approaches, and narrows the gap between unsupervised and supervised feature learning.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1803.07728.pdf&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Codes &lt;a href=&quot;https://github.com/gidariss/FeatureLearningRotNet&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Visual Representation Learning by Context Prediction</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html</id>
    <published>2021-01-08T21:22:12.000Z</published>
    <updated>2021-01-12T19:33:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf">Unsupervised Visual Representation Learning by Context Prediction</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><p>How to get <strong>a good image representation</strong></p><ul><li>The latent variables of an appropriate generative model.  --&gt; generative models<ul><li>But given an image, inferring the latent structure is <strong>intractable</strong> for even relatively simple models --&gt; to fix, use <strong>sampling</strong> to perform approximate inference.</li></ul></li><li>An embedding that can discriminate the semantics in images by distances of them. – create a supervised “pretext” task. But hard to tell whether the predictions themselves are correct.<ul><li>Reconstruction-based:  E.g., denoising autoencoders (reconstruction ), sparse autoencoders (reconstruction + sparsity penalty )</li><li>Context prediction: “skip-gram” to “filling the blank” task, and convert the prediction task to discriminate task like discriminating between real images vs. images where one patch has been replaced by a random patch from elsewhere in the dataset. But not hard enough for high-level representations</li><li>Discover object categories using hand-crafted features and various forms of clustering. But they will lose shape information. To keep more shape information, some take contour extraction or defining similarity metrics.</li><li>Video-based: since the identity of objects remains unchanged -</li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>the difficulties for generalizing CNNs models on Internet -scale datasets</li><li>context has proven to be a powerful source of automatic supervisory signal for learning representations --&gt; context can be regarded as a ‘pretext’ task to force the model to learn a good word embedding</li><li>current reconstruction-based algorithms struggle with low-level phenomena, like stochastic textures, making it hard to even measure whether a model is generating well.</li></ul></li><li>Limitations:<ul><li>generative models are rather efficiently on smaller datasets but burden on high-resolution  natural images</li><li>Some are too simple for extracting high-level representations</li><li>Hard to tell whether the model has obtained good representations.</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Provide a “self-supervised” formulation for image data, a supervised task involving predicting the context for a patch.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li>Hypothesis: Doing well on predicting patches’ positions requires understanding scenes and objects–&gt; a good visual representation</li><li>Concretely, sample random pairs of patches in one of eight spatial configurations, and present each pair to a machine learner. The algorithm must then guess the position of one patch relative to the other.</li></ul><h3 id="Implementation">Implementation</h3><h4 id="Data-preparation">Data preparation</h4><ul><li><p>Two patches are fed into network</p></li><li><p>Given an image, one patch will be sampled uniformly, then according to the position of this sampled patch, then 2nd patch will be sampled randomly from the eight possible neighboring locations.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108181759.png" alt="image-20210108181757911" style="zoom:33%;" /></li><li><p>including a gap between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</p></li><li><p>For some images ( chromatic aberration), after solving the relative location task (like by detecting the separation between green and magenta (red + blue). ), this problem will be relaxed.</p><ul><li>Shift green and magenta toward gray</li><li>Color dropping : randomly drop 2 of the 3 color channels from each patch and replace them by gaussian noise.</li></ul></li></ul><h4 id="Learning-algorithm">Learning algorithm</h4><ul><li><p>Siamese network based on AlexNet. But not all layers share weights, LRN (local response normalization ) layers won’t.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108183110.png" alt="image-20210108183110662" style="zoom:33%;" /></li><li><p><strong>Why does it works?</strong></p><ul><li>Avoid ‘trivial’ shortcuts (like boundary patterns or textures continuing between patches)–&gt; including a gap (up to 48 pixels) between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</li><li>Enhancing performance on images with  chromatic aberration.</li></ul></li></ul><h2 id="Experiments">Experiments</h2><p>Pre-Training: SGD+BN+high momentum, 4 weeks on K40 GPU.</p><table><thead><tr><th>Datasets</th><th style="text-align:center">Resizing</th><th>Preprocess</th></tr></thead><tbody><tr><td>ImageNet</td><td style="text-align:center">$150K\sim450K$ total pixels</td><td>1. sample patches at resolution $96\times 96$<br />2. mean subtraction, projecting or dropping colors, and randomly downsampling some patches to as little as 100 total pixels, and then upsampling it.</td></tr></tbody></table><h3 id="Ability-on-semantic">Ability on semantic</h3><p>Does it get similar representations for patches with similar semantics?</p><ul><li><p>check nearest neighbors by normalized correlation of $fc6$'s output. Compared with results from  random initialized model and ImageNet AlexNet.</p></li><li><p>Summary</p><ul><li>in a few cases, random (untrained) ConvNet also does reasonably well</li><li>the representations from proposed model often capture the semantic information</li></ul></li></ul><h3 id="Learnability-of-Chromatic-Aberration">Learnability of Chromatic Aberration</h3><ul><li>Patches displayed similar aberration tend to be predicted at the same location.</li><li>The effect of  color projection operation is canceled for this kind of images.</li></ul><h3 id="Object-detection">Object detection</h3><ul><li>Dataset : VOC 2007</li><li>Train: fine-tune the pretrained model (model is slight different with the previous one considering the image size in VOC) on VOC 2007.</li><li>Test: output from $fc7$ is taken.</li><li>Summary<ul><li>Pre-trained model outperforms the one trained from scratch</li><li>Obtained the best result on VOC 2007 without using labels</li><li>Robustness of the representations for one object in different datasets: acceptable</li></ul></li></ul><h3 id="Visual-data-mining">Visual data mining</h3><ul><li>Task : aims to use a large image collection to discover image fragments which happen to depict the same semantic objects</li><li>Specification for this task: sample a constellation of four adjacent patches from an image, after finding the top 100 images which have the strongest matches for all four patches, then use a type of geometric verification to filter away the images where the four matches are not geometrically consistent. Finally, rank the different constellations by counting the number of times the top 100 matches geometrically verify.</li><li>To define the geometric verification: first compute the best-fitting square $S$ to the patch centers (via least-squares), while constraining that side of $S$ be between 2/3 and 4/3 of the average side of the patches. Then compute the squared error of the patch centers relative to $S$ (normalized by dividing the sum-of-squared-errors by the square of the side of $S$). The patch is geometrically verified if this normalized squared error is less than 1.</li><li>Test: VOC 2011, Street View images from Paris</li><li>Summary<ul><li>The discovery of birds and torsos is good</li><li>The gains in terms of coverage, suggesting increased invariance for learned features</li><li>The pretext task is difficult:  for a large fraction of patches within each image, the task is almost impossible</li><li>Limitations: some loss of purity, and cannot currently determine an object mask automatically (although one could imagine dynamically adding more sub-patches to each proposed object).</li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>instance-level supervision appears to improve performance on category-level tasks</li><li>The proposed model is sensitive to objects and the layout of the rest of the image</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf&quot;&gt;Unsupervised Visual Representation Learning by Context Prediction&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
    <link href="http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html</id>
    <published>2021-01-06T16:17:12.000Z</published>
    <updated>2021-01-12T20:58:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1406.6909.pdf">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li>Supervised : labeled data with a specific CNN<ul><li>directly penalizing the derivative of the output with respect to the magnitude of the transformations, but will be sensitive to the magnitude of the applied transformation.</li></ul></li><li>Unsupervised: learning invariant representations<ul><li>Directly modeling the input distribution and are hard for jointly training multiple layers of a CNN<ul><li><strong>autoencoders</strong>:  denoising auto encoders, say reconstruct data from randomly perturbed input samples; or learn representations from videos by  enforcing a temporal slowness constraint on the feature representation learned by a linear autoencoder.</li><li>invariant to local transformations</li></ul></li><li>most aims at regularization of the latent representation</li></ul></li><li>Semi-supervised<ul><li>Regularization supervised algorithms by unlabeled data: self-training, entropy regularization</li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>the features learned by one network often generalize to new datasets</li><li>a network can be adapted to a new task by replacing the loss function and possibly the last few layers of the network and fine-tuning it to the new problem</li></ul></li><li>Limitations:<ul><li>the need for huge labeled datasets to be used for the initial supervised training</li><li>the transfer becomes less efficient the more the new task differs from the original training task</li></ul></li></ul><h2 id="Goals">Goals</h2><p>a more general extractor using unlabeled data. The extractor should satisfy two requirements:</p><ul><li>there must be at least one feature that is similar for images of the same category $y$ (invariance);</li><li>there must be at least one feature that is sufficiently different for images of different categories (ability to discriminate)</li></ul><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li>creating an auxiliary task + invariant features to transformations</li></ul><h3 id="Implementation">Implementation</h3><h4 id="Data-preparation">Data preparation</h4><ul><li>Do random selected transformation (from  a predefined  family of transformations) for sampled patches (regions containing considerable gradients so that sample a patch with probability proportional to mean squared gradient magnitude within the patch )</li><li>The family of transformations<ul><li>translation</li><li>scaling</li><li>rotation</li><li>contrast: PCA and HSV</li><li>color: works on HSV space</li><li>blur etc.</li></ul></li><li>Before feeding into model, do normalization (subtract the mean of each pixel over the whole resulting dataset)</li><li>Labeling: all transformed patches from the same seed patch are labeled by the same index</li></ul><h4 id="Learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function :</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155621.png" alt="image-20210107155618997" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155911.png" alt="image-20210107155854389" style="zoom:30%;" /><ul><li><p>After transformations, the loss for a whole class (augmented by the same seed patch ) can be taken as</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107162118.png" alt="image-20210107162118193" style="zoom:50%;" />, notice the 2nd and the 4th can be canceled.</p><ul><li>The 1st term: enforces correct classification of the average representation $\mathbb{E}<em>\alpha[g(T</em>\alpha x_i)]$ for a given input sample</li><li>The 2nd term: a regularizer enforcing all $ h(T_\alpha x_i)$ to be close to their average value, i.e., the feature representation is sought to be approximately invariant to the transformations $ T_\alpha$, note the convergence to global minimum is listed at appendix.</li></ul></li></ul></li><li><p><strong>Why does it works?</strong></p><ul><li>Previous works mostly focus on modeling the input distribution $p(x)$, based on the assumption that a good model of  $p(x)$ contains information about the category distribution  $p(y|x)$.  Therefore, to get the invariance, one will do regularization of the latent representation and obtain representation by reconstruction .</li><li>Their work  does not directly model the input distribution  $p(x)$  but learns a representation that discriminates between input samples. They argue that this <strong>allows more DOF to model the desired variability of a sample and avoid task-unnecessary reconstruction.</strong></li><li>However, their work will <strong>fail on color-relied task</strong></li></ul></li></ul><h2 id="Experiments">Experiments</h2><h3 id="Classification">Classification</h3><ul><li><p>Datasets: <strong>STL-10</strong>, CIFAR-10, Caltech-101 and Caltech-256. report mean and standard deviation</p><table><thead><tr><th>Datasets</th><th style="text-align:center">Resizing</th><th>Train</th><th>Test</th></tr></thead><tbody><tr><td>STL-10</td><td style="text-align:center">64c5-64c5-128f</td><td>10 pre-defined folds of the training data</td><td>fixed test set</td></tr><tr><td>CIFAR-10</td><td style="text-align:center">resize from $32\times 32$ to $64\times 64$</td><td>1. whole training set<br />2. 10 random selections of 400 training samples per class</td><td>1. results on CIFAR-10<br />2. average results on 10 sets.</td></tr><tr><td>Caltech-101</td><td style="text-align:center">to $150\times 150$</td><td>30 random samples per class</td><td>not more than 50 samples per class</td></tr><tr><td>Caltech-256</td><td style="text-align:center">$256\times 256$</td><td>randomly selected 30 samples per class</td><td>those except for training</td></tr></tbody></table></li><li><p>backbones of network</p><table><thead><tr><th>Network</th><th style="text-align:center">Structure</th><th>Training</th></tr></thead><tbody><tr><td>small</td><td style="text-align:center">64c5-64c5-128f</td><td>1.5 days, SGD with fixed momentum of 0.9</td></tr><tr><td>medium</td><td style="text-align:center">64c5-128c5-256c5-512f</td><td>4 days, SGD with fixed momentum of 0.9</td></tr><tr><td>large</td><td style="text-align:center">92c5-256c5-512c5-1024f</td><td>9 days, SGD with fixed momentum of 0.9</td></tr></tbody></table></li><li><p>Training: learning rate starts at 0.01， then when there was no improvement in validation error, decreased the learning rate by a factor of 3. All networks are trained on one Titan</p></li><li><p>Test features: one-vs-all linear SVM.</p></li><li><p>Summary</p><ul><li>with increasing feature vector dimensionality and number of labeled samples, training an SVM becomes less dependent on the quality of the features</li><li>Relation of <strong>the number of surrogate classes</strong> :  <strong>sampling too many, too similar images for training can even decrease the performance of the learned features</strong>. ( the discriminative loss is no longer reasonable with too many similar surrogate classes.) --&gt; fix: e.g. clustering the output features then do augmentation for clusters and feed these augmented classes as surrogate data</li><li>Relation of <strong>the number of samples per surrogate class</strong> :  around 100 samples is sufficient</li><li>Relation of **types of transformations ** : each time remove a group of transformations and check how the performance is decreased , e.g. scaling, rotation etc.  Translations, color variations and contrast variations are significantly more important. For the matching task, using blur as an additional transformation improves the performance.</li><li>Relation of **Influence of the dataset  **:  the learned features generalize well to other datasets</li><li>Relation of <strong>Influence of the Network Architecture on Classification Performance</strong>: Classification accuracy generally improves with the network size</li></ul></li></ul><h3 id="Descriptor-matching">Descriptor matching</h3><ul><li>Task: Matching of interest points</li><li>Datasets: by Mikolajczyk et al., augmented by applying  6 different types of transformations with varying strengths to 16 base images from Flickr. In addition to the transformations used before, also change the lighting and blur .</li><li>Backbones: 64c7s2-128c5-256c5-512f, named as Exemplar-CNN-blur</li><li>Training:  use unlabeled images from Flickr for training</li><li>Test and measurements: prediction is $TP$ if $IOU\ge 0.5$. Compared with SIFT and Alexnet</li><li>Summary<ul><li>Optimum patch size (or layer in CNNs): SIFT is based on normalized finite differences, and thus very robust to blurred edges caused by interpolation. In contrast, for the networks, especially for their lower layers, there is an optimal patch size. They argue that features from higher layers have access to larger receptive fields and, thus, can again benefit from larger patch sizes.</li><li>A loss function that focuses on the invariance properties (rather than class-specific features) required for descriptor matching yields better results.</li><li>Features obtained with the unsupervised training procedure outperform the features from AlexNet on both datasets</li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Pretty good on tasks: object classification , descriptor matching</li><li>emphasizes the value of data augmentation in general and suggests the use of more diverse transformations.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1406.6909.pdf&quot;&gt;Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>notes-DAPPER</title>
    <link href="http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html"/>
    <id>http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html</id>
    <published>2019-10-05T16:52:21.000Z</published>
    <updated>2021-01-12T19:34:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see <a href="https://github.com/nansencenter/DAPPER">here</a>.</p><p>This notes keep records of the problems encountered while using DAPPER.</p><a id="more"></a><ol><li>Pre-setting</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.</span><br></pre></td></tr></table></figure><p>Copy mkl_*.dll, libiomp5md.dll and <em>libiomp5md.pdb</em> from directory “./Library/bin” to the root directory of python exe.</p><p>Reference from <a href="https://blog.csdn.net/supertangcugu/article/details/89790617">here</a>.</p><ol start="2"><li>Manual</li></ol><p>Manual <a href="https://dapper.readthedocs.io/en/latest/implementation.html">online</a></p><p>For EnKF: Nx-by-N (维度数<em>样本数)<br>For ndarrays: N-by-Nx (样本数</em>维度数)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see &lt;a href=&quot;https://github.com/nansencenter/DAPPER&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This notes keep records of the problems encountered while using DAPPER.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="da" scheme="http://yoursite.com/tags/da/"/>
    
  </entry>
  
  <entry>
    <title>Notes of Mathematics</title>
    <link href="http://yoursite.com/posts/notes/2019-06-19-notes-math.html"/>
    <id>http://yoursite.com/posts/notes/2019-06-19-notes-math.html</id>
    <published>2019-06-19T13:40:09.000Z</published>
    <updated>2021-02-04T21:18:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="泛函">泛函</h2><center>    <img src="/assets/img/Rules/lines.png" width=500"></center><h3 id="不动点定理">不动点定理</h3><ol><li><p>不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。</p><p><a href="https://zhuanlan.zhihu.com/p/33885648">链接</a></p></li></ol><h3 id="紧性的利用">紧性的利用</h3><ul><li>证明存在性</li><li>在无限维空间中“模仿”有限维的欧式空间</li></ul><p>紧集是为了模仿描述欧式空间中的有界闭集合么？<br>紧=相对紧+闭</p><h3 id="流形-Manifolds">流形(Manifolds)</h3><ol><li>概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。</li></ol><h3 id="支撑集-Support">支撑集(Support)</h3><ol><li>概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合</li></ol><a id="more"></a><h2 id="数学分析">数学分析</h2><h3 id="Lipschitz连续">Lipschitz连续</h3><ol><li>若存在一个常数K，使得定义域内的任意两点x1,x2满足：<br>$ \left|f\left(x_1\right)-f\left(x_2\right)\right|=K\left|x_1-x_2\right|$<br>则称函数为Lipschitz连续函数。此性质限定了f的导函数的绝对值不超过K，规定了函数的最大局部变动幅度。</li></ol><h2 id="Statistical-learning-theory">Statistical learning theory</h2><h3 id="Introduction">Introduction</h3><p>Determine how well a model performs on unseen data</p><p>Check <a href="http://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-7-notes.pdf">here</a> for PAC, VC, uniform bound and others.</p><h2 id="Convex-Optimization">Convex Optimization</h2><p>Book <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>, <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></p><table><thead><tr><th>Terminology</th><th>Definition</th></tr></thead><tbody><tr><td>Convex function: origin</td><td>$f(\theta x+(1-\theta)y)\le \theta f(x)+(1-\theta)f(y)$</td></tr><tr><td>Convex function: 1st order (once differential )</td><td>$f(y)\ge f(x)+\nabla f(x)^T (y-x) $</td></tr><tr><td>Convex function: 2nd order (twice differential )</td><td>$\nabla^2f(x)\succeq0$, the eigenvalues won’t be negative</td></tr><tr><td>$L$-Lipschitz</td><td>$|f(x)-f(y)|\le L|x-y|$</td></tr><tr><td>$\beta$-smooth: $\beta$-Lipschitz on gradient</td><td>$|\nabla f(x)-\nabla f(y)|\le \beta|x-y|\Rightarrow \nabla^2f(x)\preceq\beta\mathrm{I}$</td></tr><tr><td>$\alpha$-strong convex, limited the domain mostly.</td><td>$f(y)-f(x)\le\nabla f(x)^T (y-x)-\frac{\alpha}{2}|y-x|^2\Rightarrow \nabla^2f(x)\succeq\alpha\mathrm{I}$</td></tr></tbody></table><center><strong>Table: Optimizer in different conditions</strong></center></br> For each optimizer, from the top line downwarding, the rate of convergence is increasing. The optimal step size is gotten by minimizing the bound.</center><table><thead><tr><th>Optimizer</th><th>Condition</th><th>Converge rate</th><th>Optimal step size</th><th>Sub-optimal gap</th><th>Bounds of the gap</th></tr></thead><tbody><tr><td>GD after $T$ steps</td><td>L-Lipschitz convex</td><td>$\mathcal{O}(\frac{1}{\sqrt{T}})$</td><td>$\gamma=\frac{|x_1-x^*|_2}{L\sqrt{T}}$</td><td>$f(\frac{1}{T}\sum\limits_{k=1}^{T}x_k)-f(x^*)$</td><td>$\le\frac{|x_1-x^*|L}{\sqrt{T}}$, <br />the initial point matters</td></tr><tr><td>GD</td><td>$\beta$-smooth<br />+convex</td><td>$\mathcal{O}(\frac{1}{T})$</td><td>$\gamma=\frac{1}{\beta}$, <br />constant and independent of $T$</td><td>$f(x_k)-f(x^*)$<br />Notice the average on all samples can be erased</td><td>$\le\frac{2\beta|x_1-x^*|^2}{k-1}$, <br />$k$ means the $k$ step. <br />Bound depends on initial point</td></tr><tr><td>Projected subGD after $T$ steps</td><td>$\alpha$-strong<br /> + $L$-Lipschitz</td><td>$\mathcal{O}(\frac{1}{T})$</td><td>$\gamma_k=\frac{2}{\alpha (k+1)}$,<br /> diminish at every step</td><td>$f(\sum\limits_{k=1}^T\frac{2k}{T(T+1)}x_k)-f(x^*)$</td><td>$\le\frac{2L^2}{\alpha (T+1)}$</td></tr><tr><td>GD</td><td>$\lambda$-strong <br />+ $\beta$-smooth</td><td>$\mathcal{O}(\exp{(-T)})$</td><td>$\gamma=\frac{2}{\lambda+\beta}$</td><td>$f(x_{t+1})-f(x^*)$<br />Notice the average on all samples can be erased</td><td>$\le\frac{\beta}{2}\exp{(-\frac{4t}{\kappa+1})}|x_1-x^*|^2$,<br />$ \kappa=\frac{\beta}{\lambda}$. <br />$k$ is the same meaning of $t$,<br /> rather than the $\kappa$ in denominator.</td></tr><tr><td>Polyak (heavy ball)</td><td>Quadratic loss</td><td>$\mathcal{O} ((\frac{\sqrt\kappa-1}{\sqrt\kappa+1})^t)\\approx\exp(-\frac{C}{\sqrt\kappa})$,<br />$\kappa=\frac{h_{max}}{h_{min}}$</td><td>$\gamma^*=\frac{(1+\sqrt\mu)^2}{h_{max}}\=\frac{(1-\sqrt\mu)^2}{h_{min}}$</td><td>$\left|\left[ \begin{matrix}   x_{t+1}-x*\ x_t-x^*    \end{matrix}  \right]\right|_2$</td><td>$\le\mathcal{O} (\rho(A)^T)=\mathcal{O} (\sqrt{\mu}^T)$.<br /> $\mu$ is the curvature</td></tr><tr><td>Nesterov NAG</td><td>$\beta$-smooth</td><td>$\mathcal{O}(\frac{1}{T^2})$</td><td></td><td>$f(y_t)-f(x^*)$</td><td></td></tr><tr><td>Nesterov NAG</td><td>$\alpha$-strong <br />+$\beta$-smooth</td><td>$\mathcal{O}(\exp(-\frac{T}{\sqrt{\kappa}}))$</td><td></td><td>$f(y_t)-f(x^*)$</td><td>$\le\frac{\alpha+\beta}{2}|x_k -x^*|^2\exp(-\frac{t-1}{\sqrt\kappa})$</td></tr><tr><td>SGD</td><td>$L$-Lipschitz by<br /> $|\tilde{g}(x)|\le L$ with prob. 1</td><td>$\mathcal{O}(\frac{1}{\sqrt T})$<br />If wants a $\epsilon$-tolerance, $T\ge\frac{B^2L^2}{\epsilon^2}$</td><td>$\gamma=\frac{B}{L\sqrt{T}}$</td><td>$\mathbb{E}[f(\bar{x})]-f(x^<em>)$, where <br />$x^</em>\in\arg\min_{x:|x|&lt;B}f(x)$</td><td>$\le\frac{BL}{\sqrt T}$</td></tr><tr><td>SGD</td><td>$\alpha$-strong+ $\mathbb{E}|\tilde{g}(x)|_*^2\le B^2$ (kind of $B$-smooth)</td><td>$\mathcal{O}(\frac{1}{T})$</td><td>$\gamma=\frac{2}{\alpha(s+1)}$</td><td>$f(\sum\limits_{s=1}^t\frac{2s}{t(t+1)}x_s)-f(x^*)$</td><td>$\le\frac{2B^2}{\alpha(t+1)}$</td></tr></tbody></table><h3 id="Estimate-sequence"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&amp;rep=rep1&amp;type=pdf">Estimate sequence</a></h3><ul><li><p>Definition</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210129143844.png" alt="image-20210129143844647" style="zoom:67%;" /></li><li><p>Properties</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131115340.png" alt="image-20210131115338221" style="zoom:50%;" /><p>for any sequence ${\lambda_k}$, satisfying $(2.2.2)$ we can derive <strong>the rate of convergence of the minimization process directly from the rate of convergence of the sequence ${\lambda_k}$.</strong></p><p>Note: below in estimate sequence, all $L$ means $L$-smooth function</p></li><li><p>How to form an estimate sequence?</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131150937.png" alt="image-20210131150935371" style="zoom:40%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131151028.png" alt="image-20210131151028446" style="zoom:40%;" /></li><li><p>How to ensure $(2.2.2)$?</p><ul><li><p>Method 1: Do scheme (2.2.6), it will generate a sequence ${x_k}_{k=0}^\infin$ such that $f(x_k)-f^<em>\le \lambda_k[f(x_0)-f^</em>+\frac{\gamma_0}{2}|x_0-x^*|^2]$. It will make sequence satisfy $(2.2.2)$</p></li><li><p>Method 2: Using gradient step</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210201094020.png" alt="image-20210201094020408" style="zoom:50%;" /></li></ul></li><li><p>Some cases</p><ul><li><font color='orange'>If in the scheme (2.2.6) we choose $\gamma_0\ge\mu$, then  $\lambda_k\le\min{(1-\sqrt{\frac{\mu}{L}})^k,\frac{4L}{(2\sqrt{L}+k\sqrt{\gamma})^2}}$.</font></li><li><font color='blue'>If in the scheme (2.2.6) we choose $\gamma_0=L$, then this scheme generates a sequence ${x_k}<em>{k=0}^{\infin}$ such that $f(x_k)-f^<em>\le L\min {(1-\sqrt{\frac{\mu}{L}})^k,\frac{4}{(k+2)^2}}|x_0-x^</em>|^2$. This  means that it’s optimal for the class $\mathcal{S}</em>{\mu,L}^{1，1}(R^n)$ with $\mu\ge0$. </font></li><li><font color='blue'>If in the scheme $(2.2.8)$ we choose $\alpha_0\ge \sqrt{\frac{\mu}{L}}$, then this scheme generates a sequence ${x_k}_{k=0}^{\infin}$ such that $f(x_k)-f^<em>\le \min {(1-\sqrt{\frac{\mu}{L}})^k,\frac{4L}{(2\sqrt{L}+k\sqrt{\gamma_0})^2}}[f(x_0)-f^</em>+\frac{\gamma_0}{2}|x_0-x^*|^2]$, where $\gamma_0=\frac{\alpha_0(\alpha_0L-\mu)}{1-\alpha_0}$. Here $\alpha_0\ge \sqrt{\frac{\mu}{L}}$ is equivalent to $\gamma_0\ge\mu$. </font></li><li></li></ul></li></ul><h3 id="Convex-sets">Convex sets</h3><h4 id="Affine-sets">Affine sets</h4><ul><li>Affine sets:<ul><li>Definition : A set $C\subseteq R^n$ is affine if the line through any two distinct points in $C$ lies in $C$, aka for any $x_1 ,x_2\in C$ and $\theta\in R$, one has $\theta x_1+(1-\theta)x_2\in C$. It indicates that the $C$ contains the linear combination of any two points in $C$.</li><li>Induction: If $C$ is an affine set, $x_1，\cdots,x_k\in C$ and $\theta_1+\cdots+\theta_k=1$, then the point $\theta_1 x_1+\cdots+\theta_kx_k$ also belongs to $C$.</li></ul></li><li>Affine hull<ul><li>Definition: the set of all affine combinations of points in some set $C\subseteq R^n$, denoted as $\mathrm{aff}C$</li><li>It’s the smallest affine set that contains $C$</li></ul></li><li>Affine dimension<ul><li>Definition: as the dimension of its affine hull.</li><li>E.g.: ${x\in R^2|x_1 ^2+x_2^2=1}$, the affine dimension is 2.</li></ul></li></ul><h4 id="Convex-sets-2">Convex sets</h4><ul><li>Definition: If every point in the set can be seen by every other point, along an unobstructed straight path between them, where unobstructed means lying in the set.</li><li>Every affine set is also convex.</li><li>Convex hull, denotes as $\mathrm{conv} C$, is the set of all convex combinations of points in $C$. It is always convex, and it’s the smallest convex set that contains $C$</li><li>More generally, suppose $p: R^n \rightarrow R$ satisfies $p(x)\ge0$ for all $x\in C$ and $\int_Cp(x)dx=1$, where $C\subseteq R^n$ is convex, then $\int_Cp(x)x dx \in C$, if the integral exists.</li></ul><h3 id="Convex-functions">Convex functions</h3><h4 id="Convex-functions-2">Convex functions</h4><ul><li><p>Definition</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117165609.png" alt="image-20210117165609361" style="zoom:50%;" /></li><li><p>All affine function are both convex and concave.</p></li><li><p>$f$ is convex if and only if for all $x\in \mathrm{dom}f$ and all $v$, the function $g(t)=f(x+tv)$ is convex.</p></li></ul><h4 id="Extended-value-extensions">Extended-value extensions</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117170246.png" alt="image-20210117170246166" style="zoom:50%;" /><h4 id="First-order-conditions">First-order conditions</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117171410.png" alt="image-20210117171410378" style="zoom:50%;" /><p>The inequality (3.2) states that for a convex function, the first-order Taylor approximation is in fact a global <em>underestimator</em> of the function. Conversely, if the first-order Taylor approximation of a function is always a global <em>underestimator</em> of the function, then the function is convex.</p><ul><li>The inequality (3.2) shows that if $\nabla f(x) = 0$, then for all $y \in \mathrm{dom} f, f(y) ≥ f(x)$, i.e., $x$ is a global minimizer of the function $f$.</li></ul><h4 id="Second-order-conditions">Second-order conditions</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117173919.png" alt="image-20210117173919136" style="zoom:67%;" /><h4 id="More-constraints-on-convex-function">More constraints on convex function</h4><h5 id="beta-smooth">$\beta$-smooth</h5><ul><li>Definition: a continuously function $f$ is $\beta$-smooth if the gradient $\nabla f$ is $\beta$-Lipschitz, that is $|\nabla f(x)-\nabla f(y)|\le\beta|x-y|$.</li><li><em><strong>If $f$ is twice differentiable then this is equivalent to the eigenvalues of the Hessians being smaller than $\beta$ at any point.</strong></em> , $\nabla^2f(x)\preceq\beta \mathrm{I}_n,\forall x$.</li><li><font color='green'>smoothness removes dependency from the averaging scheme</font></li><li><font color='cyan'>If extend the $\beta$-smooth to multi power, it’s called <a href="https://en.wikipedia.org/wiki/H">Holder condition</a></font></li><li>The bigger your function changes in gradients, the upper you have to explore.</li></ul><h5 id="alpha-strong-convexity">$\alpha$-strong convexity</h5><p>Strong convexity can significantly speed-up the convergence of first order methods.</p><ul><li><p>Definition</p><p>We say that $f:\mathcal{X}\rightarrow\mathbb{R}$ is a $\alpha$<em>-strongly convex</em> if it satisfies the following improved subgradient inequality:</p><p>$f(x)-f(y)\le\nabla f(x)^T(x-y)-\frac{\alpha}{2}|x-y|^2$. A large value of $\alpha$ will lead to a faster rate.  A $\alpha$<strong>-strong convexity for twice differential function $f$ can also be interpreted as $\nabla^2f(x)\succeq\alpha \mathrm{I}_n,\forall x$</strong>.</p></li><li><p>Strong convexity plus $\beta$-smoothness will lead to the gradient descent with a constant step-size achieves a linear rate of convergence, precisely the oracle complexity will be $O(\frac{\beta}{\alpha}\log(1/\varepsilon)), \beta\ge\alpha$. In some sense strong convexity is a dual assumption to smoothness, and in fact this can be made precise within the framework of Fenchel duality.</p></li><li><p>$\alpha$ can often be reviewed as large as the sample size. Thus reducing the number of step from <strong>sample size</strong> to $\sqrt{\mathrm{sample \quad size}}$ (cause $\kappa=\frac{\beta}{\alpha}$ for $\beta$-smooth and $\alpha$-strong function, and in basic gradient descent algorithm, to reach $\epsilon$-accuracy, it requires $\mathcal{O}(\kappa\log(\frac{1}{\epsilon}))$, and for Nesterov’s Accelerated Gradient Descent attains the improved oracle complexity of $\mathcal{O}(\sqrt{\kappa}\log(\frac{1}{\epsilon}))$) can be a huge deal, especially in large scale applications.</p></li></ul><h4 id="Examples-of-Convex-functions">Examples of Convex functions</h4><ul><li><em>Norms</em></li><li><em>Max function</em></li><li><em>Quadratic-over-linear function</em> $\frac{x^2}{y}$</li><li><em>Log-sum-exp</em>: $\log(e^{x_1}+\cdots+e^{x_n})$, which is regarded as a differentiable approximation of the max function</li><li><em>Geometric mean</em>: $(\prod\limits_{i=1}^{n}x_i)^{1/n}$, concave</li><li><em>Log-determinant</em>: $\log\det X$, concave</li></ul><p>For proofs, check Chapter $3.1.5$ of <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>.</p><h4 id="Sublevel-sets">Sublevel sets</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181258.png" alt="image-20210117181258728" style="zoom:50%;" /><h4 id="Epigraph">Epigraph</h4><p>A function is convex if and only if its epigraph is a convex set.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181415.png" alt="image-20210117181415039" style="zoom:50%;" /><h4 id="Jensen’s-inequality-and-extensions">Jensen’s inequality and extensions</h4><p>Once a function is convex, then you can get</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117185801.png" alt="image-20210117185801678" style="zoom:50%;" /> the simplest version of it is $f(\frac{x+y}{2})\le\frac{f(x)+f(y)}{2}$.</p><h4 id="Holder’s-inequality">Holder’s inequality</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190336.png" alt="image-20210117190336823" style="zoom:50%;" /><h4 id="Operations-that-preserve-convexity">Operations that preserve convexity</h4><ul><li><p>Nonnegative weighted sums: $f=w_1f_1+\cdots+w_mf_m$</p></li><li><p>Composition with an affine mapping: $g(x)=f(Ax+b)$. If $f$ is convex, so is $g$; if $f$ is concave, so is $g$.</p></li><li><p>Pointwise maximum and suprenum: $f(x)=\max{f_1(x),f_2(x)}$ and $f(x)=\max{f_1(x),\cdots,f_m(x)}$</p></li><li><p>Composition: $f(x)=h(g(x))$</p><ul><li><p>Scalar composition</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190943.png" alt="image-20210117190943043" style="zoom:50%;" /></li><li><p>Vector composition $f(x)=h(g(x))=h(g_1(x),\cdots,g_k(x))$</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117191112.png" alt="image-20210117191112754" style="zoom:50%;" /></li><li><p>Minimization</p></li></ul></li></ul><h3 id="Typical-Numerical-optimization">Typical Numerical optimization</h3><h4 id="Gradient-descent">Gradient descent</h4><p>The basic principle behind <strong>gradient descent</strong> is to make a small step in the direction that minimizes the local first order Taylor approximation of $f$ (also known as the steepest descent direction). This kind of methods will <strong>obtain an oracle complexity <em>independent of the dimension</em>.</strong></p><p>$x_{t+1}=x_t-\eta\nabla f(x_t)$</p><p>Taking $f(w)=\frac{1}{2}w^TAw-b^Tw,w\in\mathbb{R}^n$ into consideration, suppose $A$ is symmetric and invertible, then $A=Q\Lambda Q^T,\Lambda=(\lambda_1,\cdots,\lambda_n),\lambda_1\le\lambda_2\le\cdots\le\lambda_{n-1}\le\lambda_n$.</p><ul><li><p>All errors are not made equal. Indeed, there are different kinds of errors, $n$ to be exact, one for each of the eigenvectors of $A$.</p><p>$f(w^k)-f(w^*)=\sum(1-\alpha\lambda_i)^{2k}\lambda_i[x_i^0]^2$</p></li><li><p><strong>Denote the condition number $\kappa=\frac{\lambda_n}{\lambda_1}$, then the bigger the $\kappa$ is, the slower gradient descent will be</strong>, cause the condition number is a direct description of pathological curvature.</p></li><li><p><strong>The optimal step-size causes the first and last eigenvectors to converge at the same rate.</strong></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124212843.png" alt="image-20210124212843050" style="zoom:50%;" /></li></ul><h4 id="Projected-gradient-descent">Projected gradient descent</h4><ul><li><p>Subgradient</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222307.png" alt="image-20210117222306873" style="zoom:40%;" /></li><li><p>Projected subgradient descent</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222725.png" alt="image-20210117222725673" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117223015.png" alt="image-20210117222937543" style="zoom:40%;" /></li></ul><h4 id="Gradient-descent-with-momentum-Polyak’s-Momentum"><a href="https://distill.pub/2017/momentum/">Gradient descent with momentum</a> : Polyak’s Momentum</h4><ul><li><p>Sometimes SGD fail with a reason of pathological curvature of objective. (like valley, trench)</p></li><li><p>Momentum modify gradient descent by adding a short-term memory</p><p>$y_{t+1}=\beta y_t+\nabla f(x_t)\x_{t+1}=x_{t}-\alpha y_{t+1}$.</p><p>When $\beta=0$, it’s gradient descent.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124224747.png" alt="image-20210124224747333" style="zoom:50%;" /></li><li><p>Momentum allows us to crank up the step-size up by a factor of 2 before diverging.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124225628.png" alt="image-20210124225628085" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124225657.png" alt="image-20210124225656929" style="zoom:50%;" /></li><li><p>Optimize over $\beta$: The critical value of $\beta = (1 - \sqrt{\alpha \lambda_i})^2$ gives us a convergence rate (in eigenspace $i$) of $1 - \sqrt{\alpha\lambda_i}$. A square root improvement over gradient descent, $1-\alpha\lambda_i$! Alas, this only applies to the error in the $i^{th}$ eigenspace, with $\alpha$ fixed.</p></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124231402.png" alt="image-20210124231402777" style="zoom:50%;" /></li><li><p><strong>Failing</strong>: there exist strongly-convex and smooth functions for which, by choosing carefully the hyperparameters $\alpha$ and $\beta$ and the initial condition $x_0$, the heavy-ball method fails to converge.</p></li></ul><h4 id="Nesterov’s-Accelerated-Gradient-Descent">Nesterov’s Accelerated Gradient Descent</h4><ul><li><p><a href="https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/">Some notes of it</a></p></li><li><p>Iterations: starting at an arbitrary initial point $x_1=y_1$</p><p>$$y_{s+1}=x_s-\frac{1}{\beta}\nabla f(x_s),\x_{s+1}=(1+\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})y_{s+1}-\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}y_s$$</p></li><li><p><font color='blue'>Let $f$ be $\beta$-smooth and $\alpha$-strongly convex, then Nesterov’s gradient descent satisfies for $t\ge0$, $f(y_t)-f(x^<em>)\le\frac{\alpha+\beta}{2}|x_1-x^</em>|^2\exp{(-\frac{t-1}{\sqrt{\kappa}})}$ .</font></p></li><li><p><strong>Converge in $\mathcal{O}(\frac{1}{T^2})$ for smooth case, and $\mathcal{O}(\exp(-\frac{T}{\sqrt{\kappa}}))$, it guaranteed convergence for quadratic functions (and not piece-wise quadractic)</strong></p></li></ul><h4 id="Stochastic-gradient-descent">Stochastic gradient descent</h4><ul><li><p>Cases: one is $\mathbb{E}<em>{\xi}\nabla_x\ell(x,\xi)\in\part f(x)$, where $\xi$ is sampled. This method cannot be reproduced; the other is directly minimize $f(x)=\frac{1}{m}\sum\limits</em>{i=1}^{m}f_i(x)$, here gradient is reported as $\nabla f_I(x)$, where $I\in[m]$, this method can be reproduced.</p></li><li><p>Non-smooth stochastic optimization</p><ul><li>Definition: there exists $B&gt;0$ such that $\mathbb{E}|\tilde{g}(x)|_*^2\le B^2$ for all $x\in\mathcal{X}$</li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202153046.png" alt="image-20210202153043826" style="zoom:67%;" /></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202153117.png" alt="image-20210202153116696" style="zoom:67%;" /></li></ul></li><li><p>Smooth stochastic optimization and mini-batch SGD</p><ul><li><p>Definition: there exists $\sigma&gt;0$ such that $\mathbb{E}|\tilde{g}(x)-\nabla f(x)|_*^2\le \sigma^2$ for all $x\in\mathcal{X}$.</p></li><li><p><font color='orange'>smoothness does not bring any acceleration for a general stochastic<br>oracle , while in exact orale case it does.</font></p></li><li><p><font color='blue'>Stochastic smooth optimization converge in $\frac{1}{\sqrt{t}}$. Deterministic smooth optimization converge in $\frac{1}{t}$</font></p></li><li><p><font color='blue'>Mini-batch SGD converges between $\frac{1}{\sqrt{t}}$ and  $\frac{1}{t}$.</font></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202154653.png" alt="image-20210202154653243" style="zoom:67%;" /></li><li></li></ul></li></ul><h4 id="Relations">Relations</h4><h5 id="Generalization">Generalization</h5><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124232634.png" alt="image-20210124232634072" style="zoom:50%;" /><h5 id="The-momentum-SGD-and-Adaptive-optimizers">The momentum SGD and Adaptive optimizers</h5><p><a href="https://arxiv.org/pdf/1706.03471.pdf">YellowFin and the Art of Momentum Tuning</a></p><ul><li>Summary:  hand-tuning a single learning rate and momentum makes it competitive with Adam.  The proposed YellowFin (an automatic fine tuner for momentum and learning rate in SGD), can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing.</li><li>Adaptive optimizers: like Adam, AdaGrad, RmsProp</li><li>For details of paper check <a href="">here</a></li></ul><h3 id="Dimension-free-convex-optimization">Dimension-free convex optimization</h3><h4 id="Projected-subgradient-descent-for-Lipschitz-functions">Projected subgradient descent for Lipschitz functions</h4><ul><li><p><strong>Theorem</strong></p><p>Assume that $\mathcal{X}$ is contained in an Euclidean ball centered at $x_1\in \mathcal{X}$ and of radius $R$. Assume that $f$ is such that for any $x\in \mathcal{X}$ and of any $g\in\part f(x)$ (assume $\part f(x)\ne \emptyset$) one has $|g|\le L$. (This implies that $f$ is L-Lipschitz on $\mathcal{X}$, that is $|f(x)-f(y)|\le L|x-y|$)</p><p><font color='blue'><strong>The projected subgradient descent method with $\eta=\frac{R}{L\sqrt{t}}$ satisfies</strong> $f(\frac{1}{t}\sum\limits_{s=1}^{t}x_s)-f(x^*)\le\frac{RL}{\sqrt{t}}$</font></p><ul><li><p>Proof</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117225400.png" alt="image-20210117225359857" style="zoom:40%;" /></li><li><p><font color='blue'><strong>The rate is unimprovable from a black-box perspective.</strong></font></p></li></ul></li></ul><h4 id="Gradient-descent-for-smooth-functions">Gradient descent for smooth functions</h4><h5 id="Theorems-under-unconstrained-cases">Theorems under unconstrained cases</h5><p>In this section all $f$ is a convex and $\beta$-smooth function on $\mathbb{R}^n$.</p><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let $f$ be convex and $\beta$-smooth function on $\mathbb{R}^n$. Then gradient descent with $\eta=\frac{1}{\beta}$ satisfies $f(x_t)-f(x^<em>)\le\frac{2\beta|x_1-x^</em>|^2}{t-1}$ .</font></p><p>For the proof check $3.2$ in  <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a>.</p></li><li><p>Gradient descent attains a much faster rate in $\beta$-smooth situation than in the non-smooth case of the previous section.</p></li><li><p>The Definition of smooth convex functions</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117233245.png" alt="image-20210117233244773" style="zoom:50%;" /></li></ul><h5 id="The-constrained-cases">The constrained cases</h5><p>This time <em>consider the projected gradient descent algorithm $x_{t+1}=\prod_{\mathcal{X}}(x_t-\eta\nabla f(x_t))$</em></p><ul><li><strong>Lemma</strong></li></ul><p><font color='orange'>Let $x,y\in \mathcal{X},x^+=\prod_{\mathcal{X}}(x-\frac{1}{\beta}\nabla f(x))$ and $g_{\mathcal{X}}(x)=\beta (x-x^+)$ , then the following holds true: $f(x^+)-f(y)\le g_{\mathcal{X}}(x)^T(x-y)-\frac{1}{2\beta}|g_{\mathcal{X}}(x)|^2$.</font></p><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let $f$ be convex and $\beta$-smooth function on $\mathcal{X}$. Then projected gradient descent with $\eta=\frac{1}{\beta}$ satisfies $f(x_t)-f(x^<em>)\le\frac{3\beta|x_1-x^</em>|^2+f(x_1)-f(x^*)}{t}$ .</font></p><h4 id="Strong-convexity">Strong convexity</h4><h5 id="Strongly-convex-and-Lipschitz-functions"><strong>Strongly convex and Lipschitz functions</strong></h5><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210118152328.png" alt="image-20210118152328268" style="zoom:50%;" /><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let $f$ be $L$-Lipschitz and $\alpha$-strongly convex  on $\mathcal{X}$. Then projected gradient descent with $\eta_s=\frac{2}{\alpha(s+1)}$ satisfies $f(\sum\limits_{s=1}^{t}\frac{2s}{t(t+1)}x_s)-f(x^*)\le\frac{2L^2}{\alpha(t+1)}$ .</font></p><p><em>The combination of $\alpha$-strongly convex and $L$-Lipschitz means that function has to be constrained in  a bounded domain.</em></p><h5 id="Strongly-convex-and-smooth-functions"><strong>Strongly convex and smooth functions</strong></h5><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let $f$ be $\beta$-smooth and $\alpha$-strongly convex  on $\mathcal{X}$, then projected gradient descent with $\eta=\frac{1}{\beta}$ satisfies for $t\ge0$, $|x_{t+1}-x^<em>|^2\le\exp(-\frac{t}{\kappa})|x_a -x^</em>|^2$ .</font></p><p>The intuition of changing $\alpha$ and $\beta$: If increasing $\beta$, the upper bound will be decreased, and if increasing $\alpha$, the lower bound will be increased.  <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121112619.png" alt="image-20210121112605020" style="zoom:33%;" /></p></li><li><p><strong>Lemma</strong></p><p><font color='orange'>Let $f$ be $\beta$-smooth and $\alpha$-strongly convex  on $\mathbb{R}^n$, then  for all $x,y\in \mathbb{R}^n$, one has $(\nabla f(x)-\nabla f(y))^T(x-y)\ge\frac{\alpha\beta}{\alpha+\beta}|x-y|^2+\frac{1}{\beta+\alpha}|\nabla f(x)-\nabla f(y)|^2$ .</font></p></li><li><p><strong>Theorem</strong></p><p><font color='blue'> Let $f$ be $\beta$-smooth and $\alpha$-strongly convex  on $\mathbb{R}^n$, $\kappa=\frac{\beta}{\alpha}$ as the condition number. Then gradient descent with $\eta=\frac{2}{\beta+\alpha}$ satisfies $f(x_{t+1})-f(x^<em>)\le\frac{\beta}{2}\exp(-\frac{4t}{\kappa+1}|x_1-x^</em>|^2)$   </font></p></li></ul><h4 id="Lower-bound-–-black-box">Lower bound – black box</h4><ul><li><p>A black-box procedure is a mapping from “history” to the next query point, that is it maps ($x_a ,g_1,\cdots,x_t,g_t$) (with $g_s\in\part f(x_s)$) to $x_{t+1}$. To simplify, make the following assumption on the black-box procedure: $x_1=0$  and for any $t\ge0，x_{t+1}$ is in the linear span of $g_1,\cdots,g_t$, that is $$x_{t+1}\in\mathrm{Span}(g_t ,\cdots,g_t)\tag{3.15}\label{eq315}$$</p></li><li><p><strong>Theorem</strong></p><ul><li><font color='blue'> Let $t\le n,L,R&gt;0$. There exists a convex and $L$-Lipschitz function $f$ such that for any black-box procedure satisfying  $\eqref{eq315}$, $\min\limits_{1\le s\le t}f(x_s)-\min\limits_{x\in B_2®}f(x_s)\ge\frac{RL}{2(1+\sqrt{t})}$, where $B_2®={x\in\mathbb{R}^n:|x|\le R}$ . There also exists an $\alpha$-strongly convex and  $L$-Lipschitz function $f$ such that for any black-box procedure satisfying  $\eqref{eq315}$, $\min\limits_{1\le s\le t}f(x_s)-\min\limits_{x\in B_2(\frac{L}{2\alpha})}f(x_s)\ge\frac{L^2}{8\alpha t}$</font></li><li><font color='blue'> Let $t\le (n-1)/2,\beta&gt;0$. There exists a $\beta$-smooth convex function $f$ such that for any black-box procedure satisfying  $\eqref{eq315}$, $\min\limits_{1\le s\le t}f(x_s)-f(x^<em>)\ge\frac{3\beta}{32}\frac{|x_1-x^</em>|^2}{(t+1)^2}$.</font></li><li><font color='blue'> Let $\kappa\ge 1$. There exists a $\beta$-smooth and $\alpha$-strongly convex function $f:\ell_2\rightarrow \mathbb{R}$ with $\kappa=\frac{\beta}{\alpha}$ such that for any $t\ge1$ and black-box procedure satisfying  $\eqref{eq315}$ one has $f(x_t)-f(x^<em>)\ge\frac{\alpha}{2}(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})^{2(t-1)}|x_1-x^</em>|^2$. Note that for large values of the condition number $\kappa$ one has $(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})^{2(t-1)}\approx\exp(-\frac{4(t-1)}{\sqrt{\kappa}})$</font></li></ul></li></ul><h2 id="References">References</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/103359560">【凸优化笔记5】-次梯度方法（Subgradient method）</a></li><li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></li><li><a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></li><li>‘Understanding Analysis’ by Stephen Abbott. It’s a nice and light intro to analysis</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;泛函&quot;&gt;泛函&lt;/h2&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/img/Rules/lines.png&quot; width=500&quot;&gt;
&lt;/center&gt;
&lt;h3 id=&quot;不动点定理&quot;&gt;不动点定理&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33885648&quot;&gt;链接&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;紧性的利用&quot;&gt;紧性的利用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;证明存在性&lt;/li&gt;
&lt;li&gt;在无限维空间中“模仿”有限维的欧式空间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;紧集是为了模仿描述欧式空间中的有界闭集合么？&lt;br&gt;
紧=相对紧+闭&lt;/p&gt;
&lt;h3 id=&quot;流形-Manifolds&quot;&gt;流形(Manifolds)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;支撑集-Support&quot;&gt;支撑集(Support)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>About</title>
    <link href="http://yoursite.com/posts/uncategorized/2019-06-13-about.html"/>
    <id>http://yoursite.com/posts/uncategorized/2019-06-13-about.html</id>
    <published>2019-06-13T17:02:18.000Z</published>
    <updated>2020-01-23T22:44:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to Mia Feng’s Blog!</p><p>I’m doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in Montreal, Canada. I’m interested in data analysis, reinforce learning, transfer learning and democratising machine learning and AI.</p><p>I got the bachelor degree from Wuhan University, China, and then I did my master at the National University of Defense Technology, China. During my studies, I’ve processed spatial-temporal data, and financial data. I also worked as an intership of fintech in Meituan-Dianping. The main areas I have learned including machine learning, geographical information system, software engineering, data assimilation, and finance. Currently I am learning something about computational neuroscience. I want to learn more about transfer learning and visualization and explanation of neural networks.</p><p>Have a look at my <a href="https://github.com/skaudrey/cv/blob/master/cv.pdf">resume</a> for more information.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to Mia Feng’s Blog!&lt;/p&gt;
&lt;p&gt;I’m doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in Mo
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>rl-intro</title>
    <link href="http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html</id>
    <published>2019-05-26T17:02:18.000Z</published>
    <updated>2021-01-12T20:56:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver.</p><a id="more"></a><h2 id="Why-RL">Why RL?</h2><h3 id="The-difference-between-supervised-learning-and-RL">The difference between supervised learning and RL</h3><ul><li><p>task directed from fixed data sets vs goal directed learning from interaction</p></li><li><p>characteristics:</p><ul><li>RL<ul><li>trial-and-error search</li><li>delayed reward</li></ul></li><li>ML<ul><li>generalization methods: regularization, data augmentation</li><li>real-time loss</li></ul></li></ul></li><li><p>methods:</p><ul><li>RL<ul><li>exploit: get reward</li><li>exploration: make better action selection in the future.</li></ul></li><li>ML<ul><li>discriminative and generative: parameterized and semi-parameterized</li><li>supervised and unsupervised: depends on whether have labeled data</li></ul></li></ul></li></ul><center>    <img src="/assets/img/RLIntro/RLML.png" width=400"></center><h3 id="The-capacity-of-RL">The capacity of RL</h3><ul><li>sequential decision maker facing unknown or known environment</li><li>works for non i.i.d. data</li></ul><center>    <img src="/assets/img/RLIntro/RLCapacity.png" width=300"></center><h2 id="What’s-RL">What’s RL?</h2><h3 id="The-agent-environment-interaction">The agent-environment interaction</h3><center>    <img src="/assets/img/RLIntro/RLEA.png" width=300"></center><ul><li>At each step t the agent:<ul><li>Executes action at</li><li>Transform to state St</li><li>Receives scalar reward rt</li></ul></li><li>The environment:<ul><li>Receives action at</li><li>Transform to state St</li><li>Emits scalar reward rt+1</li></ul></li><li>t increments at env. step</li></ul><h3 id="The-elements-of-RL">The elements of RL</h3><ul><li><p>Policy: agent’s behaviour function, mostly is a PDF mapping state to action</p><ul><li><p>Deterministic policy</p><p>$$<br>a = \pi\left(S\right)<br>$$</p></li><li><p>Stochastic policy</p><p>$$<br>\pi\left(a|S\right)=\mathbb{P}\left(A_t=a|S_t=s\right)<br>$$</p></li></ul></li><li><p>Value function: how good is each state and/or action, the scalar value is also named reward.</p><p>$$<br>v_{\pi}\left(s\right)=\mathbb{E}\left(R_{t+1}+\gamma R_{t+1}+{\gamma}^2 R_{t+2}+\cdots|S_t=s\right)<br>$$</p><p>Mostly, to make algorithm converge, a final state will be rewarded 0, and other non-final states are rewarded as a minus value.</p></li><li><p>Model: agent’s representation of the environment, they can be modeled by TKinter, gym etc.</p><ul><li><p>e.g.: models in assimilation, maze</p><p>$$<br>\mathcal{P}<em>{ss’}^{a}=\mathbb{P}\left(S</em>{t+1}=s’|S_t=s,A_t=a\right)\<br>\mathcal{R}<em>{s}^{a}=\mathbb{E}\left(R</em>{t+1}=s’|S_t=s,A_t=a\right)<br>$$</p></li><li><p>unknown environment can be stimulated by sampling</p></li></ul></li></ul><h3 id="Classification-of-RL">Classification of RL</h3><h4 id="What-you-want">What you want</h4><pre><code>* Value Based: No Policy (Implicit), Value Function* Policy Based: Policy, No Value Function* Actor Critic: Policy, Value Function</code></pre><h4 id="What-you-knew">What you knew</h4><pre><code>* Model-free: Policy and/or Value Function, No Model* Model-based: Policy and/or Value Function, Model</code></pre><h2 id="How-to-RL">How to RL?</h2><h3 id="Markov-process-–-to-simplify">Markov process – to simplify</h3><h4 id="Markov-Process">Markov Process</h4><p>$$<br>\mathbb{P}\left(S_{t+1}\right)=\mathbb{P}\left(S_{t+1}|S_1,\cdots,S_t\right)\<br>\mathcal{P}<em>{ss’}=\mathbb{P}\left(S</em>{t+1}=s’|S_t=s\right)<br>$$</p><h4 id="Markov-Rewarded-Process">Markov Rewarded Process</h4><p>$$<br>\langle S,\mathcal{P},\mathcal{R},\gamma\rangle<br>$$<br>solve the reward from state at time t to the final state, which can be also solved by adding immediate reward and discounted value of successor state.</p><p>$$<br>v\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)=\mathbb{E}\left(R_{t+1}+\gamma v\left(S_{t+1}|S_t=s\right)\right)<br>$$</p><h4 id="Markov-Decision-Process">Markov Decision Process</h4><p>$$<br>\langle S,\mathcal{A},\mathcal{P},\mathcal{R},\gamma\rangle<br>$$</p><p>Sequential decision making.</p><ul><li><p>state-value function<br>$$<br>v_{\pi}\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)<br>$$</p></li><li><p>action-value function<br>$$<br>q_{\pi}\left(s,a\right)=\mathbb{E}_{\pi}\left(G_t|S_t=s,A_t=a\right)<br>$$</p></li></ul><h3 id="Value？Policy？">Value？Policy？</h3><h4 id="Policy-Iteration">Policy Iteration</h4><center>    <img src="/assets/img/RLIntro/policyitr.png" width=300"></center><h4 id="Value-Iteration">Value Iteration</h4><center>    <img src="/assets/img/RLIntro/valueitr.png" width=300"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>gnn</title>
    <link href="http://yoursite.com/posts/notes/2019-05-24-gnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-24-gnn.html</id>
    <published>2019-05-24T22:17:30.000Z</published>
    <updated>2019-09-23T09:27:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list the generation and improvement of graph neural networks.</p><h1>Why Use</h1><ul><li>Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,</li></ul><h1>Basic lines</h1><p>Contents in this block mainly comes from paper <a href="https://arxiv.org/pdf/1812.08434.pdf">Graph Neural Networks: A Review of Methods and Applications</a> and</p><a id="more"></a><h2 id="History">History</h2><h3 id="The-proposal-of-GNNs">The proposal of GNNs</h3><p>learn a target node’s representation by propagating neighbor information via recurrent neural architectures in an iterative<br>manner until a stable fixed point is reached<br>Computation expensive</p><ul><li><p><a href="https://www.researchgate.net/publication/4202380_A_new_model_for_earning_in_raph_domains">A new model for learning in graph domains</a></p><ul><li>Big Question: processing the graph without losing topological information<ul><li>reason<br>Traditional preprocessing methods for graphs dropped topological information, and thus leads to poor performance and generalization.</li></ul></li><li>background<br>RNN can only handle graph-level problems; Traditional methods dropped topological information.</li></ul></li><li><p>the approximation capability of GNN, <a href="https://www.researchgate.net/publication/23763868_Computational_Capabilities_of_Graph_Neural_Networks">Computational Capabilities of Graph Neural Networks</a><br>under mild generic conditions, most of the practically useful functions on graphs can be approximated in probability by GNNs up to any prescribed degree of accuracy.</p></li><li><p><a href="https://ieeexplore.ieee.org/document/4773279">Neural network for graphs: A contextual constructive approach</a></p></li><li><p><a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a></p></li></ul><h3 id="Go-to-GNNs">Go to GNNs</h3><h4 id="Spectral-based-Graph">Spectral-based Graph</h4><p>difficult to parallel or scale to large graphs,cause they need to load the whole graph into the memory.<br>relies on eigen-decomposition of the Laplacian matrix.</p><ul><li>Spectral GNN, <a href="https://arxiv.org/pdf/1312.6203v3.pdf">Spectral networks and locally connected networks on graphs</a></li><li>ChebNet, <a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a>，codes [here](<a href="https://github.com/mdeff/cnn">https://github.com/mdeff/cnn</a> graph).</li><li>1st ChebNet, <a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a>, code <a href="https://github.com/tkipf/gcn">here</a>.<br>localized in space, but the computation requirement grow exponentially, to reduce it, sampling methods are proposed. See <a href="https://arxiv.org/pdf/1801.10247.pdf">FASTGCN</a>, <a href="https://arxiv.org/pdf/1710.10568.pdf">reduce variance</a> and <a href="https://arxiv.org/pdf/1809.05343v1.pdf">adaptive sampling</a> for details.</li><li>AGCN, calculate a pairwise distance of nodes to construct a residual graph, see <a href="https://arxiv.org/pdf/1801.03226.pdf">Adaptive Graph Convolutional Neural Networks</a> for details.</li></ul><h4 id="Spatial-based-Graph-Convolution">Spatial-based Graph Convolution</h4><p>has gained more attention</p><ul><li><a href="https://arxiv.org/pdf/1706.02216.pdf">Inductive representation learning on large graphs</a></li><li><a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model cnns</a></li><li><a href="https://arxiv.org/pdf/1605.05273.pdf">Learning convolutional neural networks for graphs</a></li><li><a href="https://arxiv.org/abs/1808.03965">Large-scale learnable graph convolutional networks</a><br>[1],[4] used sampling strategy<br>the common way is to stack multiple graph convolution layer together.</li></ul><h5 id="Recurrent-based-Spatial-GCNs">Recurrent-based Spatial GCNs</h5><p>update a node’s representation recursively until a stable fixed point is reached</p><ul><li>GNNs, <a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a></li><li>GGNNs, used GRU, <a href="https://www.aclweb.org/anthology/D14-1179">Learning phrase representations using rnn encoder-decoder for statistical machine translation</a>, codes <a href="https://github.com/yujiali/ggnn">here</a>.</li><li>Stochastic Steady-state Embedding (SSE), updates the node latent representations stochastically in an asynchronous fashion, <a href="http://proceedings.mlr.press/v80/dai18a/dai18a.pdf">Learning steady-states of iterative algorithms over graphs</a>, codes [here](<a href="https://github.com/Hanjun-Dai/steady">https://github.com/Hanjun-Dai/steady</a> state embedding).</li></ul><h5 id="Composition-based-Spatial-GCNs">Composition-based Spatial GCNs</h5><ul><li>Message Passing Neural Networks (MPNNs), <a href="https://arxiv.org/pdf/1704.01212.pdf">Neural Message Passing for Quantum Chemistry</a></li><li>GraphSage, <a href="https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf">Inductive representation learning on large graphs</a>, codes <a href="https://github.com/williamleif/GraphSAGE">here</a>.</li></ul><h6 id="Miscellaneous-Variants-of-Spatial-GCNs">Miscellaneous Variants of Spatial GCNs</h6><ul><li>Diffusion Convolution Neural Networks (DCNN), the hidden node representation is get by independently convolving inputs with power series or transition probability matrix, <a href="https://arxiv.org/pdf/1511.02136.pdf">Diffusion-convolutional neural networks</a></li><li>Build GCN into a standard grid to do CNN, <a href="http://proceedings.mlr.press/v48/niepert16.pdf">Learning convolutional neural networks for graphs</a>, but it ignored the node information.</li><li>Large-scale Graph Convolution Networks (LGCN), <a href="https://arxiv.org/pdf/1808.03965.pdf">Large-scale learnable graph convolutional networks</a>, still using standard grid, but it also collects nodes’ information and draw subgraph for mini-batch training. Codes <a href="https://github.com/divelab/lgcn/">here</a>.</li><li>Mixture Model Network (MoNet), <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model CNNs</a>, introduce pseudo-coordinates and weight functions to let the weight of a node’s neighbor be determined by the relative position (pseudo-coordinates) between the node and its neighbor.</li><li><a href="https://arxiv.org/abs/1802.00910">Geniepath: Graph neural networks with adaptive receptive paths</a>, everages gating mechanisms to control the depth and breadth of a node’s neighborhood.</li><li><a href="https://persagen.com/files/misc/zhuang2018dual.pdf">Dual graph convolutional networks for graph-based semi-supervised classification</a>, one for global representation the other for local representation.</li><li><a href="https://arxiv.org/pdf/1811.10435.pdf">On filter size in graph convolutional networks</a>, introduce a hyperparameter to influence the receptive field size of a node.</li></ul><h4 id="Pooling-module">Pooling module</h4><ul><li><a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a></li><li>pooling by rearranging vertices into meaningful order, <a href="https://arxiv.org/pdf/1506.05163.pdf">Deep convolutional networks on graph-structured data</a></li><li><a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf">An end-to-end deep learning architecture for graph classification</a></li><li>DIFFPOOL pools nodes hierarchically by learning a cluster assignment matrix in each layer to get a cluster embedding, which can be combined with any standard GCN module, <a href="https://arxiv.org/pdf/1806.08804.pdf">Hierarchical graph representation learning with differentiable pooling</a></li></ul><h3 id="Graph-attention-networks">Graph attention networks</h3><p>For sequence-based tasks, in total, assigning attention weights to different neighbors when aggregating feature information, ensembling multiple models according to attention weights, and using attention weights to guide random walks.</p><ul><li><a href="https://arxiv.org/pdf/1710.10903.pdf">Graph attention networks</a>, (GAT), multi-head weights. Codes <a href="https://github.com/PetarV-/GAT">here</a>.</li><li><a href="https://arxiv.org/pdf/1803.07294.pdf">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</a>， also use multi-head, but it use a self attention mechanism to compute a different head for each head.</li><li><a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">Graph classification using structural attention</a>, Graph Attention Model (GAM), adaptively visiting a sequence of important nodes.</li><li><a href="https://arxiv.org/pdf/1710.09599.pdf">Watch your step: Learning node embeddings via graph attention</a>, factorize the co-occurrence matrix with differentiable attention weights.</li></ul><h3 id="Graph-Autoencoders">Graph Autoencoders</h3><p>had to handle the problem caused by the sparsity of adjacency matrix.</p><ul><li><a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf">Deep neural networks for learning graph representations</a>, uses the stacked denoising auto-encoders to reconstruct PPMI matrix. Codes <a href="https://github.com/ShelsonCao/DNGR">here</a></li><li><a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">Structural deep network embedding</a>, preserve nodes first-order proximity (drive representations of adjacent nodes close to each other) and second-order proximity (a node’s neighbourhood information) jointly. Codes <a href="https://github.com/suanrong/SDNE">here</a></li><li><a href="https://arxiv.org/pdf/1611.07308.pdf">Variational graph auto-encoders</a>, Graph Auto-Encoder (GAE), combined with GCN firstly. Codes <a href="https://github.com/limaosen0/Variational-Graph-Auto-Encoders">here</a></li><li><a href="https://shiruipan.github.io/pdf/CIKM-17-Wang.pdf">Mgae: Marginalized graph autoencoder for graph clustering</a>, reconstruct node’s hidden state.</li><li><a href="https://www.ijcai.org/proceedings/2018/0362.pdf">Adversarially regularized graph autoencoder for graph embedding</a>, using GANs to regularize the graph auto-encoders, recover adjacency matrix. Codes <a href="https://github.com/Ruiqi-Hu/ARGA">here</a></li><li><a href="https://www.kdd.org/kdd2018/accepted-papers/view/learning-deep-network-representations-with-adversarially-regularized-autoen">Learning deep network representations with adversarially regularized autoencoders</a>, recover node sequences rather than adjacency matrix.</li><li><a href="http://pengcui.thumedialab.com/papers/NE-RegularEquivalence.pdf">Deep recursive network embedding with regular equivalence</a>, codes <a href="https://github.com/tadpole/DRNE">here</a></li></ul><h3 id="Graph-Generative-Networks">Graph Generative Networks</h3><p>Not scalable to large graphs.</p><ul><li><a href="https://arxiv.org/pdf/1802.08773.pdf">Graphrnn: A deep generative model for graphs</a>, graph-level RNN + node-level RNN, use breadth-first-search (BFS) to sequence the nodes and Bernoulli assumption for edge generation. Codes <a href="https://github.com/snap-stanford/GraphRNN">here</a>.</li><li><a href="https://arxiv.org/pdf/1803.03324.pdf">Learning deep generative models of graphs</a>, utilize spatial-based GCNs to obtain a hidden representation of an existing graph.</li><li><a href="https://arxiv.org/pdf/1805.11973.pdf">Molgan: An implicit generative model for small molecular graphs</a>, RL+GAN+GCN</li><li><a href="https://arxiv.org/pdf/1803.00816.pdf">Net-gan: Generating graphs via random walks</a>, combines LSTM with Wasserstein GAN to generate graphs from a random-walk-based approach. As for random walk, see <a href="http://leogrady.net/wp-content/uploads/2017/01/grady2004multilabel.pdf">here</a>.</li><li><a href="https://arxiv.org/pdf/1809.02630.pdf">Constrained generation of semantically valid graphs via regularizing variational autoencoders</a></li></ul><h3 id="Graph-Spatial-Temporal-Networks">Graph Spatial-Temporal Networks</h3><ul><li><a href="https://arxiv.org/pdf/1612.07659.pdf">Structured sequence modeling with graph convolutional recurrent networks</a></li><li><a href="https://arxiv.org/pdf/1707.01926.pdf">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</a>, can work forwardly or backwardly. Codes <a href="https://github.com/liyaguang/DCRNN">here</a>.</li><li><a href="https://arxiv.org/pdf/1709.04875.pdf">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</a>, codes <a href="https://github.com/VeritasYin/STGCN_IJCAI-18">here</a>.</li><li><a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial temporal graph convolutional networks for skeleton-based action recognition</a>, extend the temporal flow as graph edges, and then assign each a label to each edge. Codes <a href="https://github.com/yysijie/st-gcn">here</a>.</li><li><a href="https://arxiv.org/pdf/1511.05298.pdf">Structural-rnn: Deep learning on spatio-temporal graphs</a>, aims at predicting nodes’ labels at each time, has nodeRNN and edgeRNN, and split nodes and edges into semantic groups. Codes <a href="https://github.com/asheshjain399/RNNexp">here</a>.</li></ul><h2 id="Main-Methodologies-–-Graph-Embedding">Main Methodologies – Graph Embedding</h2><h3 id="Matrix-Factorization">Matrix Factorization</h3><ul><li><a href="https://www.ijcai.org/proceedings/2018/0493.pdf">Discrete network embedding</a></li><li><a href="https://shiruipan.github.io/pdf/ICDM-18-Yang.pdf">Binarized attributed network embedding</a></li></ul><h3 id="Random-Walks">Random Walks</h3><ul><li><a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf">Deepwalk: Online learning of social representations</a></li></ul><h2 id="Problems">Problems</h2><ul><li>Does going deeper always work in GNNs?</li><li>How to select representative receptive field for a node?</li><li>How to work on large graphs?</li><li>How to handle dynamic and heterogeneous graph structures?</li></ul><h1>Papers</h1><h2 id="Introduction">Introduction</h2><ul><li>M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, &quot;Geometric deep learning: going beyond euclidean data,&quot;IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 18–42, 2017</li></ul><h2 id="others">others</h2><h3 id="Are-Graph-Neural-Networks-Miscalibrated"><a href="https://arxiv.org/pdf/1905.02296v1.pdf">Are Graph Neural Networks Miscalibrated?</a></h3><h3 id="Estimating-Node-Importance-in-Knowledge-Graphs-Using-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1905.08865.pdf">Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks</a></h3><p>GENI, a GNN-based method designed to deal with distinctive challenges involved with predicting node importance in KGs.</p><h3 id="Understanding-attention-in-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.02850.pdf">Understanding attention in graph neural networks</a></h3><p>We aim to better understand attention over nodes in graph neural networks and identify factors influencing its effectiveness. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 40% in some of our classification tasks</p><h3 id="Are-Graph-Neural-Networks-Miscalibrated-2"><a href="https://arxiv.org/pdf/1905.02296.pdf">Are Graph Neural Networks Miscalibrated?</a></h3><p>Graph Neural Networks (GNNs) have proven to be successful in many classification tasks, outperforming previous state-of-the-art methods in terms of accuracy</p><h3 id="Graph-Convolutional-Networks-with-EigenPooling"><a href="https://arxiv.org/pdf/1904.13107.pdf">Graph Convolutional Networks with EigenPooling</a></h3><p>To apply graph neural networks for the graph classification task, approaches to generate the \textit{graph representation} from node representations are demanded. Experimental results of the graph classification task on $6$ commonly used benchmarks demonstrate the effectiveness of the proposed framework.</p><h3 id="PAN-Path-Integral-Based-Convolution-for-Deep-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1904.10996.pdf">PAN: Path Integral Based Convolution for Deep Graph Neural Networks</a></h3><p>Experimental results show that the path integral based graph neural networks have great learnability and fast convergence rate, and achieve state-of-the-art performance on benchmark tasks.</p><h3 id="Attacking-Graph-based-Classification-via-Manipulating-the-Graph-Structure"><a href="https://arxiv.org/pdf/1903.00553.pdf">Attacking Graph-based Classification via Manipulating the Graph Structure</a></h3><p>We evaluate our attacks and compare them with a recent attack designed for graph neural networks. Results show that our attacks 1) can effectively evade graph-based classification methods; 2) do not require access to the true parameters, true training dataset, and/or complete graph; and 3) outperform the existing attack for evading collective classification methods and some graph neural network methods</p><h3 id="Deep-learning-in-bioinformatics-introduction-application-and-perspective-in-big-data-era"><a href="https://arxiv.org/abs/1903.00342">Deep learning in bioinformatics: introduction, application, and perspective in big data era</a></h3><p>After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures</p><h3 id="Constant-Time-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1901.07868.pdf">Constant Time Graph Neural Networks</a></h3><p>Recent advancements in graph neural networks (GNN) have led to state-of-the-art performance in various applications including chemo-informatics, question answering systems, and recommendation systems, to name a few</p><h3 id="A-Comprehensive-Survey-on-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1901.00596.pdf">A Comprehensive Survey on Graph Neural Networks</a></h3><p>We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories</p><h3 id="Graph-Transformation-Policy-Network-for-Chemical-Reaction-Prediction"><a href="https://openreview.net/pdf?id=r1f78iAcFm">Graph Transformation Policy Network for Chemical Reaction Prediction</a></h3><p>To this end, we propose Graph Transformation Policy Network (GTPN) – a novel generic method that combines the strengths of graph neural networks and reinforcement learning to learn the reactions directly from data with minimal chemical knowledge. Evaluation results show that GTPN improves the top-1 accuracy over the current state-of-the-art method by about 3% on the large USPTO dataset</p><h3 id="Contextualized-Non-local-Neural-Networks-for-Sequence-Learning"><a href="https://arxiv.org/pdf/1811.08600.pdf">Contextualized Non-local Neural Networks for Sequence Learning</a></h3><p>Recently, a large number of neural mechanisms and models have been proposed for sequence learning, of which self-attention, as exemplified by the Transformer model, and graph neural networks (GNNs) have attracted much attention. Specifically, we propose contextualized non-local neural networks (CN$^{\textbf{3}}$), which can both dynamically construct a task-specific structure of a sentence and leverage rich local dependencies within a particular neighborhood.</p><h3 id="Automated-Theorem-Proving-in-Intuitionistic-Propositional-Logic-by-Deep-Reinforcement-Learning"><a href="https://arxiv.org/pdf/1811.00796.pdf">Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning</a></h3><p>Using the large volume of augmented data, we train highly accurate graph neural networks that approximate the value function for the set of the syntactic structures of formulas. Within the specified time limit, our prover solved 84% of the theorems in a benchmark library, while $\texttt{tauto}$ was able to solve only 52%.</p><h3 id="Pileup-mitigation-at-the-Large-Hadron-Collider-with-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1810.07988.pdf">Pileup mitigation at the Large Hadron Collider with Graph Neural Networks</a></h3><p>We present a classifier based on Graph Neural Networks, trained to retain particles coming from high-transverse-momentum collisions, while rejecting those coming from pileup collisions. This model is designed as a refinement of the PUPPI algorithm, employed in many LHC data analyses since 2015</p><h3 id="Weisfeiler-and-Leman-Go-Neural-Higher-order-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1810.02244.pdf">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</a></h3><p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. The following work investigates GNNs from a theoretical point of view and relates them to the $1$-dimensional Weisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have the same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic (sub-)graphs</p><h3 id="Multitask-Learning-on-Graph-Neural-Networks-Learning-Multiple-Graph-Centrality-Measures-with-a-Unified-Network"><a href="https://arxiv.org/pdf/1809.07695.pdf">Multitask Learning on Graph Neural Networks - Learning Multiple Graph Centrality Measures with a Unified Network</a></h3><p>Graph neural networks (GNN), consisting of trained neural modules which can be arranged in different topologies at run time, are sound alternatives to tackle relational problems which lend themselves to graph representations. The proposed model achieves $89%$ accuracy on a test dataset of random instances with up to 128 vertices and is shown to generalise to larger problem sizes</p><h3 id="Meta-GNN-On-Few-shot-Node-Classification-in-Graph-Meta-learning"><a href="https://arxiv.org/pdf/1905.09718.pdf">Meta-GNN: On Few-shot Node Classification in Graph Meta-learning</a></h3><p>However, there are very few works applying meta-learning to non-Euclidean domains, and the recently proposed graph neural networks (GNNs) models do not perform effectively on graph few-shot learning problems. Additionally, Meta-GNN is a general model that can be straightforwardly incorporated into any existing state-of-the-art GNN</p><h3 id="MR-GNN-Multi-Resolution-and-Dual-Graph-Neural-Network-for-Predicting-Structured-Entity-Interactions"><a href="https://arxiv.org/pdf/1905.09558.pdf">MR-GNN: Multi-Resolution and Dual Graph Neural Network for Predicting Structured Entity Interactions</a></h3><p>In recent years, graph neural networks have become attractive. Experiments conducted on real-world datasets show that MR-GNN improves the prediction of state-of-the-art methods.</p><h3 id="Revisiting-Graph-Neural-Networks-All-We-Have-is-Low-Pass-Filters"><a href="https://arxiv.org/pdf/1905.09550.pdf">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</a></h3><p>In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property</p><h3 id="Multi-hop-Reading-Comprehension-across-Multiple-Documents-by-Reasoning-over-Heterogeneous-Graphs"><a href="https://arxiv.org/pdf/1905.07374.pdf">Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs</a></h3><p>We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based model (single model) achieves state-of-the-art result.</p><h3 id="IPC-A-Benchmark-Data-Set-for-Learning-with-Graph-Structured-Data"><a href="https://arxiv.org/pdf/1905.06393.pdf">IPC: A Benchmark Data Set for Learning with Graph-Structured Data</a></h3><p>The data set, named IPC, consists of two self-contained versions, grounded and lifted, both including graphs of large and skewedly distributed sizes, posing substantial challenges for the computation of graph models such as graph kernels and graph neural networks</p><h1>Datasets</h1><ul><li><a href="https://www.aminer.cn/citation">Citation Networks</a>: <a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/cora_raw.zip">Cora</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/citeseer.zip">Citeseer</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/pubmed.zip">Pubmed</a>,<a href="https://www.aminer.cn/citation">DBLP</a></li><li><a href="http://networkrepository.com/soc_BlogCatalog.php">Social Networks</a>: <a href="http://socialcomputing.asu.edu/datasets/BlogCatalog">BlogCatalog</a>,<a href="https://github.com/linanqiu/reddit-dataset">Reddit</a>,<a href="http://www.trustlet.org/downloaded_epinions.html">Epinions</a></li><li>Chemical/Biological Graphs: <a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI1.zip">NCI-1</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI109.zip">NCI-9</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/MUTAG.zip">MUTAG</a>, D&amp;D,<a href="https://github.com/bigdata-ustc/QM9nano4USTC">QM9</a>,<a href="https://tripod.nih.gov/tox21/challenge/data.jsp">Tox21</a>,<a href="http://snap.stanford.edu/graphsage/ppi.zip">PPI</a>.</li><li>Unstructured Graphs: convert <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, <a href="http://www.mattmahoney.net/dc/textdata.html">Wikipedia</a> or News Groups into graphs.</li><li>Others: <a href="https://pan.baidu.com/s/14Yy9isAIZYdU__OYEQGa_g#list/path=%2F">METR-LA</a>, <a href="https://grouplens.org/datasets/movielens/1m/">Movies-Lens1M</a>, NELL.</li></ul><p>Thanks for the links given <a href="https://www.jianshu.com/p/67137451b67f">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list the generation and improvement of graph neural networks.&lt;/p&gt;
&lt;h1&gt;Why Use&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Basic lines&lt;/h1&gt;
&lt;p&gt;Contents in this block mainly comes from paper &lt;a href=&quot;https://arxiv.org/pdf/1812.08434.pdf&quot;&gt;Graph Neural Networks: A Review of Methods and Applications&lt;/a&gt; and&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>UNet proposed by Olaf Ronneberger etc.</title>
    <link href="http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html</id>
    <published>2019-05-23T21:18:21.000Z</published>
    <updated>2021-01-12T19:33:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>The notes of paper <a href="https://arxiv.org/abs/1505.04597v1">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>.</p><a id="more"></a><h2 id="Big-Question-classification-in-pixel-level-and-thus-image-segmentation">Big Question: classification in pixel level and thus image segmentation</h2><h3 id="reason">reason</h3><ul><li>single label for a image is not enough to support segmentation.</li></ul><h3 id="background">background</h3><ul><li>why focus on it<ul><li>biomedical images: like cells segmentation.<br>Add: also appropriate for other entity segmentation</li></ul></li><li>how have been done:<ul><li>the development of deeper CNNs</li><li>using CNNs segmenting electron microscopy images</li></ul></li><li>what have been missed:<ul><li>computation efficiency and redundancy: slow cause every patch require a running of network; patch overlapping</li><li>difficult trade-off for localization and the usage of context.</li></ul></li></ul><h2 id="Methods">Methods</h2><h3 id="For-what">For what?</h3><p>Cells segmentation.</p><h3 id="Framework-of-Methods">Framework of Methods</h3><p>Convolution2D + deconvolution (upsampling 2D). The output of one downsampling layer is contracted as part of the input of the corresponding symmetric upsampling layer.</p><h3 id="Novelty">Novelty</h3><ul><li>data augmentation randomly elastic deformations: shift, rotation, gray value, random elastic deformations are the most important</li><li>replace pooling by upsampling.</li><li>No fully connection layers.</li><li>weighted the loss of touching objects (cells).</li></ul><h2 id="Details">Details</h2><h3 id="weighted-map-to-segment-overlapped-cells">weighted map to segment overlapped cells</h3><p>According to the paper, they pre-compute the weight map for each ground truth segmentation to compensate the different frequency of pixels.</p><h3 id="data-augmentation">data augmentation</h3><p>Smooth deformations using random displacements vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation.</p><h2 id="Abstract">Abstract</h2><p>The main idea in abstract are contracted NNs and data augmentation so that the new NNs can get reasonable results by fewer images.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The notes of paper &lt;a href=&quot;https://arxiv.org/abs/1505.04597v1&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>paper--Faster RCNN</title>
    <link href="http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster%20rcnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster rcnn.html</id>
    <published>2019-05-19T15:17:12.000Z</published>
    <updated>2021-01-12T20:48:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Some understanding about the details in Faster RCNN, based on the codes in tensorflow.</p><a id="more"></a><h2 id="Big-Question">Big Question</h2><h3 id="reason">reason</h3><h3 id="background">background</h3><h2 id="Region-Proposal-Network">Region Proposal Network</h2><h3 id="Some-numbers">Some numbers</h3><ul><li>The number of anchor boxes for one anchor target $k = scale \times ratios$,</li><li>The number of anchor boxes for one feature layer (which has $W \times H$ grids), will get $ W \times H \times k $ anchor boxes. Every grid in the feature map (the output of a popular CNN without FC layers) will have $k$ anchor boxes.</li><li>Not like the ROI method, the size of features are fixed, but anchor boxes are rescaled by $k$ regressors.</li></ul><h2 id="Experiments">Experiments</h2><h3 id="prove">prove</h3><ul><li>The top-ranked RPN proposals are accurate.</li><li>NMS does not harm the detection mAP and may reduce false alarms.</li></ul><h2 id="Construct">Construct</h2><h3 id="Add-loss">Add loss</h3><h3 id="Problems-using-it-processing-typhoon-data">Problems using it processing typhoon data</h3><ul><li>Does NMS lead to loss of typhoon? Not really, the texture of typhoon is obvious in image.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Some understanding about the details in Faster RCNN, based on the codes in tensorflow.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Positioning Data of FY4 AGRI.</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html</id>
    <published>2019-04-26T22:53:05.000Z</published>
    <updated>2019-04-26T15:31:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.</p><p>There are two ways for positioning, one is querying the lookup table given by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">NSMC</a>, the other is calculating by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">formulas</a>.</p><p>However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days.</p><a id="more"></a><h2 id="Querying-the-lookup-table">Querying the lookup table.</h2><p>There are two errors in the files.</p><ul><li>The first 8 bytes denote latitude, and the next 8 bytes are reserved for longitude.</li><li>The data are stored as little-endian data.</li></ul><center>    <img src="/assets/img/FY4-AGRI/lookup.png" width="400"></center><h2 id="Calculating-by-formulas">Calculating by formulas</h2><p>The formulas are OK, but the constant variable $\lambda_D$ should be measured in rad before being used.</p><center>    <img src="/assets/img/FY4-AGRI/formulas.png" width="400"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.&lt;/p&gt;
&lt;p&gt;There are two ways for positioning, one is querying the lookup table given by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;NSMC&lt;/a&gt;, the other is calculating by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;formulas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="Sensor" scheme="http://yoursite.com/tags/Sensor/"/>
    
  </entry>
  
  <entry>
    <title>A Tex Template of Cornell Notes</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2019-04-26T15:34:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these.</p><a id="more"></a><h2 id="Preliminaries">Preliminaries</h2><p>Before starting, you need install</p><ul><li>CTex</li></ul><h2 id="Install-missed-packages-of-CTex">Install missed packages of CTex</h2><h3 id="Download-the-required-packages">Download the required packages</h3><p>Search <a href="https://www.ctan.org/pkg">here</a>.</p><h3 id="Unzip-and-compile-manually-if-needed">Unzip and compile manually if needed.</h3><p>Unzip the downloaded file and jump to the directory after unzip. If need compile, run</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  pdflatex ***.ins</span><br></pre></td></tr></table></figure><h3 id="Install">Install</h3><p>Copy the compiled file folder to the CTex path: ~/CTex/CTex/tex/latex</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ texhash --admin</span><br></pre></td></tr></table></figure><p>The notes will be generated like this:</p><center>    <img src="/assets/img/CornellNotes.png" width="400"></center><h1>Acknowledgement</h1><p>Thank <a href="https://blog.csdn.net/Myriad_Dreamin/article/details/83384110">this blog</a>.</p><h1>Resources</h1><p>Tex file can be found <a href="https://github.com/skaudrey/skaudrey.github.io/tree/master/assets/notes/Cornell">here</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>Understand and debug the codes of GCN proposed by Thomas N. Kipf</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2021-01-27T14:45:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1609.02907.pdf">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>.</p><a id="more"></a><h2 id="Big-Question-semi-supervised-classification-of-graph-data">Big Question: semi-supervised classification of graph data</h2><ul><li>reason<ul><li>computation effective: semi-supervision</li><li>the complex of graphs, the information of nodes and edges are not structural information.</li></ul></li><li>background<ul><li>the improvement of GCNs: spectral GCNs</li></ul></li></ul><h2 id="Key-points">Key points</h2><h3 id="The-approximation-of-spectral-graph-convolution">The approximation of spectral graph convolution</h3><p>The lines in paper had confused me at first before I ran the codes.</p><p>The difference of graph convolution and valina convolution is the input, as the input is a graph rather than data in same dimension, the key point is how to convert data represented by node and graph to a tensor in fixed dimension.</p><p>To solve it, Thomas maps the graph into a spectral space and also, to be computational efficient, approximate the infinite coefficients by second-order Chebyshev polynomial formulas.</p><p>After those approximation, it is input into the whole network with features.</p><h3 id="Build-model">Build model</h3><p>Actually, except the complicated preprocess to represent graph G into a sparse tensor, the other step are not that complex, just the similar as what a convolution layer do.<br>$$<br>Z = f\left(\mathbf{X},A\right)=softmax\left(\hat{A}ReLU\left({\hat{A}XW^{\left(0\right)}}\right)W^{\left(1\right)}\right)<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
  </entry>
  
</feed>
