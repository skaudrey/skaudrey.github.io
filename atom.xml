<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mia&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/2d5354ebc5a8c2413323ef55a6c6d252</icon>
  <subtitle>Je marche lentement, mais je ne recule jamais.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-08-24T19:46:10.494Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Mia Feng</name>
    <email>skaudreymia@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Paper--Fall Surveys</title>
    <link href="http://yoursite.com/posts/notes/2021-08-23-notes-paper-SSL-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-08-23-notes-paper-SSL-survey.html</id>
    <published>2021-08-23T14:36:39.000Z</published>
    <updated>2021-08-24T19:46:10.494Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="https://arxiv.org/pdf/2006.08218.pdf">Self-supervised Learning: Generative or Contrastive</a></li></ul><a id="more"></a><h2 id="ssl-generative-or-contrastive">SSL: Generative or Contrastive</h2><h3 id="why">Why?</h3><h4 id="supervised-learning"><strong>Supervised Learning</strong></h4><ul><li>relies heavily on expensive manual labeling</li><li>suffers from generalization error, spurious correlations, and adversarial attacks</li><li>The characteristics of different types of falls are not taken into consideration in most of the work on fall detection surveyed. (like age, gender etc.)</li></ul><h4 id="ssl">SSL</h4><ul><li>Training data is automatically labeled by leveraging the relations between different input sensor signals.</li><li>Features<ul><li>Obtain “labels” from the data itself by using a “semi-automatic” process.</li><li>Predict part of the data from other parts.</li></ul></li></ul><h3 id="motivation-of-ssl">Motivation of SSL</h3><h4 id="mainstream-of-methods">Mainstream of methods</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823112440152.png" alt="image-20210823112440152" style="zoom:67%;" /></p><ul><li>For latent distribution <span class="math inline">\(z\)</span>: explicit in generative and contrastive methods, and implicit in GAN</li><li>Discriminator: GANs and contrastive have while generative method does not.</li><li>Objectives: generative methods use a reconstruction loss, the contrastive ones use a similarity metric and the GANs leverage distributional divergence as the loss (JS-divergence, Wasserstein distance )</li></ul><h4 id="hints">Hints</h4><ul><li>Contrastive learning is useful for almost all visual classification tasks: since the contrastive object is modeling the class-invariance between different image instances.</li><li>The art of SSL primarily lies in defining proper objectives for unlabeled data.</li></ul><h4 id="summary-of-papers">Summary of papers</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823114232256.png" alt="image-20210823114232256" /><figcaption aria-hidden="true">image-20210823114232256</figcaption></figure><h4 id="outline-of-this-paper">Outline of this paper</h4><h3 id="generative-ssl">Generative SSL</h3><h4 id="ar-model">AR model</h4><ul><li>Viewed as "Bayes net", where the probability of each variable is dependent on the previous variables.</li><li>objective: in NLP, usually maximizing the likelihood under the forward autoregressive factorization.</li><li>Examples:<ul><li>PixelRNN: lower (right) pixels are generated by conditioning on the upper (left) pixels.</li><li>For 2D images: factorize probabilities according to specific directions, and therefore masked filters</li><li>For raw audio: PixelCNN, wavenet.</li><li>GraphRNN: y decompose the graph generation process into a sequence generation of nodes and edges conditioned on the graph generated so far. The objective is the likelihood of the observed graph generation sequences.</li></ul></li><li>Pros and cons: can model the context dependency well, but the token at each position can only access its context from one direction .</li></ul><h4 id="flow-based-model">Flow-based model</h4><ul><li>Goal: estimate complex high-dimensional densities from data. It designs the mapping between <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> in invertible, but also requires that <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> have the same dimension.</li><li>Examples<ul><li>NICE and RealNVP design affine coupling layer to parameterize <span class="math inline">\(f_\theta\)</span>.</li><li>GLOW: introduces invertible 1 × 1 convolutions and simplifies RealNVP.</li></ul></li></ul><h4 id="ae-model">AE model</h4><h5 id="basic-ae">Basic AE</h5><ul><li>RBM can be regarded as a special AE.</li><li>AE model is usually a feed-forward neural network trained to produce its input at the output layer.</li><li>the linear autoencoder corresponds to the PCA method.</li><li>some interesting structures can be discovered by imposing sparsity constraints on the hidden units</li></ul><h5 id="context-prediction-model-cpm">Context prediction model (CPM)</h5><ul><li>Idea: predict contextual information based on inputs.</li><li>negative sampling is employed to ensure computational efficiency and scalability</li><li>Examples<ul><li>CBOW, Skip-Gram, FastText based on CBOW</li><li>DeepWalk: based on a similar context prediction objective. It treats random walks as the equivalent of sentences.</li><li>LINE: aims to generate neighbors based on current nodes. Use negative sampling to sample multiple negative edges to approximate the objective.</li></ul></li></ul><h5 id="denoising-ae-model">Denoising AE model</h5><ul><li>Intuition: representation should be robust to the introduction of noise.</li><li>Examples<ul><li>MLM (masked language model): randomly mask some of the tokes from the input and then predicts them based on their context information. But <strong>it assumes the predicted tokens are independent if the unmasked tokens are given,</strong> which does not hold in reality.</li><li>Bert: import a unique token to mask some tokens, but also replace the unique token with original words or random words with a small probability.</li><li>SpanBert: mask continuous random spans rather than random tokens adopted by BERT. It trains the span boundary representations to predict the masked spans.</li><li>ERINE: learn entity-level and phrase-level knowledge and further integrates knowledge in knowledge graphs into language models</li><li>GPT-GNN: asks GNN to generate masked edges and attributes.</li></ul></li></ul><h5 id="variational-ae-model">Variational AE model</h5><ul><li><p>Assumes that data are generated from underlying latent representation. The posterior distribution over a set of unobserved variable <span class="math inline">\(Z\)</span> given some data <span class="math inline">\(X\)</span> is approximated by a variational distribution <span class="math inline">\(q(z|x)\approx p(z|x)\)</span>. In variational inference, the ELBO (evidence lower bound) on the log-likelihood of data is maximized during training.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823135504436.png" alt="image-20210823135504436" /><figcaption aria-hidden="true">image-20210823135504436</figcaption></figure><p>The 1st term of ELBO is a regularizer forcing the posterior to approximate the prior. The second term is the likelihood of reconstructing the original input data based on latent variables.</p></li><li><p>Examples</p><ul><li>VQ-VAE: aims to learn discrete latent variables motivated by the fact that many modalities are inherently discrete, such as language, speech, and images. VQ-VAE relies on vector quantization (VQ) to learn the posterior distribution of discrete latent variables.</li><li>VQ-VAE-2: enlarge the scale and enhance the autoregressive priors by a powerful PixelCNN prior</li><li>VGAE: VAE combined with GCN as the encoder, with an objective to reconstruct the adjacency matrix of the graph by measuring node proximity.</li><li>DVNE: s Gaussian node embedding to model the uncertainty of nodes. 2-Wasserstein distance is used to measure the similarity between the distributions for its effectiveness in preserving network transitivity</li><li>vGraph: node representation learning and community detection. Assumed that each node can be generated from a mixture of communities.</li></ul></li></ul><h4 id="hybrid-generative-models">Hybrid generative models</h4><h5 id="arae">AR+AE</h5><ul><li>Examples<ul><li>MADE: modify autoencoder by masking its parameters to respect AR constraints.</li><li>PLM (permutation language model): AR+ auto encoding .</li><li>XLNet: introduces PLM, enables learning bidirectional contexts by <strong>maximizing the expected likelihood over all permutations</strong> of the factorization order. It also integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL into pre-training, which can model long-range dependency better that Transformer.</li></ul></li></ul><h5 id="ae-flow-based-models">AE + flow-based models</h5><ul><li>Examples<ul><li>GraphAF: to molecule graph generation as a sequential decision process. Follow flow-based method, it defines an invertible transformation from a base distribution to a molecular graph structure. Dequantization technique is utilized to convert discrete data into continuous data.</li></ul></li></ul><h4 id="pros-and-cons">Pros and cons</h4><ul><li>Ability: recover the original data distribution without assumptions for downstream tasks.</li><li>Shortcomings<ul><li>far less competitive than contrastive self-supervised learning in some classification scenarios because contrastive learning’s goal naturally conforms the classification objective: MOCO, SimCLR, BYOL, SwAV.</li><li>Point-wise nature of the generative objective has some inherent defects. The MLE is based on all the samples <span class="math inline">\(x\)</span> we hope to model and the context information <span class="math inline">\(c\)</span> is conditionally constrained.<ul><li>Sensitive and conservative distribution: when <span class="math inline">\(p(x|c)\rightarrow 0\)</span>, MLE loss becomes super large, making generative model extremely sensitive to rare samples.</li><li>Low-level abstraction objective: the representation distribution in MLE is mostly modeled at <span class="math inline">\(x\)</span>'s level, while most of the classification tasks target at high-level abstraction.</li></ul></li><li>Generative - contrastive SSL abandons the point-wise objective.</li></ul></li></ul><h3 id="contrastive-ssl">Contrastive SSL</h3><p>The contrastive models show the potential of discriminative models for representation. They aim at "learn to compare" through a NCE (noise contrastive estimation ) objective formatted as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823152108301.png" alt="image-20210823152108301" style="zoom:80%;" /></p><p>With more dissimilar pairs involved, we have the InfoNCE formulated as</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823152312022.png" alt="image-20210823152312022" /><figcaption aria-hidden="true">image-20210823152312022</figcaption></figure><h4 id="context-instance-contrast">Context-instance Contrast</h4><p>Also called as global-local contrast, focusing on modeling the belonging relationship between the local feature of a sample and its global context representation. There are two main types of context-instance contrast:</p><ul><li>PRP (predict relative position): learn relative positions between local components. The global context serves as an implicit requirement for predicting these relations. (such as understanding what an elephant looks like is critical for predicting relative position between its head and tail)</li><li>MI (maximize mutual information): learn the direct belonging relationships between local parts and global context. Ignore relative positions between local parts.</li></ul><h5 id="predict-relative-position">Predict relative position</h5><ul><li>The PRP is also knowns as pretext task, such as jigsaw, rotation. It may also serve as tools to create hard positive samples.</li><li>Examples<ul><li>NSP (next sentence prediction ): for a sentence, the model is asked to distinguish the following and a randomly sampled one. But in RoBERTa, the NSP loss is removed since some argue that NSP may hurt performance.</li><li>ALBERT propose SOP task (sentence order prediction ): two sentences that exchange their position are regarded as a negative sample, making the model concentrate on the semantic meaning’s coherence.</li></ul></li></ul><h5 id="mi">MI</h5><ul><li><p>Target at maximizing the association between two variables. To reduce the computation price, in practice, MI is maximized by maximizing its lower bound with an NCE objective</p></li><li><p>Examples</p><ul><li>Deep InfoMax: maximizing the MI between a local patch and its global context.</li><li>CPC (contrastive predictive coding ): inspired by deep infomax. It maximizes the association between a segment of audio and its context audio. The negative context vectors are taken at the same time.</li><li>AMDIM: enhances the positive association between a local feature and its context. Instead picking a negative image which has the same context, it picks the image which is taken from a different view of the positive image.</li><li><em>CMC</em>: extends AMDIM, which take the image in several different views as the positive samples, while the negative sample is another irrelevant sampled image. It <em>measure the instance-instance similarity rather than context-instance similarity</em>.</li><li>InfoWord: maximize the mutual information between a global representation of a sentence and n-grams in it. The context is induced from the sentence with selected n-grams being masked, and the negative contexts are randomly picked out from the corpus.</li><li>DGI (deep graph InfoMax): take a nodes representation as the local feature and the average of randomly samples 2-hop neighbors as the context. To generate negative samples on one single graph, DGI corrupt the original context by keeping the sub-graph structure and permuting the node features.</li><li>InfoGraph: follow DGI, learn graph-level representation.</li><li>Similar as CMC: <a href="https://arxiv.org/pdf/2006.05582.pdf">paper</a> learns node and graph representations by maximizing MI between node representations of one view and graph representation of another view and vice versa. They find that graph diffusion is the most effective way to yield augmented positive sample pairs in graph learning.</li><li>In <a href="https://arxiv.org/abs/1905.12265">paper</a> , they attempt to unify graph pre-training in two strategies. One is structural prediction at node-level, where they propose context prediction to maximized the MI between the k-hop neighborhood's representations and its context graph. For node-level/graph-level strategy, they propose attribute mask to predict a node's attribute according to its neighborhood, which is a generative objective similar to token masks in bert.</li><li><span class="math inline">\(\mathrm{S}^2\)</span>GRL: separate nodes in the context graph into k-hop context subgraphs and maximizes their MI with target node, respectively. <font color=red>(There are k negative samples?)</font></li></ul></li><li><p>Cons</p><p>Existing graph pre-training work is only applicable for a specific domain, while graph pre-training tends to learn inductive biases across graphs.</p></li></ul><h5 id="improvements">Improvements</h5><ul><li>Some argue that the models above is only loosely connected to MI by showing that <strong><em>an upper bound MI estimator leads to ill-conditioned and lower performance representations.</em></strong>--&gt; <font color="blue">More should be attributed to encoder architecture and a negative sampling strategy related to metric learning.</font></li><li>And therefore in metric learning: <strong><em>perform hard positive sampling while increasing the negative sampling strategy</em></strong>.</li></ul><h4 id="instance-instance-contrast">Instance-instance Contrast</h4><ul><li>Directly studies the relationships between different samples' instance-level local representations as what metric learning does.</li></ul><h5 id="cluster-discrimination">Cluster discrimination</h5><ul><li>The motivation is to pull similar images near in the embedding space.</li><li>Examples<ul><li>Deep Cluster: leverage clustering to yield pseudo labels and asks a discriminator to predict images' labels. In details, K-means to cluster pseudo labels and then the discriminator predicts whether two samples are from the same cluster and back-propagates to the encoder.<ul><li>In DeepCluster, samples are assigned to mutual-exclusive clusters. But LA identifies neighbors separately for each examples.</li><li>DeepCluster optimizes a cross-entropy discriminative loss, while LA employs an objective function that directly optimizes a local soft-clustering metric.</li></ul></li><li>LA (local aggregation): improve the cluster-based method's boundary.<ul><li>LA identifies neighbors separately for each examples.</li><li>LA employs an objective function that directly optimizes a local soft-clustering metric.</li></ul></li><li>VQ-VAE: similar as LA. For the feature matrix encoded from an image, VQ-VAE substitutes each 1-dimensional vector in the matrix to the nearest one in an embedding dictionary.</li><li>ClusterFit: help in the generalization of other pre-trained models. Introduce a cluster prediction fine-tuning stage similar to DeepCluster.</li><li>SwAV: to improve the time-consuming two-stage training. Use online clustering ideas and multi-view data augmentation strategies into the cluster discrimination approach. To reduce time price, they propose an online computing strategy to label the images in different views.</li><li>M3S: in graph learning. Given little labeled data and many unlabeled data, for every stage, M3S first pretrain as DeepCluster does and then compares these pseudo labels with those predicted by the model being supervised trained on labeled data. . Only top-k confident labels are added into a labeled set for the next stage of semi-supervised training.</li></ul></li></ul><h5 id="instance-discrimination">Instance Discrimination</h5><ul><li>The prototype is InstDisc.</li><li>Examples<ul><li>CMC: adopt multiple different views of an image as positive samples and take another one as the negative. But it's constrained by the idea of Deep Infomax, which only samples one negative sample for each positive one.</li><li>MoCo: leverage instance discrimination via momentum contrast, which substantially increases the amount of negative samples.<ul><li>It designs the <strong>momentum contrast learning with two encoders (query and key),</strong> which prevents the fluctuation of loss convergence in the beginning period</li><li>to enlarge negative samples’ capacity, MoCo employs a queue (with K as large as 65536) to save the recently encoded batches as negative samples.</li><li>But the positive sample strategy is too simple: a pair of positive representations come from the same sample without any transformation or augmentation.</li></ul></li><li>PIRL: based on MoCo, adds jigsaw augmentation.</li><li>SimCLR: hard positive sample strategy by introducing data augmentation in 10 forms. The augmentation leverages several different views to augment the positive pairs. To handle the large-scale negative samples problem, it chooses a batch size of N as large as 8196. <strong>Techniques in SimCLR can also further improve MoCo's performance.</strong></li><li>InfoMin: more into augmenting positive samples. They argue that we should select those views with less mutual information for better augmented views in contrastive learning. To do so, they first propose an unsupervised method to minimize mutual information between views, but this will result in a loss of information for predicting labels, say a pure blank view. Then a semi-supervised method is proposed to find views sharing only label information.</li><li>BYOL: discards negative sampling in SSL but achieves an even better result over InfoMin. <em>They argue that negative samples may not be necessary in this process</em>.<ul><li>If we use a fixed randomly initialized network to serve as the key encoder, the representation produced by query encoder would still be improved during training.</li><li>BYOL proposes an architecture with an exponential moving average strategy to update the target encoder just as MoCo does.</li><li>The loss is mean square error, which is robust to smaller batch size.</li><li>The batch size in BYOL is not as critical as what's in MoCo and SimCLR.</li></ul></li><li>SimSiam: study how necessary is negative sampling.<ul><li>They show that the most critical component in BYOL is the stop gradient operation, which makes the target representation stable.</li><li>It converges faster than MoCo, SimCLR, and BYOL with even smaller batch sizes.</li></ul></li><li>ReLIC: argue that contrastive pre-training teaches the encoder to causally disentangle the invariant content and style in an image.<ul><li>They propose to add an extra KL-divergence regularizer between prediction logits of an image's different views.</li></ul></li><li>GCC (graph contrastive coding): leverage instance discrimination as the pretext task for structural information pre-training.<ul><li>For each node, they sample two subgraphs independently by random walks with restart and use top eigenvectors from their normalized graph Laplacian matrices as nodes' initial representations.</li><li>Then they use GNN to encode them and calculate the InfoNCE loss.</li></ul></li><li>GraphCL: studies the data augmentation strategies, propose four different augmentation methods based on edge perturbation and node dropping. They show that the appropriate combination of these strategies can yield even better performance.</li></ul></li></ul><h4 id="ss-contrastive-pre-training-for-semi-supervised-self-training">SS contrastive pre-training for semi-supervised self-training</h4><ul><li>No matter how self-supervised learning models improve, they are still the only powerful feature extractor, and to transfer to the downstream task, we still need labels more or less.</li><li>In self-training, a model is trained on the small amount of labeled data and then yield labels on unlabeled data. Only those data with highly confident labels are combined with original labeled data to train a new model. We iterate this procedure to find the best model.<ul><li>Student-teacher</li><li>The improvements from pre-training and self-training are orthogonal to each other. The model with joint pre-training and self-training is the best.</li></ul></li><li>SimCLR v2 adopts the conclusion above<ul><li>Do SS pre-training as SimCLR v1， with some minor architecture modification and a deeper ResNet.</li><li>Fine tune the last few layers with only 1% or 10% of original ImageNet labels.</li><li>Use the fine-tuned network as teacher to yield labels on unlabeled data to train a smaller student ResNet-50.</li></ul></li></ul><h4 id="pros-and-cons-1">Pros and cons</h4><ul><li>usually light-weighted and perform better in discriminative downstream applications</li><li>Problems remain to be solved<ul><li>Scale to natural language pre-training</li><li>Sampling efficiency: hints from BYOL and SimSiam. The role that negative sampling plays in contrastive learning is still not clear.</li><li>data augmentation: in theory, why data augmentation can boost contrastive learning's performance is still not clear.</li></ul></li></ul><h3 id="generative-contrastive-adversarial-ssl">Generative-Contrastive (Adversarial) SSL</h3><p>Adversarial learning learns to reconstruct the original data distribution rather than the samples by minimizing the distributional divergence.</p><h4 id="generate-with-complete-input">Generate with Complete Input</h4><ul><li>Capturing the sample's complete information.</li><li>To extract the implicit distribution out <span class="math inline">\(p(z)\)</span>,<ul><li>AAE: the generator in GAN is an implicit autoencoder, which can be replaced by an explicit variational autoencoder (VAE).<ul><li>AAE substitutes the KLH divergence function for a discriminative loss.</li></ul></li></ul></li></ul><h4 id="recover-with-partial-input">Recover with Partial Input</h4><ul><li>Provide models with partial input and ask them to recover the rest parts. Similar as masked bert but this works in an adversarial manner.</li><li>Examples<ul><li>Colorization: given one color channel L in an image and predicting the value of two other channels A, B. The encoder and decoder networks can be set to any form of convolutional neural network.</li><li>Inpainting: ask the model to predict an arbitrary part of an image given the rest of it. Then a discriminator is employed to distinguish the inpainted image from the original one.</li><li>SRGAN: follows the same idea in inpainting,</li></ul></li></ul><h4 id="pre-trained-language-model-ptm">Pre-trained Language model (PTM)</h4><ul><li>Focus on maximum likelihood estimation based on pretext task.</li><li>Examples<ul><li>ELECTRA: outperform BERT.<ul><li>The generator is a small masked language model (MLM)</li><li>The discriminator will predict which words are replaced.</li><li>Training steps: first warming-up the generator by MLM pretext task. Then train with the discriminator.</li></ul></li><li>WKLM: perform Replaced Token Detection (RTD) at the entity-level.</li></ul></li></ul><h4 id="graph-learning">Graph learning</h4><ul><li>Adopt adversarial training<ul><li>ANE (adversarial network embedding) designs a generator that is updated in two stages: the generator encodes sampled graph into target embedding and computes traditional NCE with a context encoder like Skip-gram; discriminator will distinguish embedding from the generator and sampled one from a prior distribution.</li><li>GraphGAN: model the link prediction task and follow the original GANs style discriminative objective to distinguish directly at node-level rather than representation-level.</li><li>GraphSGAN: use the adversarial method in semi-supervised graph learning with the motivation that marginal nodes cause most classification errors in the graph. Between clusters, there are density gaps where few samples exist. They prove that we can complete classification theoretically if we generate enough fake samples in density gaps. The generator will generate fake nodes in density gaps during the training.</li></ul></li></ul><h4 id="domain-adaption-and-multi-modality-representation">Domain adaption and multi-modality representation</h4><ul><li>GAN can help on domain adaption: <a href="https://arxiv.org/abs/1505.07818">[1]</a>, <a href="https://arxiv.org/abs/1805.05151">[2]</a>, <a href="https://arxiv.org/abs/1505.07818">[42]</a>, <a href="https://www.researchgate.net/publication/318224334_Adversarial_Representation_Learning_for_Domain_Adaptation">[113]</a>.</li><li>Leverage adversarial sampling to improve the negative samples' quality: <a href="https://aclanthology.org/N18-1133.pdf">[16]</a>, <a href="https://arxiv.org/abs/1809.11017">[138]</a></li></ul><h4 id="pros-and-cons-2">Pros and cons</h4><ul><li>Challenges<ul><li>Limited applications in NLP and graph.</li><li>Easy to collapse</li><li>Not for feature extraction: Contrastive learning is more practical in extraction.</li></ul></li></ul><h3 id="theory-behind-ssl">Theory behind SSL</h3><h4 id="gan">GAN</h4><h5 id="divergence-matching">Divergence matching</h5><ul><li>Different divergence functions leads to different GAN variants. <a href="https://arxiv.org/abs/1606.00709">Paper</a> discusses the effects of various choices of divergence functions.</li></ul><h5 id="disentangled-representation">Disentangled representation</h5><ul><li>GAN shows its superior potential in learning disentangled features empirically and theoretically.</li><li>InfoGAN proposes to learn disentangled representation with DCGAN.<ul><li>Since mutual information is hard to compute, they leverage the variational inference approach to estimate its lower bound.</li></ul></li><li><a href="https://arxiv.org/abs/1811.10597">GAN dissection</a>: apply causal analysis into understanding GAN. They identify the correlations between channels in the convolutional layers and objects in the generated images, and examine whether they are causally-related with the output.</li><li><a href="https://openreview.net/pdf?id=SJxDDpEKvH">Paper</a> examines the channels' conditional independence via rigorous counterfactual interventions over them. They show that in BigGAN, it's possible to disentangle backgrounds and objects.</li></ul><h4 id="maximizing-lower-bound">Maximizing Lower Bound</h4><h5 id="evidence-lower-bound">Evidence lower bound</h5><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824130843214.png" alt="image-20210824130843214" style="zoom:80%;" /></p><p>ELBO is the lower bound of the optimization target KL divergence. VAE maximizes the ELBO to minimize the difference between <span class="math inline">\(q_\phi(z|x),p_\theta(z|x)\)</span>.</p><h5 id="mutual-information">Mutual information</h5><ul><li>Maximizes the MI of the input and its representation with joint density <span class="math inline">\(p(x|y\)</span> and marginal densities <span class="math inline">\(p(x),p(y)\)</span>.</li><li>Examples<ul><li>Deep Infomax maximizes the MI of local and global features and replaces KL-divergence with JS-divergence, which is similar to GAN.</li><li>Instance Discrimination directly optimizes the proportion of gap of positive pairs and negative pairs. One of the commonly used estimators is InfoNCE. And prove that useful to use large negative samples(large values of N. But then the other testify that increasing the number of negative samples does not necessarily help.</li><li>Maximizing the lower bound (MI and ELBO) is not sufficient to learn useful representations.</li><li>MI maximization can be analyzed from the metric learning view. By rewriting the InfoNCE MI as the triplet loss, it is corresponding to the expectation of the multi-class k-pair loss.</li></ul></li></ul><h4 id="contrastive-ss-representation-learning">Contrastive SS representation learning</h4><h5 id="relationship-with-supervised-learning">Relationship with Supervised learning</h5><ul><li><p>How contrastive pre-training benefits supervised learning?</p><ul><li><p><strong><em>SSL cannot learn more than supervised learning, but make it with few labels.</em></strong></p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824134109250.png" alt="image-20210824134109250" /><figcaption aria-hidden="true">image-20210824134109250</figcaption></figure></li></ul></li><li><p>SSL trained neural networks are more robust t adversarial examples, label corruption and common input corruptions. It also benefits OOD detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods.</p></li></ul><h5 id="understanding-contrastive-loss">Understanding Contrastive Loss</h5><ul><li><p>Split the contrastive loss into two terms</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824134816367.png" alt="image-20210824134816367" /><figcaption aria-hidden="true">image-20210824134816367</figcaption></figure><p>where the first term aims at “alignment” and the second aims at “uniformity” of sample vectors on a sphere given the normalization condition. They show that these two terms have a large agreement with downstream tasks.</p><ul><li>They show that by directly optimizing the two loss, it is consistently better than contrastive loss. And both these terms are necessary for a good representation.</li></ul></li><li><p>It's doubtful that whether alignment and uniformity are necessarily in the form of upper two losses. We may still achieve uniformity via other techniques such as exponential moving average, batch normalization, regularization and random initialization.</p></li></ul><h5 id="generalization">Generalization</h5><p>It is unclear why the learned representations should also lead to better performance on downstream tasks.</p><ul><li><a href="https://arxiv.org/abs/1902.09229">Paper</a> propose a conceptual framework to analyze contrastive learning on average classification tasks.<ul><li>Under the context of only 1 negative sample, it is proved that optimizing unsupervised loss benefits the downstream classification tasks.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824142746586.png" title="fig:" alt="image-20210824142746586" /></li><li>They argue that enlarging the number of negative samples does not hold for contrastive learning and shows that it can hurt performance when the negative samples exceed a threshold.</li></ul></li><li>Noise Contrastive Estimation(NCE) [49] explains that increasing the number of negative samples can provably improve the variance of learning parameters</li></ul><h3 id="discussion-and-future-directions">Discussion and future directions</h3><ul><li>Theoretical foundation<ul><li><a href="https://arxiv.org/abs/1902.09229">Paper</a> proposes a conceptual framework to analyze the contrastive objective's function in generalization ability.</li><li><a href="https://arxiv.org/abs/1907.13625">Paper</a> proves that the sampling strategies and architecture design may count more.</li></ul></li><li>Transferring to downstream tasks<ul><li>pre-training task selection problem: By ALBERT, NSP for bert may hurt its performance.</li><li>NAS to design pre-training tasks for a specific downstream task automatically.</li></ul></li><li>Transferring cross datasets (inductive learning)</li><li>Exploring potential of sampling strategies<ul><li>leverage super large amounts of negative samples and augmented positive samples, whose effects are studied in deep metric learning.</li><li>How to further release the power of sampling is still an unsolved and attractive problem.</li></ul></li><li>Early degeneration for contrastive learning<ul><li>the contrastive objectives often get trapped into embedding spaces’ early degeneration problem, which means that the model over-fits to the discriminative pretext task too early, and therefore lost the ability to generalize.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2006.08218.pdf&quot;&gt;Self-supervised Learning: Generative or Contrastive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
  </entry>
  
  <entry>
    <title>Book-Graph Representation Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-05-22-notes-book-grl.html"/>
    <id>http://yoursite.com/posts/notes/2021-05-22-notes-book-grl.html</id>
    <published>2021-05-22T16:10:39.000Z</published>
    <updated>2021-05-30T12:33:53.738Z</updated>
    
    <content type="html"><![CDATA[<p>Book <a href="https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf">Graph Representation Learning</a></p><a id="more"></a><h2 id="chapter-1-introduction">Chapter 1: Introduction</h2><h3 id="graphs">Graphs</h3><ul><li>Types and represent edges<ul><li>Simple graphs: at most one edge between each pair of nodes, no loop and undirected.</li><li>To represent graphs, adjacent matrix can be use. It shows as a symmetric matrix under simple graphs and may not symmetric under directed graphs.</li><li>Heterogeneous and multiplex graphs<ul><li>Heterogeneous graphs: both nodes and graphs are heterogeneous.</li><li>Multiplex: graph can be decomposed in a set of <span class="math inline">\(k\)</span> layers. Every layers corresponds to a unique relation, denoted as intra-layer edge, and those between layers are inter-layer edges. <strong>E.g., the graph of skeleton clips.</strong></li></ul></li></ul></li><li>Feature information<ul><li>If heterogeneous, each types of nodes may have different dimensions of attributes.</li></ul></li><li>Tasks on graphs with machine learning<ul><li>node classification: like whether users are bots or not, aka predict the label of node.<ul><li>Note that the <strong>nodes in a graph</strong> are not independently and identically distributed, which <strong>doesn't satisfy the requirements of i.i.d. in supervised learning.</strong></li><li>To inference the label, one idea is to use <em>homophily</em>, which assume that nodes tend to shared attributes with their neighbors in the graph. The other idea is <em>structural equivalence</em>, which assumes that nodes with similar local neighborhood structures will have similar labels.</li></ul></li><li>Relation prediction<ul><li>Like predicting the missing interactions.</li><li>Similar problems met in node classification (i.i.d.), and requires inductive biases that are specific to the graph domain.</li></ul></li><li>Clustering and community detection<ul><li>infer latent community structures given only the input graph.</li></ul></li><li>Graph classification, regression and clustering.<ul><li>In graph clustering, the goal is to learn an unsupervised measure of similarity between pairs of graphs.</li></ul></li></ul></li></ul><h2 id="chapter-2-background-traditional-approaches">Chapter 2: Background &amp; Traditional Approaches</h2><h3 id="graph-statistics-and-kernel-methods">Graph statistics and kernel methods</h3><h4 id="node-level-statistics">Node-level statistics</h4><ul><li><p>Node degree: simply counts the number of edges incident to a node, which will measure how many neighbors a node has.</p></li><li><p>Node centrality: To measure the importance of a node in a graph.</p><ul><li><p>Eigenvector centrality <span class="math inline">\(e_u\)</span></p><ul><li><p><span class="math inline">\(e_u=\frac{1}{\lambda}\sum\limits_{v\in V} \mathrm{A}[u,v]e_v,\forall u\in \mathcal{V}\)</span>., it measures that satisfies the recurrence in above equation corresponds to an eigenvector of the adjacency matrix.</p></li><li><p>The vector of centrality values is given by the eigenvector corresponding to the largest eigenvalues of <span class="math inline">\(\mathrm{A}\)</span>.</p></li><li><p><strong>The eigenvector centrality ranks the likelihood that a node is visited on a random walk of infinite length on the graph.</strong></p></li><li><p>After <span class="math inline">\(t\)</span> iteration, the eigenvector centrality will contain the number of length-<span class="math inline">\(t\)</span> paths arriving at each node.</p></li></ul></li><li><p>Betweeness centrality</p><ul><li>Measures how often a node lies on the shortest path between two other nodes.</li></ul></li><li><p>Closeness centrality</p><ul><li>Measures the average shortest path length between a node and all other nodes.</li></ul></li><li><p>More: M. Newman. Networks. Oxford University Press, 2018. 1, 12, 13, 108, 109</p></li></ul></li><li><p>The clustering coefficient</p><ul><li><p>Measure the structural distinction using variations of the clustering coefficient, which Measures the proportion of closed triangles in a node's local neighborhood.</p></li><li><p>The local variant of clustering coefficient is calculated by:</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522153036032.png" alt="image-20210522153036032" style="zoom:50%;" /></p></li><li><p>It Measures how tightly clustered a node's neighborhood is.</p></li><li><p>In social and biological sciences, they tend to have far higher clustering coefficients than one would expect if edges were sampled randomly.</p></li></ul></li><li><p>Closed Triangles, Ego Graphs, and Motifs</p><ul><li>The global clustering coefficient. Similar equation but this time count in the node's ego graph.</li><li>A node's ego graph: the subgraph containing that node, its neighbors, and all the edges between nodes in its neighborhood.</li><li>The general version of these ideas is counting arbitrary motifs or graphlets within a node's ego graph. E.g.: triangles, cycles of particular length etc.</li></ul></li></ul><h4 id="graph-level-features-and-graph-kernels">Graph-level features and graph kernels</h4><ul><li><p>Bag of nodes</p><ul><li>Aggregate node-level statistics</li><li>E.g., the histograms, the statistics based on the degrees, centralities and clustering coefficients of the nodes.</li><li>May miss important global properties in graph</li></ul></li><li><p>The Weisfieler-Lehman Kernel</p><ul><li><p>Iterative neighborhood aggregation. Extract node-level features that contain more information than just their local Ego graph, and then to aggregate these features into a representation.</p></li><li><p>The most well-known one is the Weisfieler-Lehman (WL) algorithm.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522162120249.png" alt="image-20210522162120249" style="zoom:57%;" /></p><ul><li>In other words, the WL kernel is computed by measuring the difference between the resultant label sets for two graphs.</li><li>Useful while solving the isomorphism problem: approximate graph isomorphism is to check whether or not two graphs have the same label set after <span class="math inline">\(K\)</span> rounds of the WL algorithm.</li></ul></li></ul></li><li><p>Graphlets and path-base methods</p><ul><li>Graphlets: count the occurrence of different small subgraph structures, but it's computationally difficult.</li><li>Path-based methods: examine the different kinds of paths that occur in the graph. E.g. <a href="https://www.aaai.org/Papers/ICML/2003/ICML03-044.pdf">random walk</a> , the <a href="https://ieeexplore.ieee.org/document/1565664">shortest path kernel</a> that is similar as random walk but uses only the shortest-paths between nodes.</li></ul></li></ul><h3 id="neighborhood-overlap-detection">Neighborhood overlap detection</h3><ul><li>The features mentioned above don't quantify the relationships among nodes, and thus won't work well on relation prediction.</li><li>The simplest neighborhood overlap measure: just counts the number of neighbors that two nodes share.</li><li>Hope that node-node similarity Measures computed on the training edges will lead to accurate predictions about the existence of test edges.</li></ul><h4 id="local-overlap-measures">Local overlap measures</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522180625718.png" alt="image-20210522180625718" style="zoom:80%;" /></p><ul><li>Use the functions of the number of common neighbors two nodes share.<ul><li>Sorensen index: normalized by the sum of the node degrees.</li><li>Salton index that normalizes by the product of the degrees of u and v.</li><li>Jaccard overlap: normalized by the degree of union u and v.</li></ul></li><li>Consider the importance of common neighbours, give more weight to common neighbours that have low degree.<ul><li>Resource Allocation index (RA): counts the inverse degrees of the common neighbours.</li><li>Adamic-Adar index (AA): similar as RA but use the inverse logarithm of the degrees.</li></ul></li></ul><h4 id="global-overlap-measures">Global overlap measures</h4><ul><li><p>Katz Index</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522180701697.png" alt="image-20210522180701697" style="zoom:67%;" /></p><ul><li>The most basic global overlap statistic. Simply count the number of paths of all lengths between a pair of nodes.</li><li>This index is a geometric series of matrices.</li><li>It's strongly biased by node degree which will give higher overall similarity scores when considering high-degree nodes cause they will generally be with more paths.</li></ul></li><li><p>Leicht, Holme, and Newman (LHN) similarity</p><ul><li><p>Considering the ratio between the actual number of observed paths and the number of expected paths between two nodes.</p></li><li><p>To compute expectation, the <em>configuration model</em> is relied on. Under a random configuration model, the likelihood of an edge is simply proportional to the product of the two node degrees. But this heuristic calculation is intractable.</p></li><li><p>To approximate, the number of paths between two nodes grows by the largest eigenvalue of adjacent matrix. (The fact that the largest eigenvalue can be used to approximate the growth in the number of paths).</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522182722632.png" alt="image-20210522182722632" style="zoom:67%;" />, and the solution to the matrix series can be written as <span class="math display">\[\mathrm{S_{LNH}}=2\alpha m\lambda_1 \mathrm{D^{-1}(I-\frac{\beta}{\lambda_1}A)^{-1}D^{-1}}\]</span></p></li></ul></li><li><p>Random walk methods</p><ul><li>rather than exact count of paths over the graph in previous introduced index, consider random walks.</li><li>A measure of importance specific to node u will be obtained since the random walks are continually being teleported back to that node.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522183303798.png" alt="image-20210522183303798" style="zoom:80%;" /></li></ul></li></ul><h3 id="graph-laplacians-and-spectral-methods">Graph Laplacians and spectral methods</h3><p>learning to cluster the nodes in a graph.</p><h4 id="graph-laplacians">Graph Laplacians</h4><ul><li><p>Unnormalized Laplacian</p><ul><li><p><span class="math inline">\(\mathrm{L=D-A}\)</span></p></li><li><p>It's symmetric and positive semi-definite. <span class="math inline">\(\mathrm{L}\)</span> has <span class="math inline">\(|V|\)</span> non-negative eigenvalues.</p></li><li><p><span class="math inline">\(\mathrm{x^TLx}=\sum\limits_{(u,v)\in\mathcal{E}}(\mathrm{x}[u]-\mathrm{x}[v])^2\)</span></p></li><li><p>If the graph contains <span class="math inline">\(K\)</span> connected components, then there exists an ordering of the nodes in the graph such that the Laplacian matrix can be written as <span class="math display">\[\mathrm{L}={\begin{bmatrix}\mathrm{L}_1 &amp;  &amp; &amp; \\&amp; \mathrm{L}_2 &amp; &amp; \\ &amp;  &amp; \ddots&amp; \\  &amp;  &amp; &amp;  \mathrm{L}_K\\\end{bmatrix}},\]</span> where each blocks is a valid graph Laplacian of a fully connected subgraph of the original graph. The spectrum of <span class="math inline">\(\mathrm{L}\)</span> is the union of the eigenvalues of the <span class="math inline">\(\mathrm{L}_K\)</span> matrices and the eigenvectors are the union of the eigenvectors of all the <span class="math inline">\(\mathrm{L}_K\)</span> matrices with 0 values filled at the positions of the other blocks.</p></li></ul></li><li><p>Normalized Laplacians</p><ul><li><span class="math inline">\(\mathrm{L_{sym}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}}\)</span>, while random walk Laplacian is defined as <span class="math inline">\(\mathrm{L_{RW}=D^{-1}L}\)</span>.</li><li>For <span class="math inline">\(\mathrm{L_{sym}}\)</span>, the properties of previous mentioned holds but with the eigenvectors for the 0 eigenvalue scaled by <span class="math inline">\(\mathrm{D^{\frac{1}{2}}}\)</span>. For <span class="math inline">\(\mathrm{L_{RW}}\)</span>, the properties hold exactly.</li></ul></li><li><p>These methods just allow to cluster nodes that are already in disconnected components. The Laplacian can be used to get an optimal clustering of nodes within a fully connected graph.</p></li></ul><h4 id="graph-cuts-and-clustering">Graph cuts and clustering</h4><ul><li><p>Graph cuts</p><ul><li>An optimal cluster means that a partition that minimizes the cut value, which is the count of how many edges cross the boundary between the partition of nodes. Theoretically the methods tend to simply make clusters that consists of a single node.</li><li>One way to solve this is minimizing the <em>Ratio Cut</em>, which penalizes the solution of choosing small cluster sizes.</li><li>Another popular solution is minimize the Normalized Cut (NCut), which enforces that all clusters have a similar number of edges incident to their a nodes.</li></ul></li><li><p>Approximating the RatioCut with the Laplacian Spectrum</p><ul><li><p>The ratio cut minimization problem can be approximated as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524082410207.png" alt="image-20210524082410207" style="zoom:40%;" /><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524082433260.png" alt="image-20210524082433260" style="zoom:50%;" /></p></li><li><p>The formula above is NP-hard, after simplification, it will be :</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524082658389.png" alt="image-20210524082658389" style="zoom:67%;" />, and the solution of this problem is the second smallest eigenvector of <span class="math inline">\(\mathrm{L}\)</span>.</p></li></ul></li><li><p>Generalized spectral clustering</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524104844842.png" title="fig:" alt="image-20210524104844842" /></li></ul></li></ul><h1 id="part-i-node-embeddings">Part I: Node Embeddings</h1><h2 id="chapter-3-neighborhood-reconstruction-methods">Chapter 3 Neighborhood Reconstruction Methods</h2><p>Goal: encode nodes as low-dimensional vectors that summarize their graph position and the structure of their local graph neighborhood.</p><h3 id="an-encoder-decoder-perspective">An encoder-decoder perspective</h3><p>An encoder maps each node in the graph into a low-dimensional vector or embedding, a decoder takes the embeddings and uses them to reconstruct information about each node's neighborhood.</p><h4 id="the-encoder">The encoder</h4><ul><li>Most works rely on <em>shallow embedding</em>, where this encoder function is simply an embedding lookup based on the node ID.</li><li>The generalized encoders are called GNNs.</li></ul><h4 id="the-decoder">The decoder</h4><ul><li>They might predict the neighbors of one given node or one row in the graph adjacency matrix.</li><li>The standard practice is to define pairwise decoders, which can be interpreted as predicting the relationship or similarity between pairs of nodes.</li><li>While reconstruction, the goal is to optimize the encoder and decoder to minimize the reconstruction loss.</li></ul><h4 id="optimizing-an-encoder-decoder-model">Optimizing an encoder-decoder model</h4><ul><li>Usually use SGD, and minimize the disparity of decoded latent distance and real distance.</li></ul><h4 id="typical-encoder-decoder-methods">Typical encoder-decoder methods</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524113343676.png" alt="image-20210524113343676" style="zoom:67%;" /></p><h3 id="factorization-based-approaches">Factorization-based approaches</h3><p>Methods below are classified as factorization methods case their loss functions can be minimized using factorization algorithms like SVD.</p><p>The goal of these methods is to learn embeddings for each node such that the inner product between the learned embedding vectors approximates some deterministic measure of node similarity.</p><p>The inner product methods all employ deterministic measures of node similarity.</p><h4 id="laplacian-eigenmaps-le">Laplacian eigenmaps (LE)</h4><ul><li><p>Builds upon the spectral clustering</p></li><li><p>Formula: <span class="math display">\[\mathcal{L}=\sum\limits_{(u,v)\in\mathcal{D}}\mathrm{DEC}(\mathrm{z}_u,\mathrm{z}_v)\cdot\mathrm{S}[u,v],\]</span> which penalizes the model when very similar nodes have embeddings that are far apart.</p></li><li><p>If the reconstructed <span class="math inline">\(\mathrm{S}\)</span> satisfies the properties of a Laplacian matrix, then the node embeddings that minimize the loss are identical to the solution for a spectral clustering.</p></li></ul><h4 id="inner-product-methods">Inner-product methods</h4><ul><li>More recent work employs inner-product, which assumes that the similarity between two nodes, e.g., the overlap between their local neighborhoods is proportional to the dot product of their embeddings.</li><li>Examples:<ul><li>Graph factorization: <a href="https://dl.acm.org/doi/10.1145/2488388.2488393">Distributed large-scale natural graph factorization</a></li><li>GraRep: <a href="https://dl.acm.org/doi/10.1145/2806416.2806512">Learning graph representations with global structural information</a></li><li>HOPE: <a href="https://dl.acm.org/doi/10.1145/2939672.2939751">Asymmetric transitivity preserving graph embedding</a></li><li>All of them used the mean squared error as loss function. They differ primarily in how they define <span class="math inline">\(\mathrm{S[u,v]}\)</span>. GF uses the adjacency matrix and sets <span class="math inline">\(\mathrm{S=A}\)</span>, GraRep defines <span class="math inline">\(\mathrm{S}\)</span> based on powers of the adjacency matrix and HOPE supports general neighborhood overlap measures.</li></ul></li></ul><h3 id="random-walk-embedding">Random walk embedding</h3><p>Use stochastic measures of node neighborhood overlap.</p><h4 id="deepwalk-and-node2vec">Deepwalk and node2vec</h4><ul><li><p>The two methods differ on the inner-product decoder (the notions of node similarity and neighborhood reconstruction). They all optimize embeddings to encode the statistics of random walks. Mathematically, the goal is to learn embeddings so that <span class="math display">\[\mathrm{DEC(z}_u,\mathrm{z}_v)\triangleq\frac{\mathrm{z}_u^\top\mathrm{z}_v}{\sum\limits_{v_k\in\mathcal{V}}\mathrm{z}_u^\top\mathrm{z}_k}\approx p_{\mathcal{G,T}}(v|u),\label{4}\]</span> where <span class="math inline">\(p_{\mathcal{G,T}}(v|u)\)</span> is the probability of visiting <span class="math inline">\(v\)</span> on a length-<span class="math inline">\(T\)</span> random walk starting at <span class="math inline">\(u\)</span>, and <span class="math inline">\(T\)</span> is the range.</p></li><li><p>To train the random walk embeddings, use the cross-entropy loss.</p></li><li><p>DeepWalk employs a hierarchical softmax to approximate equation above, where the normalizing factor is approximated using <em>negative samples</em> in the <a href="https://arxiv.org/abs/1607.00653">following way</a> <span class="math display">\[\mathcal{L}=\sum\limits_{(u,v)\in\mathcal{D}}-\log(\sigma(\mathrm{z}_u^\top\mathrm{z}_v))-\gamma\mathbb{E}_{v_n\sim P_n{\mathcal{V}}}[\log(-\sigma(\mathrm{z}_u^\top\mathrm{z}_v))]\]</span></p></li><li><p>DeepWalk employ uniformly random walks to define the visiting probability and node2vec use hyperparameters to allow the probabilities to smoothly interpolate between walks.</p></li></ul><h4 id="line">LINE</h4><ul><li><a href="http://dx.doi.org/10.1145/2736277.2741093">Large-scale information network embeddings</a>. Rather than explicitly leverage random walks, it shares ideas from DeepWalk and node2vec. It combines two encoder-decoder objectives.</li><li>It has two objectives, the 1starting is <span class="math inline">\(\mathrm{DEC(z}_u,\mathrm{z}_v)=\frac{1}{1+e^{-\mathrm{z}_u^\top\mathrm{z}_v}}\)</span>, and the 2nd has the same equation as <span class="math display">\[\eqref{4}\]</span>,but takes the KL-divergence to encode two-hop adjacency information.</li><li>Instead of random walks, it explicitly reconstructs 1st and 2nd order neighborhood information.</li></ul><h4 id="additional-variants-of-the-random-walk">Additional variants of the random-walk</h4><ul><li>biasing or modifying the random walks.<ul><li>consider random walks that skip over nodes, <a href="https://arxiv.org/pdf/1605.02115.pdf">Perozzi et al.</a></li><li>define random walks based on the structural relationships between nodes: <a href="http://dx.doi.org/10.1145/3097983.3098061">Ribeiro et al.</a></li></ul></li></ul><h3 id="random-walk-and-matrix-factorization">Random walk and matrix factorization</h3><ul><li>https://dl.acm.org/doi/10.1145/3159652.3159706</li><li>Random walk are closely related to matrix factorization.</li><li>The embeddings learned by DeepWalk are closely related to the spectral clustering embeddings, but DeepWalk embeddings control the influence of different eigenvalues through T.</li><li>The disadvantages of shallow embedding<ul><li>It doesn't share any parameters between noes in the encoder, and therefore statistically and computationally inefficient.</li><li>It doesn't leverage node features in the encoder.</li><li>It's inherently trans-ductive, which means it only generate embeddings for nodes that were present during the training phase.</li></ul></li></ul><h2 id="chapter-4-multi-relational-data-and-knowledge-graphs">Chapter 4: Multi-relational data and knowledge graphs</h2><p>The knowledge graph completion is to predict missing edges in the graph, generally. Below only covers the node embeddings way in graph completion.</p><h3 id="reconstruction-multi-relational-data">Reconstruction multi-relational data</h3><ul><li><p>Note the edges have multiple types, and the input for decoder will be a pair of nodes and types of the edge.</p></li><li><p>One simple way is <a href="https://icml.cc/2011/papers/438_icmlpaper.pdf">RESCAL</a>: <span class="math display">\[\mathrm{DEC}(u,\tau,v)=\mathrm{z}_u^\top\,\mathrm{R}_\tau\mathrm{z}_v,\]</span> where the embeddings <span class="math inline">\(\mathrm{z}\)</span> and relation matrices <span class="math inline">\(\mathrm{R}\)</span> are all learnable.</p></li><li><p>While solving the Reconstruction error, it's like tensor factorization.</p></li><li><p>Nearly all multi-relational embedding methods simply define the similarity measure directly based on the adjacency tensor, or to say they all try to reconstruct immediate neighbors from the low-dimensional embeddings.</p></li></ul><h3 id="loss-functions">Loss functions</h3><h4 id="cross-entropy-with-negative-sampling">Cross-entropy with negative sampling</h4><ul><li>The formula</li></ul><p><span class="math display">\[\mathcal{L}=\sum\limits_{(u,\tau,v)\in\mathcal{E}}-\log(\sigma(\mathrm{DEC(z}_u,\tau,\mathrm{z}_v)))-\gamma\mathbb{E}_{v_n\sim P_{n,u}{\mathcal{V}}}[\log(\sigma(-\mathrm{DEC(z}_u,\tau,\mathrm{z}_{v_n}))],\]</span></p><p>where <span class="math inline">\(P_{n,u}(\mathcal{V})\)</span> denotes a negative sampling distribution. The 1st term denotes the log-likelihood that we predict "true" for an edge that does actually exist in the graph, and the 2nd term is the expected log-likelihood that is correctly predicted "false" for an edge that does not exist in the graph.</p><p>After approximating with Monte Carlo, the loss will be <span class="math display">\[\mathcal{L}=\sum\limits_{(u,\tau,v)\in\mathcal{E}}-\log(\sigma(\mathrm{DEC(z}_u,\tau,\mathrm{z}_v)))-\sum\limits_{v_n\in \mathcal{P}_{n,u}}[\log(\sigma(-\mathrm{DEC(z}_u,\tau,\mathrm{z}_{v_n}))],\]</span> , where <span class="math inline">\(\mathcal{P}_{n,u}\)</span> is a set of nodes sampled from <span class="math inline">\(P_{n,u}(\mathcal{V})\)</span>.</p><ul><li>How to define negative sampling distribution?<ul><li>Simply use a uniform distribution over all nodes, but this may get false negative.</li><li>Sample negative samples that satisfy a predefined type constraints.</li><li>Draw negative samples for both the head node and the tail node of the relation.</li></ul></li></ul><h4 id="max-margin-loss">Max-Margin loss</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524174533295.png" alt="image-20210524174533295" style="zoom:67%;" /></p><p>Contrastive estimation to get the negative sample.</p><h3 id="multi-relational-decoders">Multi-relational decoders</h3><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524233843533.png" alt="image-20210524233843533" style="zoom:80%;" /></p><h4 id="methods">Methods</h4><ul><li>RESCAL</li></ul><p>But it's computationally expensive.</p><ul><li>Translational Decoders<ul><li><p><em><a href="https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf">TransE</a></em>, the likelihood of an edge is proportional to the distance between the embedding of the head node and the tail node, after translating the head node according to the relation embedding. <span class="math display">\[\mathrm{DEC(z}_u,\tau,\mathrm{z}_{v})=-\|\mathrm{z}_u+\mathrm{r}_\tau-\mathrm{z}_{v}\|\]</span></p></li><li><p>Limitation: simplicity.</p></li><li><p>The variants of TransE</p><ul><li><p><a href="">TransX</a></p><p>Import a trainable transformation that depend on the relation <span class="math inline">\(\tau\)</span>. <span class="math display">\[\mathrm{DEC(z}_u,\tau,\mathrm{z}_{v})=-\|g_{1,\tau}(\mathrm{z}_u)+\mathrm{r}_\tau-g_{2,\tau}(\mathrm{z}_{v})\|\]</span></p></li><li><p><a href="https://persagen.com/files/misc/wang2014knowledge.pdf">TransH</a></p><p>Project the entity embeddings onto a learnable relation-specific hyperplane-defined by the normal vector <span class="math inline">\(\mathrm{w}_r\)</span>-before performing translation.</p></li></ul></li></ul></li><li>Multi-Liner dot products<ul><li>Also known as <a href="https://arxiv.org/abs/1412.6575">DistMult</a>.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525081148389.png" alt="image-20210525081148389" style="zoom:80%;" /></li><li>Generalizing the dot-product decoder from simple graphs.</li><li>One <strong>limitation</strong> is that it can only encode symmetric relations while there are many directed graph and thus they are asymmetric.</li></ul></li><li>Complex decoders<ul><li><p><a href="http://proceedings.mlr.press/v48/trouillon16.pdf">ComplEx</a>, augmenting the DistMult by employing complex-valued embeddings. It's defined as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525081551755.png" alt="image-20210525081551755" style="zoom:80%;" /></p></li><li><p><a href="https://arxiv.org/abs/1902.10197">RotatE</a> defies the decoder as rotations in the complex plane as:</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525082511044.png" alt="image-20210525082511044" style="zoom:80%;" /></p><p>each dimension of the relation embedding can be represented as <span class="math inline">\(\mathrm{r}_\tau[i]=e^{i\theta_{r,i}}\)</span> and thus corresponds to a rotation in the complex plane.</p></li></ul></li></ul><h4 id="representational-abilities">Representational abilities</h4><p>The Multi-relational decoders can represent different logical patterns on relations.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525083801306.png" alt="image-20210525083801306" style="zoom:80%;" /></p><ul><li>Symmetry and anti-symmetry<ul><li>Symmetric decoders: DistMult</li><li>Anti-symmetric decoders: TransE</li></ul></li><li>Inversion: implies the existence of another with opposite directionality.</li><li>Compositionality:<ul><li>RESCAL: <span class="math inline">\(\mathrm{R_{\tau_3}=R_{\tau_2}R_{\tau_1}}\)</span></li><li>TransE: <span class="math inline">\(\mathrm{r_{\tau_3}=r_{\tau_2}+r_{\tau_1}}\)</span></li></ul></li></ul><h1 id="part-ii-gnns">Part II: GNNs</h1><h2 id="chapter-5-the-gnn-model">Chapter 5 The GNN model</h2><p>GNNs are more complex than shallow embedding, they will generate representations of nodes that actually depend on the structure of the graph.</p><p>A key desideratum of GNNs is that they should be permutation invariant.</p><h3 id="neural-message-passing">Neural message passing</h3><p>After GNN, the embeddings contain structure-based information and feature-based information. However, the feature based information is in their k-hop neighborhoods.</p><p>The message passing in GNN can be taken as using update and aggregate functions.</p><h4 id="the-basic-gnn">The basic GNN</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525122930060.png" alt="image-20210525122930060" style="zoom:67%;" /></p><ul><li>The message passing in basic GNN framework is like a standard multi-layer perception.</li></ul><h4 id="with-self-loop">With self-loop</h4><ul><li>Add self-loops to omit the explicit update step.</li><li>This can alleviate overfitting, but also limit the expressivity of the GNN, sample the information coming the nodes neighbours cannot be differentiated from the information from the node itself.</li><li>In basic GNN, adding self-loops means sharing parameters between the <span class="math inline">\(\mathrm{W}_{self}\)</span> and <span class="math inline">\(\mathrm{W}_{neigh}\)</span> matrices.</li></ul><h3 id="generalized-neighborhood-aggregation">Generalized neighborhood aggregation</h3><h4 id="neighborhood-normalization">Neighborhood normalization</h4><ul><li><p>Simply normalized the aggregation operation based upon the degrees of the nodes involved: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526114126952.png" alt="image-20210526114126952" style="zoom:50%;" /></p></li><li><p>Symmetric normalization: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526114205704.png" alt="image-20210526114205704" style="zoom:50%;" /></p></li><li><p>Combining the symmetric-normalized aggregation along with the basic GNNs update function results in a first-order approximation of a spectral graph convolution.</p></li><li><p>GCNs</p><ul><li><p>They use symmetric-normalized aggregation as well as the self-loop update approach.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526114617065.png" alt="image-20210526114617065" style="zoom:67%;" /></p></li><li><p>In GNN, the use of normalization can lead to a loss of information.</p></li><li><p>Normalization is most helpful in tasks where noes feature information is far more useful than structural information, or where there is a very wide range of node degrees that can lead to instabilities during optimization.</p></li></ul></li></ul><h4 id="set-aggregation">Set aggregation</h4><p>The embeddings of neighbours, there is no natural ordering of a nodes' neighbours, and any aggregation function we define must thus be permutation invariant.</p><ul><li><p>Set pooling</p><ul><li><p>Define an aggregation function based on permutation invariant, can be implemented by just adding some MLP layers, e.g.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526122458994.png" alt="image-20210526122458994" style="zoom:67%;" /></p><p>this always lead to small increases in performance, though with the risk of overfitting.</p></li><li><p>Or use element-wise maximum or minimum to replace summation.</p></li></ul></li><li><p>Janossy pooling</p><ul><li><p>More powerful, apply a permutation-sensitive function and average the result over many possible permutations. In practice, the permutation-sensitive function is defined to be an LSTM.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526124251769.png" alt="image-20210526124251769" style="zoom:80%;" /></p></li><li><p>If the set of permutations <span class="math inline">\(\prod\)</span> is equal to all possible permutations, then the aggregator is also a universal function approximator for sets. Like simple summation based set pooling.</p></li><li><p>In practice, Janossy pooling employs one of two approaches</p><ul><li>Sample a random subsets of possible permutations during each application of the aggregator, and only sum over that random subset.</li><li>Employ a canonical ordering of the nodes in the neighborhood set, e.g., order the nodes in descending order according to their degree, with ties broken randomly.</li></ul></li><li><p>Janossy-style pooling can improve upon set pooling in a number of synthetic evaluation setups.</p></li></ul></li></ul><h4 id="neighborhood-attention">Neighborhood attention</h4><ul><li><p>Assign an attention weight or importance to each neighbour, which is used to weigh this neighbor's influence during the aggregation step.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526130931074.png" alt="image-20210526130931074" style="zoom:67%;" /></p></li><li><p>Popular variants of attention include the bilinear attention model and variations of attention layers using MLPs.</p></li><li><p>Multi heads methods are popular, which is also known as transformer.</p><ul><li>The basic idea behind transformers is to define neural network layers entirely based n the attention operation. The basic transformer layer is extractly equivalent to a GNNs layer using multi-headed attention if we assume that the GNNs receives a fully connected graph input.</li><li>The time complexity is the square of the number of nodes cause each pairs attention need to be calculated.</li><li>Attention can influence the inductive bias of GNNs.</li></ul></li></ul><h3 id="generalized-update-methods">Generalized update methods</h3><ul><li><p>One popular way is <a href="https://arxiv.org/abs/1706.02216">GraphSAGE</a>, which introduced the idea of generalized Neighborhood aggregation.</p></li><li><p>Over-smoothing of GNN: after several iterations of GNN message passing, the representations for all the nodes in the graph can become very similar to one another.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526135929759.png" alt="image-20210526135929759" style="zoom:80%;" /></p></li><li><p>The measure of how much the initial embedding of node <span class="math inline">\(u\)</span> influences the final embedding of node <span class="math inline">\(v\)</span> in the GNN is proportional to the probability of visiting node <span class="math inline">\(v\)</span> on a length-<span class="math inline">\(k\)</span> random walk starting from node <span class="math inline">\(u\)</span>.</p></li></ul><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526140143590.png" alt="image-20210526140143590" style="zoom:80%;" /></p><p>As <span class="math inline">\(k\rightarrow\infin\)</span>, the influence of every node approaches the stationary distribution of random walks over the graph, meaning that local neighborhood information is lost.</p><ul><li>When using simple GNN models, and especially those with the self-loop update approach-building deeper models can actually hurt performance.</li></ul><h4 id="concatenation-and-skip-connections">Concatenation and skip-connections</h4><p>Like using vector concatenation or skip connection that try to directly preserve information from previous rounds of message passing during the update step.</p><ul><li>Do concatenation to preserve more node-level information during message passing<ul><li>The key intuition is that we encourage the model to disentangle information during message passing--separating the information coming from the neighbors from the current representation of each model.</li></ul></li><li>Do linear interpolation between the previous representation and the representation that was updated based on the neighborhood information.</li><li>In practice, these techniques tend to be most useful for node classification tasks with moderately deep GNNs, and excel on tasks that exhibit homophily.</li></ul><h4 id="gated-updates">Gated updates</h4><p>One way to view the GNN meaning passing is that the aggregation function is receiving an observation from the neighbors, which is then used to update the hidden state of each node. Simply replace the hidden state argument of the RNN update function with the node's hidden state, and replace the observation vector with the message aggregated from the local neighborhood.</p><ul><li><a href="https://arxiv.org/abs/1511.05493">GRU based</a></li><li><a href="https://arxiv.org/abs/1802.03685">LSTM based</a></li></ul><h4 id="jumping-knowledge-connection">Jumping knowledge connection</h4><p>To improve the quality of the final node representations</p><ul><li><a href="https://arxiv.org/abs/1806.03536">Jumping knowledge connections</a>: One simple way is to simply leverage the representations at each layer of message passing, rather than only using the final layer output.</li><li>With max-pooling and LSTM attention layers the result is improved.</li></ul><h3 id="edge-features-and-multi-relational-gnns">Edge features and Multi-relational GNNs</h3><h4 id="relational-gnns">Relational GNNs</h4><ul><li><p><a href="https://arxiv.org/abs/1703.06103">RGCN</a>: relational graph convolutional network</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526152624397.png" alt="image-20210526152624397" style="zoom:67%;" /></p><ul><li>Augment the aggregation function to accommodate multiple relation types by specifying a separate transformation matrix per relation type.</li><li>The Multi-relational aggregation in RGCN is thus analogous to the basic a GNN approach with normalization, but we separately aggregate information across different edge types.</li></ul></li><li><p>Parameter sharing</p><ul><li><p>For RGCN, the increase of number of parameters is caused by that each edge type requires a trainable matrix.</p></li><li><p>To fix this, <a href="https://arxiv.org/abs/1703.06103">Schlichtkrull et al.</a> proposed to share with basis matrices, aka an alternative view of the parameter sharing RGCN approach is that we are learning an embedding for each relation, as well as a tensor that is shared across all relations.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526160241907.png" alt="image-20210526160241907" style="zoom:80%;" /></p></li></ul></li><li><p>Extensions and variations</p><ul><li>define separate aggregation matrices per relation as relational GNNs.<ul><li><a href="http://dx.doi.org/10.1093/bioinformatics/bty294">without parameter sharing</a></li><li>http://dx.doi.org/10.18653/v1/d17-1159</li><li><a href="https://arxiv.org/pdf/1911.06962.pdf">RGCN+attention</a></li></ul></li></ul></li></ul><h4 id="attention-and-feature-concatenation">Attention and feature concatenation</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526161216469.png" alt="image-20210526161216469" style="zoom:80%;" /></p><p>https://www.aclweb.org/anthology/D19-1458/</p><h3 id="graph-pooling">Graph pooling</h3><p>To get <strong>representations in graph-level</strong> cause the goal is to pool together the node embedding in order to learn an embedding of the entire graph.</p><ul><li><p>Set pooling approaches</p><ul><li>One popular way it taking a sum or mean of the node embeddings. This is sufficient for small graphs.</li><li>The other popular way is using a combination of LSTM and attention to pool the node embeddings.<ul><li>The way is like what's done is one-head bert, the output of graph-level representation is the concatenation of output from different timestep.</li></ul></li></ul></li><li><p>Graph coarsening approaches</p><p>To exploit the structure of the graph. One popular strategy to accomplish this is to perform graph clustering or coarseing as a means to pool the node representations.</p><ul><li>Estimate assignment matrices<ul><li>Use spectral clustering (the decomposition of adjacent matrix to estimate assignment matrix (from node representations to graph-level representations.))</li><li><a href="https://arxiv.org/abs/1806.08804">Employ another GNN to predict cluster assignments</a></li></ul></li><li>Use the assignment matrices to coarsen the graph.</li></ul></li></ul><h3 id="generalized-message-passing">Generalized message passing</h3><p>Leverage edge and graph-level information at each stage of message passing</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526164837669.png" alt="image-20210526164837669" style="zoom:67%;" /></p><h2 id="chapter-6-gnns-in-practice">Chapter 6 GNNs in practice</h2><p>optimization, regularization and application-based</p><h3 id="applications-and-loss-functions">Applications and loss functions</h3><ul><li><p>GNNs for node classification</p><ul><li>If fully supervised, use the negative log-likelihood loss and softmax to denote the predicted probability.</li><li>transductive and inductive nodes<ul><li>transductive nodes: nodes used while training but not covered while calculating loss. Semi-supervised means that the GNN is tested on transductive nodes.</li><li>inductive: not used in either the loss computation or the GNN message passing operations during training.</li></ul></li></ul></li><li><p>GNNs for graph classification</p><ul><li>similar loss functions but use graph-level representations.</li></ul></li><li><p>GNNs for relation prediction</p><ul><li>use the pairwise node embedding loss functions.</li></ul></li><li><p>Pretraining</p><ul><li><p>Pre-training the GNN using one of the neighborhood reconstruction losses.</p><ul><li><p>E.g., pre-train a GNN to reconstruct missing edges in the graph before fine-tuning on a node classification loss. However, <strong><a href="https://arxiv.org/abs/1809.10341">Veličković et al</a> found that a randomly initialized GNN is equally strong compared to one pre-trained on a neighborhood reconstruction loss.</strong></p></li><li><p>DGI (deep graph infomax): maximize the mutual information between node embeddings and graph embeddings. The basic idea is the GNN model must learn to generate node embeddings that can distinguish between the real graph and its corrupted counterpart.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526172943470.png" alt="image-20210526172943470" style="zoom:67%;" /></p></li></ul></li></ul></li></ul><h3 id="efficiency-concerns-and-node-sampling">Efficiency concerns and node sampling</h3><ul><li><p>Graph-level implementations</p><ul><li>Use sparse matrix multiplications and add self-loops to avoid redundant computations, but it requires operating on tested entire graph and all node features simultaneously, which may not be feasible due to memory limitations.</li></ul></li><li><p>Sampling and mini-batch</p><p>Work with a subset of nodes during message passing.</p><ul><li>The challenge is we cannot simply Run message passing on a subsets of the nodes in a graph without losing information.</li><li>One way is subsampling node neighbors. First select a set of target nodes for a batch and then to recursively sample the neighbors of these nodes in order to ensure that the connectivity of the graph is maintained. Subsample the neighbors of each node, using a fixed sample size to improve the efficiency of batched tensor operations.</li></ul></li></ul><h3 id="parameter-sharing-and-regularization">Parameter sharing and regularization</h3><p>Like L2 regularization, dropout and layer normalization that both work on GNNs and CNNs.</p><ul><li>Parameter sharing across layers<ul><li>use the same parameters in all the aggregate and update functions in the GNN. It's most effective in GNNs with more than six layers, and is often used in conjunction with gated update functions.</li></ul></li><li>Edge dropout<ul><li>randomly remove edges in the adjacency matrix during training, with the intuition that this will make the GNN less prone to overfitting and more robust to noise in the adjacency matrix.</li><li>The neighborhood subsampling approaches lead to this kind of regularization as a side effect, making it a very common strategy in large-scale GNN applications.</li></ul></li></ul><h2 id="chapter-7-theoretical-motivations">Chapter 7 Theoretical motivations</h2><h3 id="gnns-and-graph-convolutions">GNNs and graph convolutions</h3><p>Generalize the notion of convolutions to general graph structured data.</p><h4 id="convolutions-and-the-fourier-transform">Convolutions and the Fourier transform</h4><ul><li>The Fourier analysis<ul><li>The coefficients of Fourier series tell the amplitude if the complex sinusoidal component <span class="math inline">\(e^{-\frac{i2\pi}{N}k}\)</span>.</li><li>The high-frequency components have a large <span class="math inline">\(k\)</span> and vary quickly while the low-frequency components have small <span class="math inline">\(k\)</span> and vary more slowly.</li></ul></li><li>Translation equivalent: translating a signal and then convolving it by a filter is equivalent to convolving the signal and then translating the result.</li></ul><h4 id="from-time-signals-to-graph-signals">From time signals to graph signals</h4><ul><li>Each point in time <span class="math inline">\(t\)</span> is represented as a node and the edges in graph thus represent how the signal propagates</li><li>Represent operations such as time-shifts using the adjacency and Laplacian matrices of the graph.</li><li>Multiplying a signal by the adjacency matrix propagates signals from node to node, and multiplication by the Laplacian computes the difference between a signal at each node and its immediate neighbors.</li><li>shifts and convolutions on time-varying discrete signals can be represented based on the adjacency matrix and Laplacian matrix of a chain graph.</li><li>The convolution operation matrix satisfies translation equivalence.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210527233655918.png" alt="image-20210527233655918" style="zoom:80%;" />, which means that the convolved signal <span class="math inline">\(\mathrm{Q_hx}[u]\)</span> at each node <span class="math inline">\(u\in\mathcal{V}\)</span> will correspond to some mixture of the information in the node's <span class="math inline">\(N\)</span>-hop neighborhood, with the <span class="math inline">\(\alpha_0,\alpha_1,\cdots,\alpha_N\)</span> controlling the strength of the information coming from different hops. Defining <span class="math inline">\(\mathrm{Q_h}\)</span> in this way guarantees that our filter commutes with the adjacency matrix, satisfying a generalized notion of translation equivariance.</li><li><strong>By stacking multiple message passing layers, GNNs are able to implicitly operate on higher-order polynomials of the adjacency matrix.</strong></li><li>The symmetric normalized Laplacian or symmetric normalized adjacency matrix are usually taken as the convolutional filters cause<ul><li>They have bounded spectrums and thus numerically stable.</li><li>They are simultaneously diagonalizable, which means that they share the same eigenvectors.</li></ul></li></ul><h4 id="spectral-graph-convolutions">Spectral graph convolutions</h4><ul><li><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210527235621190.png" alt="image-20210527235621190" style="zoom:80%;" /></p><p>The Laplace operator (<span class="math inline">\(\Delta\)</span>) tells us the average difference between the function value at a point and function values in the neighboring regions surrounding this point. The Laplacian matrix is regarded as a discrete analog of the Laplace operator since it allows to quantity the difference between the value at a node and the values at that node's neighbors.</p></li><li><p>The eigenfunctions of Laplace operator is corresponds to the complex exponentials, which means the eigenfunctions of <span class="math inline">\(-\Delta e^{2\pi ist}\)</span> are the same complex exponentials that make up the modes of the frequency domain in the Fourier transform, with the corresponding eigenvalue indicating the frequency.</p></li><li><p>Graph convolution can be represented as polynomials of the Laplacian (or one of its normalized variants).</p></li><li><p>Given the graph Fourier coefficients <span class="math inline">\(\mathrm{U^\top f}\)</span> of a signal <span class="math inline">\(\mathrm{f}\)</span> as well as the graph Fourier coefficients <span class="math inline">\(\mathrm{U^\top h}\)</span> of some filter <span class="math inline">\(\mathrm{h}\)</span>, we can compute a graph convolution via element-wise products as <span class="math display">\[  \mathrm{f\star_\mathcal{G}h=U(U^\top f \circ U^\top h)}  \]</span> Then represent convolutions in the spectral domain based on the graph Fourier coefficients <span class="math inline">\(\theta_h=\mathrm{U^\top h \in \mathbb{R}^{|\mathcal{V}|}}\)</span>.</p></li><li><p>One way is to learn a nonparametric filter by <strong>directly optimizing <span class="math inline">\(\theta_h\)</span> (spectral filter)</strong> and defining the convolution as <span class="math inline">\(\mathrm{f\star_\mathcal{G}h=U(U^\top f \circ }\theta_h)=(\mathrm{Udiag}(\theta_h)\mathrm{U}^\top)\mathrm{f}\)</span>. But this way has no real dependency on the structure of the graph and may not satisfy many of the properties that we want from a convolution, e.g. locality.</p></li><li><p>To make sure the spectral filter <span class="math inline">\(\theta_h\)</span> is corresponds to a meaningful convolution on the graph, another way is to parameterize <span class="math inline">\(\theta_h\)</span> based on the eigenvalues of the Laplacian, e.g., a degree <span class="math inline">\(N\)</span> polynomial of the eigenvalues of the Laplacian and thus ensure the filtered signal at each node depends on information in its <span class="math inline">\(k\)</span>-hop neighborhood.</p><p><span class="math inline">\(\mathrm{f\star_\mathcal{G}h}=(\mathrm{Up_N}(\Lambda)\mathrm{U}^\top)\mathrm{f}=p_N(\mathrm{L})\mathrm{f}\)</span>.</p></li><li><p>The filter (e.g., Fourier) coefficients cannot be simply interpreted as corresponding to different frequencies.</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528101813914.png" alt="image-20210528101813914" style="zoom:80%;" /></li><li>The smallest eigenvector of the Laplacian corresponds to a signal that varies from node to node by the least amount on the graph, the second-smallest eigenvector corresponds to a signal that varies the second smallest amount and so on.</li><li>Laplacian eigenvectors can be used to assign nodes to communities so that we minimize the number of edges that go between communities. The <strong>Laplacian eigenvectors define signals that vary in a smooth way across the graph, with the smoothest signals indicating the coarse-gained community structure of the graph.</strong></li></ul></li></ul><h4 id="convolution-inspired-gnns">Convolution inspired GNNs</h4><ul><li><p>Purely convolutional approaches</p><ul><li>Directly optimize <span class="math inline">\(\theta_h\)</span> or parameterize it</li><li>Methods<ul><li><a href="https://arxiv.org/abs/1312.6203">Bruna et al.</a> : Nonparametric spectral filter and parametric spectral filter by a cubic spline.</li><li><a href="https://arxiv.org/abs/1606.09375">Defferrard et al.</a>, parameterize the spectral filter by Chebyshev polynomials. Chebyshev polynomials have an efficient recursive formulation and have various properties that make them suitable for polynomial approximation.</li><li><a href="https://arxiv.org/abs/1901.01484">Liao et al.</a>,: learn polynomials of the Laplacian based on the Lanczos algorithm.</li><li>Beyond real-valued polynomials of the Laplacian (or the adjacency matrix): employ more general parametric rational complex functions of the Laplacian.<ul><li><a href="http://dx.doi.org/10.1109/tsp.2018.2879624">Levie et al.</a>: Cayley polynomials of the Laplacian</li><li><a href="https://arxiv.org/abs/1901.01343">Bianchi et al</a>: ARMA filters.</li></ul></li></ul></li></ul></li><li><p><a href="https://arxiv.org/abs/1609.02907">GCNs</a> and connections to message passing</p><ul><li><p>A basic GCN layer is defined as <span class="math display">\[\mathrm{H}^{(k)}=\sigma (\tilde{\mathrm{A}}\mathrm{H}^{(k-1)}\mathrm{W}^{(k)}),\\\tilde{\mathrm{A}}=\mathrm{(D+I)^{-\frac{1}{2}}(I+A)(D+I)^{-\frac{1}{2}}},\]</span> where <span class="math inline">\(\mathrm{W}^{(k)}\)</span> is a learnable parameter matrix. The model was initially motivated as a combination of a simple graph convolution (based on the polynomial <span class="math inline">\(\mathrm{I+A}\)</span>), with a learnable weight matrix and a nonlinearity.</p></li><li><p>The notion of message passing can be viewed as corresponding to a simple form of graph convolutions combined with additional trainable weights and nonlinearites.</p></li><li><p>Stacking multiple rounds of message passing in a basic GNN is analogous to applying a low-pass convolutional filter, which produces a smoothed version of the input signal on the graph.</p><ul><li>The multiplication <span class="math inline">\(\mathrm{A}^K_{sym}\mathrm{X}\)</span> of the input node features by a high power of the adjacency matrix can be interpreted as convolutional filter based on the lowest-frequency signals of the graph Laplacian. Because multiplying a signal by high powers of <span class="math inline">\(\mathrm{A_{sym}}\)</span> corresponds to a convolutional filter based on the lowest eigenvalues of <span class="math inline">\(\mathrm{L_{sym}}\)</span>, i.e., it produces a low-pass filter.</li><li>The deeper, the convolution filters are simpler.</li></ul></li></ul></li><li><p>GNNs without message passing</p><ul><li><p>Simplify GNNs by removing the iterative message passing process. The models are generally defined as <span class="math display">\[\mathrm{Z=MLP}_\theta(f(\mathrm{A)MLP_\phi(\mathrm{A})})\]</span></p><ul><li><p><a href="https://arxiv.org/pdf/1902.07153.pdf">Wu et al.</a> define <span class="math inline">\(f(\mathrm{A})=\mathrm{\tilde{A}}^k\)</span>, with <span class="math inline">\(\tilde{\mathrm{A}}\)</span> is the symmetric normalized adjacency matrix.</p></li><li><p><a href="https://arxiv.org/abs/1810.05997">Klicpera et al.</a> defines <span class="math inline">\(f\)</span> by analogy to the personalized PageRank algorithm as <span class="math display">\[f(\mathrm{A})=\alpha(\mathrm{I-(1-\alpha)\tilde{A}})^{-1}=\alpha\sum\limits_{k=0}^{\infin}(\mathrm{I-\alpha\tilde{A}})^k,\]</span> cause we often do not need to interleave trainable neural networks with graph convolution layers. We can simply use neural networks to learn feature transformations at the beginning and end of the model and apply a deterministic convolution layer to leverage the graph structure. Like GAT.</p></li></ul></li><li><p>Using the symmetric normalized adjacency matrix with self-loops leads to effective graph convolutions.</p><ul><li><a href="https://arxiv.org/pdf/1902.07153.pdf">Wu et al.</a> proves that adding self-loops shrinks the spectrum of corresponding graph Laplacian by reducing the magnitude of the dominant eigenvalue.</li><li>Intuitively, adding self-loops decrease the influence of far-away noes and makes the filtered signal more dependent on local neighborhoods on the graph.</li></ul></li></ul></li></ul><h3 id="gnns-and-probabilistic-graphical-models">GNNs and probabilistic graphical models</h3><p>View the embeddings for each node as latent variables that are inferred.</p><h4 id="hilbert-space-embeddings-of-distributions">Hilbert space embeddings of distributions</h4><ul><li><p>The density <span class="math inline">\(p(\mathrm{x})\)</span> based on its expected value under the feature map <span class="math inline">\(\phi\)</span> is: <span class="math display">\[\mu_\mathrm{x}=\int_{\mathbb{R}^m}\phi(\mathrm{x})p(\mathrm{x})d\mathrm{x}\]</span> The formula will be injective under the assumption of Hilbert space embeddings of distributions. Then <span class="math inline">\(\mu_\mathrm{x}\)</span> can serve as a sufficient statistics for <span class="math inline">\(p(\mathrm{x})\)</span>. Then any computations we want to perform on <span class="math inline">\(p(\mathrm{x})\)</span> can be equivalently represented as functions of the embedding <span class="math inline">\(\mu_\mathrm{x}\)</span>. One well-known feature map <span class="math inline">\(\phi\)</span> is Gaussian radial basis function.</p></li><li><p>In the context of the connection to GNNs, the takeaway is simply that we can represent distributions <span class="math inline">\(p(\mathrm{x})\)</span> as embeddings <span class="math inline">\(\mu_\mathrm{x}\)</span> in some feature space.</p></li></ul><h4 id="graphs-as-graphical-models">Graphs as graphical models</h4><ul><li><p>The notion of dependence between nodes is viewed as a formal, probabilistic way.</p></li><li><p>Assume that a graph defines a Markov random field,</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528155046247.png" alt="image-20210528155046247" style="zoom:80%;" /></p><ul><li>Intuitively, <span class="math inline">\(\Phi(\mathrm{x}_v,\mathrm{z}_v)\)</span> indicates the likelihood of a node feature vector <span class="math inline">\(\mathrm{x}_v\)</span> given its latent node embedding <span class="math inline">\(\mathrm{z}_v\)</span>, while <span class="math inline">\(\Psi\)</span> controls the dependency between connected nodes.</li><li>Assume that node features are determined by their latent embeddings, and the latent embeddings for connected nodes are dependent on each other.</li></ul></li><li><p>GNNs will try to seek to implicitly learn the <span class="math inline">\(\Psi,\Phi\)</span> by leveraging the Hilbert space embedding idea.</p></li></ul><h4 id="embedding-mean-field-inference">Embedding mean-field inference</h4><p>The goal is to infer latent representations for all the nodes in the graph that can explain the dependencies between the observed node features.</p><ul><li><p>The key step is computing the posterior <span class="math inline">\(p(\{\mathrm{z}_v\}|\{\mathrm{x}_v\})\)</span>, i.e., computing the likelihood of a particular set of latent embeddings given the observed features. But the accurate solution is intractable, one way is to approximate it.</p></li><li><p>One way to approximate the posterior is to employ mean-field variational inference, by which the posterior is approximated as <span class="math inline">\(p(\{\mathrm{z}_v\}|\{\mathrm{x}_v\})\approx q(\{\mathrm{z}_v\})=\prod\limits_{v\in\mathcal{V}}q_v(\mathrm{z}_v)\)</span>. The key intuition in mean-field inference is that we assume that the posterior distribution over the latent variables factorizes into <span class="math inline">\(\mathcal{V}\)</span> independent distributions, one per node.</p><ul><li><p>The standard approach to solve is to minimize the KL divergence between the approximate posterior and the true posterior. But directly minimize it is impossible cause evaluating the <strong>KL divergence</strong> requires knowledge of the true posterior.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528213649200.png" alt="image-20210528213649200" style="zoom:80%;" /></p><ul><li>To minimize the KL divergence easily, one way is to use the techniques from variational inference.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528213729244.png" alt="image-20210528213729244" style="zoom:80%;" /></li><li>The approximate posterior at timestep <span class="math inline">\(t\)</span> for each latent node embedding is a function of the node's feature <span class="math inline">\(\mathrm{z}_x\)</span> and the marginal distributions (marginalized on the node's neighbors.) at previous timestep. Therefore, <strong>using variational inference to infer the posterior is really like message passing.</strong></li><li><strong>The key distinction</strong> is that the mean-field message passing equations operate over distributions rather than embeddings (what used in GNN).</li></ul></li></ul></li><li><p>Another way is try to learn embeddings in an end-to-end way, or to say rather than specifying a concrete probabilistic model, one can simply learn embeddings that could correspond to some probabilistic model.</p><ul><li><a href="https://arxiv.org/abs/1603.05629">Dai et al.</a> define <span class="math inline">\(f\)</span> in an analogous manner to a basic GNN and thus at each iteration the updated Hilbert space embedding for node <span class="math inline">\(v\)</span> is a function of its neighbors' embedding as well as its feature inputs.</li></ul></li></ul><h4 id="gnns-and-pgms-more-generality">GNNs and PGMs more generality</h4><ul><li>Different variants of message passing can be derived based on different approximate inference algorithms. <a href="https://arxiv.org/abs/1603.05629">Dai et al.</a></li><li>How GNNs can be integrated more generally into PGM models.<ul><li><a href="https://arxiv.org/abs/1905.06214">Qu et al.</a></li><li><a href="https://arxiv.org/abs/1906.02111">Zhang et al.</a></li></ul></li></ul><h3 id="gnns-and-graph-isomorphism">GNNs and graph isomorphism</h3><p>The motivation of GNNs based on connections to graph isomorphism testing.</p><h4 id="graph-isomorphism-and-representational-capacity">Graph isomorphism and representational capacity</h4><ul><li>Graph isomorphism<ul><li>The goal of graph isomorphism is to declare whether or not the given two graphs are isomorphic. If so, the graphs are essentially identical.</li><li>Formally, given adjacency matrix <span class="math inline">\(\mathrm{A}_1,\mathrm{A}_2\)</span> and node features <span class="math inline">\(\mathrm{X}_1,\mathrm{X}_2\)</span>, then two graphs are isomorphic if and only if there exists a permutation matrix <span class="math inline">\(\mathrm{P}\)</span> such that <span class="math inline">\(\mathrm{PA_1P^\top=A_2,PX_1=X_2}\)</span></li></ul></li><li>The challenges of graph isomorphism<ul><li>The simple definition</li><li>Testing for graph isomorphism<ul><li>A naive approach to test for isomorphism would involve the optimization problem<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528231925373.png" alt="image-20210528231925373" style="zoom:60%;" /> with the computation complexity <span class="math inline">\(\mathcal{O}(|V|!)\)</span>.</li><li>Therefore, this is regarded as NP-indeterminate. No general polynomial time algorithms are known for this problem.</li></ul></li></ul></li><li>Graph isomorphism and representational capacity<ul><li>Graph isomorphism gives a way to quantify the representational power of different learning approaches. E.g., evaluating the power by asking how useful the representations would be for testing graph isomorphism.</li><li>In practice, no representation learning algorithm is going to be “perfect".</li></ul></li></ul><h4 id="the-weisfieler-lehman-algorithm">The Weisfieler-Lehman algorithm</h4><ul><li><p>The steps of 1-WL</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528233109761.png" alt="image-20210528233109761" style="zoom:80%;" /></p></li><li><p>The WL algorithm is known to converge in at most <span class="math inline">\(|V|\)</span> iterations and is known to known to successfully test isomorphism for a broad class of graph.</p></li><li><p>WL may fail for some graphs e.g. when the graph consists of multi disconnected subgraphs.</p></li></ul><h4 id="gnns-and-the-wl-algorithm">GNNs and the WL algorithm</h4><ul><li><p>GNNs aggregate and update node embeddings using NNs while WL aggregates and updates discrete labels.</p></li><li><p>Formally, the relation between GNNs and WL is</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528233713630.png" alt="image-20210528233713630" style="zoom:80%;" /></p><p>It tells that GNNs are no more powerful than the WL algorithm when we have discrete information as node features.</p></li><li><p>If the WL algorithm assigns the same label to two nodes, then any message-passing GNN will also assign the same embedding to these two nodes.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528234128699.png" alt="image-20210528234128699" style="zoom:80%;" /></p></li><li><p>If we define the message passing updates as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529080919406.png" alt="image-20210529080919406" style="zoom:80%;" /></p><p>then this GNN is sufficient to match the power of the WL algorithm.</p></li><li><p>To make GNN is as powerful as WL, the aggregate and update function need to be injective, which means that they need to map every unique input to a unique output value. But this cannot be satisfied usually if the aggregate functions are using a average of the neighbor embeddings.</p></li><li><p>GIN (graph isomorphism network) has few parameters but is still as powerful as the WL algorithm.</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529084928011.png" alt="image-20210529084928011" style="zoom:80%;" /></li></ul></li><li><p>In summary, MP-GNNs (message passing GNNs) are no more powerful than the WL algorithm.</p></li></ul><h4 id="beyond-the-wl-algorithm">Beyond the WL algorithm</h4><ul><li><p>Relational pooling</p><p>Considering the failure cases of the WL algorithm. Message passing approaches generally fail to identify closed triangles in a graph, which is a critical limitation.</p><ul><li>To address the limitation, Murphy et al. consider MP-GNNs with unique node ID features. They simply add a unique, one-hot indicator feature (node ID) for each node.<ul><li>However, this doesn't solve the problem and rather import a new and equally problematic issue that MP-GNNs is no longer permutation equivariant since the adding of node IDs. Specifically, assigning a unique ID to each node fixes a particular node ordering for the graph, which breaks the permutation equivariance.</li></ul></li><li>The other way is <a href="https://arxiv.org/abs/1903.02541">Relational Pooling (PR) approach</a>, which involves marginalizing over all possible node permutations. In practice, it will sum over all possible permutation matrices recovers the permutation invariance.<ul><li>The limitation are<ul><li>its computation complexity.</li><li>We have no way to characterize how much more powerful that PR-GNNs are.</li></ul></li><li>But RP can achieve strong results using various approximation to decrease the computation cost.</li></ul></li></ul></li><li><p>The <span class="math inline">\(k\)</span>-WL test and <span class="math inline">\(k\)</span>-GNNs</p><p>Improving GNNs by adapting generalizations of the WL algorithm.</p><ul><li><span class="math inline">\(k\)</span>-WL works on subgraphs in size <span class="math inline">\(k\)</span>, which can be used to test graph isomorphism by comparing the multi-sets for two graphs.<ul><li>It introduces a hierarchy of representation capacity. For any <span class="math inline">\(k\ge2\)</span> we have that the <span class="math inline">\((k+1)\)</span>-WL test is strictly more powerful than the <span class="math inline">\(k\)</span>-WL test.</li><li>To intimate this, <a href="http://dx.doi.org/10.1609/aaai.v33i01.33014602">Morris et al</a> develop a <span class="math inline">\(k\)</span>-GNN that is a differentiable and continuous analog of the <span class="math inline">\(k\)</span>-WL algorithm. They learn embeddings associated with subgraphs, rather than noes, and the message passing occurs according to subgraph neighborhoods.</li></ul></li><li>Graph kernel methods based on the <span class="math inline">\(k\)</span>-WL test.</li></ul></li><li><p><a href="https://arxiv.org/abs/1905.11136">Invariant and Equivariant <span class="math inline">\(k\)</span> -order GNNs</a></p><ul><li>MP-GNNs are equivariant to node permutations. And permuting the input to an MP-GNNs simply results in the matrix of output node embeddings being permuted in an analogous way.</li><li>MP-GNNs can also be permutation invariant at the graph level. The pooled graph-level embedding does not change when different node orderings are used.</li><li><a href="https://arxiv.org/abs/1905.11136">Maron et al</a> propose a general form of GNN-like models based on permutation equivariant/invariant tensor operations.<ul><li>For a given input, both equivariant and invariant linear operators on this input will correspond to tensors that satisfy the fixed point in <span class="math inline">\(\mathrm{P}\star\mathcal{L}=\mathcal{L},\forall \mathrm{P}\in \mathcal{P}\)</span>, but the number of channels in the tensor will differ depending on whether it is an equivariant or invariant operator.</li><li>The fixed point can be constructed as a linear combination of a set of fixed basis elements.</li><li>The equivariant linear layers involve tensors that have up to <span class="math inline">\(k\)</span> different channels.</li><li>Constructing <span class="math inline">\(k\)</span>-order invariant models for <span class="math inline">\(k&gt;3\)</span> is generally computationally intractable. The built <span class="math inline">\(k\)</span>-order GNNs are equally powerful as the <span class="math inline">\(k\)</span>-WL algorithm.</li></ul></li></ul></li></ul><h1 id="part-iii-generative-graph-models">Part III Generative Graph models</h1><h2 id="chapter-8-traditional-graph-generation-approaches">Chapter 8 Traditional graph generation approaches</h2><p>The goal of graph generation is to build models that can generate realistic graph structures. The key challenge in graph generation is generating graphs that have certain desirable properties.</p><p>Traditional approaches to graph generation generally involve specifying some kind of generative process, which defines how the edges in a graph are created.</p><p>A more through survey and discussion is <a href="https://global.oup.com/academic/product/networks-9780198805090?cc=us&amp;lang=en&amp;">Newman's (1,12,13,108,109).</a></p><h3 id="erdos-renyi-model">Erdos-Renyi model</h3><ul><li>ER model may be the simplest and most well-known generative model of graphs. It simply assumes that the probability of an edge occurring between any pairs of nodes is equal to <span class="math inline">\(r\)</span>.</li><li>To generate a random ER graph, just simply choose how many nodes we want, set the density parameter <span class="math inline">\(r\)</span> and then use equation to generate the adjacency matrix.</li><li>The downside of the ER model is that it doesn't generate very realistic graphs. The graph properties like degree distribution, existence of community structures, node clustering coefficients and tensors occurrence of structural motifs are not captured.</li></ul><h3 id="stochastic-block-models">Stochastic block models</h3><ul><li>SBMs seek to generate graphs with community structure.</li><li>SBMs are based on blocks, every node has a probability that it belongs to block <span class="math inline">\(i\)</span>, edge probabilities are defined by a block-to-block probability matrix. To generate graph, for each node assign a class (block) by sampling and then sample edges for each pair of nodes.</li><li>By controlling the edge probabilities within and between different blocks, one can generate graphs that exhibit community structure.</li><li>The nodes have a probability <span class="math inline">\(\alpha\)</span> of having an Eden with another node that assigned to the same community and a smaller probability of having an Eden with another node that is assigned to a different community.</li><li>The variations including approaches for bipartite graphs, graphs with node features, as well as approaches to infer SBMs parameters from data.</li><li>However, SBMs is limited in that it fails to capture the structural characteristic of individuals nodes that are present in most real-world graphs. In SBMs, the structure of individual communities is relatively homogeneous in that all the nodes have similar structural.</li></ul><h3 id="preferential-attachment">Preferential attachment</h3><ul><li><p>Preferential attachment (PA) attempts to capture the inhomogeneous of communities (real-world degree distributions). It's built based on the assumption that many real-world graphs exhibit power-law degree distributions.</p></li><li><p>The power law distributions are heavy tailed, which means that a probability distribution goes to zero for extreme values slower than an exponential distribution. It also means that there is a large number of nodes with small degrees but also have a small number of nodes with extremely large degrees.</p></li><li><p>The steps of PA:</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529144054529.png" alt="image-20210529144054529" style="zoom:80%;" /></p></li><li><p>The key idea is that the PA model connects new nodes to existing nodes with a probability that is proportional to the existing nodes' degrees.</p></li><li><p>The generation process of PA is autoregressive, the edge probabilities are defined on an iterative approach.</p></li></ul><h3 id="traditional-applications">Traditional applications</h3><p>Historically, the methods introduced above have been used in two key applications:</p><ul><li>Generating synthetic data for benchmarking and analysis tasks</li><li>Creating Null models<ul><li>We can investigate the extent to which different graph characteristics are probable under different generative models.</li></ul></li></ul><p>The traditional approaches can generate graphs, but they lack the ability to learn a generative model from data.</p><h2 id="chapter-9-deep-generative-models">Chapter 9 Deep generative models</h2><p>Focus on the simple and general variants of VAEs, GANs and autoregressive models. All below will only focus on generating graph structure.</p><h3 id="variational-autoencoder-approaches">Variational autoencoder approaches</h3><ul><li><p>Under VAEs, the key idea behind can be summarized as : the goal is to train a probabilistic decoder model from which one can sample realistic graphs by conditioning on a latent variable. Or to say, the goal is to learn a conditional distribution over adjacency matrices.</p></li><li><p>The target is the decoder that generate graph from latent variable, but encoder and decoder are trained together.</p></li><li><p>The components that required:</p><ul><li><p>A probabilistic encoder model <span class="math inline">\(q_\phi\)</span></p><p>It takes a graph as input. Generally , in VAEs the representation trick with Gaussian random variables is used to design this function.</p></li><li><p>A probabilistic decoder model <span class="math inline">\(p_\theta\)</span></p><p>The decoder takes a latent representation as input and uses this input to specify a conditional distribution over graphs. Specifically, it defines a conditional distribution over the entries of the adjacency matrix.</p></li><li><p>A prior distribution over the latent space</p><p>Usually a standard Gaussian prior is used.</p></li></ul></li><li><p>With these components, the loss is minimizing the evidence likelihood lower bound (ELBO)</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529181330038.png" alt="image-20210529181330038" style="zoom:80%;" />, with the basic idea that seek to maximize the reconstruction ability of the decoder.</p><ul><li>The motivation behind the ELBO is rooted in the theory of variational inference.</li><li>The goals will be satisfied under this optimization method<ul><li>The sampled latent representations encode enough information to allow our decoder to reconstruct the input.</li><li>The latent distribution is as close as possible to the prior. It's important if one wants to generate new graphs after training : they can generate new graphs by sampling from the prior and feeding these latent embeddings to the decoder.</li></ul></li></ul></li></ul><h4 id="node-level-latent">Node-level latent</h4><p>Encoding and decoding graphs based on node embeddings. The key idea is that the encoder generates latent representations for each node in the graph.</p><p>As a generative model, the node-level method is limited.</p><ul><li><a href="https://arxiv.org/pdf/1611.07308.pdf%5D">Kipf et al.</a> proposed VGAE (variational graph autoencoder)<ul><li>Encoder model: can be based on any of the GNN architectures. In particular, two GNNs are used to generate mean (for each node in the input graph) and variance parameters separately. Once they are computed, tensors set of latent node embeddings can be sampled.</li><li>Decoder model: predict the likelihood of all the edges in the graph, given a matrix of sampled node embeddings. In VGAE, thy use a dot-product decoder.</li><li>The reconstruction error is a binary cross-entropy over the edge probabilities.</li></ul></li><li>Limitations: It's limited especially when a dot-product decoder is used. The decoder has no parameters, so the model is not able to generate non-trivial graph structures without a training graph as input.</li><li><a href="http://proceedings.mlr.press/v97/grover19a.html">Grover et al.</a> propose to augment the decoder with an iterative GNN based decoder.</li></ul><h4 id="graph-level-latent">Graph-level latent</h4><p>The encoder and decoder functions are modified to work with graph-level latent representations.</p><ul><li>Encoder model: It can be an arbitrary GNN model augmented with a pooling layer. Again there are two separate GNNs to parameterize the mean and variance of a posterior normal distribution over latent variables.</li><li>Decoder model: One original Graph-VAE model proposed to combine a basic MLP with a Bernoulli distributional assumption. Simply independent Bernoulli distribution for each edge, and the overall log-likelihood objective is equivalent to set of independent binary cross-entropy loss function on each edge.<ul><li>The challenges while implementing with MLP:<ul><li>Have to assume a fixed number of nodes</li><li>We don't know the correct ordering of the rows and columns in <span class="math inline">\(\tilde{A}\)</span> when we are computing the reconstruction loss.</li></ul></li></ul></li><li>Limitations: using graph-level latent representations introduces the issue of specifying node orderings. And with MLP, currently limits the application of the basic graph-level VAE to small graphs with hundred of nodes or less.</li></ul><h3 id="adversarial-approaches">Adversarial approaches</h3><p>VAE suffers from serious limitations--such as the tendency for VAEs to produce blurry outputs in the image domain.</p><ul><li>Some works<ul><li><a href="https://arxiv.org/pdf/1805.11973.pdf">De Cao and Kipf et al.</a> propose one that is similar to the graph-level VAE.<ul><li>The generator is a MLP that generates a matrix of edge probabilities given a seed vector.</li><li>The discrete adjacency matrix is generated by sampling independent Bernoulli variables for each edge.</li><li>The discriminator employ any GNN-based graph classification model.</li></ul></li></ul></li><li>Benefits: GAN-based methods remove the complication of specifying a node ordering in the loss computation as long as the discriminator model is permutation invariant.</li><li>Limitations : GAN-based approaches to graph generation have so far received less attention and success than their variational counterparts.</li></ul><h3 id="autoregressive-methods">Autoregressive methods</h3><p>Both VAE-based approaches and basic GANs that discussed before use simple MLPs to generate adjacency matrices. Autoregressive methods can decode graph structures from latent representations, they will combine GANs and VAEs.</p><h4 id="modeling-edge-dependencies">Modeling edge dependencies</h4><p>Previous it's assumed that the edges are independent for convenience, but this is not true in real world.</p><ul><li><p>In autoregressive model, it's assumed that edges are generated sequentially and that the likelihood of each edge can be conditioned on the edges that have been previously generated.</p></li><li><p>Assume the lower-triangular portion of the adjacency matrix <span class="math inline">\(\mathrm{A}\)</span> is denoted as <span class="math inline">\(\mathrm{L}\)</span>.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210530073545690.png" alt="image-20210530073545690" style="zoom:80%;" /></p></li></ul><h4 id="recurrent-models-for-graph-generation">Recurrent models for graph generation</h4><ul><li><a href="https://arxiv.org/abs/1802.08773">GraphRNN</a> : use a hierarchical RNN to model the edge dependencies.<ul><li>The 1st RNN in this model is a graph-level RNN that is used to model the state of the graph that has been generated so far. It involves a stochastic sampling process to generate the discrete edges. In this way, the graph RNN is able to generate diverse samples of graphs even when a fixed initial embedding is used.</li><li>The 2nd RNN, termed a the node-level RNN, generates the entries of <span class="math inline">\(\mathrm{L}\)</span> lower triangular adjacency matrix in an autoregressive manner. It will take the graph-level hidden state as its input and then sequentially generate the binary values of <span class="math inline">\(\mathrm{L}\)</span>, assuming a conditional Bernoulli distribution for each entry.</li><li>The node level RNN is initialized at each time-step with the current hidden state of the graph-level RNN.</li><li>Both RNNs can be optimized to maximize the likelihood of the training graphs using the teaching forcing strategy, meaning that the ground truth of <span class="math inline">\(\mathrm{L}\)</span> are always used to update the RNNs during training. But computing the likelihood requires the assumption that a particular a ordering over the generated nodes.</li><li>It's more capable of generating grid-like structures, compared to the basic graph-level VAE.</li><li>Limitations: It still generates unrealistic artifacts (e.g. long chains) when trained on samples of grids. It can be difficult to train and scale to large graphs due to the need to backpropagate through many steps of RNN recurrence.</li></ul></li><li><a href="https://arxiv.org/abs/1910.00760">GRAN</a>: generate graphs by using a GNN to condition on the adjacency matrix that has been generated so far. GRAN models dependencies between edges. It maintains the autoregressive decomposition of the generation process.<ul><li>It uses GNNs to model the autoregressive generation process.</li><li>One can model the conditional distribution of each row of the adjacency matrix by running a GNN on the graph that has been generated so far.</li><li>Since there are no node attributes associated with the generated nodes, the input feature matrix <span class="math inline">\(\tilde{\mathrm{X}}\)</span> to the GNN can simply contain randomly sampled vectors.</li><li>The key benefit of GRAN compared with GraphRNN is that it does not need to maintain a long and complex history in a graph-level RNN.</li><li>To use GRAN on large graphs, one improvement is that multiple nodes can be added simultaneously in a single block rather than adding nodes one at a time.</li></ul></li></ul><h3 id="evaluating-graph-generation">Evaluating graph generation</h3><p>Quantitatively compare the different models introduced previouly?</p><ul><li>Currently is to analyze different statistics of the generated graphs and to compare the distribution of statistics for the generated graphs to a test set.</li><li>Compute the distance between the statistic's distribution on the test graph and generated graph using a distributional measure, such as the total variation distance.</li><li>The statistics that are used including degree distributions, graphlets counts, and spectral features with distributional distances computed using variants of the total variation score and the 1st Wassertein distance.</li></ul><h3 id="molecule-generation">Molecule generation</h3><ul><li>The goal of molecule generation is to generate molecular graph structures that are both valid (e.g., chemically stable) and ideally have some desirable properties (e.g., medicinal properties or solubility).</li><li>Domain-specific knowledge for both model design and evaluation.</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Building models that can infer latent graph structures beyond the input graph that we are given is a critical direction for pushing forward graph representation learning.</li><li>Message-passing GNNs are inherently bounded by the WL isomorphism test. They suffer from over-smoothing, being limited to simple convolutional filters, and being restricted to tree-structured computation graphs.</li></ul><h3 id="section"></h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Book &lt;a href=&quot;https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf&quot;&gt;Graph Representation Learning&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
  </entry>
  
  <entry>
    <title>Paper--3D photography on your desk</title>
    <link href="http://yoursite.com/posts/notes/2021-03-04-notes-paper-cv-3dshape.html"/>
    <id>http://yoursite.com/posts/notes/2021-03-04-notes-paper-cv-3dshape.html</id>
    <published>2021-03-05T03:35:00.000Z</published>
    <updated>2021-04-28T23:14:51.990Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.2656&amp;rep=rep1&amp;type=pdf">3D photography on your desk, 1998</a>, <a href="http://www.vision.caltech.edu/bouguetj/ICCV98/">website</a></p><p>Reference: <a href="https://ieeexplore.ieee.org/abstract/document/1087109?casa_token=RZ6-EVaSgrIAAAAA:QwrjPXnkBwNtWBQR_RsNxixV2Q7NZ4qj9DFuPG4PkI3rm8J_6NZQUq4spi3Op-xpmGSEwCNuyA">A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses</a></p><a id="more"></a><h1 id="reference">Reference</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/1087109?casa_token=RZ6-EVaSgrIAAAAA:QwrjPXnkBwNtWBQR_RsNxixV2Q7NZ4qj9DFuPG4PkI3rm8J_6NZQUq4spi3Op-xpmGSEwCNuyA">A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses</a></p><h2 id="camera-calibration">Camera calibration</h2><h3 id="types">Types</h3><ol type="1"><li><p><strong>2D image coordinates to 3D information</strong></p><ul><li><p>The Types of 3D information to be inferred</p><ul><li><p>3D information concerning the location of the object, target, or feature.</p><p>Camera calibration will offer a way of determining a ray in 3D space that the object point must lie on.</p></li><li><p>3D information concerning the position and orientation of the moving camera relative to the target world coordinate system.</p><p>Like robot camera.</p></li></ul></li></ul></li><li><p><strong>3D information to 2D image coordinates</strong></p><ul><li>If given hypothetical 3D location of the object, the 2D image coordinates can be estimated.</li></ul></li></ol><h3 id="requirement">Requirement</h3><p>autonomous, accurate, reasonably efficient, versatile, need only common off-the-shelf camera and lens</p><h3 id="previous">Previous</h3><h4 id="full-scale-nonlinear-optimization">Full-scale nonlinear optimization</h4><ul><li>Advantage: allows easy adaption of any arbitrarily accurate yet complex model for imaging.</li><li>Problems: the requirement of a good initial guess and computer-intensive full-scale nonlinear search</li><li>Approaches<ul><li>Classical approach: accurate cause the large number of unknowns and images in high resolution from rather than solid-state image array like CCD</li><li>Direct linear transformation (DLT): only using linear equations, but pure DLT only works fine without lens distortion. DLT confirms that low-resolution images from like CCD can also be used for accurate calibration.</li><li>Sobel, Gennery, Lowe<ul><li>Sobel: nonlinear equation, 18 parameters</li><li>Gennery: iteratively by minimizing the error of epipolar constraints without using 3D coordinates of calibration points. But it's error-prone.</li></ul></li></ul></li></ul><h4 id="computing-perspective-transformation-matrix-using-linear-equation-solving">Computing perspective transformation matrix using linear equation solving</h4><ul><li>Advantage : no requirement of nonlinear optimization</li><li>Problems: Cannot take lens distortion into consideration; the number of unknowns in linear equations is much larger than the real DoF. If lens distortion is not considered, then the perspective matrix can be solved by OLS.</li><li>If the field of view is narrow and the object distance is large, then ignoring distortion should cause more error.</li></ul><h4 id="two-plane-method">Two-plane method</h4><ul><li>Advantage : only linear equations need be solved</li><li>Problems: the number of unknowns is much larger than DoF; the formula used between 2D and 3D is empirically.</li><li>No restrictions needed for the extrinsic camera parameters, but the relative orientation between the camera coordinate system and the object world coordinate system is required. The nonlinear leans distortion theoretically cannot be corrected.</li></ul><h4 id="geometric-technique">Geometric technique</h4><ul><li>Advantage : no linear search is needed</li><li>Problems: no lens distortion be carried; the requirement of focal length, uncertainty of image scale factor is not allowed</li></ul><h2 id="two-stage-calibration">Two-stage calibration</h2><h3 id="goal">Goal</h3><ul><li>Reduce the number of parameters that need to be estimated by applying a constraint. The constraint is <strong><em>radial alignment constraint.</em></strong></li><li>Radial alignment constraint: a function of the relative rotation and translation between the camera and the calibration points.</li><li>The single-plane calibration points are used so the plane must be parallel to image plane.</li><li>If the DLT-type linear approximation is used, the distortion cannot be ignored unless a very narrow angle lens is used.</li></ul><h3 id="camera-model">Camera model</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306222342885.png" alt="image-20210306222342885" style="zoom:30%;" /></p><ol type="1"><li><p>Rigid body transformation from the object world coordinate system <span class="math inline">\((x_w, y_w, z_w)\)</span> to the camera 3D coordinate system $(x, y, z) $. <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306222752281.png" alt="image-20210306222752281" style="zoom:25%;" />. <span class="math inline">\(R,T\)</span> need to be calibrated. Translation is calibrated before rotation.</p><p><strong>However, how can one know <span class="math inline">\((x_w, y_w, z_w)\)</span>?</strong></p></li><li><p>Transformation from 3D camera coordinate<span class="math inline">\((x, y, z)\)</span> to ideal (undistorted) image coordinate <span class="math inline">\((X_u, Y_u)\)</span>. <span class="math inline">\(X_u=f\frac{x}{z},Y_u=f\frac{y}{z}\)</span>.</p></li><li><p>Calibrate radial lens distortion <span class="math inline">\(k_1,k_2\)</span>. Experimentally only one <span class="math inline">\(k\)</span> of radial lens distortion will work fine. The more will cause numerical instability.</p><p><span class="math inline">\(X_d(1+k_1r^2+k_2r^4+\cdots)=X_u,\\Y_d(1+k_1r^2+k_2r^4+\cdots)=Y_u,\\r=\sqrt{X_d^2+Y_d^2}\)</span>.</p></li><li><p>Real image coordinate <span class="math inline">\((X_d,Y_d)\)</span> to computer image coordinate <span class="math inline">\((X_f,Y_f)\)</span>. <span class="math inline">\(S_x\)</span> is gonna be calibrated.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306224649098.png" alt="image-20210306224649098" style="zoom:40%;" /></p></li></ol><p>Get all four steps together and suppose <span class="math inline">\(C_x=0，C_y=0\)</span>, the final formula between image coordinates <span class="math inline">\((X,Y)\)</span> and real world 3D coordinates <span class="math inline">\((x_w,y_w,z_w)\)</span> is</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306231044495.png" alt="image-20210306231044495" style="zoom:40%;" />.</p><p>Now that given image coordinates <span class="math inline">\((X,Y)\)</span> and real world 3D coordinates <span class="math inline">\((x_w,y_w,z_w)\)</span> , the camera can be calibrated.</p><h3 id="implementation-by-a-monoview-coplanar-set-of-points">Implementation by a monoview coplanar set of points</h3><p>Before started, make sure <span class="math inline">\((x_w,y_w,z_w)\)</span> is out of the field view and not close to the <span class="math inline">\(y\)</span> axis so as to avoid <span class="math inline">\(T_y=0\)</span>.</p><ol type="1"><li><p>Compute 3D Orientation , Position (<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>) and sclerosis factor</p><ul><li><p>Compute the distorted image coordinates <span class="math inline">\((X_d,Y_d)\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307113715726.png" alt="image-20210307113715726" style="zoom:50%;" />, where <span class="math inline">\(C_x,C_y\)</span> are supposed as the center of the image frame (aka they cannot be calibrated), <strong><span class="math inline">\(s_x\)</span> is not calibrated here but from a priori.</strong></p></li><li><p>Compute the five unknowns <span class="math inline">\(T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307115018316.png" alt="image-20210307115018316" style="zoom:33%;" />, this requires at least 6 points.</p></li><li><p>Compute <span class="math inline">\((r_1,\cdots,r_9,T_x,T_y)\)</span> from <span class="math inline">\((T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5)\)</span></p><ul><li><p>Compute <span class="math inline">\(|T_y|\)</span> from <span class="math inline">\((T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5)\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307115902149.png" alt="image-20210307115902149" style="zoom:40%;" /></p></li><li><p>Determine the sign of <span class="math inline">\(T_y\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307121434690.png" alt="image-20210307121434690" style="zoom:40%;" /></p><p>This sign reversal of <span class="math inline">\(T_y\)</span> causes <span class="math inline">\((x,y)\)</span> to become <span class="math inline">\(-(x,y)\)</span>. But because <span class="math inline">\(X_d,x\)</span> have the same sign, <span class="math inline">\(Y_d, y\)</span> have the same sign, then <strong>only one of the two signs for <span class="math inline">\(T_y\)</span> is valid</strong> .</p></li><li><p>Compute the 3D rotation <span class="math inline">\(R\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122232570.png" alt="image-20210307122232570" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122414379.png" alt="image-20210307122414379" style="zoom: 50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122549000.png" alt="image-20210307122549000" style="zoom:50%;" /></p></li></ul><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122644888.png" alt="image-20210307122644888" style="zoom:50%;" /></p></li></ul></li><li><p>Compute effective focal length, distortion coefficients and <span class="math inline">\(z\)</span> position.</p><p>Here, <span class="math inline">\(k_1=0\)</span>.</p><ul><li><p>Compute an approximation of <span class="math inline">\(f, T_z\)</span> by ignoring lens distortion.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122549000.png" alt="image-20210307122549000" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122644888.png" alt="image-20210307122644888" style="zoom:50%;" /></p></li><li><p>Compute exact solution for <span class="math inline">\(f,T_z,k_1\)</span></p><p>Solve <span class="math inline">\((8b)\)</span>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307124032524.png" alt="image-20210307124032524" style="zoom:50%;" />, with <span class="math inline">\(f,T_z,k_1\)</span> as unknowns using standard optimization scheme such as steepest descent. The approximation of <span class="math inline">\(f,T_z\)</span> in previous step can be used as initial guess, and the initial guess of <span class="math inline">\(k_1\)</span> can be taken as zero.</p></li></ul></li></ol><h3 id="implementation-using-monoview-noncoplanar-points">Implementation using monoview noncoplanar points</h3><p>When <span class="math inline">\(s_x\)</span> is unknown . Now a coplanar set of calibration points is required. Now <span class="math inline">\(z_w\)</span> is no longer identical zero.</p><ol type="1"><li><p>Compute 3D Orientation , Position (<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>) and scale factor</p><ul><li><p>Compute the distorted image coordinates <span class="math inline">\((X_d,Y_d)\)</span> <span class="math display">\[X_{di}={d_x}&#39;(X_{fi}-C_x)\\Y_{di}=d_y(Y_{fi}-C_y)\]</span> where <span class="math inline">\(C_x,C_y\)</span> are supposed as the center of the image frame (aka they cannot be calibrated), <strong>here, the real <span class="math inline">\(s_x\)</span> is absorbed into the unknowns for the liner equation in equations below.</strong></p></li><li><p>Compute the seven unknowns <span class="math inline">\(T_y^{-1}s_xr_1,T_y^{-1}s_xr_2,T_y^{-1}s_xr_3,T_y^{-1}s_xT_x,T_y^{-1}s_xr_4,T_y^{-1}s_xr_5,T_y^{-1}s_xr_6\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307132957951.png" alt="image-20210307132957951" style="zoom:30%;" />, this requires at least 8 points.</p></li><li><p>Compute <span class="math inline">\((r_1,\cdots,r_9,T_x,T_y)\)</span> from <span class="math inline">\(T_y^{-1}s_xr_1,T_y^{-1}s_xr_2,T_y^{-1}s_xr_3,T_y^{-1}s_xT_x,T_y^{-1}s_xr_4,T_y^{-1}s_xr_5,T_y^{-1}s_xr_6\)</span></p><ul><li><p>Compute <span class="math inline">\(|T_y|\)</span> from <span class="math inline">\((T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5)\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133504840.png" alt="image-20210307133504840" style="zoom:50%;" /></p></li><li><p>Determine the sign of <span class="math inline">\(T_y\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307121434690.png" alt="image-20210307121434690" style="zoom:40%;" /></p><p>This sign reversal of <span class="math inline">\(T_y\)</span> causes <span class="math inline">\((x,y)\)</span> to become <span class="math inline">\(-(x,y)\)</span>. But because <span class="math inline">\(X_d,x\)</span> have the same sign, <span class="math inline">\(Y_d, y\)</span> have the same sign, then <strong>only one of the two signs for <span class="math inline">\(T_y\)</span> is valid</strong> .</p></li><li><p><strong><em>Determine s_x</em></strong>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133615299.png" alt="image-20210307133615299" style="zoom:33%;" /></p></li><li><p>Compute the 3D rotation <span class="math inline">\(R\)</span></p><p>​ <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133750413.png" alt="image-20210307133750413" style="zoom:50%;" />, <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133819256.png" alt="image-20210307133819256" style="zoom:50%;" /></p></li></ul></li></ul></li><li><p>Compute effective focal length, distortion coefficients and <span class="math inline">\(z\)</span> position.</p><p>Here, <span class="math inline">\(k_1=0\)</span>.</p><ul><li><p>Compute an approximation of <span class="math inline">\(f, T_z\)</span> by ignoring lens distortion.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122549000.png" alt="image-20210307122549000" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122644888.png" alt="image-20210307122644888" style="zoom:50%;" /></p></li><li><p>Compute exact solution for <span class="math inline">\(f,T_z,k_1\)</span></p><p>Solve <span class="math inline">\((8b)\)</span>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307124032524.png" alt="image-20210307124032524" style="zoom:50%;" />, with <span class="math inline">\(f,T_z,k_1\)</span> as unknowns using standard optimization scheme such as steepest descent. The approximation of <span class="math inline">\(f,T_z\)</span> in previous step can be used as initial guess, and the initial guess of <span class="math inline">\(k_1\)</span> can be taken as zero.</p></li></ul></li></ol><h1 id="paper">Paper</h1><p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.2656&amp;rep=rep1&amp;type=pdf">3D photography on your desk, 1998</a>, <a href="http://www.vision.caltech.edu/bouguetj/ICCV98/">website</a></p><h2 id="why">Why?</h2><ul><li>Knowing the 3D shape helps a lot, and the progress and computers and computer graphics also encourage the recovering of 3D shape</li><li>Previous<ul><li>commercial 3D scanners: <strong>accurate</strong> but expensive and bulky. Use motorized transport of temporal object and active (laser, LCD projector) lighting of the scene</li><li>Passive cues contain information on 3D shape: stereoscopic disparity, texture, motion parallax, defocus, shadows, shading and specularities, occluding contours and other surface discontinuities amongst them. Stereoscopic disparity is popular way but it suffers from the requirement of two cameras and failing on untextured faces</li></ul></li></ul><h2 id="goals">Goals</h2><p>Simple and inexpensive approach for extracting the 3D shape of objects</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Illuminate the camera (facing the object) by desk-lamp, then the user moves a pencil in front of the light source casting a moving shadow on the object, the 3D shape of object will be recovered by the spatial and temporal location of the observed shadow.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210304224912139.png" alt="image-20210304224912139" style="zoom:30%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210305085918925.png" alt="image-20210305085918925" style="zoom:40%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307200112887.png" alt="image-20210307200112887" style="zoom: 33%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307200146056.png" alt="image-20210307200146056" style="zoom:33%;" /></p><h3 id="implementation">Implementation</h3><h4 id="calibration"><strong>Calibration</strong></h4><ul><li><p><strong>Camera calibration</strong></p><ul><li><p>recover the intrinsic camera parameters (<span class="math inline">\(f,K,s_x,(u_0,v_0)\)</span>) and the location of the desk plane with respect to camera.</p></li><li><p>The method is from <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1087109">here</a>. But because the object used is planar, the optical center cannot be calibrated exactly, so the optical center is assumed to be the center of the image.</p><ul><li><p>Specifically, this method considering <strong>lens distortion</strong>. And the steps for calibration is</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210305220451049.png" alt="image-20210305220451049" style="zoom:67%;" /></p><ul><li><p><em>Why there are lens distortion</em> ? On some cheap camera, the captured pixel on image is rectangle but not square. This will lead to lens distortion. Under lens distortion, suppose the image point under ideal case is <span class="math inline">\((X_u,Y_u)\)</span>, and after lens distortion the image point is <span class="math inline">\((X_d,Y_d)\)</span>, <span class="math inline">\(K\)</span> is the lens distortion coefficient, then</p><p><span class="math inline">\(r^2={X_u}^2+{Y_u}^2,\\ X_d=X_u(1+Kr^2),Y_d=Y_u(1+Kr^2)\)</span></p></li><li><p>From the image above, <span class="math inline">\(P_u\)</span> is the image coordinates of <span class="math inline">\(P\)</span> in real 3D world under ideal case (no lens distortion ), and <span class="math inline">\(P_d\)</span> is the image coordinates considering lens coordinates. Then the formulas are:</p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307144904684.png" alt="image-20210307144904684" /><figcaption aria-hidden="true">image-20210307144904684</figcaption></figure></li></ul></li></ul></li></ul></li><li><p><strong>lamp calibration</strong> : to determine the 3D location of the point light source <span class="math inline">\(S\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307152240120.png" alt="image-20210307152240120" style="zoom:33%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307152308342.png" alt="image-20210307152308342" style="zoom:30%;" /></p><ul><li>Given the height of pencil: <span class="math inline">\(h\)</span>, the pencil will be orthogonal to the desk.</li><li>Measure the bottom of the pencil (<span class="math inline">\(\bar{b}\)</span>) and the tip of the shadow <span class="math inline">\(\bar{t}_s\)</span> in captured image, then according to the calibrated camera, the coordinates of pencil bottom in 3D world (denoted as <span class="math inline">\(K\)</span>) and the tip of shadow in real 3D world (<span class="math inline">\(T_s\)</span>) can be estimated. Then the tip of pencil in 3D world (<span class="math inline">\(T\)</span>) is estimated by <span class="math inline">\(h\)</span>.</li><li>The light source <span class="math inline">\(S\)</span> will be the intersection of two rays <span class="math inline">\(TT_S\)</span>. Therefore, two position of pencils will help figure the light source out.</li></ul></li></ul><h4 id="spatial-and-temporal-shadow-edge-localization"><strong>Spatial and temporal shadow edge localization</strong></h4><p>Notice temporal shadow will be scanned from the left to the right side of the scene and thus the right edge of the shadow corresponds to the front edge of the temporal profile</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307175819939.png" alt="image-20210307175819939" style="zoom:33%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307175915876.png" alt="image-20210307175915876" style="zoom:30%;" /></p><ul><li><strong>Two steps to be done</strong><ul><li><strong>Localize</strong> the edge of the shadow that is directly projected on the tabletop <span class="math inline">\((\bar{x}_{top}(t),\bar{x}_{bot}(t))\)</span> at every time instant <span class="math inline">\(t\)</span> (every frame).</li><li><strong>Estimate the time</strong> <span class="math inline">\(t_s(\bar{x}_c)\)</span> (shadow time) where the edge of the shadow passes through any given pixel <span class="math inline">\(\bar{x}_c=(x_c,y_c)\)</span> in the image.</li></ul></li><li>Cause the pixels corresponding to regions in the scene are not illuminated by the lamp, so they don't provide any relevant depth information. And that's why only processing pixels with contrast value (<span class="math inline">\(I_{max}(x,y)-I_{min}(x,y)\)</span>) larger than predefined threshold <span class="math inline">\(I_{thresh}\)</span>, which is 70 in this paper.</li><li>Adaptive threshold image <span class="math inline">\(I_{shadow}(x,y)=(I_{max}(x,y)+I_{min}(x,y))/2\)</span></li><li>No spatial filtering is used cause it would generate undesired blending in the final depth estimates.</li><li>But when the light source is not close to an ideal point source, the predefine threshold <span class="math inline">\(I_{shadow}\)</span> (as a mean) is not optimum. The shadow edge profile becomes shallower as the distance between the stick and the surface increase.</li><li>To <strong>do it real-time,</strong> as the images <span class="math inline">\(I(x,y,t)\)</span> are acquired , one needs to update at each frame five different arrays <span class="math inline">\(I_{max}(x,y),I_{min}(x,y),I_{contrast}(x,y),I_{shadow}(x,y),t_{s}(x,y)\)</span>.<ul><li>For one pixel <span class="math inline">\((x,y)\)</span>, the maximum brightness <span class="math inline">\(I_{max}(x,y)\)</span> is collected at the 1st frame</li><li>The <span class="math inline">\(I_{min}(x,y), I_{contrast}(x,y)\)</span> is updated as time going. Once <span class="math inline">\(I_{contrast}(x,y)\)</span> crosses <span class="math inline">\(I_{thresh}\)</span><font color='red'> (<strong><em>larger than 70??</em></strong> which means now there is a shadow on pixel (x,y)?? )</font>, the adaptive threshold <span class="math inline">\(I_{shadow}(x,y)\)</span> starts being computed and updated at every frame. This process goes on till the pixel brightness <span class="math inline">\(I(x,y,t)\)</span> is larger than than <span class="math inline">\(I_{shadow}(x,y)\)</span> at the 1st time. <strong>This time instant is registered as the shadow time <span class="math inline">\(t_s(x,y)\)</span>.</strong></li></ul></li></ul><h4 id="triangulation"><strong>Triangulation</strong></h4><ul><li>The real 3D point <span class="math inline">\(P\)</span> is gonna be the intersection of line <span class="math inline">\(O_c\bar{x}_c\)</span> and the given pixel <span class="math inline">\(\bar{x}_c\)</span>'s shadow plane.</li><li>The shadow time <span class="math inline">\(t_s(\bar{x}_c)\)</span> acts as an index to the shadow plane list <span class="math inline">\(\prod(t)\)</span>. Besides, the final plane <span class="math inline">\(\prod(t_s(\bar{x}_c))\)</span> will from plane <span class="math inline">\(\prod(t_0-1)\)</span> and <span class="math inline">\(\prod{t_0}\)</span> if <span class="math inline">\(t_0-1&lt;t_s{(\bar{x}_c)}&lt;t_0\)</span> and <span class="math inline">\(t_0\)</span> integer.</li><li>After the range data are recovered, a mesh can be used to build the 3D surface.</li></ul><h3 id="noise-sensitivity">Noise sensitivity</h3><ul><li><p>For quantifying the effect of the noise in the measurement data <span class="math inline">\(\{x_{top}(t), x_{bot}(t), t_{s}(\bar{x}_c)\}\)</span> on the final reconstructed scene depth map, he analysis of <strong>the variance of the induced noise on the depth estimation <span class="math inline">\(Z_c\)</span>, aka <span class="math inline">\(\sigma_{Z_c}\)</span>,</strong> will help. In a word, <span class="math inline">\(\sigma_{Z_c}\)</span> can quantify the uncertainties on the depth estimation <span class="math inline">\(Z_c\)</span> at every pixel <span class="math inline">\(\bar{x}_c\)</span>, and also constitute a good indicator of the overall accuracies in reconstruction (since most of the errors are located along the <span class="math inline">\(Z\)</span> direction of the camera frame ).</p></li><li><p>The variance of the induced noise on the depth estimation <span class="math inline">\(Z_c\)</span>, aka <span class="math inline">\(\sigma_{Z_c}\)</span>, is derived by taking the 1st order derivatives of <span class="math inline">\(Z_c\)</span> with respect to the 'new' noisy input <span class="math inline">\(x_{top},x_{bot},\bar{x}_c\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307205307729.png" alt="image-20210307205307729" style="zoom:33%;" />, <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307205329890.png" alt="image-20210307205329890" style="zoom:30%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307205408384.png" alt="image-20210307205408384" style="zoom:33%;" /></p></li><li><p>From the equation <span class="math inline">\(\sigma_{x_c}=\frac{\sigma_I}{|I_x(\bar{x}_c)|}\)</span>, <span class="math inline">\(\sigma_{x_c}\)</span> does not depend on the local shadow speed. Therefore , decreasing the scanning speed would not increase the accuracy . However, <strong>better slow down while scanning when the shadow edge is sharper</strong> so as to get good samples for every pixel. But with slow scanning speed, an appropriate low-pass filter before extraction of <span class="math inline">\(t_{s}(\bar{x}_c)\)</span> is required for good accuracy.</p></li><li><p>Numerically, most of the variations in the variance <span class="math inline">\(\sigma_{Z_c}^2\)</span> are due to the variation of volume <span class="math inline">\(V\)</span> within a single scan. And Therefore the <strong>reconstruction noise is systematically larger in portions of the scene further away from the lamp</strong>.</p><ul><li><p><strong>To avoid the systematic error,</strong> one may take two scans of the same scene with the lamp at two different locations (on the left and right side of the camera say)</p></li><li><p><strong><em>The final depth is estimated as</em></strong> <span class="math display">\[Z_c=w_LZ_c^L+w_RZ_c^R\]</span> where <span class="math inline">\(Z_c^L,Z_c^R\)</span> are the two estimates (from left and right) of the same depth <span class="math inline">\(Z_c\)</span>.</p><ul><li><strong>If they are gaussian distributed,</strong> and independent, then using</li></ul><p><span class="math display">\[w_L=\frac{\sigma_{Z_R}^2}{\sigma_{Z_R}^2+\sigma_{Z_L}^2}=\frac{\alpha^2}{1+\alpha^2},\\w_R=\frac{\sigma_{Z_L}^2}{\sigma_{Z_R}^2+\sigma_{Z_L}^2}=\frac{1}{1+\alpha^2},\\\alpha=\frac{V_L}{V_R}\]</span></p><p>for averaging.</p><p>But this suffers from degradation of the overall final reconstruction cause may the <span class="math inline">\(Z_c^L,Z_c^R\)</span> are not gaussian .</p><ul><li>To avoid the problem mentioned above, another solution is sigmoid. <span class="math display">\[w_L=\frac{1}{1+\exp(-\beta\Delta V)},\\w_R=\frac{1}{1+\exp(\beta\Delta V)},\\\Delta V=\frac{V_L^2-V_R^2}{V_L^2+V_R^2}=\frac{\alpha^2-1}{\alpha^2+1}\]</span> The positive coefficient <span class="math inline">\(\beta\)</span> controls the amount of diffusion between the left and the right regions. As <span class="math inline">\(\beta\)</span> tends to infinity, merging reduces to hard decision: <span class="math inline">\(Z_c=Z_c^L\)</span> if <span class="math inline">\(V_L&gt;V_R\)</span> and <span class="math inline">\(Z_c=Z_c^R\)</span> otherwise. This will help reduce tends estimation error and obtain more coverage of the scene.</li></ul></li><li><p>The global accuracy depends on the scanning . This paper scan vertically, so the average relative depth error <span class="math inline">\(|\frac{\sigma_{Z_c}}{Z_c}|\)</span> is inversely proportional proportion to <span class="math inline">\(|\cos\xi|\)</span>. The best value will be got while <span class="math inline">\(\xi=0, \xi=\pi\)</span>. aka <strong>lamp standing either tot he right (<span class="math inline">\(\xi=0\)</span> ) or to the left (<span class="math inline">\(\xi=\pi\)</span> ) of the camera.</strong></p></li></ul></li></ul><h2 id="issues">Issues</h2><h3 id="point-light-source"><strong>Point light source</strong></h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210414221836181.png" alt="image-20210414221836181" style="zoom:67%;" /></p><h3 id="real-time-implementation"><strong>real-time implementation </strong></h3><p><strong>while estimating the shadow time</strong></p><p>All that one needs to do is update at each frame five different arrays <span class="math inline">\(I_{max}(x; y), I_{min}(x; y), I_{contrast}(x; y),\)</span> <span class="math inline">\(I_{shadow}(x; y)\)</span> and the shadow time <span class="math inline">\(t_s(x; y)\)</span>, as the images <span class="math inline">\(I(x; y; t)\)</span> are acquired. For a given pixel <span class="math inline">\((x; y)\)</span>, the maximum brightness <span class="math inline">\(I_{max}(x; y)\)</span> is collected at the very beginning of the sequence (the first frame), and then, as time goes, the incoming images are used to update the minimum brightness <span class="math inline">\(I_{min}(x; y)\)</span> and the contrast <span class="math inline">\(I_{contrast}(x; y)\)</span>. Once <span class="math inline">\(I_{contrast}(x; y)\)</span> crosses <span class="math inline">\(I_{thresh}\)</span>, the adaptive threshold <span class="math inline">\(I_{shadow}(x; y)\)</span> starts being computed and updated at every frame (and activated). This process goes on until the pixel brightness <span class="math inline">\(I(x; y; t)\)</span> crosses <span class="math inline">\(I_{shadow}(x; y)\)</span> for the first time (in the upwards direction). That time instant is registered as the shadow time <span class="math inline">\(t_s(x; y)\)</span>. In that form of implementation, the left edge of the shadow is tracked instead of the right one, however the principle remains the same.</p><h3 id="shadow-time"><strong>shadow time</strong></h3><ul><li><strong>Function</strong>: works as an index to the shadow plane list so as to using the intersection to locate <span class="math inline">\(P\)</span>.</li><li><strong>Accuracy</strong>: Since <span class="math inline">\(t_s(\bar{x}_c)\)</span> is estimated at sub-frame accuracy, the final plane <span class="math inline">\(\prod(t_s(\bar{x}_c))\)</span> actually results from linear interpolation between the two planes <span class="math inline">\(\prod(t_0-1)\)</span> and <span class="math inline">\(\prod(t_0)\)</span> if <span class="math inline">\(t_0-1 &lt; t_s(\bar{x}_c) &lt; t_0\)</span> and <span class="math inline">\(t_0\)</span> integer.</li></ul><h3 id="accuracy-of-depth-estimation">Accuracy of depth estimation</h3><ul><li>The accuracy increases as the sharpness of image increases.</li><li>Remove the lamp reflector improve the accuracy.</li><li>Decreasing the scanning speed would not increase accuracy.</li><li>To guarantee the accuracy of sharp edges of object, temporal pixel profile must be sufficiently sampled within the transition area of the shadow edge. Therefore, <strong>the sharper the shadow edge, the slower the scanning speed will help.</strong></li><li><span class="math inline">\(\sigma_{Z_c}\)</span> is a good indicator of the overall accuracies while reconstruction, since most of the errors are located along the <span class="math inline">\(Z\)</span> direction of the camera frame.</li><li>As the shadow moves into the opposite direction of the lamp, the absolute value of the volume <span class="math inline">\(|V|\)</span> strictly decreases and making <span class="math inline">\(\sigma_{Z_c}\)</span> larger. Therefore, the reconstruction noise is systematically larger in portions of the scene further away from the lamp.</li></ul><h2 id="experiments">Experiments</h2><h3 id="calibration-accuracies">Calibration accuracies</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307223201885.png" alt="image-20210307223201885" style="zoom:33%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307223230315.png" alt="image-20210307223230315" style="zoom:33%;" /></p><ul><li>For camera calibration,<ul><li>10 images of the checkboard are taken, nearly 90 corners on the checkboard (8*9).</li><li>the relative error of radial distortion is larger than others.</li></ul></li><li>In lamp calibration,<ul><li>collect 10 images of the pencil shadow</li><li><span class="math inline">\(\bar{S}_c\)</span> is the coordinate vector of the light source in the camera frame, points <span class="math inline">\(b,t_s\)</span> were manually extracted from the images.</li><li>The calibration accuracy is about 3mm, which is sufficient for final shape recovery.</li></ul></li></ul><h3 id="scene-reconstructions">Scene reconstructions</h3><ul><li>Planarity of the plane<ul><li>There is a decrease of approximately <span class="math inline">\(6\%\)</span> in residual standard deviation after quadratic warping. The global geometric deformations are negligible compared to local surface noise. It indicates that the errors of calibration for not induce significant global deformations on the final reconstruction.</li><li><strong>Why 0.23mm/5cm?</strong></li></ul></li><li>Geometry of the corner<ul><li>The overall reconstructed structure does not have any major noticeable global deformation.</li><li>The <strong>errors</strong> (which errors? The surface noise?) are the order of 0.3mm in most experiments.</li></ul></li><li>Angle scene<ul><li>With bulb naked, there is a significant improvement in the sharpness of the projected shadow compared the shadow captured with lamp reflector.</li></ul></li></ul><h3 id="outdoor">Outdoor</h3><ul><li>outdoors where the sun may be used as a calibrated light source (given latitude, longitude, and time of day).</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Simple and low cost system to extracting surface shape of objects. It can be used in real time. The accuracies on the final reconstruction are reasonable (at most <span class="math inline">\(1\%\)</span> or 0.5mm noise error)</li><li>it easily scales to larger scenarios indoors and outdoors.</li><li>Future work:<ul><li>like using the sun as the light source if measuring outdoors.</li><li>multiple view integration so as to move freely the object in front ode the camera and lamp between scans.</li><li>Incorporate a geometrical model of extended light source to the shadow edge detection process.</li><li></li></ul></li></ul><h2 id="qa">QA</h2><ul><li><p>Briefly explain, referring to the figure, how to obtain a 3D reconstruction at from a camera, a light source, a stick, and one or two shots.</p><ul><li>preliminaries: calibrated lamp <span class="math inline">\(S\)</span> and camera <span class="math inline">\(O_c\)</span>.</li><li>Slowly scan the target object by the stick and saved as a video.<ul><li>Do the statistics to get <span class="math inline">\(I_{max},I_{min}\)</span> and then define <span class="math inline">\(I_{shadow}\)</span>.</li><li>Check the video, for the image of each object's point (manually defined, such as each pixel of this object) <span class="math inline">\(\bar{x}_c:(x_{c},y_{c})\)</span> in the whole video, use <span class="math inline">\(I_{shadow}\)</span> and image intensity, draw the temporal shadow and locate the <span class="math inline">\(t_s(x_c,y_c)\)</span> (the index of frame).</li></ul></li><li>For each point in object <span class="math inline">\(\bar{x}_c\)</span>:<ul><li>Check the shadow time <span class="math inline">\(t_s(x_c,y_c)\)</span> and pick the corresponding image.</li><li>Set <span class="math inline">\(x_{bot}(t), x_{top}(t)\)</span> on the picked image, and then find its corresponding 3D points <span class="math inline">\(A(t),B(t)\)</span> respectively.</li><li>Define the shadow plane as the plane consists of <span class="math inline">\(S,A(t),B(t)\)</span>.</li><li>The 3D point of <span class="math inline">\(x_c\)</span>, denoted as <span class="math inline">\(P\)</span> is the intersection of shadow plane and ray <span class="math inline">\(O_cx_c\)</span>.</li></ul></li><li>After the 3D coordinates of all interested points <span class="math inline">\(\bar{x}_c\)</span> are detected, 3D reconstruction can be completed.</li></ul></li><li><p>Referring to the figure, explain what the reference points A (t) and B (t) are for.</p><p>Points <span class="math inline">\(A(t)\)</span> and <span class="math inline">\(B(t)\)</span> are the 3D points, and used to define the shadow plane by light source <span class="math inline">\(S\)</span> and these two points.</p></li><li><p>To find the internal and external parameters of the camera, the article offers a method of calibration with a single image of a checkerboard placed on the desktop, but in this case it is necessary know the main point (center of the image). What could we do if we don't know the point and we want to estimate it?</p><ul><li><p><del><em>Method 1：2D-3D calibration</em></del></p><p><del>Use the dodecahedron as the 3D object to calibrate camera. The optical center is the unknown parameter of camera intrinsic matrix <span class="math inline">\(\mathrm{K}\)</span>. Because the DoF of <span class="math inline">\(\mathrm{K}\)</span> is 8, at least 4 pairs of non-coplanar points (2D-3D) are required during calibration.</del></p></li><li><p><em>Method 2：plane calibration</em></p><p>Still, use the checkboard, but this time move the checkboard so as to get at least two images of checkboard with different pose (keep at least one pose on the desk), each image and checkboard pair will generate at least 4 pairs of 2D-3D points. With at least 2 homographies (induced from the images), the IAC, denoted as <span class="math inline">\(\omega\)</span> can be detected and thus <span class="math inline">\(\mathrm{K}\)</span> is detected by Cholesky decomposition cause <span class="math inline">\(\omega=\mathrm{K}^{-T}\mathrm{K}^{-1}\)</span>. The optical center is in <span class="math inline">\(\mathrm{K}\)</span>.</p></li></ul></li><li><p>The article says that if we use two planes perpendicular (<span class="math inline">\(\pi_h\)</span> and <span class="math inline">\(\pi_v\)</span>) rather than a single plane, we do not need calibrate the light source. Why?</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210415102209642.png" alt="image-20210415102209642" style="zoom:30%;" /></p><p>The requirement of light source <span class="math inline">\(S\)</span> is for the detection of shadow plane so as to get the intersection of shadow plane and ray <span class="math inline">\(O_cx_c\)</span>. Three non-coplanar points define a plane. Here, <span class="math inline">\(A(t), B(t)\)</span> are detected by calibrated camera <span class="math inline">\(O_c\)</span> and images of them. Suppose <span class="math inline">\(A(t),B(t)\)</span> is on <span class="math inline">\(\pi_v\)</span> and any one shadow point <span class="math inline">\(S&#39;\)</span> is on <span class="math inline">\(\pi_h\)</span>. Since <span class="math inline">\(\pi_h \perp\pi_v\)</span>, <span class="math inline">\(S&#39;\)</span> is easy to be detected. Armed with these three points <span class="math inline">\(A(t),B(t), S\)</span>, the shadow plane can be located.</p></li><li><p>Why must the light source used be “point”? Explain. <strong>Give an example of a non-point light source that would also be adequate.</strong></p><ul><li><p>If the light source is not close to an ideal point source, the mean value between maximum and minimum brightness may not always constitute the optimal value for the threshold image <span class="math inline">\(I_{shadow}\)</span>. Indeed, the shadow edge profile becomes shallower as the distance between the stick and the surface increases. In addition, it deforms asymmetrically as the surface normal changes. These effects could make the task of detecting the shadow boundary points challenging.</p></li><li><p>naked bulb: like just remove the lamp reflector in paper.</p></li></ul></li><li><p>The article says that if we use a vertical pencil, we can calibrate the light source. Explain how, based on the illustration to the right. State how many images are needed and explain why.</p><ul><li>Given the height of pencil: <span class="math inline">\(h\)</span>, the pencil will be orthogonal to the desk.</li><li>Measure the bottom of the pencil (<span class="math inline">\(\bar{b}\)</span>) and the tip of the shadow <span class="math inline">\(\bar{t}_s\)</span> in captured image, then according to the calibrated camera, the coordinates of pencil bottom in 3D world (denoted as <span class="math inline">\(K\)</span>) and the tip of shadow in real 3D world (<span class="math inline">\(T_s\)</span>) can be estimated. Then the tip of pencil in 3D world (<span class="math inline">\(T\)</span>) is estimated by <span class="math inline">\(h\)</span>.</li><li>The light source <span class="math inline">\(S\)</span> will be the intersection of two rays <span class="math inline">\(TT_S\)</span>. Therefore, two position of pencils will help figure the light source out.</li></ul></li><li><p>Explain the concepts of temporal shadow and spatial shadow and what are their roles in the reconstruction process.</p><ul><li>spatial shadow helps to detect the column index of <span class="math inline">\(A(t),B(t)\)</span>'s images. After finding this, the real position of <span class="math inline">\(A(t),B(t)\)</span> can be detected by the calibrated camera <span class="math inline">\(O_c\)</span> and the shadow plane can be defined also. Then the 3D coordinates can be detected as the intersection of shadow plane and ray <span class="math inline">\(O_cx_c\)</span>.</li><li>temporal shadow works as an index to the shadow plane list so as to using the intersection to locate <span class="math inline">\(P\)</span>.</li></ul></li><li><p>Explain for the following elements of the assembly how it could be modified and to improve the precision of the 3D reconstruction. In each case, justify your answers.</p><ul><li><p>choice of light source:</p><ul><li>point light source, like what mentioned in the paper just remove the lamp reflector. Or can use candle without reflector. May can also use a board with only one small hole in front of the light, but this should be adjusted to make sure all required shadows are created (difficult in practical). <em>The reason is a point light source creates more precise shadows.</em></li><li>may can move the light source further away from the object to make shadow more clear.</li></ul></li><li><p>choice of stick used to shade:</p><p>make sure the edge of stick is sharp (like a square stick), and long enough to cover the target object. Because the sharper the stick, its shadow edge will be more clear and easier to be detected and thus to locate <span class="math inline">\(\bar{x}_{bot},\bar{x}_{top}\)</span>. The long enough stick will generate required shadow.</p></li><li><p>spatial resolution of the camera (image size):</p><p>the higher the spatial resolution, the more detailed the image (more pixels and smaller points), and thus the more detailed the reconstruction. To make sure the high spatial resolution, one can decrease the distance between camera and object or fix the camera but prolong the focal length.</p></li><li><p>temporal resolution of the camera (number of images per second):</p><p>make the scan speed faster will increase the temporal resolution. The sharper the shadow edge, the higher the temporal resolution will help. If the scan speed is too fast, then one can see a blurred shadow that hurts the reconstruction accuracy, but this can be fixed by reducing the exposure.</p></li></ul></li><li><p>The article suggests that the same method can be used outdoors. Based on the illustration to the right, explain how this can be done.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210415103710164.png" alt="image-20210415103710164" style="zoom:35%;" /></p><ul><li>Calibration of light source (sun) and camera<ul><li>The camera calibration can be done by checkboard.</li><li>The calibration of light source: use the latitude, longitude, and cause the sun is very far away from the earth, each ray can be taken as a parallel ray.</li></ul></li><li>Take the video of shadow image and take the note of time of the day. Similarly, calculate <span class="math inline">\(I_{max},I_{min}\)</span> and then calculate <span class="math inline">\(I_{shadow}\)</span>. Draw the temporal profile and denote the shadow time <span class="math inline">\(t_s(x_c,y_c)\)</span> as the time of day.</li><li>For each point in object <span class="math inline">\(\bar{x}_c\)</span>:<ul><li>Check the shadow time <span class="math inline">\(t_s(x_c,y_c)\)</span> and pick the corresponding image.</li><li>Set <span class="math inline">\(x_{bot}(t), x_{top}(t)\)</span> on the picked image, and then find its corresponding 3D points <span class="math inline">\(A(t),B(t)\)</span> respectively.</li><li>Define the shadow plane as the plane consists of <span class="math inline">\(S,A(t),B(t)\)</span>.</li><li>The 3D point of <span class="math inline">\(x_c\)</span>, denoted as <span class="math inline">\(P\)</span> is the intersection of shadow plane and ray <span class="math inline">\(O_cx_c\)</span>.</li></ul></li><li>After the 3D coordinates of all interested points <span class="math inline">\(\bar{x}_c\)</span> are detected, 3D reconstruction can be completed.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.2656&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;3D photography on your desk, 1998&lt;/a&gt;, &lt;a href=&quot;http://www.vision.caltech.edu/bouguetj/ICCV98/&quot;&gt;website&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Reference: &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/1087109?casa_token=RZ6-EVaSgrIAAAAA:QwrjPXnkBwNtWBQR_RsNxixV2Q7NZ4qj9DFuPG4PkI3rm8J_6NZQUq4spi3Op-xpmGSEwCNuyA&quot;&gt;A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="cv" scheme="http://yoursite.com/tags/cv/"/>
    
      <category term="3d shape" scheme="http://yoursite.com/tags/3d-shape/"/>
    
  </entry>
  
  <entry>
    <title>Papers--The reviews of anomaly detection</title>
    <link href="http://yoursite.com/posts/notes/2021-03-03-notes-paper-anomal-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-03-03-notes-paper-anomal-survey.html</id>
    <published>2021-03-04T03:35:39.000Z</published>
    <updated>2021-06-05T23:54:15.954Z</updated>
    
    <content type="html"><![CDATA[<p>Paper I: <a href="https://arxiv.org/pdf/2007.02500.pdf">Deep Learning for Anomaly Detection: A Review</a></p><a id="more"></a><h1 id="paper-i-deep-learning-for-anomaly-detection-a-review">Paper I: <a href="https://arxiv.org/pdf/2007.02500.pdf">Deep Learning for Anomaly Detection: A Review</a></h1><h2 id="why">Why?</h2><ul><li>outlier detection or novelty detection</li><li>Deep anomaly detection aims at learning feature representations or anomaly scores via neural networks for the sake of anomaly detection.</li></ul><h2 id="problem-complexities-and-challenges">Problem complexities and challenges</h2><h3 id="major-problem-complexities">Major problem complexities</h3><ul><li>Unknownness: Anomalies are associated with many unknowns</li><li>Heterogeneous anomaly classes: one class of anomalies may demonstrate completely different abnormal characteristics from another class of anomalies.</li><li>Rarity and class imbalance: it is difficult to collect a large amount of labeled abnormal instances.</li><li>Diverse types of anomaly:<ul><li>Point anomalies: individual instances that are anomalous w.r.t. the majority of other individual instances</li><li>Conditional anomalies: contextual anomalies, also refer to individual anomalous instances but in a specific context.</li><li>Group anomalies: a subset of data instances anomalous as a whole w.r.t. the other data instances.</li></ul></li></ul><h3 id="main-challenges-tackled-by-deep-anomaly-detection">Main challenges tackled by deep anomaly detection</h3><ul><li>CH1: low anomaly detection recall rate: How to reduce false positives and enhance detection recall rates</li><li>CH2: anomaly detection in high-dimensional and/or not-independent data:<ul><li>High-dimensional anomaly detection<ul><li>Performing anomaly detection in a reduced lower dimensional space spanned by a small subsets of original features or newly constructed features.</li><li>But challenges on identifying intricate (e.g., high-order, nonlinear and heterogeneous) feature interactions and couplings.</li></ul></li><li>Guarantee the new feature space preserved proper information for specific detection methods.<ul><li>Due to the aforementioned unknowns and heterogeneities of anomalies.</li></ul></li><li>Detect anomalies from instances that may be dependent on each other.</li></ul></li><li>CH3: data-efficient learning of normality/abnormality<ul><li>Fully supervised anomaly detection is often impractical.</li><li>Unsupervised methods do not have any prior knowledge of true anomalies. They rely heavily on their assumption on the distribution of anomalies.</li><li>Weakly supervised anomaly detection<ul><li>How to learn expressive normality/abnormality representations with a small amount of labeled anomaly data.</li><li>How to learn detection models that are generalized to novel anomalies uncovered by the given labeled anomaly data.</li></ul></li></ul></li><li>CH4： Noise-resilient anomaly detection<ul><li>Large-scale anomaly-contaminated unlabeled data</li><li>The amount of noises can differ significantly from datasets and noisy instances may be irregularly distributed in the data space.</li></ul></li><li>CH5：Detection of complex anomalies<ul><li>The generation from point anomalies to conditional anomalies and group anomalies</li><li>How to incorporate the concept of conditional/group anomalies into anomaly measures/models.</li><li>The detection of anomalies with multiple heterogeneous data sources.</li></ul></li><li>CH6： Anomaly explanation<ul><li>Have anomaly explanation algorithms that provide straightforward clues about why a specific data instance is identified as anomaly.</li><li>A main challenge to well balance the model’s interpretability and effectiveness.</li></ul></li></ul><h2 id="addressing-the-challenges-with-deep-anomaly-detection">Addressing the challenges with deep anomaly detection</h2><h3 id="preliminaries">Preliminaries</h3><ul><li>Deep anomaly detection aims at learning a feature representation mapping function <span class="math inline">\(\phi (·) : X\mapsto Z\)</span> or an anomaly score learning function <span class="math inline">\(\tau (·): X\mapsto R\)</span> in a way that anomalies can be easily differentiated from the normal data instances in the space yielded by the <span class="math inline">\(\phi\)</span> or <span class="math inline">\(\tau\)</span> function.</li></ul><h3 id="categorization-of-deep-anomaly-detection">Categorization of deep anomaly detection</h3><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602121703067.png" alt="image-20210602121703067" style="zoom:70%;" /></p><h2 id="deep-learning-for-feature-extraction">Deep learning for feature extraction</h2><ul><li>The deep learning components work purely as dimensionality reduction only.</li><li>Assumptions: The feature representations extracted by deep learning models preserve the discriminative information that helps separate anomalies from normal instances.</li><li>Research lines<ul><li>Directly use popular pre-trained deep learning models such as VGG etc.<ul><li>An unmasking process: iteratively train a binary classifier to separate one set of video frames from its subsequent video frames in a sliding window, with the most discriminant features removed in each iteration step. The power of unmasking framework relies heavily on the quality of the features.</li><li>Using features extracted from a dynamically updated sampling pool of video frames is found to improve the performance of the framework.</li><li>Pretrained model and fine-tuning.</li></ul></li><li>Explicitly train a deep feature extraction model rather than a pre-trained model for the downstream anomaly scoring.<ul><li>Methods<ul><li>Three separate autoencoder networks are trained to learn low-dimensional features for respective appearance, motion, and appearance-motion joint representations for video anomaly detection.</li><li>Unsupervised classification approaches to enable anomaly scoring in the projected space. Use cluster methods to assign pseudo labels and then do one-vs-the-rest classification. The classification probabilities are used to define frame-wise anomaly scores.</li><li>Graph anomaly detection: learn the representations of graph vertices by minimizing autoencoder-based reconstruction loss and pairwise distances of neighbored graph vertices.</li></ul></li></ul></li></ul></li><li>Advantages<ul><li>A large number of state-of-the-art (pre-trained) deep models and off-the-shelf anomaly detectors are readily available.</li><li>Deep feature extraction offers more powerful dimensionality reduction than popular linear methods.</li><li>It is easy-to-implement given the public availability of the deep models and detection methods.</li></ul></li><li>Disadvantages<ul><li>The fully disjointed feature extraction and anomaly scoring often lead to suboptimal anomaly scores.</li><li>Pre-trained deep models are typically limited to specific types of data.</li></ul></li><li>Challenges Targeted<ul><li>The lower-dimensional space often helps reveal hidden anomalies and reduces false positives (CH2).</li><li>May not preserve sufficient information for anomaly detection as the data projection is fully decoupled with anomaly detection.</li><li>Allows to leverage multiple types of features and learn semantic-rich detection models, and then reduce CH1.</li></ul></li></ul><h2 id="learning-feature-representation-of-normality">Learning feature representation of normality</h2><h3 id="generic-normality-feature-learning">Generic normality feature learning</h3><ul><li>Learns the representations of data instances by optimizing a generic feature learning objective function that is not primarily designed for anomaly detection.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602130709282.png" alt="image-20210602130709282" style="zoom:67%;" /></li></ul><h4 id="autoencoders">Autoencoders</h4><ul><li><p>Aims to learn some low-dimensional feature representation space on which the given data instances can be well reconstructed. The learned feature representations are enforced to learn important regularities of the data to minimize reconstruction errors, anomalies are difficult to be reconstructed from the resulting representations and thus have large reconstruction errors.</p></li><li><p>Assumptions: Normal instances can be better restructured from compressed space than anomalies.</p></li><li><p>Formally,</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602132252366.png" alt="image-20210602132252366" style="zoom:70%;" /></p></li><li><p>Methods</p><ul><li>Sparse AE: encourage sparsity in the activation units of the hidden layer.</li><li>Denoising AE: learning representations that are robust to small variations by learning to reconstruct data from some predefined corrupted data instances rather than original data.</li><li>Contractive AE: takes a step further to learn feature representations that are robust to small variations of the instances around their neighbors. By adding a penalty term based on the Forbenius norm of the Jacobian matrix of the encoder's activations.</li><li>Variational AE: introduces regularization into the representation space by encoding data instances using a prior distribution over the latent space.</li></ul></li><li><p>Implementations</p><ul><li>Replicator NNs</li><li>RandNet: learning an ensemble of AEs</li><li><a href="https://git.io/JfYG5">RDA</a>: motivated by robust PCA, it attempts to iteratively decompose term original data into 2 subsets, normal instance set and anomaly set. This is achieved by adding a sparsity penalty <span class="math inline">\(\ell_1\)</span> or grouped penalty <span class="math inline">\(\ell_{2,1}\)</span> into its RPCA-alike objective function to regularize the coefficients of the anomaly set.</li></ul></li><li><p>For more complex data</p><ul><li>Adapting the network architecture to the type of input data, they embeds the encoder-decoder scheme into the full procedure of these methods, such as CNN-AE, LSTM-AE, Conv-LSTM-AE, GCN-AE etc.</li><li>First use AEs to learn low-dimensional representations of the complex data and then learn to predict these learned representations.<ul><li>denoising AE is combined with RNNs to learn normal patterns of multivariate sequence data,</li></ul></li></ul></li><li><p>Advantages</p><ul><li>The idea of AEs is straightforward and generic to different types of data.</li><li>Different types of powerful AE variants can be leveraged to perform anomaly detection.</li></ul></li><li><p>Disadvantages</p><ul><li>The learned feature representations can be biased by infrequent regularities and the presence of outliers or anomalies in the training data.</li><li>The objective function of the data reconstruction is designed for dimension reduction or data compression, rather than anomaly detection.</li></ul></li><li><p>Challenges</p><ul><li>CH2: attributed graph data etc.</li><li>CH1: reduce false positives</li><li>CH4: RPCA</li></ul></li></ul><h4 id="gans">GANs</h4><p>Some form of residual between the real instance and the generated instance are then defined as anomaly score.</p><ul><li><p>Assumption: Normal data instances can be better generated than anomalies from the latent feature space of the generative network in GANs</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfGgc">AnoGAN</a>: computational inefficiency in the iterative search of latent representation <span class="math inline">\(\boldsymbol{\mathrm{z}}\)</span>.</p><ul><li>To alleviate the inefficiency<ul><li><a href="https://git.io/JfGgG">EBGAN</a> : based on BiGAN, discriminate the pair of instances <span class="math inline">\((\mathrm{x}, 𝐸(\mathrm{x}))\)</span> from the pair <span class="math inline">\((𝐺(\mathrm{z}),\mathrm{z})\)</span>. EBGAN is extended to ALAD by adding two more discriminators with one discriminator trying to discriminate the pair <span class="math inline">\((\mathrm{x}, \mathrm{x})\)</span> from $(,𝐺(𝐸())) $and another one trying to discriminate the pair <span class="math inline">\((\mathrm{z},\mathrm{z})\)</span> from <span class="math inline">\((\mathrm{z}, 𝐸(𝐺(\mathrm{z})))\)</span>.</li><li><a href="https://git.io/JfZRn">fast AnoGAN</a>: share the same spirit of EBGAN.</li><li><a href="https://git.io/JfZ8v">ALAD</a>: the extension of EBGAN</li></ul></li></ul></li><li><p><a href="https://git.io/JfGgn">GANomaly</a>: further improves the generator over the previous work by changing the generator network to an encoder-decoder-encoder network and adding two more extra loss function.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602143617119.png" alt="image-20210602143617119" style="zoom:60%;" /></p></li><li><p>Combined with wassertein GAN or adversarially learn end-to-end one-class classification.</p></li></ul></li><li><p>Advantages</p><ul><li>Demonstrated superior capability in generating realistic instances, especially on images.</li><li>A large number of existing GAN-based models and <a href="https://arxiv.org/abs/1710.07035">theories</a> may be adapted for anomaly detection.</li></ul></li><li><p>Disadvantages</p><ul><li>The training of GANs can suffer from multiple problems, such as failure to converge and mode collapse.</li><li>The generator network can be misled and generates data instances out of the manifold of normal instances.</li><li>The GANs-based anomaly scores can be suboptimal since they are built upon the generator network with the objective designed for data synthesis rather than anomaly detection.</li></ul></li><li><p>Challenges</p><ul><li>CH2， CH1</li></ul></li></ul><h4 id="predictability-modeling">Predictability modeling</h4><p>Learn feature representations by predicting the current data instances using the representations of the previous instances within a temporal window as the context. The prediction errors can be used to define the anomaly scores</p><ul><li><p>Assumptions: Normal instances are temporally more predictable than anomalies</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/Jf4pc">FFP</a>: Using like U-net prediction, popular in video anomaly detection.</p><p>U-Net as the frame generator, and measure the objective loss by intensity, gradient and optical flow. After training, for a given video frame <span class="math inline">\(\mathrm{x}\)</span>, a normalized Peak Signal-to-Noise Ratio based on the prediction difference is used to define the anomaly score.</p><ul><li><a href="https://git.io/Jf4pW">LSA</a></li></ul></li><li><p>Another way is based on autoregressive models that assume each element in a sequence is linearly dependent on the previous elements.</p><ul><li>At the evaluation stage, the reconstruction error and the log-likelihood are combined to define the anomaly score.</li></ul></li></ul></li><li><p>Advantages</p><ul><li>A number of sequence learning techniques can be adapted and incorporated into this approach.</li><li>This approach enables the learning of different types of temporal and spatial dependencies.</li></ul></li><li><p>Disadvantages</p><ul><li>This approach is limited to anomaly detection in sequence data.</li><li>The sequential predictions can be computationally expensive.</li><li>The learned representations may suboptimal for anomaly detection as its underlying objective is for sequential predictions rather than anomaly detection.</li></ul></li><li><p>Challenges</p><ul><li>CH1&amp;CH2</li><li>CH5</li></ul></li></ul><h4 id="self-supervised-classification">Self-supervised classification</h4><ul><li><p>Learns representations of normality by building self-supervised classification models and identifies instances that are inconsistent to the classification models as anomalies.</p></li><li><p>Evaluate the normality of data instances by their consistency to a set of predictive models, with each model learning to predict one feature based on the rest of the other feature.</p></li><li><p>Focuses on image data and builds the predictive models by using feature transformation-based augmented data.</p></li><li><p>Assumptions: Normal instances are more consistent to self-supervised classifiers than anomalies.</p></li><li><p>Methods</p><ul><li><a href="https://git.io/JfZRW">GT</a>: Like horizontal flipping, translations, and rotations. <strong>The classification scores of each test instance w.r.t. different <span class="math inline">\(𝑇_𝑗\)</span> are then aggregated to compute the anomaly score.</strong></li><li><a href="https://git.io/Jf4pl"><span class="math inline">\(E^3\)</span>outlier</a> : Training data contains normal instances only. <a href="https://proceedings.neurips.cc/paper/2019/file/6c4bb406b3e7cd5447f7a76fd7008806-Paper.pdf">UOD</a><ul><li><em>The gradient magnitude induced by normal instances is normally substantially larger than outliers during the training of such self-supervised multiclass classification models;</em></li><li><em>The network updating direction is also biased towards normal instances.</em></li></ul></li></ul><p><strong>Normal instances often have stronger agreement with the classification model than anomalies.</strong></p><p>Negative entropy-based anomaly scores perform generally better than average prediction probability and maximum prediction probability.</p></li><li><p>Advantages</p><ul><li>They work well in both the unsupervised and semi-supervised settings.</li><li><em>Anomaly scoring is grounded by some intrinsic properties of gradient magnitude and its updating.</em></li></ul></li><li><p>Disadvantages</p><ul><li>The feature transformation operations are often data-dependent. And the operations (rotation) only work on images.</li><li>The consistency-scores are derived upon the classification scores rather than an integrated module in the optimization, and thus they may be suboptimal.</li></ul></li><li><p>Challenges</p><ul><li>CH1 &amp; CH2</li><li>CH4</li></ul></li></ul><h3 id="anomaly-measure-dependent-feature-learning">Anomaly measure-dependent feature learning</h3><p>Learning feature representations that are specifically optimized for one particular existing measure. They incorporates an existing anomaly measure <span class="math inline">\(f\)</span> into the feature learning objective function to optimize the feature representations specifically for <span class="math inline">\(f\)</span>.</p><h4 id="distance-based-measure">Distance-based measure</h4><ul><li>Aims to learn feature representations that are specifically optimized for a specific type of distance-based anomaly measures.</li><li>Like DB outliers, <span class="math inline">\(k\)</span>-nearest neighbor distance, average <span class="math inline">\(k\)</span>-nearest neighbor distance, relative distance and random nearest neighbor distance. But they fail to work effectively in high dimensional data.</li><li>Assumption : anomalies are distributed far from their closet neighbors while normal instances are located in dense neighbors.</li><li>Methods<ul><li><a href="https://git.io/JfZRg">REPEN</a>: Random neighbor distance-based: The representations are optimized so that the nearest neighbor distance of pseudo-labeled anomalies in random subsamples is substantially larger than that of pseudo-labeled normal instances. The pseudo labels are generated by some off-the-shelf anomaly detectors. The loss function is built upon the hinge loss.</li><li><a href="https://git.io/RDP">RDP</a>: The other uses the distance between optimized representations and randomly projected representations of the same instances to guide the representation learning. Solving <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602202900471.png" alt="image-20210602202900471" style="zoom:50%;" /> is equivalent to have a knowledge distillation from a random NNs and helps learn the frequency of different underlying patterns in the data.</li><li>At the evaluation stage, the function of <span class="math inline">\(f\)</span> is used to compute the anomaly score.</li></ul></li><li>Advantages<ul><li>The distance-based anomalies are straightforward and well defined with rich theoretical supports in the literature.</li><li>They work in low-dimensional representation spaces and can effectively deal with high-dimensional data that traditional distance-based anomaly measures fail.</li><li>They are able to learn representations specifically tailored for themselves.</li></ul></li><li>Disadvantages<ul><li>The extensive computation involved in most of distance-based anomaly measures</li><li>Their capabilities may be limited by the inherent weaknesses of the distance-based anomaly measures.</li></ul></li><li>Challenges<ul><li>CH1&amp;CH2</li><li>CH3,CH4</li></ul></li></ul><h4 id="one-class-classification-measure">One-class classification measure</h4><p>Aims to learn feature representations customized to subsequent one-class classification-based anomaly detection. Learn a description of a set of data instances to detect whether new instances conform to the training data or not.</p><ul><li><p>One way is to learn representations that are specifically optimized for these traditional one-class classification models, like one-class SVM.</p></li><li><p>Assumption: all normal instances come from a single class and can be summarized by a compact model, to which anomalies do not conform.</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfGgl">AE-1SVM</a>, <a href="https://git.io/JfGgZ">OC_NN</a>: Deep one-class SVM, learn the one-class hyperplane from the neural network-enabled low-dimensional representation space rather than the original input space.</p><p>The formula is <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602205007059.png" alt="image-20210602205007059" style="zoom:33%;" /> Any instances that have $𝑟 − ^_𝑖 $ can be reported as anomalies. It has two benefits:</p><ul><li>It can leverage deep networks to learn more expressive features for downstream anomaly detection</li><li>It can also help remove the computational expensive pairwise distance computation in the kernel functions.</li><li>Based on one-class SVM, one may use a random mapping to map latent representation <span class="math inline">\(\mathrm{z}\)</span> to Fourier features since many kernel functions can be approximated by random Fourier features.</li></ul></li><li><p><a href="https://git.io/JfZRR">Deep SVDD</a>, <a href="https://git.io/JfOkr">Deep SAD</a>: Another way is deep models for SVDD, which aims at Learning a minimum hyperplane characterized by a center <span class="math inline">\(\mathrm{c}\)</span> and a radius <span class="math inline">\(r\)</span> so that the sphere contains all training data instances.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602210101160.png" alt="image-20210602210101160" style="zoom:67%;" /></p><p>It's shown that <span class="math inline">\(\mathrm{c}\)</span> as trainable parameters can lead to trivial solutions. <span class="math inline">\(\mathrm{c}\)</span> can be fixed as the mean of the feature representations yield by performing a single initial forward pass. The key idea is to minimize the distance of labeled normal instances to the center while at the same time maximizing the distance of known anomalies to the center.</p></li></ul></li><li><p>Advantages</p><ul><li>The one-class classification-based anomalies are well studied in the literature and provides a strong foundation of deep one-class classification-based methods.</li><li>The representation learning and one-class classification models can be unified to learn tailored and more optimal representations.</li><li>They free the users from manually choosing suitable kernel functions in traditional one-class model.</li></ul></li><li><p>Disadvantages</p><ul><li>The one-class models may work ineffectively in datasets with complex distributions within the normal class.</li><li>The detection performance is dependent on the one-class classification-based anomaly measure.</li></ul></li><li><p>Challenges : CH1&amp;CH2， CH3</p></li></ul><h4 id="clustering-based-measure">Clustering-based measure</h4><p>Aims at learning representations so that anomalies are clearly deviated from the clusters in the newly learned representation space.</p><ul><li><p>Methods use like cluster size, distance to cluster centers, distance between cluster centers, and cluster membership to define clusters.</p></li><li><p>Assumptions: Normal instances have stronger adherence to clusters than anomalies.</p></li><li><p>Many methods are explored based on the motivation that the performance of clustering methods is highly dependent on the input data.</p></li><li><p>The deep clustering methods typically consist of two modules: performing clustering in the forward pass and learning representations using the cluster assignment as pseudo class labels in the backward pass.</p><ul><li><p>The clustering loss can be initialized with a kmeans loss, a spectral clustering loss, an agglomerative loss or a GMM loss.</p></li><li><p>The auxiliary loss can be an autoencoder-based reconstruction loss to learn robust and/or local structure preserved representations.</p></li><li><p>The cluster assignments in the resulting function is used to compute anomaly scores.</p></li><li><p>The aforementioned deep clustering methods are focused on learning optimal clustering results, but the learned representations may not be able to well capture the abnormality of anomalies. <a href="https://www.researchgate.net/publication/330625995_A_Unified_Unsupervised_Gaussian_Mixture_Variational_Autoencoder_for_High_Dimensional_Outlier_Detection">Papers</a></p><p><a href="https://git.io/JfZR0">DAGMM</a>:</p><ul><li>The cluster loss is GMM loss and the auxiliary loss is autoencoder-based reconstruction loss.</li><li>The auxiliary loss is an an autoencoder-based reconstruction loss, but to learn deviated representations of anomalies.</li></ul></li></ul></li><li><p>Advantages</p><ul><li>A number of deep clustering methods and theories can be utilized to support the effectiveness and theoretical foundation of anomaly detection.</li><li>Learn specifically optimized representations that help spot the anomalies easier than on the original data, especially when dealing with intricate data sets.</li></ul></li><li><p>Disadvantages</p><ul><li>The performance of anomaly detection is heavily dependent on the clustering results.</li><li>The clustering process may be biased by contaminated anomalies in the training data, which in turn leads to less effective representations.</li></ul></li><li><p>Challenges: CH1&amp;CH2， CH4</p></li></ul><h2 id="end-to-end-anomaly-score-learning">End-to-end anomaly score learning</h2><p>Aims at learning scalar anomaly scores in an end-to-end fashion. It has a NN that directly learns the anomaly scores. Methods here won't be limited by the inherent disadvantages of the incorporated anomaly measures. There are two design directions: one focuses on how to synthesize existing anomaly measures and neural network models, while another focuses on devising novel loss functions for direct anomaly score learning.</p><h3 id="ranking-models">Ranking models</h3><p>Aims to directly learn a ranking model, such that data instances can be sorted based on an observable ordinal variable associated with the absolute/relative ordering relation of the abnormality.</p><ul><li><p>Assumptions: There exists an observable ordinal variable that captures some data abnormality.</p></li><li><p>Methods</p><ul><li><p>One line is to devise ordinal regression -based loss functions to drive the anomaly scoring neural network.</p><ul><li>Two-class ordinal regression.<ul><li>The end-to-end anomaly scoring network takes <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{N}\)</span> as inputs and learns to optimize the anomaly scores such that the data inputs of similar behaviors as those in <span class="math inline">\(\mathcal{A(N)}\)</span> receive large (small) scores as close <span class="math inline">\(𝑐_1 (𝑐_2)\)</span> as possible, resulting in larger anomaly scores assigned to anomalous frames than normal frames.</li></ul></li></ul></li><li><p>Weakly-supervision</p><ul><li><p><a href="https://git.io/JfZRz">MIL</a>: the model is optimized to learn larger anomaly scores for the pairs of two anomalies than the pairs with one anomaly or none.</p><p>MIL ranking model, directly learn the anomaly score for each video segment. Its key objective is to guarantee that the maximum anomaly score for the segments in a video that contains anomalies somewhere is greater than the counterparts in a normal video.<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605124035832.png" alt="image-20210605124035832" /></p><ul><li><font color='red'>Why not <span class="math inline">\(\min\)</span> at the last term in the 1st term?</font></li><li>The first term is to guarantee the relative anomaly score order, i.e., the anomaly score of the most abnormal video segment in the positive instance bag is greater than that in the negative instance bag. The last two terms are extra optimization constraints, in which the former enforces score smoothness between consecutive video segments while the latter enforces anomaly sparsity, i.e., each video contains only a few abnormal segments.</li></ul></li></ul></li></ul></li><li><p>Advantages：</p><ul><li>The anomaly scores can be optimized directly with adapted loss functions.</li><li>They are generally free from the definitions of anomalies by imposing a weak assumption of the ordinal order between anomaly and normal instances.</li><li>This approach may build upon well-established ranking techniques and theories from areas like learning to rank.</li></ul></li><li><p>Disadvantages</p><ul><li>At least some form of labeled anomalies are required.</li><li>Methods may not be able to generalize to unseen anomalies cause they are designed to detect labeled anomalies.</li></ul></li><li><p>Challenges:</p><ul><li>CH1&amp;CH2;</li><li>CH3</li><li>CH6</li><li>CH4</li></ul></li></ul><h3 id="prior-driven-models">Prior-driven models</h3><p>Use a prior distribution to encode and drive the anomaly score learning. The prior may be imposed on either the internal module or the learning output of the score learning function.</p><ul><li><p>Assumptions: The imposed prior captures the underlying (ab)normality of the dataset.</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfZRw">DevNet</a>: enforce a prior on the anomaly scores. It uses a Gaussian prior to encode the anomaly scores and enable the direct optimization very well. The deviation loss is built upon contrastive loss.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605131355985.png" alt="image-20210605131355985" /><figcaption aria-hidden="true">image-20210605131355985</figcaption></figure><ul><li>Driven by the deviation loss, it will push the anomaly scores of normal instances as close as possible to 𝜇 while guaranteeing at least 𝑚 standard deviations between 𝜇 and the anomaly scores of anomalies.</li><li>The loss is equivalent to enforcing a statistically significant deviation of the anomaly score of the anomalies from that of normal instances in the upper tail.</li><li>It's also interpretable.</li></ul></li></ul></li><li><p>Advantages</p><ul><li>The anomaly scores can be directly optimized w.r.t. a given prior.</li><li>It provides a flexible framework for incorporating different prior distributions into the anomaly score learning.</li><li>The prior can also result in more interpretable anomaly scores than the other methods.</li></ul></li><li><p>Disadvantages</p><ul><li>It's difficult to design a universally effective prior for all anomaly detection application.</li><li>The efficiency of model depends on how the picked prior fits the underlying distribution.</li></ul></li><li><p>Challenges</p><ul><li>CH1&amp;CH2</li><li>CH1&amp;CH3</li><li>CH4</li></ul></li></ul><h3 id="softmax-likelihood-models">Softmax Likelihood models</h3><p>Learning anomaly scores by maximizing the likelihood of events in the training data. Normal instances are presumed to be high-probability events whereas anomalies are prone to be low-probability events. Tools like NCE are used.</p><ul><li>Assumptions: Anomalies and normal instances are respectively low- and high-probability events.</li><li>Methods<ul><li>Use log negative likelihood. Learning the likelihood function 𝑝 is equivalent to directly optimizing the anomaly scoring function.</li><li>But the original likelihood is computed costly, NCE is used to alleviate. For each instance <span class="math inline">\(\mathrm{x}\)</span>, <span class="math inline">\(k\)</span> noise samples <span class="math inline">\(\mathrm{x}_{1, \cdots,k}\sim Q\)</span> are generated from some synthetic known ‘noise’ distribution <span class="math inline">\(Q\)</span>.</li></ul></li><li>Advantages<ul><li>Different types of interactions can be incorporated into the anomaly score learning process.</li><li>The anomaly scores are faithfully optimized w.r.t. the specific abnormal interactions we aim to capture.</li></ul></li><li>Disadvantages<ul><li>The computation of the interactions can be very costly when the number of features/elements in each data instance is large.</li><li>The anomaly score learning is heavily dependent on the quality of the generation of negative samples.</li></ul></li><li>Challenges<ul><li>CH2&amp;CH5</li><li>CH1</li></ul></li></ul><h3 id="end-to-end-one-class-classification">End-to-end one-class classification</h3><p>Train a one-class classifier that learns to discriminate whether a given instance is normal or not. It does not rely on any existing one-class classification measures. Methods like adversarially learned one-class classification are used. It learns a one-class discriminator of the normal instances so that it well discriminates those instances from adversarially generated pseudo anomalies. The goal is to learn a discriminator and this discriminatory will be directly used as anomaly scorer.</p><ul><li><p>Assumptions :</p><ul><li>Data instances that are approximated to anomalies can be effectively synthesized.</li><li>All normal instances can be summarized by a discriminative one-class model.</li></ul></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfZRw">ALOCC</a>:</p><ul><li>The key idea is to train two deep networks, with one network trained as the one-class model to separate normal instances from anomalies while the other network trained to enhance the normal instances and generate distorted outlier. The generator is based on a denoising AE.</li><li>The outliers are randomly sampled from some classes other than the classes where the normal instances come from.</li><li>But this method may be unavailable in many domains cause the reference outliers are beyond the given data.</li></ul></li><li><p><a href="https://git.io/JfYGb">OCAN</a>, <a href="https://git.io/Jf4pR">FenceGAN</a>: generate fringe data instances based on the given training data and use them as negative reference instances to enable the training of the one-class discriminator.</p><ul><li><p>OCAN: The generator is trained to generate data instances that are complementary to the training data.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605190140300.png" alt="image-20210605190140300" style="zoom:67%;" /></p><p>The 1st two terms are devised to generate low-density instances in the original feature space, and the last term is to help better generate data instances within the original data space.</p><p>The objective of the discriminatory is enhanced with an extrovert conditional entropy loss to enable the detection with high confidence.</p></li><li><p><a href="https://git.io/Jf4pR">FenceGAN</a>: generate data instances tightly lying at the boundary of the distribution of the training data, which is achieved by introducing two loss functions into the generator that enforce the generated instances to be evenly distributed along a sphere boundary of the training data.</p></li><li><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605191312110.png" alt="image-20210605191312110" /><figcaption aria-hidden="true">image-20210605191312110</figcaption></figure><p>The first term is called encirclement loss that enforces the generated instances to have the same discrimination score, ideally resulting in instances tightly enclosing the training data. The second term is called dispersion loss that enforces the generated instances to evenly cover the whole boundary.</p></li></ul></li><li><p><a href="https://git.io/Jf4p0">OCGAN</a>: uniformly distributed instances can be generated to enforce the normal instances to be distributed uniformly across the latent space.</p></li><li><p>An ensemble of generator is used with each generator synthesizing boundary instances for one specific cluster of normal instances.</p></li></ul></li><li><p>Advantages</p><ul><li>Its anomaly classification model is adversarially optimized in an end-to-end fashion.</li><li>It can be developed and supported by the affluent techniques and theories of adversarial learning and one-class classification.</li></ul></li><li><p>Disadvantages</p><ul><li>It is difficult to guarantee that the generated reference instances well resemble the unknown anomalies.</li><li>The instability of GANs may lead to generated instances with diverse quality and consequently unstable anomaly classification performance.</li><li>Its applications are limited to semi-supervised anomaly detection scenarios.</li></ul></li><li><p>Challenges</p><ul><li>CH1 &amp; CH2</li></ul></li></ul><h2 id="algorithms-and-datasets">Algorithms and datasets</h2><h3 id="representative-algorithms">Representative algorithms</h3><ul><li><p>Most methods operate in an unsupervised or semi-supervised mode.</p></li><li><p>Deep learning tricks like data augmentation, dropout and pre-training are under-explored.</p></li><li><p>The network architecture used is not that deep.</p></li><li><p>(leaky) ReLU is the most popular one</p></li><li><p>diverse backbone networks can be used to handle different types of input data.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605192143473.png" alt="image-20210605192143473" /><figcaption aria-hidden="true">image-20210605192143473</figcaption></figure></li></ul><h3 id="datasets-with-real-anomalies">Datasets with Real anomalies</h3><ul><li><a href="https://github.com/GuansongPang/anomaly-detection-datasets">The datasets</a></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605192324925.png" title="fig:" alt="image-20210605192324925" /></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Contributions<ul><li>Problem nature and challenges: some unique problem complexities underlying anomaly detection and the resulting largely unsolved challenges.</li><li>Categorization and formulation: three principled frameworks: deep learning for generic feature extraction, learning representations of normality, and end-to-end anomaly score learning</li><li>Comprehensive literature review</li><li>Future opportunities</li><li>Source codes and datasets</li></ul></li><li>Exploring anomaly-supervisory signals<ul><li>The key issue for these formulations is that their objective functions are generic.</li><li>Explore new sources of anomaly-supervisory signals that lie beyond the widely-used formulations such as data reconstruction and GANs, and have weak assumptions on the anomaly distribution.</li><li>Develop domain-driven anomaly detection by leveraging domain knowledge.</li></ul></li><li>Deep weakly-supervised anomaly detection<ul><li>Leveraging deep neural networks to learn anomaly-informed detection models with some weakly-supervised anomaly signals.<ul><li>Utilize a small number of accurate labeled anomaly examples to enhance detection models.</li></ul></li><li>Unknown anomaly detection: aim to build detection models that are generated from the limited labeled anomalies to unknown anomalies.</li><li>Data-efficient anomaly detection or few-shot anomaly detection: given only limited anomaly examples.</li></ul></li><li>Large scale normality leaning<ul><li>Since it is difficult to obtain sufficient labeled data</li><li>The goal is to first learn transferable pre-trained representation models from large-scale unlabeled data in an unsupervised/self-supervised mode, and then fine-tune detection models in a semi-supervised mode.</li><li>May need to be domain/application-specific.</li></ul></li><li>Deep detection of complex anomalies<ul><li>conditional/group anomalies</li><li>Multimodal anomaly detection</li></ul></li><li>Interpretable and actionable deep anomaly detection<ul><li>The abnormal feature selection methods but may render the explanation less useful</li></ul></li><li>Novel applications and settings<ul><li>Out-of-distribution detection: closely related area. It is generally assumed that fine-grained normal class labels are available during training.</li><li>Curiosity learning: learning a bonus reward function in reinforcement learning with sparse rewards. Augmenting the environment with a bonus reward in addition to the original sparse rewards from the environment.</li><li>non-IID anomaly detection: e.g., the abnormality of different instances/features is interdependent and/or heterogeneous. May be confused with anomaly instances.</li><li>detection of adversarial examples, anti-spoofing in biometric systems, and early detection of rare catastrophic events.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper I: &lt;a href=&quot;https://arxiv.org/pdf/2007.02500.pdf&quot;&gt;Deep Learning for Anomaly Detection: A Review&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
      <category term="anomaly" scheme="http://yoursite.com/tags/anomaly/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Fall Surveys</title>
    <link href="http://yoursite.com/posts/notes/2021-02-07-notes-paper-fall-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-02-07-notes-paper-fall-survey.html</id>
    <published>2021-02-07T13:46:39.000Z</published>
    <updated>2021-03-05T03:34:36.201Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Fall detection<ul><li>Paper I: <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full">Elderly Fall Detection Systems: A Literature Survey, 2020</a></li><li>Paper II: <a href="https://dl.acm.org/doi/10.1145/2769493.2769540">A survey on vision-based fall detection, 2015</a></li><li>Paper III: <a href="https://www.sciencedirect.com/science/article/pii/S2352864815000681">3D depth image analysis for indoor fall detection of elderly people, 2015</a></li><li>Paper IV: <a href="https://ieeexplore.ieee.org/document/9186685">Deep learning based systems developed for fall detection: a review, 2020</a></li><li>Paper V: <a href="https://ieeexplore.ieee.org/abstract/document/8869737">Implementation of Fall Detection System Based on 3D Skeleton for Deep Learning Technique, 2019</a></li><li>Paper VI: <a href="https://ieeexplore.ieee.org/document/8369778">Human fall-down event detection based on 2D skeletons and deep learning approach</a></li><li></li></ul></li><li>Human activity recognition<ul><li>Paper VII: <a href="https://link.springer.com/content/pdf/10.1007/s11042-020-09004-3.pdf">Vision-based human activity recognition: a survey, 2020</a></li><li>3D and depth data<ul><li>Paper VIII: <a href="https://www.sciencedirect.com/science/article/pii/S0167865514001299">Human activity recognition from 3d data: a review, 2014</a></li><li><a href="https://arxiv.org/abs/1711.08362">RGB-D-based Human Motion Recognition with Deep Learning: A Survey, 2017</a></li></ul></li><li>3D skeleton-based human representations<ul><li><a href="https://arxiv.org/abs/1601.01006">Space-Time Representation of People Based on 3D Skeletal Data: A Review, 2016</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S0031320315004392">3D skeleton-based human action classification: A survey,2015</a></li><li><a href="https://d1wqtxts1xzle7.cloudfront.net/47833181/Crowd_analysis_A_survey20160806-21965-155miur.pdf?1470471444=&amp;response-content-disposition=inline%3B+filename%3DCrowd_analysis_a_survey.pdf&amp;Expires=1614620280&amp;Signature=gO5XOCbzmA4O~6zc1hli7UqnkZmethCye13xIqVW58A~NTeZYwbbxSs3vZsO4E9~73WX7gYBapzo3quA7UV5jFDRfaDQ6v0ds8dA3BDhB5ys2PlxRFWxEmPsfGAPSp7G6inWLRrfw89L2xXRnX-KM1caNEnqcsg18OD9zf8LU3aovB4hXyB0kvMtc2T2FXdg1HdlQbjqVAlZmrcSl2Y98j1Gr4it23BLSbmUmwZpYAtVA4WqUwFihyqQco5XHX3dhJn7eUdKTOc6QdqQ2KumIhXBwnHSR8TOF9StECcxoUlOf9fcrEgRH4tDauMCsVqgCWJkANhI4~lp0nJEPP21fQ__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">Crowd analysis: a survey, 2008</a></li><li><a href="https://ieeexplore.ieee.org/document/6909476">Human action recognition by representing 3d skeletons points in a lie group, CVPR 2014</a></li></ul></li><li>knowledge-based HAR activity recognition<ul><li><a href="https://www.sciencedirect.com/science/article/pii/S0957417416302913">A survey on using domain and contextual knowledge for human activity recognition in video streams.,2016</a></li></ul></li><li>Abnormal HAR<ul><li><a href="https://www.sciencedirect.com/science/article/pii/S0952197618301775">A review of state-of-the-art techniques for abnormal human activity recognition, 2018</a></li><li></li></ul></li></ul></li></ul><a id="more"></a><h1 id="fall-detection">Fall detection</h1><h2 id="paper-i-2020">Paper I: 2020</h2><p><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full">Elderly Fall Detection Systems: A Literature Survey, 2020</a></p><h3 id="why">Why?</h3><h4 id="types-of-falls"><strong>Types of falls</strong></h4><ul><li>Types<ul><li>forward, lateral and backward in <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B35">El-Bendary et al.</a></li><li>forward, backward, left-side, right-side, blinded-forward and blinded-backward in <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B95">Putra et al.</a></li><li>fall lateral left lie on the floor, fall lateral left and sit up from floor, fall lateral right and lie on the floor, fall lateral and left sit up from the floor, fall forward and lie on the floor, and fall backward and lie on the floor in <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B23">Chen et al</a></li></ul></li><li>Elderly people may suffer from longer duration of falls, because of motion with low speed in the activity of daily living</li><li>The characteristics of different types of falls are not taken into consideration in most of the work on fall detection surveyed. (like age, gender etc.)</li></ul><h4 id="previous-work-and-falls">Previous work and falls</h4><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>Paper</th><th>contents</th><th>summary</th></tr></thead><tbody><tr class="odd"><td><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B18">Chaudhuri et al. (2014)</a></td><td>fall detection devices for people of different ages (excluding children) from several perspectives, including background, objectives, data sources, eligibility criteria, and intervention methods.</td><td>most of the studies were based on synthetic data</td></tr><tr class="even"><td><strong><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B131">Zhang et al. (2015)</a></strong></td><td>vision-based fall detection systems and their related benchmark data sets. Methods are divided into four categories, namely <strong>individual single RGB cameras, infrared cameras, depth cameras, and 3D-based methods using camera arrays</strong>. Methods are also divided to rely on the activity/inactivity of the subjects, shape (width-to-height ratio), and motion.</td><td>Non-vision sensors are not included</td></tr><tr class="odd"><td><strong><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B16">Cai et al. (2017)</a></strong></td><td><strong>depth cameras</strong>, reviewed the benchmark data sets acquired by Microsoft Kinect and similar cameras.</td><td>helpful for looking for benchmark data sets</td></tr><tr class="even"><td><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B21">Chen et al. (2017a)</a></td><td>vision- and non-vision-based systems</td><td>fusion of depth cameras and inertial sensor resulted in a system that is more robust</td></tr><tr class="odd"><td><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B50">Igual et al. (2013)</a></td><td>low-cost cameras and accelerometers embedded in smartphones may offer the most sensible technological choice for the investigation of fall detection. They also reported three main challenges: (i) real-world deployment performance, (ii) usability, and (iii) acceptance.</td><td>another option of sensors</td></tr></tbody></table><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210207202446658.png" alt="image-20210207202446658" style="zoom:40%;" /></p><h3 id="goals">Goals</h3><p>provide a literature survey of work conducted on elderly fall detection using sensor networks and IoT in terms of data acquisition, data analysis, data transport and storage, sensor networks and Internet of Things (IoT) platforms, as well as security and privacy.</p><h3 id="how">How?</h3><h4 id="the-components-of-system">The components of system</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210207204832364.png" alt="image-20210207204832364" style="zoom:60%;" /></p><h4 id="sensors">Sensors</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210207230722406.png" alt="image-20210207230722406" style="zoom:50%;" /></p><ul><li>Individual wearable sensors<ul><li>possible choices: accelerometers, gyroscopes, glucometers, pressure sensors, ECG (Electrocardiography), EEG (Electroencephalography), or EOG (Electromyography)</li><li><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B14">Bourke et al. (2007)</a> found that accelerometers are regarded as the most popular sensors for fall detection. Smart phones are more practical compared with wearable sensors.</li></ul></li><li>Individual vision sensors<ul><li>Possible choices: infrared, RGB, RGB-D etc.</li><li>main challenge of vision-based detection is the potential violation of privacy</li><li>Almost use synthetic falling dataset. <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B13">Boulard et al. (2014)</a> has actual fall data and the other by <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B108">Stone and Skubic (2015)</a> has mixed data.</li></ul></li><li>Individual ambient sensors<ul><li>Possible choices: active infrared RFID,pressure, smart tiles, magnetic switches, doppler radar, ultrasonic and microphone.</li></ul></li><li>Subjects<ul><li>simulated data from OpenSim contributed to an increase in performance to the resulting models</li><li>transfer learning which adapt to subjects who were not represented in the training data</li><li>reinforcement learning for different subjects</li></ul></li></ul><h4 id="sensor-fusion">Sensor fusion</h4><ul><li><p>Types</p><p><img src="https://www.frontiersin.org/files/Articles/520978/frobt-07-00071-HTML/image_m/frobt-07-00071-g007.jpg" style="zoom:50%;" /></p></li><li><p>feature fusion is the most popular approach, followed by decision fusion.</p></li></ul><h3 id="trends-challenges">Trends &amp; challenges</h3><h4 id="trends">Trends</h4><ul><li>Sensor fusion</li><li>ML,DL and RL</li><li>With 5G wireless networks</li><li>Data augmentation<ul><li>Personalized data: the historical medical and behavioral data of Individuals (<a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B35">El-Bendary et al. (2013)</a> and <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B84">Namba and Yamada (2018b)</a>)</li><li>Use skeletal models for simulation: <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B79">Mastorakis et al. (2007</a>, <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B78">2018)</a>, applied the skeletal model simulated in Opensim,</li></ul></li><li>Fog computing : Intel RealSense includes a 28 nanometer processor</li></ul><h4 id="challenges">Challenges</h4><ul><li>The rarity of data of real falls : only <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B70">Liu et al. (2014)</a> used a data set with nine real falls along with 445 simulated ones.</li><li>Detection in real-time</li><li>Security and privacy</li><li>Platform of sensor fusion</li><li>Limitation of locations: indoor and outdoor environments</li><li>Scalability and flexibility</li></ul><h2 id="paper-ii-vision">Paper II: vision</h2><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1077.2575&amp;rep=rep1&amp;type=pdf">A survey on vision-based fall detection, 2015</a></p><h3 id="goal">Goal</h3><ol type="1"><li><p>focus on recent vision-based fall detection techniques, including depth-based methods;</p></li><li><p>discuss the publicly available fall datasets.</p></li></ol><h3 id="fall-datasets">Fall datasets</h3><p>[1] Definition and performance evaluation of a robust svm based fall detection solution.</p><p>[2] Multiple cameras fall dataset</p><p>The EDF dataset mentioned in this paper, I cannot find the original paper except for <a href="http://vlm1.uta.edu/~athitsos/publications/zhang_isvc2014.pdf">this one</a> , in which EDF and OCCU are both created. But if it's the paper I list, the description of EDF in this paper is not totally correct.</p><table><colgroup><col style="width: 11%" /><col style="width: 15%" /><col style="width: 10%" /><col style="width: 9%" /><col style="width: 22%" /><col style="width: 30%" /></colgroup><thead><tr class="header"><th>-</th><th>SDUFall</th><th>EDF?</th><th>OCCU</th><th>[1]</th><th>[2]</th></tr></thead><tbody><tr class="odd"><td>camera type</td><td>1 kinetic</td><td>2 kinetics</td><td>2 kinetics</td><td>1 RGB camera</td><td>8 calibrated RGB camera</td></tr><tr class="even"><td>camera viewpoints</td><td>1</td><td>2</td><td>2</td><td>NaN</td><td>8</td></tr><tr class="odd"><td>fall type</td><td>falls with different directions</td><td>eight fall directions</td><td><strong>occluded falls</strong></td><td>falls with different directions</td><td>forward, backward falls, falls from sitting down and loss of balance</td></tr><tr class="even"><td>#falls</td><td>200</td><td>320</td><td>60</td><td>192</td><td>200</td></tr><tr class="odd"><td>actions in daily life?</td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td></tr><tr class="even"><td>#scenarios</td><td>1</td><td>1</td><td>1</td><td>4 (home, coffee room, office, lecture room)</td><td>24</td></tr><tr class="odd"><td>#subjects</td><td>20</td><td>10</td><td>5</td><td></td><td>1</td></tr><tr class="even"><td>resolution</td><td><span class="math inline">\(320\times240\)</span></td><td><span class="math inline">\(320\times240\)</span></td><td></td><td><span class="math inline">\(320\times240\)</span></td><td></td></tr><tr class="odd"><td>frame rate</td><td>30 fps</td><td>25 fps</td><td>30 fps</td><td>25 fps</td><td></td></tr><tr class="even"><td>year</td><td>2014</td><td>2008</td><td>2014</td><td>2012</td><td>2010</td></tr></tbody></table><h3 id="vision-based-fall-detectors">Vision-based fall detectors</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Comment: <span class="keyword">the</span> classification rules <span class="keyword">for</span> <span class="keyword">each</span> section are <span class="keyword">not</span> <span class="keyword">the</span> same.</span><br></pre></td></tr></table></figure><h4 id="using-single-rgb-camera">Using single RGB camera</h4><p>Firstly classify by features, then the tasks of it is mentioned</p><ul><li>Shape-related features : based on width to height aspect ratio of the person.<ul><li>background separation to get silhouette and then use SVM for classification:</li><li>shape deformation from silhouettes and use shape deformation based GMM for classification</li><li>ellipse shape for body modeling and GMM for extracting moving object.</li><li>2 HMMs for classify falls and normal activities</li><li>extracting projection histograms of the segmented body silhouette and then use it as feature vector, complete posture classification by KNN.</li></ul></li><li>Motion pattern<ul><li>human motion analysis (analyzing the energy of the motion active area ) + human silhouette shape variations to detect slip-only and fall events</li><li>applying Integrated Time Motion Image (ITMI) to fall detection. ITMI is the calculated the PCs of typical video clip for representing a motion pattern.</li><li>threshold-based by the last few frames (falling, the magnitude of the fall, the maximum velocity of the fall ) etc.</li><li>extract the 3D head trajectory using a single calibrated camera.</li><li>extract foreground human silhouette via background modeling. Ellipse fitting for human body and analyze silhouette motion by an integrated normalized motion energy image.</li></ul></li><li>Inactivity detection<ul><li>use ceiling-mounted, wide-angle cameras with vertically oriented optical axes to reduce the influences of occlusion.</li><li>use learned models of spatial context in conjunction with a tracker</li><li>STHF descriptor by <a href="https://ieeexplore.ieee.org/document/6916794">Charfi et al</a>.</li></ul></li></ul><h4 id="using-multiple-cameras">Using multiple cameras</h4><p>Classified by tasks</p><ul><li>3D reconstruction<ul><li><strong>volume distribution</strong> along the vertical axis, check <a href="https://scholar.google.ca/scholar?q=Fall+detection+using+body+volume+reconstruction+and+vertical+repartition+analysis&amp;hl=zh-CN&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart">here</a>.</li><li>multiple cameras + a hierarchy of fuzzy logic to detect falls. Use voxel person (linguistic summarization of temporal fuzzy inference curves)</li></ul></li><li>Multiple viewpoint<ul><li>Use LHMM (layered hidden Markov model)</li><li>combining decisions from different camera</li><li>Using the measures of humans heights and occupied areas.</li></ul></li></ul><h4 id="using-depth-cameras">Using depth cameras</h4><ul><li>Using the distance from the top of the person to the floor<ul><li>the distance of human centroid to floor is smaller than threshold and the person does not move for a certain seconds</li><li>Other features like head to floor distance, person area and shape's major length to width</li></ul></li><li>analyzing how a human has moved during the last frames<ul><li>characterize the vertical state of a segmented 3D object for each frame. Then compute a confidence by the features extracted on ground event which are fed into an ensemble of decision trees.</li><li>Bayes network on fall detection, including duration, total head drop, maximum speed, smallest head height and fraction of frames for which the head drops</li><li>calculating the velocity based on the contraction or expansion of the width, height and depth of the 3D bounding box.</li></ul></li><li>human body key joints<ul><li>3D body joints at each frame are extracted by randomized decision tree, and the 3D trajectory of the head joint is used to determine whether the fall action has occurred.</li><li>3D depth for main detection (structure similarity and vertical height of the person), RGB for out-of-the-rage of the depth camera (the width-height ratio of the detected human bounding box is for recognizing different activities).</li><li>an action is represented by a bag of curvature scale space features (BoCSS) of human silhouettes. Or represent an action as Fisher Vector (on CSS). Or check whether the width-height ratio of temporal bounding box is greater than the predefined threshold, if it is, then it's fall. Otherwise check 2D velocity and the 3D centroid information</li></ul></li></ul><h3 id="conclusions">Conclusions</h3><ul><li><p>Methods with cameras</p><table><colgroup><col style="width: 20%" /><col style="width: 38%" /><col style="width: 40%" /></colgroup><thead><tr class="header"><th>method</th><th>merits</th><th>demerits</th></tr></thead><tbody><tr class="odd"><td>single RGB camera</td><td>* no requirement of camera calibration<br />* inexpensive</td><td>* case specific and viewpoint-dependent<br />* occlusion problem</td></tr><tr class="even"><td>calibrated multi-camera systems</td><td>* viewpoint independent <br />* No occlusion</td><td>* careful and time-consuming calibration<br />* repeatedly calibration if any of them moved</td></tr><tr class="odd"><td>depth camera</td><td>* viewpoint independent <br />* No occlusion</td><td>* price</td></tr></tbody></table></li><li><p>Tips for benchmark</p><ul><li>both falls and activities of daily life (ADL) are requried</li><li>include various falls</li><li>include different camera viewpoints: to verify whether the proposed algorithms are viewpoint-independent</li><li>real falls: consider the distribution of volunteers like gender, ages etc.</li></ul></li><li><p>Tips for fall detection</p><ul><li>combine with Other type of data like sound,</li></ul></li></ul><h2 id="paper-iii-3d-depth">Paper III: 3D depth</h2><p><a href="https://www.sciencedirect.com/science/article/pii/S2352864815000681">3D depth image analysis for indoor fall detection of elderly people, 2015</a></p><h3 id="previous">Previous</h3><ul><li><p>Existing fall detection methods</p><ul><li><p>wearable sensor based</p><p>small , cheap, wearable, but high drift</p></li><li><p>ambient sensor-based</p><p>low detection precision of these sensors, and the precision is distance-relied and thus more sensors are required if with a bigger room</p></li><li><p>computer-vision based</p><p>more robust and less intrusive.</p><ul><li>methods with a single RGB camera</li><li>3d-based methods Using multiple cameras</li><li>depth camera</li></ul></li></ul></li><li><p>Usually <strong>shape relative features of human motion analysis and inactivity detection are used as clues for detecting falls.</strong></p></li><li><p>moment functions are powerful while describing the human shape</p><ul><li>On grey or color images an ellipse is more accurate than a bounding box</li><li>depth camera can offer 3D data so as to work on shape analysis</li></ul></li><li><p>Threshold based fall detection</p><ul><li>Features that can be used: head-ground distance gap and head-shoulder distance gap, the orientation of the ellipse of the human object and the motion of the human object</li><li>methods: DT, NNs, SVM, Bayesian Belief Network</li></ul></li></ul><h3 id="goal-1">Goal</h3><p>deals with the fall detection of the single elderly person in the home environments</p><h3 id="how-1">How?</h3><h4 id="idea">Idea</h4><ul><li>extracting silhouette of moving Individual by extracting background frame</li><li>with the horizontal and vertical projection histogram statistics the depth images are converted to disparity map</li><li>coefficients of the human body are calculated to determine the direction of individual</li><li>threshold based: centroids of the human body to the floor plane and the angle between the human body and the floor plane</li></ul><h4 id="method">Method</h4><p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S2352864815000681-gr1.jpg" style="zoom:67%;" /></p><ul><li><p>Data preprocessing</p><ul><li><p>depth data: only adopt kinetic data in trusted range (1.2-3.8m)</p></li><li><p>extracting silhouette of the moving individual: subtracting the median-filtered background depth frame</p></li><li><p>floor plane: disparity map. <strong>Floor plane will be a noticeable slant and thick straight line in the disparity map</strong></p><p>$= $, for kinetic, <span class="math inline">\(f=580\)</span> pixels, <span class="math inline">\(b=7.5\)</span> cm, and <span class="math inline">\(d\)</span> is the distance between one point in the space TPM the center of the kinetic.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210215180234037.png" alt="image-20210215180234037" style="zoom:80%;" /></p><p>The floor equation then is <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210215202222673.png" alt="image-20210215202222673" style="zoom:70%;" /></p></li><li><p>The orientation of human body: after estimating the ellipse of human body on the image plane, then estimate orientation</p></li><li><p>calculate the distance from the centroid of the human body to the floor plane and the angles between the body and the floor plane.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210215215635773.png" alt="image-20210215215635773" style="zoom:70%;" /></p></li></ul></li></ul><h3 id="experiments">Experiments</h3><ul><li>The dataset is based on their self-captured dataset.-- <strong>Cannot find the sharing link of dataset</strong></li><li>Only two clips' trajectories are listed, no accuracy data shown</li></ul><h3 id="conclusions-1">Conclusions</h3><ul><li>the method may be taken as a candidate</li><li>the result of experiments is not convince.</li></ul><h2 id="paper-iv-dl">Paper IV: DL</h2><p><a href="https://ieeexplore.ieee.org/document/9186685">Deep learning based systems developed for fall detection: a review, 2020</a></p><h3 id="previous-1">Previous</h3><ul><li>the reviewed fall detection systems have some general steps, combined with sensing, data processing, fall event recognition and emergency alert system to rescue the victim.</li><li></li></ul><h3 id="goal-2">Goal</h3><p>presenting a summary and comparison of existing state-of-the-art deep learning based fall detection systems to facilitate future development in this field.</p><p>The categorization focuses on how the different principal methods (CNN, LSTM, and AE) handle the event data captured by sensors.</p><h3 id="how-2">How?</h3><p>Methods used for fall detection</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210219211345800.png" alt="image-20210219211345800" style="zoom:67%;" /></p><h4 id="cnn-based-fall-detection-systems">CNN based fall detection systems</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210221105345137.png" alt="image-20210221105345137" style="zoom:67%;" /></p><h5 id="cnn">CNN</h5><ul><li><a href="http://eprints.bournemouth.ac.uk/29421/1/activity-recognition-indoor.pdf">Adhikari et al</a> : based on videos images from RGB-D camera .<ul><li>used their own dataset</li><li>poor sensitivity when the user was in crawling, bending and sitting positions. The system also works in a selected environment. Developed on a single-person scenario</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8302004">Li et al</a>: extract human shape deformation by CNNs<ul><li>used dataset URFD</li><li>not tested on real dataset</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8732857">Yhdego et al.</a>:<ul><li>convert the data from accelerometer to image by continuous wavelet transform</li><li>use transfer learning for these images</li><li>use URFD dataset</li></ul></li><li><a href="https://dl.acm.org/doi/pdf/10.1145/3136755.3136802">Yu et al</a><ul><li>extract human body silhouette by background subtraction. CNN is for preprocessing the silhouette</li><li>codebook background subtraction</li></ul></li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-93659-8_53">Shen et al.</a><ul><li>cloud</li><li>deep-cut NNs to detect the key points of human body, feed key points into NNs</li></ul></li><li><a href="https://pubmed.ncbi.nlm.nih.gov/30959877/">Santos et al</a><ul><li>IoT, fog computing</li><li>used three open datasets</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8551564">Zhou et al</a> : multi-sensor fusion<ul><li>STFT is for extracting the time frequency (TF) micro-motion features</li><li>Two Alexnet and one SSD (single shot multi-box detector) net are for classifying the TF features</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8662651">He et al.</a>: FD-CNN net<ul><li>collected data is mapped into 3-channel RGB bitmap image</li><li>the image plus SisFall and MobiFall datasets for training FD-CNN</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8664624">Sadreazami et al</a> : CNN for time-series data (radar)</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-030-20257-6_22">Sortis et al</a> : raw accelerometer data, CUSUM (cumulative sum) algorithm</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8295206">Lu et al</a>: use optical flow images, pretrained 3D CNN on different dataset, transfer learning. Accurate for single-person detection but may not work well on multi-person detection</li><li><a href="https://ieeexplore.ieee.org/document/8813332">Wang et al</a>: tri-axial accelerometer, gyroscope sensor in the smart insole. CNN for improving the accuracy level (it's used directly on the raw sensor data)</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8796672">Zhang and Zhu</a>: CNN that works on raw 3-axis accelerometer data streams.</li><li><a href="https://ieeexplore.ieee.org/document/8787213">Camerio et al</a>: multi-stream model, takes high-level handcrafted feature generators. CNN for optical flow， RGB and human estimated pose. Use URFD and FDD datasets for training</li><li><a href="https://pubmed.ncbi.nlm.nih.gov/32155936/">Casilari et al</a> :CNN for recognizing the pattern from the tri-axial transportable accelerometer. Using multiple dataset for training. For real-life performance, LSTM is used</li><li><a href="https://www.sciencedirect.com/science/article/pii/S0010482519303816">Espinosa et al</a> : Using UP-fall detection multi-modal dataset. <strong>Only vision-based</strong> .</li></ul><h5 id="d-cnn">1D CNN</h5><ul><li><a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/el.2018.6117">Cho and Yoon</a> : SVD on accelerometer data for 1Data CNN. Works for triaxial acceleration data.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8869737">Tsai and Hsu et al.</a>: feature extraction algorithm for converting the depth image to skeleton information . Only seven highlighted feature points are picked from the skeleton joints. Performs well on NTU RGB+D</li></ul><h5 id="d-cnn-1">3D CNN</h5><ul><li><a href="https://ieeexplore.ieee.org/document/8622342">Ranhnemoonfar et al.</a>: Kinect depth camera, Adam optimizer. SDUFall dataset for training.</li><li><a href="https://ieeexplore.ieee.org/document/8779504">Li et al.</a>: pre-impact fall detection. Pretrained model + new samples for fine tuning.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/7946918">Hwang et al.</a>: 3D-CNN for continuous motion data from depth cameras. Using data augmentation. Using TST fall detection dataset</li><li><a href="https://www.researchgate.net/publication/333852390_Human_Fall_Recognition_using_the_Spatiotemporal_3D_CNN">Kasturi et al.</a>: kinetic camera. the data fed into 3DCNN is a staked cube. Tested on UR fall detection dataset</li></ul><h5 id="fof-cnn">FOF CNN</h5><p>FOF: feedback optical flow convolution</p><ul><li><a href="https://ieeexplore.ieee.org/document/8101471">Hsieh and Jeng</a> : IoT, used Feature Feedback Mechanism Scheme (FFMS) and 3D-CNN. KTH dataset.</li></ul><h4 id="lstm-based-fall-detection-systems">LSTM based fall detection systems</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210221123633848.png" alt="image-20210221123631712" style="zoom:67%;" /></p><h5 id="lstm-with-3d-cnn">LSTM with 3D CNN</h5><ul><li><a href="https://ieeexplore.ieee.org/abstract/document/8256202">Lu et al.</a>: trained an extractor only on kinetic data, LSTM+ spatial visual attention. Use Sports-1M dataset, and fall dataset of Multiple Cameras.</li></ul><h5 id="lstm-with-rnn-and-cnn">LSTM with RNN and CNN</h5><ul><li><a href="https://ieeexplore.ieee.org/document/8369778">Lie et al.</a>: CNN for extracting 2D skeletons from RGB camera, LSTM for classifying actions. Has an online version</li><li><a href="https://ieeexplore.ieee.org/document/8615759">Abobakr et al.</a>: Kinect RGB-D camera. Convolutional LSTM + ResNet for visual feature extraction, LSTM for sequence modeling and logistics regression for fall detection. Use URFD public dataset, can work in real-time</li><li><a href="https://iopscience.iop.org/article/10.1088/1742-6596/1267/1/012044/meta">Xu et al.</a>: IoT, acceleration data from a tri-axial accelerometer as input . Outperfoms SVM+CNN model,</li><li><a href="https://journals.sagepub.com/doi/full/10.1177/1550147717703257">Tao and Yun</a>: body posture and human biomechanics equilibrium, fed depth camera data to RNN+LSTM and extracting 3D skeletons. Then by computing center of mass (COM) positions and the region of base support, falls are detected.</li><li><a href="https://www.hindawi.com/journals/wcmc/2019/9507938/">Ajerla <em>et al.</em></a> : used edge devices like a laptop for computing, MetaMotionR sensor (tri-axial accelerometer), subset of the MobiAct public dataset for training,</li><li><a href="https://www.researchgate.net/publication/332780940_Edge-AI_in_LoRa-based_Health_Monitoring_Fall_Detection_System_with_Fog_Computing_and_LSTM_Recurrent_Neural_Networks">Queralta <em>et al.</em></a>: edge computing and fog computing. LSTM+RNN</li><li><a href="https://core.ac.uk/download/pdf/286564597.pdf">Luna-Perejon <em>et al.</em></a>: LSTM+GRU, accelerometer data as input, SisFall dataset for training.</li><li><a href="https://www.researchgate.net/publication/328310209_Embedded_Real-Time_Fall_Detection_with_Deep_Learning_on_Wearable_Devices">Torti et al.</a>: Micro-Controller Unit (MCU) with Tri-axial accelerometers, LSTM, tensorflow, SisFall dataset</li><li><a href="https://link.springer.com/chapter/10.1007/978-981-10-7419-6_25">Theodoridis <em>et al.</em></a> : acceleration measurements, RNN, URFD dataset + random 3D rotation augmentation for training. Comparing 4 type of model: LSTM-Acc, LSTM-Acc Rot, Acc + SVM-Depth, UFT</li><li><a href="https://www.researchgate.net/publication/326622687_Convolutional_recurrent_neural_networks_for_posture_analysis_in_fall_detection">Hsiu-Yu <em>et al.</em></a> : kinetics, posture Types, LSTM compared with CNN, fusion images as input from RGB images after extracting body shape by GMM and optical flow.</li></ul><h5 id="lstm-with-rcn-and-rnn">LSTM with RCN and RNN</h5><ul><li><a href="https://www.semanticscholar.org/paper/Co-Saliency-Enhanced-Deep-Recurrent-Convolutional-Ge-Gu/37b625459bd85caba627a7516741c50c05117344">Ge <em>et al.</em></a>: kinect, co-saliency-enhanced RCN, input video clips, RCN + (RNN+LSTM) to label the output.</li></ul><h4 id="ae-based-fall-detection-systems">AE based fall detection systems</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210222085226504.png" alt="image-20210222085226504" style="zoom:80%;" /></p><ul><li><a href="https://ieeexplore.ieee.org/document/7485147">Jokanovic <em>et al.</em></a>: time-frequency (TF) analysis. two stacked AE and a softmax layer.</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-95095-2_18">Droghini et al.</a>: acoustic fall detection, DT (downstream threshold) classifier.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8803671">Zhou and Komuro et al.</a>: VAE+3D-CNN residual block, reconstruction error for detecting fall actions. Dataset High-quality simulated fall dataset (HQFD) and Le2i dataset.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8283539">Seyfioglu et al.</a>: 3 layer-CAE. unsupervised pretraining for the convolutional layers, radar</li></ul><h3 id="conclusion">Conclusion</h3><ul><li>Sensors:<ul><li>RGB camera: not privacy and thus cannot be used in bathroom which with highly risk. Hacker</li><li>Depth camera: only depth, but can be used in like bathroom</li><li>Accelerator: cheap and the data captured are easy to be used,</li></ul></li><li>Vision based methods really depend on the background</li></ul><h2 id="paper-v-3d-skeletons">Paper V: 3D skeletons</h2><p><a href="https://ieeexplore.ieee.org/abstract/document/8869737">Implementation of Fall Detection System Based on 3D Skeleton for Deep Learning Technique</a></p><h3 id="previous-2">Previous</h3><ul><li>Kinect: depth sensor + RGB camera + microphone array</li><li>silhouette normalization : sensitive to the background</li><li>high computation price</li></ul><h3 id="goal-3">Goal</h3><p>real-time fall detection</p><h3 id="idea-1">Idea</h3><ul><li><p>Methods: seven highlight feature points+ pruned CNNs</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210226225948892.png" alt="image-20210226225948892" style="zoom:30%;" /></p><ul><li>Foreground segmentation: GMM, depth information</li><li>Labeling: use the area size to determine whether the object is human or not.</li><li>Thinning: Use Zhang Suen's rapid thinning method. Dilation + erosion</li><li>Searching: just Using the joints on arms and head, they <strong>argue that when a falling event will occur, the joints above the waist will have enormous change.</strong></li></ul></li><li><p>Input: 7 3D joints in 30 frames, (30，21).Use conv1d (so as to prune the number of parameters)</p></li></ul><h3 id="experiments-1">Experiments</h3><ul><li><p>Dataset: NTU RGB+D</p></li><li><p>Performance</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210226232235263.png" alt="image-20210226232235263" style="zoom:40%;" /></p></li><li><p>system: 15 frames per second in real-time</p></li></ul><h3 id="conclusion-1">Conclusion</h3><ul><li>depth image for skeleton extraction, conv1d for parameter pruning</li><li>15 frames per second for real-time implementation.</li></ul><h2 id="paper-vi-3d-skeletons">Paper VI: 3D skeletons</h2><p><a href="https://ieeexplore.ieee.org/document/8369778">Human fall-down event detection based on 2D skeletons and deep learning approach</a></p><h3 id="previous-3">Previous</h3><ul><li>Most of the existing skeleton-based action recognition approaches model actions based on well-designed hand-crafted local features.<ul><li>depth value+HMM etc.</li><li>single RGB image + NNs (CNNs, RNNs)</li></ul></li></ul><h3 id="idea-2">Idea</h3><ul><li>CNN for extracting skeletons and LSTM for final detection</li><li>Skelton extractor:<ul><li>input: 2D RGB image (1920*1080)</li><li>DeeperCut skeleton extraction: resnet backbone, output 14 joints (“forehead”, “chin”, and left and right “shoulder”, “elbow”, “wrist”, “hip”, “knee”, “ankle”, the mean of hip, the mean of chin and central-hip. The last two joints are for robustness with respect to background, so as to build a translation-invariant skeleton model).</li></ul></li><li>LSTM:<ul><li>input: 8 frames, try to classify 5 actions. Supervised training. (8*28), where (28=2*14)</li></ul></li></ul><h3 id="experiments-2">Experiments</h3><ul><li>800 training, 255 validation and 250 test. Manually remove the wrong skeletons (incomplete skeletons) by like a threshold (sum of the distances between each joint and the centroid is calculated)</li><li>trigger rule: during the last 30 outputs,<ul><li>current-time output is “lying” and the previous 19 outputs contain more than 14 “danger” statuses, an alarm will be triggered.</li><li>current output belongs to “safe” status and the previous 8 outputs contain over 5 “safe” statuses, then the alarm signal will be reset.</li></ul></li><li><strong>8 frames per second</strong></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210228193517428.png" alt="image-20210228193517428" style="zoom:40%;" /></li></ul><h1 id="human-activity-recognition">Human Activity Recognition</h1><h2 id="paper-vii">Paper VII</h2><p>Paper VII: <a href="https://link.springer.com/content/pdf/10.1007/s11042-020-09004-3.pdf">Vision-based human activity recognition: a survey</a></p><h3 id="goal-4">Goal</h3><p>review and summarize the progress of HAR systems from the computer vision perspective.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210228233502592.png" alt="image-20210228233502592" style="zoom:50%;" /></p><h3 id="har">HAR</h3><ul><li>Related: determining and naming activities using sensory observations<br /></li><li>Goal: labeling the same activity with the same label even when performed by different persons under different conditions or styles</li><li>Methods: usually by an activity detection task, which includes the temporal identification and localization. Formally, the activity recognition task is divided into classification and detection</li><li>HAR systems are influenced by two technologies: contact-based and remote methods.<ul><li>contact-based: require the physical interaction of the user with the command acquisition machine or device. Sensors like accelerometers, multi-touch screens etc. But it is not that popular now, cause the complicated sensors and price to make it easily to be accepted and wore</li><li>remote based: vision, societal trust, no requirement of ordinary users and thus non-intrusive</li></ul></li><li>Contributions:<ul><li>New update on this rarely focused theme</li><li>thoroughly analysis</li><li>multiple classification methods so as to analyze: detection, tracking and classification stages; feature extraction progress; input data modalities; supervision level; evaluation methods;</li></ul></li></ul><h3 id="benchmarks">Benchmarks</h3><ul><li>CONVERSE: complex conversational interactions</li><li>ALSAN:</li></ul><h3 id="previous-4">Previous</h3><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210301221944071.png" alt="image-20210301221944071" /><figcaption aria-hidden="true">image-20210301221944071</figcaption></figure><h3 id="har-approaches">HAR approaches</h3><h4 id="har-approaches-according-to-feature-extraction-process">HAR approaches according to feature extraction process</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210301232609121.png" alt="image-20210301232608959" style="zoom:50%;" /></p><ul><li><p>The main steps of handcrafted-based features:</p><ul><li>foreground detection that corresponds to action segmentation</li><li>feature selection and extraction by an expert</li><li>classification of action represented by the extracted features</li></ul></li><li><p>The spatial and temporal representations of actions</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210301233924987.png" alt="image-20210301233924987" style="zoom:50%;" /></p><ul><li><p>Based on spatial cues (spatial representation )</p><ul><li>body models: use kinematic joint model, can be 3D model or directly recognize on 2D model</li><li>image models: holistic representation of actions that use a regular grid bounded by ROI centered around the person. E.g., silhouettes, contours, motion history images, motion energy images and optical flow.</li><li>spatial statistics: use a set of statistics of local features from the surrounding regions. E.g., calculated spatial-temporal interest points (STIP) of the image and assign each region to a set of features.<ul><li>volume-based: rely on features like texture, color, posture etc. Recognizing actions by the similarity between two action volumes. <strong>Only works fine on simple action or gesture.</strong> Like SIFT.</li><li>trajectory-based: represent joint positions with 2D or 3D points and then track them. The tracked changes in the posture is used for classification. Noise, view and /or illumination changes robust. Like HOG, HOF etc.</li></ul></li></ul></li><li><p>Based on temporal cues (temporal representation )</p><ul><li>action grammars: represent the action as a sequence of moments, and each moments is described by its own appearance and dynamics. Features are grouped into similar configurations called states and temporal transition between these states are learned. Like HMM, CRF, regression models and context-free grammars</li><li>action templates: representing the appearance of temporal blocks of features and dynamics called templates. Take representations of dynamics from several frames. Methods like Fourier Transform, Wavelet representations and trajectories of body parts are templates that cane be used</li><li>temporal statistics: use statistical models to describe the distributions of unstructured features over time.</li></ul></li><li><p>Appearance based approaches: <em>can be classified according to either shape or motion based characteristics.</em></p><p>Use 2D or 3D depth images and are based on shape features, motion features or any combination of both features. One advance is the skeleton-based recognition. Methods of it can be classified as (1) joint locations: consider the skeleton as a set of points; (2) joint angles: assume the human body as a system of rigid connected segments and the movement as an evolution of their spatial configuration.</p><ul><li>shape based methods: local shape features such as contour points, local region, silhouette and geometric features from the Human image or video after foreground segmentation.</li><li>motion based methods: optical flow and motion history volume from extracting action representation. methods like vector quantization of motion descriptors is used for action recognition. It uses histograms of optical flow and classifiers of bag-of-words.</li><li>hybrid methods: shape + motion features</li></ul></li></ul></li><li><p>Feature learning based methods for extracting representations</p><ul><li>traditional approaches<ul><li>dictionary learning: provides a sparse representation of the input data. It's proved that the use of over-complete dictionaries can produce even more compact representations</li><li>genetic programming: search a space of possible solutions without having any prior knowledge and can discover functional relationships between features in data enabling its classification. GP is used to construct holistic descriptors that allow to maximize the performance of action recognition tasks.</li><li>bayesian networks: PGMs, some use PGMs to represent and capture the semantic relationships among action units, and the correlations of the action unit intensities</li></ul></li><li>deep-learning-based methods<ul><li>generative methods: the main goal of these models is to understand the data distribution including the features that belong to each class. Methods like AE, VAE, GANs</li><li>discriminative methods: DNN,RNN, CNN. E.g., propose long-term temporal convolutions + high-quality optical flow</li><li>hybrid models</li></ul></li></ul></li></ul><h4 id="har-approaches-according-to-the-recognition-stages">HAR approaches according to the recognition stages</h4><ul><li>First stage methods (detection)<ul><li>skin color: used to detect the desire body part. May face problems since the chosen color space or when the objects of the scenario whose color is close to that of the skin. <strong>Seems not that robust</strong></li><li><strong>????</strong>Shape: the contours of the body part shape. This kind of methods are independent of the camera view, skin color and conditions of lighting.</li><li>pixel values: appearance is represented as pixel values change between images of a sequence according to the activity</li><li>3d models: build matches between characteristics of the model based on various features of the images. These methods are independent ode the viewpoint</li><li>motion: Using like the difference in brightness of pixels of two successive images</li><li><strong>????</strong>anisotropic-diffusion: based on the extension of the successful anisotropic diffusion based segmentation to the whole video sequence to improve the detection</li></ul></li><li>Second stage methods (tracking)<ul><li><p>temporal based methods: models are used to follow body parts</p><ul><li>features tracking based on the correlation: try to track the regions that contains body parts. These models require the part being tracked remains in the same neighborhood in the successive images. E.g., use 3D information of depth maps; optical flow</li><li>contours based tracking: snakes . initially place a contour close to the ROI (region of interest ), then it's warped in an interactive way using active shape models in each frame to make to snake converge. Sensitive to color intensity variations and smoothing and softening of contours</li></ul></li><li><p>optimal estimation</p><ul><li>evaluate the state of moving systems from series of measures. In HAR, they are used to estimate the movements of the human body. In real-time systems. Limitations against cluttered backgrounds.</li><li>similar to KF</li></ul></li><li><p>particle filters</p><p>following the body parts and their configurations in complex environments. The location of one body part is modelled by a set of particles</p></li><li><p>cam shift</p><p>based on mean shift algorithm (it uses the models of appearance based on density to represent the targets).</p></li></ul></li><li>Third stage methods (classification)<ul><li>SVM: used with kernel like Gaussian, RBF or linear kernel</li><li>Naive Bayesian classifier:</li><li>KNN: sensitive to local fatal structure</li><li>Kmeans: sensitive to data structures.</li><li>mean shift clustering: no requirement of prior knowledge about the number of clusters and no limit of the forms</li><li>machines finite state: states (the static gestures and postures) and transitions (temporal and /or probabilistic constraints). However, the models need to be changed everytime a new gesture appears. Computational expensive.</li><li>HMMs:</li><li>dynamic time wrapping: calculates the distances between each pair of possible points from two signals. Used for estimating and detecting the movement in a video sequence</li><li>NNs</li></ul></li></ul><h4 id="har-approaches-according-to-the-source-of-the-input-data">HAR approaches according to the source of the input data</h4><ul><li>Uni-modal methods<ul><li>space-time methods: time and the 3D representation f the body to locate activities in space. But sensitive to noise and occlusion. Cannot works on complex actions</li><li>stochastic methods: models like Markov Model. The training is difficult cause the amount of parameters.</li><li>rule-based methods: characterize the activity using a set of rules or attributes</li><li>shape-based methods: shape features to represent and recognize activities. Dependent on the viewpoint, occlusion, people clothing and sensitive to lighting variations</li></ul></li><li>Multi-modal methods<ul><li>emotional methods: associate visual and textual features</li><li>behavior methods: recognize the behavior methods.</li><li>social networks-based methods: allow recognition of social events and interactions</li></ul></li></ul><h4 id="har-approaches-according-to-the-ml-supervision-level">HAR approaches according to the ML supervision level</h4><ul><li>supervised: mostly used to classify and recognize short term actions</li><li>unsupervised: outperform on finding spatio-temporal patterns of motion. computationally complex, less accurate and trustworthy.</li><li>semi-supervised: hybrid.</li></ul><h3 id="activities-type">Activities type</h3><p>Divided by the complexity</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210303222430563.png" alt="image-20210303222430563" style="zoom:40%;" /></p><ul><li>elementary human actions: simple atomic activities</li><li>gestures: a language or part of the non-verbal communication which can be employed to express significant ideas or orders</li><li>behaviors: a set of physical actions and reaction of individuals in specific situations</li><li>interactions: reciprocal actions or exchanges between two entities or more</li><li>group actions:</li><li>events: social actions between individuals</li></ul><h4 id="body-parts">Body parts</h4><ul><li>hand gestures: be tracked to <strong>detect the communication between individuals</strong></li><li>foot: detect shifting and movements of people or Other actions</li><li><strong>facial expressions: interpret specific Types of human activities, especially for handicapped or disabled people</strong></li><li>full body: posturers and human actions by the whole body</li></ul><h3 id="the-types-of-input-data">The Types of Input Data</h3><h4 id="image-or-videos">image or videos</h4><ul><li>HAR on static images: when activity is distinguishable compared to others by its characteristic.</li><li>HAR by videos: offer extra information related to prior and post event, then the relation between two successive frames can be established</li></ul><h4 id="single-viewpoint-or-multi-view-acquisition">single viewpoint or multi-view acquisition</h4><ul><li>single view acquisition</li><li>multiple view acquisition</li></ul><h3 id="evaluations">Evaluations</h3><h4 id="validation-means">Validation means</h4><ul><li>One platform, validated on different Types of datasets (differ on actions) acquired by this platform.</li><li>Different platforms, different datasets but all be evaluated so as to test the generalization across datasets</li><li>Compared with results in literature</li></ul><h4 id="datasets-benchmarks">Datasets (Benchmarks)</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210304202438344.png" alt="image-20210304202438344" style="zoom:67%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210304202924893.png" alt="image-20210304202924893" style="zoom:50%;" /></p><h4 id="evaluation-metrics">Evaluation metrics</h4><ul><li><p>sensitivity: also known as TP, recall or probability of detection. It determines the failure of the system to detect actions</p><p><span class="math inline">\(Sensitivity =\frac{TP}{TP+FN}\)</span></p></li><li><p>precision: also known as PPV (positive prediction value). <span class="math inline">\((1-precision)\)</span> determines the probability of the recognizer incorrectly identifying a detected activity</p><p><span class="math inline">\(Precision=\frac{TP}{TP+FP}\)</span></p></li><li><p>Specificity: also known as false positive rate (FPR).</p><p><span class="math inline">\(Specificity=\frac{TN}{TN+FP}\)</span></p></li><li><p>Negative predictive value (NPV): also known as negative precision. It measures the system sensitivity to negative class.</p><p><span class="math inline">\(NPV=\frac{TN}{TN+FN}\)</span></p></li><li><p>F_measure: the harmonic mean of precision and recall. It gives information of tests accuracy. Best with 1 and worst with 0.</p><p>$F_measure=2 $</p></li><li><p>Accuracy: the percentage of correct predictions relative to the total number of samples</p><p><span class="math inline">\(Accuracy =\frac{\#CorrectPredictions}{\#Predictions}\\Accuracy =\frac{TP+TN}{\#samples}\)</span></p></li><li><p>Likelihood ratio: the likelihood of an activity predicted when it matches the ground truth compared to the likelihood when it's predicted wrongly.</p><p><span class="math inline">\(LR+=\frac{Sensitivity}{1-Specificity},\\ LR-=\frac{1-Sensitivity}{Specificity}\)</span></p></li><li><p>AUC: it's 1 if predicted perfectly.</p></li><li><p>Confusion matrix</p></li><li><p>IoU: intersection over Union, also known as Jaccard index or Jaccard similarity coefficient.</p><p><span class="math inline">\(IoU=\frac{AreaOfOverlap}{AreaOfUnion}\)</span></p></li></ul><h3 id="limitations-and-challenges">Limitations and Challenges</h3><h4 id="limitations">Limitations</h4><p>Show various issues that may affect the effectiveness of HAR system.</p><ul><li>specific to the methods used during the various phase of the recognition process.<ul><li>methods based on the form or the appearance<ul><li>like colorimetric segmentation: confuse the objects of the scene and body parts</li><li>variation in appearance or clothing of people</li></ul></li></ul></li><li>related to the acquisition devices, experimentation environment or various applications of the system.<ul><li>light variations: affect the image quality and then features</li><li>perspective change: if data is acquired by single view it would be a big problem<ul><li>self-occlusion: body parts occlude each other</li><li>occlusion of another object</li><li>partial occlusion of human body parts</li></ul></li><li>variety of gestures linked to the complex structure of human activities and the similarity between classes of different actions.<ul><li>due to data association</li></ul></li><li>noise, complex or moving backgrounds and unstructured scenes, and scale variation</li></ul></li><li>rely on their own recorded dataset to test performance<ul><li>call of a benchmark</li></ul></li></ul><h4 id="challenges-1">Challenges</h4><ul><li>the requirement of continuous monitoring and generate reliable answers at the right time</li><li>modeling and analyzing interactions between people and objects with an appropriate level of accuracy</li><li>societal challenges: acceptance by the society, privacy, side effects of installation<ul><li>privacy: HAR systems on smartphones may be a way out</li></ul></li><li>HAR system should be independent on users' age, color, size or capacity to use</li><li>gestures independence and gestures spotting from continuous data streams<ul><li>detect and recognize various gestures under different background conditions</li><li>tolerant with the scalability and growth of gestures</li></ul></li><li>context-aware: improve applications in its domain</li><li>daily life activities: complex videos and hard to be modeled; overlapping of starting and ending time of each particular activity; discrimination between intentional and involuntary actions</li><li>HAR through missed parts of video, recognition more than one activities performed by one person at the same time, early recognition and prediction of actions</li><li>The implementation of DL HAR system: memory constraint, high number of parameters update, collection an fusion of large multi-modal variant data for the training process, deployment of different architectures of DL based methods in smartphones or wearable devices</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Fall detection
&lt;ul&gt;
&lt;li&gt;Paper I: &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full&quot;&gt;Elderly Fall Detection Systems: A Literature Survey, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper II: &lt;a href=&quot;https://dl.acm.org/doi/10.1145/2769493.2769540&quot;&gt;A survey on vision-based fall detection, 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper III: &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2352864815000681&quot;&gt;3D depth image analysis for indoor fall detection of elderly people, 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper IV: &lt;a href=&quot;https://ieeexplore.ieee.org/document/9186685&quot;&gt;Deep learning based systems developed for fall detection: a review, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper V: &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8869737&quot;&gt;Implementation of Fall Detection System Based on 3D Skeleton for Deep Learning Technique, 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper VI: &lt;a href=&quot;https://ieeexplore.ieee.org/document/8369778&quot;&gt;Human fall-down event detection based on 2D skeletons and deep learning approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Human activity recognition
&lt;ul&gt;
&lt;li&gt;Paper VII: &lt;a href=&quot;https://link.springer.com/content/pdf/10.1007/s11042-020-09004-3.pdf&quot;&gt;Vision-based human activity recognition: a survey, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D and depth data
&lt;ul&gt;
&lt;li&gt;Paper VIII: &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0167865514001299&quot;&gt;Human activity recognition from 3d data: a review, 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.08362&quot;&gt;RGB-D-based Human Motion Recognition with Deep Learning: A Survey, 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;3D skeleton-based human representations
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1601.01006&quot;&gt;Space-Time Representation of People Based on 3D Skeletal Data: A Review, 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0031320315004392&quot;&gt;3D skeleton-based human action classification: A survey,2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://d1wqtxts1xzle7.cloudfront.net/47833181/Crowd_analysis_A_survey20160806-21965-155miur.pdf?1470471444=&amp;amp;response-content-disposition=inline%3B+filename%3DCrowd_analysis_a_survey.pdf&amp;amp;Expires=1614620280&amp;amp;Signature=gO5XOCbzmA4O~6zc1hli7UqnkZmethCye13xIqVW58A~NTeZYwbbxSs3vZsO4E9~73WX7gYBapzo3quA7UV5jFDRfaDQ6v0ds8dA3BDhB5ys2PlxRFWxEmPsfGAPSp7G6inWLRrfw89L2xXRnX-KM1caNEnqcsg18OD9zf8LU3aovB4hXyB0kvMtc2T2FXdg1HdlQbjqVAlZmrcSl2Y98j1Gr4it23BLSbmUmwZpYAtVA4WqUwFihyqQco5XHX3dhJn7eUdKTOc6QdqQ2KumIhXBwnHSR8TOF9StECcxoUlOf9fcrEgRH4tDauMCsVqgCWJkANhI4~lp0nJEPP21fQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA&quot;&gt;Crowd analysis: a survey, 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/6909476&quot;&gt;Human action recognition by representing 3d skeletons points in a lie group, CVPR 2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;knowledge-based HAR activity recognition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0957417416302913&quot;&gt;A survey on using domain and contextual knowledge for human activity recognition in video streams.,2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Abnormal HAR
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0952197618301775&quot;&gt;A review of state-of-the-art techniques for abnormal human activity recognition, 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
      <category term="fall" scheme="http://yoursite.com/tags/fall/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Graph Level Anomaly Detection</title>
    <link href="http://yoursite.com/posts/notes/2021-01-28-notes-paper-anomal-graphlevel-SSL.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-28-notes-paper-anomal-graphlevel-SSL.html</id>
    <published>2021-01-28T18:33:39.000Z</published>
    <updated>2021-07-13T02:31:58.919Z</updated>
    
    <content type="html"><![CDATA[<p>Project <a href="http://snap.stanford.edu/class/cs224w-2019/project/26424135.pdf">Graph Level Anomaly Detection</a></p><p>Paper <a href="https://www.osti.gov/servlets/purl/1214009">Multi-Level Anomaly Detection on Time-Varying Graph Data</a></p><a id="more"></a><h2 id="why">Why?</h2><p>Anomaly detection at a graph level rather than node level or links level.</p><h2 id="goals">Goals</h2><p>modeling a comprehensive representation of a graph’s local and high level structural features, as well as a challenging problem because of the unique properties of graph based data, such as long dependencies and size variability.</p><h2 id="previous">Previous</h2><ul><li>You et al. propose an autoregressive approach to graph generation that is trained sequentially on existing graphs and then generates them at inference time by breaking the process into a sequence of node and edge formations</li><li>You et al.’s work in an RL approach to goal-directed molecular graph generation. Using partially observed subgraphs (discussed later) and an action-based generation framework v</li></ul><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li><p>Formulate two unsupervised learning objectives for graph level anomaly detection. Namely, we compare 1) generative modeling for graph likelihood estimation and 2) a novel method based on masked graph representation learning.</p></li><li><p>look to learn meaningful representations over a family of graphs by modeling <strong>one step edge completion problems.</strong></p></li></ul><h3 id="implementation">Implementation</h3><h2 id="experiments">Experiments</h2><h2 id="conclusion">Conclusion</h2><h2 id="multi-level-anomaly-detection-on-time-varying-graph-data">Multi-Level Anomaly Detection on Time-Varying Graph Data</h2><p><a href="https://www.osti.gov/servlets/purl/1214009">Here</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Project &lt;a href=&quot;http://snap.stanford.edu/class/cs224w-2019/project/26424135.pdf&quot;&gt;Graph Level Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paper &lt;a href=&quot;https://www.osti.gov/servlets/purl/1214009&quot;&gt;Multi-Level Anomaly Detection on Time-Varying Graph Data&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Graph Embedded Pose Clustering for Anomaly Detection</title>
    <link href="http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html</id>
    <published>2021-01-15T20:17:13.000Z</published>
    <updated>2021-01-21T22:51:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1912.11850.pdf">Graph Embedded Pose Clustering for Anomaly Detection</a></p><p>Code <a href="https://github.com/amirmk89/gepc">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li><p>Anomaly detection task</p><ul><li>Fine-grained anomaly detection: Detecting abnormal variations of an action: e.g. an abnormal type of walking</li><li>Coarse-grained anomaly detection: Defining normal actions and regard other action as abnormal. Aka there are multiple poses regarded as normal actions, rather than a single normal action.</li></ul></li><li><p>Video anomaly detection</p><ul><li>Reconstructive models: learn a feature representation for each sample and attempt to reconstruct a sample based on that embedding, often using <strong>Autoencoder</strong>. Samples poorly reconstructed are considered anomalous.</li><li>Predictive models: model the current frame based on a set of previous frames, often relying on recurrent neural networks or 3D convolutions. Samples poorly predicted are considered anomalous.</li><li>Reconstructive + predictive models</li><li>Generative models: used to reconstruct, predict or model the distribution of the data, often using Variational Autoencoders (VAEs) or GANs. E.g. the differences in gradient-based features and optical flow.</li></ul></li><li><p>GNNs</p><p>The point is the weighted adjacency matrix.</p><ul><li>Temporal and multiple adjacency extensions. (ST-GCN)</li><li>Graph attention networks. (2s-AGCN)</li></ul></li><li><p>Deep clustering models</p><p>Provide useful cluster assignments by optimizing a deep model under a cluster inducing objective.</p></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Skeleton-based methods make the analysis independent of nuisance parameters such as viewpoint or illumination.</li></ul></li><li>Limitations<ul><li>Traditional RGB-based anomaly detection methods have to consider many trivial information (viewing direction, illumination, background clutter etc.), and those data are sparse in human pose.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Generating action words from skeleton-based graphs and then classify actions into normal and abnormal (anomaly detection). With an aim at it can work both on fine-grained and coarse-grained task.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Map graphs into representation space and then cluster them so as to get action words. At last, Dirichlet process based mixture is used for classifying normal and abnormal.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115162120.png" alt="image-20210115162120660" style="zoom:50%;" /></p><h3 id="data-preparation">Data Preparation</h3><ul><li>Similar skeleton graph as what used in ST-GCN.</li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Backbone: ST-GCN</p></li><li><p>ST-GCAE network</p><ul><li><p>GCN block</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115185527.png" alt="image-20210115185526918" style="zoom:50%;" /></p><p>The block will be used in SAGC</p></li><li><p><strong>SAGC</strong> block</p><p>Each adjacency type is applied with its own GCN, using separate weights.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115190041.png" alt="image-20210115190041541" style="zoom:50%;" /></p><ul><li><p>Adjacency matrices</p><table><colgroup><col style="width: 8%" /><col style="width: 38%" /><col style="width: 28%" /><col style="width: 24%" /></colgroup><thead><tr class="header"><th>matrix</th><th>sharing</th><th>level</th><th>Dimension</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\mathrm{A}\)</span></td><td>fixed and shared by all layers</td><td>body-part connectivity over node relations</td><td><span class="math inline">\([V,V]\)</span>, <span class="math inline">\(V\)</span> is the number of nodes</td></tr><tr class="even"><td><span class="math inline">\(\mathrm{B}\)</span></td><td>individual at each layer, applied equally to all samples</td><td>dataset level keypoint relations</td><td><span class="math inline">\([V,V]\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\mathrm{C}\)</span></td><td>is different for different sample</td><td>sample specific relations</td><td><span class="math inline">\([N,V,V]\)</span>, <span class="math inline">\(N\)</span> is the batch size</td></tr></tbody></table></li></ul></li><li><p>ST-GCAE</p><p>The encoder uses large temporal strides with an increasing channel number to compress an input sequence to a latent vector. The decoder uses temporal up-sampling layers and additional graph convolutional blocks.</p></li></ul></li><li><p><strong>Deep embedded cluster</strong></p><ul><li><p>The input is the embedding from ST-GCAE, denoted as <span class="math inline">\(\mathrm{z}_i\)</span> for sample <span class="math inline">\(i\)</span></p></li><li><p>Soft-assignment --clustering layer</p><p>The probability <span class="math inline">\(p_{ik}\)</span> for the <span class="math inline">\(i\)</span>-th sample to be assigned to the <span class="math inline">\(k\)</span>-th cluster is:</p><p><span class="math inline">\(p_{ik}=Pr(y_i=k|\mathrm{z}_i,\Theta)=\frac{exp(\theta^T_k\mathrm{z}_i)}{\sum\limits_{k&#39;=1}^{K}exp(\theta^T_{k&#39;}\mathrm{z}_i)}\)</span>, where <span class="math inline">\(\Theta\)</span> is the clustering layer’s parameters. (Simple softmax)</p></li><li><p><strong>Optimize clustering layer</strong></p><ul><li>Objective: Minimize the KL-divergence between the current model probability clustering prediction <span class="math inline">\(P\)</span> and a target distribution <span class="math inline">\(Q\)</span>. The target distribution aims to strengthen current cluster assignments by normalizing and pushing each value closer to a value of either 0 or 1.</li><li><strong>EM</strong> style. In expectation step, the entire model is fixed and the target distribution <span class="math inline">\(Q\)</span> is updated. In maximization stage, the model is optimized to minimize the clustering loss <span class="math inline">\(L_{cluster}\)</span>.</li></ul></li></ul></li><li><p>Anomaly classifier--<strong>normality scoring??</strong></p><ul><li>Two types of multimodal distributions. One is at the cluster assignment level; the other is at the soft-assignment vector level.</li><li>DPMM based. Classifier is fitted by soft-assignment vector (e.g., for class <span class="math inline">\(i\)</span> the softmax result) and then it can do inference.</li></ul></li><li><p>Model</p><p>Feeding the embedding from ST-GCAE to clustering layer, then fixing decoder, fine-tune the encoder in ST-GCAE and clustering layer by combined loss. After fine tuning, using DPMM-based classifier for final inference.</p><ul><li><p>Loss function</p><ul><li><p><strong>Reconstruction loss</strong> <span class="math inline">\(L_{rec}\)</span>: <span class="math inline">\(\ell_2\)</span> loss between the original temporal pose graphs and those reconstructed by ST-GCAE, <strong>used in pre-training stage</strong>, for training the whole ST-GCAE.</p></li><li><p><strong>Clustering loss</strong> <span class="math inline">\(L_{cluster}\)</span>, combined with reconstruction loss and used for fine-tuning encoder of ST-GCAE+clustering layer</p><p><span class="math inline">\(L_{cluster}=KL(Q||P)=\sum\limits_i\sum\limits_kq_{ik}\log\frac{q_{ik}}{p_{ik}},\\ q_{ik}=\frac{p_{ik}/(\sum_{i&#39;}p_{i&#39;k})^{\frac{1}{2}}}{\sum_{k&#39;}p_{ik&#39;}/(\sum_{i&#39;}p_{i&#39;k&#39;})^{\frac{1}{2}}}\)</span></p></li><li><p>Combined loss</p><p><span class="math inline">\(L_{combined}=L_{rec}+\lambda\cdot L_{cluster}\)</span>, this loss is for training encoder and clustering layer, which means the decoder is fixed while using it.</p></li></ul></li><li><p>Optimization</p><ul><li>encoder: reconstruction loss + cluster loss</li><li>decoder: reconstruction loss</li><li>clustering layer: cluster loss</li></ul></li></ul></li></ul><h2 id="experiments">Experiments</h2><ul><li><p>Dataset</p><ul><li>ShanghaiTech: 130 abnormal events captured in 13 different scenes with complex lighting conditions and camera angles.<ul><li>training set contains only normal examples</li><li>test set contains both normal and abnormal examples</li><li>2D pose</li></ul></li><li>Kinetics-based: Kinetics-250 and NTU-RGBD. Actions in each set are sampled randomly or meaningfully. In Kinetics dataset, remove actions that focus only on slightly part joints' movements, like hair braiding.<ul><li><em>Few vs. Many</em>: few normal actions (<span class="math inline">\(3\sim5\)</span>) in the training set and many abnormal (<span class="math inline">\(10\sim 11\)</span> hundreds) actions in the test set</li><li><em>Many vs. Few</em>: switch the training set and test set in experiment above.</li></ul></li></ul></li><li><p>Preprocessing</p><ul><li>Pre-extracting 2D pose from ShanghaiTech Campus</li></ul></li><li><p>Input features</p><ul><li>The coordinates of joints</li><li>For ShanghaiTech: The embeddings of the patch around each joint (from one of the pose estimation model's hidden layers)</li></ul></li><li><p>Test Algorithms on coarse-grained (Kinetics and NTU-RGBD)</p><ul><li><p>Autoencoder reconstruction loss: ST-GCAE reached convergence prior to the deep clustering fine-tuning stage.</p></li><li><p>Autoencoder based one-class SVM: fit a one-class SVM using the encoded pose sequence representation</p></li><li><p>Video anomaly detection methods: Train Future frame prediction model and the skeleton trajectory model. Anomaly scores for each video are obtained by averaging the per-frame scores.</p></li><li><p>Classifier softmax scores: supervised baseline. Anomaly score is by either using the softmax vector's max value or by using the Dirichlet normality score</p></li><li><p>Test video in fixed size but with sliding-window if the test video with unknown frames</p></li></ul></li><li><p>Evaluation metrics</p><ul><li>Frame-level score: the maximal score over all the people in the frame</li><li>AUC as the combined score over all frames of one test</li></ul></li><li><p>Summary</p><ul><li><p>On ShanghaiTech (fine-grained): Patches ST-GCAE outstands.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121170448.png" alt="image-20210121170448502" style="zoom:40%;" /></p></li><li><p>On coarse grained dataset, ST-GCAE outperforms, but better on meaningful actions. <em>A good skeleton help ST-GCAE</em> (NTU-RGBD has better detection on skeletons cause the depth data is known.)</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121171357.png" alt="image-20210121171357539" style="zoom:30%;" /></p></li><li><p><em>Failed cases</em>: occlusions, high-speed action like cycling, non-person related abnormal like bursting into a vehicle.</p></li><li><p><strong>Ablation study:</strong> adding some abnormal actions into normal actions</p><ul><li>ST-GACE on NTU-RGBD (<strong><em>only dropping, touching and Rand8 dataset are tested</em></strong>): ST-GCAE loses on average less than <span class="math inline">\(10\%\)</span> of performance when trained with <span class="math inline">\(5\%\)</span> abnormal actions added as noises.</li></ul></li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>The use of embedded pose graphs and a Dirichlet process mixture for video anomaly detection;</li><li>A new coarse-grained setting for exploring broader aspects of video anomaly detection;</li><li>State-of-the-art AUC of 0.761 for the ShanghaiTech Campus anomaly detection benchmark.</li></ul><h2 id="remarks"><font color='blue'>Remarks</font></h2><ul><li>The reconstruction (learning representations of graph) is mixed with clustering in the final loss, will this be good? Won't the trivial information of clustering influence the reconstruction?</li><li>The training set for clustering layer is initialized by the K-Means centroids, won't the initialization methods matter?</li><li><strong>The embeddings of patches around each joint outperforms the simple joint coordinates</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1912.11850.pdf&quot;&gt;Graph Embedded Pose Clustering for Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/amirmk89/gepc&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html</id>
    <published>2021-01-15T01:51:32.000Z</published>
    <updated>2021-01-15T20:20:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/lshiwjx/2s-AGCN">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Convolutional DL based methods manually structure the skeleton as a sequence of joint-coordinate vectors or as a pseudo-image, which is fed into RNNs or CNNs to generate the prediction.</li><li>Skeleton-based action recognition<ul><li>Design handcrafted features to model human body, but they are barely satisfactory.</li><li>DL-based: CNN-based methods are generally more popular than RNN-based methods. But both fail to fully represent the structure of the skeleton data.</li><li>GCN-based: ST-GCN, eliminates the meed for designing handcrafted part assignment or traversal rules.</li></ul></li><li>GNNs<ul><li><strong>Spatial perspective</strong>: directly perform the convolution filters on the graph vertexes and their neighbors, which are extracted and normalized based on manually designed rules.</li><li>Spectral perspective: use the eigenvalues and eigenvectors of the graph Laplace matrices. They perform the graph convolution in the frequency domain with the help of graph Fourier transform.</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>The second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods.</li></ul></li><li>Limitations:<ul><li>Representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependency between correlated joints</li><li>In GCN-based skeleton action recognition, the topology of the graph is set manually and thus may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks.</li><li>ST-GCN: 1) The skeleton graph is heuristically predefined and represents only the physical structure of the human body. 2) The fixed topology of graph limiting the flexibility and capacity to model the multilevel semantic information. 3) One fixed graph structure may not be optimal for all the samples of different action classes. Like hands-related actions and legs-related actions</li></ul></li></ul><h2 id="goals">Goals</h2><p>Propose a improved ST-GCN (graph convolutional based model), so as to use 2nd order information and improve the accuracy of action recognition based on skeletons.</p><p>Make the graph is unique for different layers and samples.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Modification on ST-GCN</p><ul><li>Two types of graphs:<ul><li>Global graph: represents the common pattern for all the data</li><li>Individual graph: represents the unique pattern for each data</li></ul></li><li>Second-order information: the length and directions of bones are formulated as a vector pointing from its source joint to its target joint.</li></ul><h3 id="data-preparation">Data Preparation</h3><ul><li>The structure of the graph follows the work of ST-GCN.</li></ul><h3 id="a-look-at-st-gcn">A look at ST-GCN</h3><p>Graph convolution in ST-GCN: <span class="math inline">\(f_{out}(v_{ti})=\sum\limits_{v_{tj}\in \mathcal{B}_i}\frac{1}{Z_{ti}(v_{tj})}f_{in}(v_{j})\cdot \mathrm{w}(l_{ti}(v_{tj}))\)</span>, follows spatial configuration partitioning.</p><ul><li>Graph convolution in spatial dimension</li></ul><p><span class="math inline">\(f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}_k(f_{in}\mathrm{A}_k)\odot\mathrm{M}_k\)</span>, where <span class="math inline">\(\mathrm{M}_k\)</span> is an <span class="math inline">\(N\times N\)</span> attention map that indicates the importance of each vertex. <span class="math inline">\(\mathrm{A}_k\)</span> <strong>determines whether there are connections between two vertexes and <span class="math inline">\(\mathrm{M}_k\)</span> determines the strength of the connections.</strong></p><ul><li>Graph convolution in temporal dimension: <span class="math inline">\(K_t\times 1\)</span>convolution on the output feature map</li></ul><p><em>The model is calculated based on a predefined graph, which may not be a good choice.</em></p><h3 id="implementation">Implementation</h3><ul><li><p>Adaptive graph convolutional network (<strong>AGCN</strong>)</p><p><strong>BN+9 of adaptive graph convolutional blocks + global average pooling + softmax classifier</strong></p><ul><li><p>Adaptive graph convolutional layer:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142649.png" alt="image-20210115142649202" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115133332.png" alt="image-20210115133332722" style="zoom:50%;" /></p><p><span class="math inline">\(f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}_kf_{in}(\mathrm{A}_k+\mathrm{B}_k+\mathrm{C}_k)\)</span>. The adjacency matrix is now divided into three parts</p><ul><li><p><span class="math inline">\(\mathrm{A}_k\)</span>: same as <span class="math inline">\(N\times N\)</span> adjacency matrix <span class="math inline">\(\mathrm{A}_k\)</span> in ST-GCN, it <em>represents the physical structure of the human body</em>.</p></li><li><p><span class="math inline">\(\mathrm{B}_k\)</span>: An <span class="math inline">\(N\times N\)</span> adjacency matrix. It's trainable. It acts as <span class="math inline">\(\mathrm{M}_k\)</span> (attention mechanism) in ST-GCN, influenced by the connections between two joints and also the strength of the connections.</p></li><li><p><span class="math inline">\(\mathrm{C}_k\)</span>: a similarity matrix calculated by the normalized embedded Gaussian function with vectors embedded by <span class="math inline">\(1\times 1\)</span> convolutional layer.</p><p><span class="math inline">\(\mathrm{C}_k=softmax(\mathrm{f}_{in}^T\mathrm{W}_{\theta k}^{T}\mathrm{W}_{\phi k}\mathrm{f}_{in})\)</span>, where <span class="math inline">\(\mathrm{W}_\theta,\mathrm{W}_\phi\)</span> are the parameters of the embedding functions <span class="math inline">\(\theta,\phi\)</span>, respectively.</p></li></ul></li><li><p>Adaptive graph convolutional block</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115132928.png" alt="image-20210115132928144" style="zoom:33%;" /></p></li></ul></li><li><p>Model: two stream networks</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142748.png" alt="image-20210115142748418" style="zoom:50%;" /></p><ul><li>J-stream: the input data are joints, as what's depicted in AGCN.</li><li>B-stream: the input data are bones.<ul><li>A bone is the vector pointing from its source joint to its target joint. This vector will contain both length and direction of a bone. An empty bone is added so as to make sure the B-stream has similar quantity of input as J-stream.</li></ul></li></ul><p>Finally, the <em>softmax</em> scores of the two streams are added to obtain the fused score and do prediction.</p></li><li><p>Loss function</p><p>Cross-entropy</p></li><li><p>Why does it work?</p><ul><li>Considering bones (2nd information)</li><li>Offers trainable attention matrix <span class="math inline">\(\mathrm{B}_k\)</span> and the similarity evaluation of <span class="math inline">\(\mathrm{C}_k\)</span> to estimate the strength of connection. Both of them offer more possible connections and provide more flexibility.</li></ul></li></ul><h2 id="experiments">Experiments</h2><ul><li>Dataset: Kinetics and NTU-RGBD<ul><li>NTU-RGBD: If the number of bodies in the sample is less than 2, the second body is padded with 0.</li><li>Kinetics: Same data augmentation as done in ST-GCN.</li></ul></li><li>Training: SGD with Nesterov momentum (0.9), batch size is 64. The weight decay is set to 0.0001.<ul><li>NTU-RGBD: learning rate is set as 0.1 and is divided by 10 at the 30th epoch and 40th epoch. The training process is ended at the 50th epoch.</li><li>Kinetics: The learning rate is set as 0.1 and is divided by 10 at the 45th epoch and 55th epoch. Training ends at the 65th epoch.</li></ul></li><li>Evaluation metrics</li><li>NTU-RGBD: top-1 accuracy<ul><li>Kinetics : top-1 and top-5 accuracy</li></ul></li></ul><h3 id="ablation-study">Ablation study</h3><ul><li><p>Adaptive graph convolutional block</p><p>Manually delete one of the graphs and estimate.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115150909.png" alt="image-20210115150909820" style="zoom:33%;" /></p><ul><li>Given each connection, a weight parameter is important, which also proves the importance of the adaptive graph structure</li></ul></li><li><p>Visualization of the learned graphs</p><p>Denote the strength of joints by dot size, the bigger the stronger connection.</p><ul><li><p>A higher layer in AGCN contains higher-level information, comparing the dot size in different layers</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115151421.png" alt="image-20210115151421543" style="zoom:50%;" /></p></li><li><p>The diversity for different sample in the same layer is proved.</p></li></ul></li><li><p>Two-stream framework</p><p>The two-stream method outperforms the one-stream-based methods either the J-stream or the B-stream.</p></li></ul><h3 id="compared-with-sota">Compared with SOTA</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115144443.png" alt="image-20210115144443887" style="zoom:50%;" /></p><ul><li>Question 1: <font color='red'> ResNet helps?</font></li><li>Question 2: <font color='red'>How about compared with methods based on RGB or optical flow ?</font> In paper ST-GCN their model fails to those models.</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>An adaptive graph convolutional network is proposed.</li><li>The second-order information of the skeleton data is explicitly formulated and combined with the first-order information using a two-stream framework, which brings notable improvement for the recognition performance.</li><li>On two large-scale datasets for skeleton-based action recognition, the proposed 2s-AGCN exceeds the SOTA by a significant margin.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf&quot;&gt;Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/lshiwjx/2s-AGCN&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html</id>
    <published>2021-01-13T02:58:39.000Z</published>
    <updated>2021-01-15T18:26:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/yysijie/st-gcn">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Human action recognition: can be solved by appearance, depth, optical flows, and body skeletons.</li><li>Models for graph<ul><li>Recurrent neural networks</li><li>GNNs:<ul><li>Spectral perspective: the locality of the graph convolution is considered in the form of spectral analysis.</li><li><strong>Spatial perspective</strong>: the convolution filters are applied directly on the graph nodes and their neighbors.</li></ul></li></ul></li><li>Skeleton Based Action Recognition<ul><li>Handcrafted feature based methods: design several handcrafted features to capture the dynamics of joint motion. E.g., covariance matrices of joint trajectories, relative positions of joints, rotations and translations between body parts.</li><li>Deep learning methods: recurrent neural networks and temporal CNNs. Many emphasize <strong>the importance of modeling the joints within parts of human bodies.</strong></li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Earlier methods of using skeletons for action recognition simply employ the joint coordinates at individual time steps to form feature vectors, and apply temporal analysis thereon. <strong>They do not explicitly exploit the spatial relationships among the joint</strong>.</li><li>Most existing methods which explore spatial relationship rely on hand-crafted parts or rules to analyze the spatial patterns.</li><li>Traditional CNNs are not suitable for 2D or 3D skeletons (graphs rather than data in grids).</li></ul></li><li>Limitations:<ul><li>Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization.</li><li>Models using hand-crafted parts are difficult to be generalized to others</li><li>These parts used in DL based methods are usually explicitly assigned using domain knowledge, which is not automatic and practical.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Build a better model for dynamics of human body skeletons. Specifically, a new method that can automatically capture the patterns embedded in the <strong>spatial configuration of the joints as well as the temporal dynamics</strong> is required.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Skeletons in frames are connected as natural spatial-temporal graph, then using GCN.</p><h3 id="data-preparation">Data Preparation</h3><ul><li>The feature vector on a node <span class="math inline">\(F(v_{ti})\)</span> consists of coordinate vectors, as well as estimation confidence, of the i-th joint on frame t.</li><li>Construct the spatial temporal graph on the skeleton sequences in two steps. First, the joints within one frame are connected with edges according to the connectivity of human body structure. Then each joint will be connected to the same joint in the consecutive frame.</li><li>Both 18 joints skeleton model or 25 joints skeleton model work fine.</li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Model</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113115442.png" alt="image-20210113115429152" style="zoom:50%;" /></p><p>ResNet mechanism is applied on each ST-GCN unit.</p><table><thead><tr class="header"><th>Layer name</th><th>configuration</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(1\sim3\)</span></td><td>64 channels</td></tr><tr class="even"><td><span class="math inline">\(4\sim6\)</span></td><td>128 channels</td></tr><tr class="odd"><td><span class="math inline">\(7\sim9\)</span></td><td>256 channels, 9 temporal kernel size</td></tr><tr class="even"><td>Global pooling+softmax</td><td></td></tr></tbody></table></li><li><p>Loss function</p><ul><li><p><strong>Sampling function</strong> <span class="math inline">\(\mathbb{p}\)</span> enumerates the neighbors of location <span class="math inline">\(x\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113155452.png" alt="image-20210113155452481" style="zoom:40%;" /></p><p>In this paper, <strong>the 1-neighbor set</strong> of joint nodes are used.</p></li><li><p><strong>The filter weights</strong> <span class="math inline">\(\mathrm{w} (v_{ti},v_{tj})\)</span> are shared everywhere on the input image.</p><p>Build a mapping <span class="math inline">\(l_{ti}:B(v_{ti})\rightarrow\{0,\cdots,K−1\}\)</span> which maps a node in the neighborhood to its subset label. The weight function <span class="math inline">\(\mathrm{w}(v_{ti}, v_{tj}):B(v_{ti})\rightarrow R^c\)</span> can be implemented by indexing a tensor of <span class="math inline">\((c,K)\)</span> dimension or</p><p><span class="math inline">\(\mathrm{w}(v_{ti},v_{tj})=\mathrm{w}&#39;(l_{ti}(v_{tj})\)</span></p><ul><li><p>Labeling strategies (the definition of <span class="math inline">\(l_{ti}\)</span>)</p><p><img src="C:%5CUsers%5C10457%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210113195834069.png" alt="image-20210113195834069" style="zoom:50%;" /></p><ul><li><p>Uni-labeling</p><p>Make the whole neighbor set itself as subset. Then feature vectors on every neighboring node will have a inner product with the same weight vector. Formally, <span class="math inline">\(K=1,l_{ti}(v_{tj})=0,\forall i,j\in V\)</span></p></li><li><p>Distance partitioning</p><p><span class="math inline">\(d=0\)</span> refers to the root node itself and remaining neighbor nodes are in the <span class="math inline">\(d =1\)</span> subset. Formally , <span class="math inline">\(K=2,l_{ti}(v_{tj})=d(v_{tj},v_{ti})\)</span></p></li><li><p>Spatial configuration partitioning</p><p>Three subsets: 1) the root node itself; 2)centripetal group and 3) centrifugal group.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113200600.png" alt="image-20210113200600480" style="zoom:33%;" />, where <span class="math inline">\(r_i\)</span> is the average distance from gravity center to joint <span class="math inline">\(i\)</span> over all frames in the training set.</p></li></ul></li><li><p>Learnable edge importance weighting</p><p>One joint appears in multiple body parts should have different importance in modeling the dynamics of these parts. A learnable mask M is added on every layer of spatial temporal graph convolution.</p></li></ul></li><li><p>The <strong>spatial graph convolution</strong></p><p><span class="math inline">\(f_{out}(v_{ti})=\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(\mathbb{p}(v_{ti},v_{tj}))\cdot \mathrm{w}(v_{ti},v_{tj})=\\\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(v_{tj})\cdot \mathrm{w}(l_{ti}(v_{tj}))\)</span>, <span class="math inline">\(Z_{ti}(v_{tj})=|\{v_{tk}|l_{ti}(v_{tk})=l_{ti}(v_{tj})\}|\)</span> is the cardinality of the corresponding subset. It's for balancing the contributions of different subsets to the output.</p></li><li><p><strong>Spatial temporal modeling</strong></p><p>Extend neighbors so as to include temporally connected joints</p><p><span class="math inline">\(B(v_{ti})={v_{qj}|d(v_{tj},v_{ti})\le K,|q-t|\le \lfloor\Gamma/2\rfloor}\)</span>, where <span class="math inline">\(\Gamma\)</span> is temporal kernel size (controls the temporal range to be included in the neighbor graph). Then , the sampling function is <span class="math inline">\(l_{ST}(v_{qj})=l_{ti}(v_{tj})+(q-t+\lfloor\Sigma/2\rfloor)\times K\)</span>.</p></li><li><p>The final convolution formula:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113222927.png" alt="image-20210113222926988" style="zoom:40%;" />, where <span class="math inline">\(\Lambda_j^{ii}=\sum_k(A_j^{ik})+\alpha\)</span>, <span class="math inline">\(\alpha=0.001\)</span> is to avoid empty rows in <span class="math inline">\(A_j\)</span>. To add learnable mask <span class="math inline">\(M\)</span>, displace <span class="math inline">\(A_j\)</span> as <span class="math inline">\(A_j\otimes M\)</span>.</p></li></ul></li><li><p>Why does it work?</p><ul><li>Parts restrict the modeling of joints trajectories within “local regions” compared with the whole skeleton, thus forming a hierarchical representation of the skeleton sequences.</li></ul></li></ul><h2 id="experiments">Experiments</h2><ul><li><p>Dataset:</p><ul><li><p>Kinetics (unconstrained action recognition dataset), provides only raw video clips without skeleton data.</p><ul><li><p>Augmentation: To avoid overfitting,two kinds of augmentation are used to replace dropout layers when training on the Kinetics dataset. 1) affine transformations, 2) sampling part of the frames from the whole frame and testing by a whole frame. <font color=Blue>May that's a way for avoiding the two consecutive frame are too similar?</font></p></li><li><p>Videos to skeletons</p><p>To work on skeletons, openpose is used for extracting. Concretely, resize all videos to the resolution of 340 × 256 and convert the frame rate to 30 FPS. Then OpenPose toolbox is used to estimate the location of 18 joints on every frame of the clips.</p></li><li><p>Final features:</p><p>Finally <strong>the clips are represented by a tensor in shape</strong> <span class="math inline">\((3,T,18,2)\)</span>, where 18 is the number of joints, 2 is the number of people and 3 is the number of features (X,Y,C), C is the confidence.</p></li></ul></li><li><p>NTU-RGBD ( in-house captured action recognition dataset)</p><ul><li>Already annotated with 25 3D joints</li><li>Each clip is guaranteed to have at most 2 subjects</li></ul></li></ul></li><li><p>Training: SGD with a learning rate of 0.01. <span class="math inline">\(lr\)</span> decay by 0.1 after every 10 epochs.</p></li><li><p>Evaluation metrics</p><ul><li><p>Kinetics</p><p>Test on validation set. Using <strong>top-1 and top-5 classification accuracy</strong></p></li><li><p>NTU-RGBD</p><p>Report top-1 recognition accuracy.</p><ul><li><strong>Cross-subject</strong>: Train on one subset of actors and test on the remaining actors.</li><li><strong>Cross-view</strong>: Train on skeletons from camera views 2 and 3, and test on those from camera view 1.</li></ul></li></ul></li></ul><h3 id="ablation-study">Ablation study</h3><p>Applied on Kinetics dataset.</p><ul><li><p>Spatial temporal graph convolution</p><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th>Model</th><th>configuration</th></tr></thead><tbody><tr class="odd"><td><em>baseline-TCN</em></td><td>squeeze Spatial dimension, concatenate all input joint locations to form the input features at each frame <span class="math inline">\(t\)</span>.</td></tr><tr class="even"><td><em>local convolution</em></td><td>The input data are the same format, but with unshared convolution filters.</td></tr></tbody></table></li><li><p>Partition strategies: same as what described before. <em>Distance partitioning*</em> is as intermediate between the distance partitioning and uni-labeling. The filters in this setting only differs with a scaling factor -1, or to say <span class="math inline">\(\mathrm{w}_0=-\mathrm{w}_1\)</span>.</p></li><li><p>Learnable edge importance weighting</p><p>This setting is named as <em>ST-GCN+Imp</em>.</p></li><li><p>Results</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114194501.png" alt="image-20210114194452831" style="zoom:50%;" /></p><ul><li>Better performance of ST-GCN based models could justify the power of the spatial temporal graph convolution in skeleton based action recognition</li><li>Distance partitioning* achieves better performance than uni-labeling, which again demonstrate <strong>the importance of the partitioning with multiple subsets</strong>.</li><li>ST-GCN model with learnable edge importance weights can learn to express the joint importance.</li></ul></li></ul><h3 id="compared-with-sota">Compared with SOTA</h3><p><strong>Model setting: ST-GCN+Learnable weights+Spatial configuration partitioning</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204800.png" alt="image-20210114204800283" style="zoom:30%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204722.png" alt="image-20210114204722008" style="zoom:30%;" /></p><ul><li>Kinetics: ST-GCN is able to outperform previous representative approaches, but under-perform methods in RGB or optical flow.</li><li>NTU-RGBD: No data augmentation before training. It outperforms all other selected candidates.</li><li>The skeleton based model ST-GCN can provide complementary information to RGB and optical flow models.</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Propose ST-GCN, a generic graph-based formulation for modeling dynamic skeletons, which is the first that applies graph-based neural networks for action recognition.</li><li>Propose several principles in designing convolution kernels in ST-GCN to meet the specific demands in skeleton modeling.</li><li>On two large scale datasets for skeleton-based action recognition, the proposed model achieves superior performance as compared to previous methods using hand-crafted parts or traversal rules, with considerably less effort in manual design.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1801.07455.pdf&quot;&gt;Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/yysijie/st-gcn&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Self-supervised Learning on Graphs, Deep Insights and New Directions</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html</id>
    <published>2021-01-12T03:00:00.000Z</published>
    <updated>2021-01-24T16:54:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2006.10141.pdf">Self-supervised Learning on Graphs: Deep Insights and New Directions</a></p><p><a href="https://github.com/ChandlerBang/SelfTask-GNN">Codes</a></p><a id="more"></a><h2 id="why">Why?</h2><ul><li>Nodes in graphs present unique structure information and they are inherently linked indicating not independent and identically distributed (or i.i.d.).</li><li>(SSL) has been introduced in both the image and text domains to alleviate the need of large labeled data by deriving labels for the significantly more unlabeled data.</li><li>To fully exploit the unlabeled nodes for GNNs, SSL can be naturally harnessed for providing additional supervision.</li><li>The challenges of graph to use SSL:<ul><li>graphs are not restricted to these rigid structures.</li><li>each node in a graph is an individual instance and has its own associated attributes and topological structures</li><li>instances (or nodes) are inherently linked and dependent of each other.</li></ul></li></ul><h2 id="goals">Goals</h2><ul><li><p>Focus on advancing GNNs for node classification where GNNs leverage both labeled and unlabeled nodes on a graph to jointly learn node representations and a classifier that can predict the labels of unlabeled nodes on the graph. Aims at gain insights on when and why SSL works for GNNs and which strategy can better integrate SSL for GNNs.</p></li><li><p><strong><em>Focus on semi-supervised node classification task</em></strong></p><p><span class="math inline">\(\min\limits_{\theta}\mathcal{L}_{task}(\theta,\mathrm{A,X},\mathcal{D}_L)=\sum\limits_{(v_i,y_i)\in\mathcal{D}_L}\ell(f_{\theta}(\mathcal{G})_{v_i},y_i)\)</span></p></li></ul><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121192249.png" alt="image-20210121192249653" style="zoom:40%;" /></p><h2 id="previous-work">Previous work</h2><h3 id="examples">Examples</h3><ul><li><a href="https://arxiv.org/abs/1902.11038">Multi-stage self-supervised learning for graph convolutional networks on graphs with few labels</a> utilize the clustering assignments of node embeddings as guidance to update the graph neural networks.</li><li><a href="https://arxiv.org/abs/2003.01604">Self-supervised graph representation learning via global context prediction</a> proposed to use the global context of nodes as the supervisory signals to learn node embeddings.</li></ul><h3 id="basic-pretext-task-on-graphs">Basic pretext task on graphs</h3><h4 id="structure-information-adjacency-matrix-mathrma">Structure information (Adjacency matrix <span class="math inline">\(\mathrm{A}\)</span>)</h4><p>Construct self-supervision information for the unlabeled nodes based on their local structure information, or how they relate to the rest of the graph</p><h5 id="local-structure-information"><strong>Local structure information</strong></h5><ul><li><p>Node property</p><ul><li><p>use node degree as a representative local node property for self-supervision while leaving other node properties (or the combination) as one future work</p></li><li><p>Formally, let <span class="math inline">\(d_i=\sum\limits_{j=1}^{N}\mathrm{A}_{ij}\)</span> denote the degree of <span class="math inline">\(v_i\)</span> and construct the associated loss of the SSL pretext task as</p><p><span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}(f_{\theta&#39;}(\mathcal{G})_{v_i}-d_i)^2\)</span>, where <span class="math inline">\(\mathcal{D}_U\)</span> denote the set of unlabeled nodes and associated pretext task labels in the graph.</p></li><li><p>Assumption: The node property information is related to the specific task of interest.</p></li></ul></li><li><p><strong>EdgeMask</strong></p><ul><li><p>Build pretext task based on the connections between two nodes in the graph. Specifically, <strong>one can first randomly mask some edges and then the model is asked to reconstruct the masked edges</strong>.</p></li><li><p>Formally, first mask <span class="math inline">\(m_e\)</span> edges denotes as the set <span class="math inline">\(\mathcal{M}_e\subset\varepsilon\)</span> and also sample the set <span class="math inline">\(\bar{\mathcal{M}_e}=\{(v_i,v_j)|v_i,v_j\in\mathcal{V},(v_i,v_j)\notin\varepsilon\}\)</span>, <span class="math inline">\(|\bar{\mathcal{M}_e}|=|\mathcal{M}_e|=m_e\)</span> . Then the SSL pretext task is to predict whether there exist a link between a given node pair.</p><p><span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\\\frac{1}{|\mathcal{M}_e|}\sum\limits_{(v_i,v_j)\in\mathcal{M}_e}\ell(f_w(|f_{\theta&#39;}(\mathcal{G})_{v_i}-f_{\theta&#39;}(\mathcal{G})_{v_j}|),1)+\frac{1}{|\bar{\mathcal{M}_e}|}\sum\limits_{(v_i,v_j)\in\bar{\mathcal{M}_e}}\ell(f_w(|f_{\theta&#39;}(\mathcal{G})_{v_i}-f_{\theta&#39;}(\mathcal{G})_{v_j}|),0)\)</span>, where <span class="math inline">\(\ell(\cdot,\cdot)\)</span> is the cross entropy loss, <span class="math inline">\(f_w\)</span> linearly maps to 1-dimension.</p></li><li><p>Expecting to help GNN learn information about local connectivity.</p></li></ul></li></ul><h5 id="global-structure-information"><strong>Global structure information</strong></h5><p>Not only based on the node itself or limited to its immediate local neighborhood, but also considering the position of the node in the graph.</p><ul><li>PairwiseDistance<ul><li>Maintain global topology information through a pairwise comparison. Or to say, pretext task will be able to distinguish/predict the distance between different node pairs.</li><li>The measurements of distance vary.</li><li>If use the shortest path length <span class="math inline">\(p_{ij}\)</span> as a measure of the distance, then for all node pairs <span class="math inline">\(\{(v_i,v_j)|v_i,v_j\in\mathcal{V}\}\)</span>, they are grouped into four categories: <span class="math inline">\(p_{ij}=\{1,2,3,\ge4\}\)</span>, <span class="math inline">\(4\)</span> is because of the computing price and accuracy (the more neighbors , the more unrelated noises are included.) Practically, randomly sample a certain amount of node pairs <span class="math inline">\(S\)</span> used for SSL during epoch. Then the SSL loss is <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|S|}\sum\limits_{(v_i,v_j)\in S}\ell(f_w(|f_{\theta&#39;}(\mathcal{G})_{v_i}-f_{\theta&#39;}(\mathcal{G})_{v_j}|),C_{p_{ij}})\)</span>, where <span class="math inline">\(C_{p_{ij}}\)</span> is the corresponding distance category of <span class="math inline">\(p_{ij}\)</span>.</li></ul></li><li><strong><em>Distance2Clusters</em></strong><ul><li>Predicting the distance from the unlabeled nodes to predefined graph clusters. Thus enforce the representations to learn a global positioning vector of each of the nodes.</li><li>First partitioning the graph to get <span class="math inline">\(k\)</span> clusters <span class="math inline">\(\{C_1,C_2,\cdots,C_k\}\)</span> by METIS graph partitioning methods. Denote the node with highest degree as center node <span class="math inline">\(c_j\)</span> in each cluster.</li><li>Formally, the SSL will optimize <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-d_i\|^2\)</span>, where <span class="math inline">\(\mathrm{d}_i\)</span> is the distance vector between node <span class="math inline">\(v_i\)</span> and each center.</li></ul></li></ul><h4 id="attribute-information-nodes-matrix-mathrmx">Attribute information (Nodes matrix <span class="math inline">\(\mathrm{X}\)</span>)</h4><p>Guide the GNN to ensure certain aspects of node/neighborhood attribute information is encoded in the node embeddings after a SSL attribute-based pretext.</p><ul><li><p>AttributeMask</p><ul><li>Let GNN learn attribute information via pretext</li><li>Randomly mask (e.g. set to zero ) the features of <span class="math inline">\(m_a\)</span> nodes <span class="math inline">\(\mathcal{M}_a\subset\mathcal{V}, |\mathcal{M}_a|=m_a\)</span>, then SSL will try to construct these features. Formally , <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{M}_a|}\sum\limits_{v_i\in\mathcal{M}_a}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-\mathrm{x}_i\|^2\)</span>, where <span class="math inline">\(\mathrm{x}_i\)</span> is the dense features after PCA.</li></ul></li><li><p>PairewiseAttrSim</p><ul><li><p>The similarity two nodes have in the input feature space is not guaranteed in the learned representations due to the GNN aggregating features from the two nodes local neighborhoods.</p></li><li><p>Specifically,</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210122233255.png" alt="image-20210122233252928" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210122233404.png" alt="image-20210122233403875" style="zoom:50%;" /></p></li><li><p>Only constrain the intra-class distance</p></li></ul></li></ul><h3 id="merge-pretext-task-on-graphs">Merge pretext task on Graphs</h3><ul><li><p>Joint Training</p><ul><li><p>Optimize the SSL loss (i.e., <span class="math inline">\(\mathcal{L}_{self}\)</span>) and supervised loss (i.e., <span class="math inline">\(\mathcal{L}_{task}\)</span>) simultaneously.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123174253.png" alt="image-20210123174253225" style="zoom:50%;" /></p></li><li><p>The overall objective is <span class="math inline">\(\min\limits_{\theta,\theta&#39;}\mathcal{L}_{task}(\theta,\mathrm{A,X},\mathcal{D}_L)+\lambda(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)\)</span>, where <span class="math inline">\(\lambda\)</span> is the hyperparameter to control the distribution of self-supervision.</p></li></ul></li><li><p>Two-stage training</p><p>Fine tuning the model which is pretrained on pretext task on downstream dataset.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123181120.png" alt="image-20210123181120517" style="zoom:50%;" /></p></li></ul><h2 id="analysis">Analysis</h2><ul><li><p>Targets: Understand what SSL information works for GNNs, which strategies can better integrate SSL for GNNs, and further analyze why SSL is able to improve GNNs</p></li><li><p>Datasets: Cora, Citeseer, Pubmed</p></li><li><p>Training: Adam, learning rate <span class="math inline">\(0.01\)</span>, <span class="math inline">\(L_2\)</span> regularization <span class="math inline">\(5e-4\)</span>, dropout rate <span class="math inline">\(0.5\)</span>, <span class="math inline">\(128\)</span> hidden units across all self-supervised information and GCN, top-K=bottom-K=<span class="math inline">\(3\)</span>. <span class="math inline">\(\lambda\)</span> in range <span class="math inline">\(\{0,0.001,0.01,0.1,1,5,10,50,100,500,1000\}\)</span>, <span class="math inline">\(m_e,m_a\)</span> in <span class="math inline">\(\{10\%,20\%\}\)</span> the size of <span class="math inline">\(|V|\)</span>.</p></li><li><p><strong>Two-stage training</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123200058.png" alt="image-20210123200058736" style="zoom:50%;" /></p><ul><li>the configuration of one graph convolutional layer for feature extraction, one graph convolutional layer for the adaptation of node classification and one linear layer for the adaptation of pretext task works very well for all three strategies</li><li>In most cases, the strategy of “Tune all" achieves the best performance--&gt; fine tune for downstream task is necessary.</li></ul></li><li><p><strong>SSL for GNNs</strong></p><ul><li><p><em>Joint training vs. Two-stage Training</em></p><p><strong><em>Joint training outperforms the Two-stage training in most settings.</em></strong></p></li><li><p><em>What SSL works for GNNs</em></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123202214.png" alt="image-20210123202214101" style="zoom:28%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123202231.png" alt="image-20210123202231858" style="zoom:30%;" /></p><ul><li>SSL for GNNs will improve the accuracy for downstream task</li><li>Across all datasets , the best performing method is a pretext task developed from <em>global structure information.</em></li><li>self-supervised information from both the structure and attributes have potentials</li><li>For the structure information, the global pretext tasks are likely to provide much more significant improvements compared to the local ones.</li></ul></li><li><p><em>Why SSL Works for GNNs</em></p><ul><li>GCN for node classification is naturally semi-supervised that has explored the unlabeled nodes, those (SSL pretext) failed to improve GCNs is argued resulted in GCN has already learned that information.</li><li>GCN is unable to naturally learn the global structure information and employing pairwise node distance prediction as the SSL task can help boost its performance for the downstream task.</li></ul></li><li><p>The capability of pretext representations maintaining similarity</p><p>The most popular similarity for graph is structural equivalence and regular equivalence (规则的等效节点是那些不一定具有相同邻居但具有自身相似的邻居的节点).</p><ul><li>Authors argue pretext task can maintain these two similarities by changing the definition of task (like nodes attribute task or distance between a pair of nodes can maintain structure similarity and regular equivalence.</li></ul></li></ul></li></ul><h2 id="advanced-pretext-task-on-graphs">Advanced pretext task on graphs</h2><p>Pretext tasks are built with the intuition of adapting the notion of regular equivalence to having neighbors with similar node labels (or regular task equivalence). Specifically, <strong>if every node constructs a pretext vector based on information in regards to the labels from their neighborhood, then two nodes having similar (or dissimilar) vectors will be encouraged to be similar (or dissimilar) in the embedding space.</strong></p><h3 id="proposed-tasks">Proposed Tasks</h3><h4 id="distance2labeled">Distance2Labeled</h4><ul><li>Modify Distance2Cluster. Propose to predict the distance vector from each node to the labeled nodes (i.e., <span class="math inline">\(\mathcal{V}_L\)</span>) as the pretext task. <em>For class <span class="math inline">\(c_j\in\{1，\cdots,K\}\)</span> and unlabeled node <span class="math inline">\(v_i\in\mathcal{V}_U\)</span>, the distance vector <span class="math inline">\(\mathrm{d}_i\)</span> for node <span class="math inline">\(v_i\)</span> is defined as three shortest path length (average, minimum, maximum) from <span class="math inline">\(v_i\)</span> to all labeled nodes in class <span class="math inline">\(c_i\)</span>.</em><br /></li><li>Formally, the objective is <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-d_i\|^2\)</span>.</li></ul><h4 id="contextlabel">ContextLabel</h4><ul><li>Considering the sparsity of labels, use similarity based function which utilize structure , attributes , and the current labeled nodes to <strong>construct a neighbor label distribution context vector <span class="math inline">\(\bar{\mathrm{y}}_i\)</span></strong> for each nodes as follows: <span class="math inline">\(f_s({\mathrm{A,X},\mathcal{D}_L,\mathcal{V}_U})\rightarrow\{\bar{\mathrm{y}}_i|v_i\in\mathcal{V}_U\}\)</span>. Specifically , the <span class="math inline">\(c\)</span>-th item of <span class="math inline">\(\bar{\mathrm{y}}\)</span> is: <span class="math inline">\(\bar{\mathrm{y}}_{ic}=\frac{|\mathcal{N}_{\mathcal{V}_L}(v_i,c)|+|\mathcal{N}_{\mathcal{V}_U}(v_i,c)|}{|\mathcal{N}_{\mathcal{V}_L}(v_i)|+|\mathcal{N}_{\mathcal{V}_U}(v_i)|},c=1,\cdots,K\)</span>. (For the neighbors of node <span class="math inline">\(v_i\)</span> (including unlabeled and labeled neighbors), the ratio of neighbors in class <span class="math inline">\(c\)</span>)</li><li>Formally, the objective is <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-\mathrm{y}_i\|^2\)</span></li><li>The labels of nodes (aka <span class="math inline">\(f_s\)</span>) can be generated by LP (Label propagation ) or ICA (Iterative Classification Algorithm). But these will import weak labels that are too noisy.</li></ul><h4 id="ensemblelabel">EnsembleLabel</h4><ul><li>Ensemble various functions <span class="math inline">\(f_s\)</span>. <span class="math inline">\(\bar{y}_i=\arg\max_c\sigma_{LP}(v_i)+\sigma_{ICA}(v_i),c=1,\cdots,K\)</span></li><li>The objective is the same as ContextLabel method.</li></ul><h4 id="correctedlabel">CorrectedLabel</h4><ul><li>Enhance ContextLabel by iteratively improving the context vectors. GNN <span class="math inline">\(f_{\theta}\)</span> is trained on both the original (e.g., <span class="math inline">\(\bar{\mathrm{y}}_i\)</span>) and corrected (e.g., <span class="math inline">\(\hat{\mathrm{y}}_i\)</span>) context distributions.</li><li>Formally, the loss is <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U,\hat{\mathcal{D}}_U)=\\\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-\bar{\mathrm{y}}_i\|^2+\alpha(\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-\hat{\mathrm{y}}_i\|^2)\)</span>, where the 1st and second terms are to fit the original and corrected context distributions respectively, and <span class="math inline">\(\alpha\)</span> controls the contribution from the corrected context distribution. <span class="math inline">\(\hat{y}_i=\arg\max_c\frac{1}{p}\sum\limits_{l=1}^p\cos(f_{\theta&#39;}(\mathcal{G})_{v_i},\mathrm{z}_{cl}),c=1,\cdots,K\)</span>. Where <span class="math inline">\(p\)</span> indicates the prototype nodes in top-<span class="math inline">\(p\)</span> largest <span class="math inline">\(\rho\)</span> values, indicating <strong>the nodes if the measurements (<span class="math inline">\(\rho\)</span>) of their neighbors' label similarity is in top-<span class="math inline">\(p\)</span>.</strong>. Concretely, the similarity of labels' similarity is defined as <span class="math inline">\(\rho_i=\sum\limits_{j=1}^m\mathrm{sign}(\mathrm{S}_{ij}-S_c)\)</span>, where <span class="math inline">\(\mathrm{S}_{ij}\)</span> is the cosine similarity between two nodes based on their embeddings, <span class="math inline">\(S_c\)</span> indicating a constant value (which is selected as the value rank in top <span class="math inline">\(40\%\)</span> in <span class="math inline">\(\mathrm{S}\)</span>).</li><li>In other words, the average similarity between <span class="math inline">\(v_i\)</span> and <span class="math inline">\(p\)</span> prototypes is used to represent the similarity between <span class="math inline">\(v_i\)</span> and the corresponding class, and then assign the class <span class="math inline">\(c\)</span> having the largest similarity to <span class="math inline">\(v_i\)</span>.</li></ul><h3 id="experiments-for-evaluating">Experiments for evaluating</h3><ul><li><p>Experiment settings</p><ul><li>Datasets: Cora, Citeseer, Pubmed and Reddit</li><li>Model: 2-layer GCN as the backbone, with hidden units of 128, <span class="math inline">\(L_2\)</span> regularization <span class="math inline">\(5e−4\)</span>, dropout rate <span class="math inline">\(0.5\)</span> and learning rate <span class="math inline">\(0.01\)</span>. For the SSL loss, the hidden representations from the first layer of GCN are fed through a linear layer to solve SSL pretext task. Jointly train SSL and GCNs. <span class="math inline">\(\lambda\)</span> ranges in <span class="math inline">\(\{1, 5, 10, 50, 100, 500\}\)</span>. <span class="math inline">\(\alpha\)</span> ranges in <span class="math inline">\(\{0.5, 0.8, 1, 1.2, 1.5\}\)</span>.</li><li>Measurements: the average accuracy with standard deviation.</li></ul></li><li><p>Analysis</p><ul><li><p>Performance comparison: Though they argue the performance exist, but seems not that significant. They summarize: <strong>label correction can better</strong> extend label information to unlabeled nodes than ensemble, but it's much less inefficient. A tradeoff must be taken in.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124113911.png" alt="image-20210124113911642" style="zoom:50%;" /></p></li><li><p>Fewer Labeled Samples</p><ul><li><p>By randomly sampling 5 or 10 nodes per class for training and the same number of nodes for validation.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124114147.png" alt="image-20210124114147761" style="zoom:50%;" /></p></li><li><p><strong>SelfTask achieves even greater improvement when the labeled samples are fewer and consistently outperforms the state-of-the-art baselines.</strong></p></li><li><p><font color='red'>Why with fewer samples per class SelfTask can be even better?</font></p></li></ul></li><li><p>Parameter Analysis</p><ul><li><p>Only the sensitivity of the best model <em>SelfTaskCorrectedLabel-ICA</em> is evaluated. Vary <span class="math inline">\(\lambda\)</span> in the range of <span class="math inline">\(\{0, 0.1, 0.5, 1, 5, 10, 50, 100\}\)</span> and <span class="math inline">\(\alpha\)</span> from 0 to 2.5 with an interval of 0.25.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124114646.png" alt="image-20210124114646842" style="zoom:50%;" /></p></li><li><p>The performance of this model first increases with the increase of <span class="math inline">\(\lambda\)</span>, which controls the contribution of SSL pretext task.</p></li><li><p>The using of correction is confirmed.</p></li><li><p>They don't report sensitivity on other datasets, which should have been in supplementary.</p></li></ul></li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Present detailed empirical study to understand when and why SSL works for GNNs and which strategy can better work with GNNs.</li><li>Propose a new direction SelfTask to build advanced pretext tasks which further exploit task-specific self-supervised information, and demonstrate that our advanced method achieves state-of-the-art performance.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2006.10141.pdf&quot;&gt;Self-supervised Learning on Graphs: Deep Insights and New Directions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ChandlerBang/SelfTask-GNN&quot;&gt;Codes&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Predicting What You Already Know Helps, Provable Self-Supervised Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html</id>
    <published>2021-01-11T20:00:00.000Z</published>
    <updated>2021-01-12T19:38:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2008.01064.pdf">Predicting What You Already Know Helps: Provable Self-Supervised Learning</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li><p>Pretext tasks</p><ul><li><strong>Reconstruct images from corrupted versions or just part it</strong>: including denoising auto-encoders, image inpainting, and split-brain autoencoder</li><li><strong>Using visual common sense</strong>, including predicting rotation angle, relative patch position, recovering color channels, solving jigsaw puzzle games, and discriminating images created from distortion.</li><li><strong>Contrastive learning</strong>: learn representations that <strong>bring similar data points closer</strong> while pushing randomly selected points further away or <strong>maximize a contrastive-based mutual information lower bound</strong> between different views</li><li><strong>Create auxiliary tasks</strong>: The natural ordering or topology of data is also exploited in video-based, graph-based or map-based self-supervised learning. For instance, the pretext task is to determine the correct temporal order for video frames.</li></ul></li><li><p>Theory for self-supervised learning: contrastive learning</p><ul><li>Contrastive learning may not work when conditional independence holds only with additional latent variables</li></ul><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th style="text-align: left;">Theory</th><th style="text-align: left;">Limitations</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Shows shows guarantees for contrastive learning representations on linear classification tasks using a class conditional independence assumption</td><td style="text-align: left;">Not handle approximate conditional independence</td></tr><tr class="even"><td style="text-align: left;">Contrastive learning representations can linearly recover any continuous functions of the underlying topic posterior under a topic modeling assumption for text</td><td style="text-align: left;">The <strong>assumption of independent sampling of words</strong> that they exploit is <strong>strong</strong> and <strong>not generalizable to other domains</strong> like images</td></tr><tr class="odd"><td style="text-align: left;">Studies contrastive learning on the hypersphere through intuitive properties like alignment and uniformity of representations</td><td style="text-align: left;">No connection made to downstream tasks</td></tr><tr class="even"><td style="text-align: left;">A mutual information maximization view of contrastive learning</td><td style="text-align: left;">Some issues point by paper [45]</td></tr><tr class="odd"><td style="text-align: left;">Explain negative sampling based methods use the theory of noise contrastive estimation</td><td style="text-align: left;"><strong>guarantees are only asymptotic and not for downstream tasks.</strong></td></tr><tr class="even"><td style="text-align: left;">Conditional independence assumptions and redundancy assumptions on multiple views are used to analyze co-training</td><td style="text-align: left;">not for downstream task</td></tr></tbody></table></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Forming the pretext tasks:<ul><li>Colorization: can be interpreted as <span class="math inline">\(p(X_1,X_2|Y)=p(X_1|Y)\times p(X_2|Y)\)</span>, aka <span class="math inline">\(X_1,X_2\)</span> are independently conditioned on <span class="math inline">\(Y\)</span></li><li>Inpainting: <span class="math inline">\(p(X_1,X_2|Y,Z)=p(X_1|Y,Z)\times p(X_2|Y,Z)\)</span>,aka the inpainted <span class="math inline">\(X_2\)</span> is conditionally independent of <span class="math inline">\(X_2\)</span> (the remainder) given <span class="math inline">\(Y,Z\)</span>.</li></ul></li><li>The only way to solve the pretext task is to first implicitly predict <span class="math inline">\(Y\)</span> and then predict <span class="math inline">\(X_2\)</span> from <span class="math inline">\(Y\)</span></li></ul></li><li>Limitations:<ul><li>The underlying principles of self-supervised learning are still mysterious since it is a-priori unclear why predicting what we already know should help.</li></ul></li></ul><h2 id="goals">Goals</h2><p><strong><em>What conceptual connection between pretext and downstream tasks ensures good representations?</em></strong></p><p><strong><em>What is a good way to quantify this?</em></strong></p><h2 id="how">How?</h2><h3 id="notations">Notations</h3><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th style="text-align: left;">Symbol</th><th style="text-align: left;">Meaning</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><span class="math inline">\(\mathbb{E}^L[Y|X]\)</span></td><td style="text-align: left;">the best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span></td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(\Sigma_{XY|Z}\)</span></td><td style="text-align: left;">partial covariance matrix between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z\)</span></td></tr><tr class="odd"><td style="text-align: left;"><span class="math inline">\(X_1,X_2\)</span></td><td style="text-align: left;">the input variable and the target random variable for the pretext tasks</td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(Y\)</span></td><td style="text-align: left;">label for the downstream task</td></tr><tr class="odd"><td style="text-align: left;"><span class="math inline">\(P_{X_1X_2Y}\)</span></td><td style="text-align: left;">the joint distribution over <span class="math inline">\(\mathcal{X}_1 \times \mathcal{X}_2 \times \mathcal{Y}\)</span></td></tr></tbody></table><h3 id="idea">Idea</h3><ul><li>Under approximate condition independence (CI) (quantified by the norm of a certain partial covariance matrix), show similar sample complexity improvements.</li><li>Testify pretext task helps when CI is approximately satisfied in text domain.</li><li>Demonstrate on a real-world image dataset that a pretext task-based linear model outperforms or is comparable to many baselines.</li></ul><h3 id="formalize-ssl-with-pretext-task">Formalize SSL with pretext task</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111171426.png" alt="image-20210111171415185" style="zoom:50%;" /></p><p>It will be estimated by:</p><ul><li><strong>approximation erro</strong>r:<img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173318.png" alt="image-20210111173318840" style="zoom:33%;" />, where <span class="math inline">\(f^*=\mathbb{E}[Y|X_1]\)</span> is the optimal predictor for the task</li><li><strong>estimation error</strong>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173520.png" alt="image-20210111173520930" style="zoom:25%;" />, it's the difference between Predicting <span class="math inline">\(Y\)</span> directly by <span class="math inline">\(X_1\)</span> and Predicting by the representations from pretext task</li></ul><h2 id="experiments">Experiments</h2><ul><li></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks</li><li>Quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) <strong>allows us to learn representations that can solve the downstream task with drastically reduced sample complexity</strong> by just training a linear layer on top of the learned representation.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2008.01064.pdf&quot;&gt;Predicting What You Already Know Helps: Provable Self-Supervised Learning&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Colorful Image Colorization</title>
    <link href="http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html</id>
    <published>2021-01-10T22:15:39.000Z</published>
    <updated>2021-01-12T21:19:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1603.08511.pdf">Colorful Image Colorization</a></p><p>Code <a href="http://richzhang.github.io/colorization/">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>Predicting colors in free way: taking the image’s <span class="math inline">\(L\)</span> channel as input and its <span class="math inline">\(ab\)</span> channels as the supervisory signal--&gt; but tend to look desaturated, one explanation is using loss functions that encourage conservative predictions</p><ul><li><p>Non-parametric methods: given an input grayscale image, first define one or more color reference images. Then, transfer colors onto the input image from analogous regions of the reference image(s).</p></li><li><p>Parametric methods: learn prediction functions from large datasets of color images at training time, posing the problem as either regression onto continuous color space or classification of quantized color values. --&gt; Work in this paper is also classification task.</p></li><li><p>Concurrent work on colorization</p><table><colgroup><col style="width: 10%" /><col style="width: 37%" /><col style="width: 45%" /><col style="width: 6%" /></colgroup><thead><tr class="header"><th>Paper</th><th style="text-align: center;">loss</th><th>CNNs</th><th>Dataset</th></tr></thead><tbody><tr class="odd"><td>Larsson et al.</td><td style="text-align: center;">un-rebalanced classification loss</td><td>hypercolumns on a VGG</td><td>ImageNet</td></tr><tr class="even"><td>Iizuka et al.</td><td style="text-align: center;">regression loss</td><td>two-stream architecture in which fuse global and local features</td><td>Places</td></tr><tr class="odd"><td>This paper</td><td style="text-align: center;">classification loss, with rebalanced rare classes,</td><td>a single-stream, VGG-styled network with added depth and dilated convolutions</td><td>ImageNet</td></tr></tbody></table></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Even in gray images, the semantics of the scene and its surface texture provide ample cues for many regions in each image</li><li>Color prediction is inherently multimodal --&gt; sparks for a loss tailored to their work</li></ul></li><li>Limitations:<ul><li>Loss only cares Euclidean distance: If an object can take on a set of distinct ab values, the optimal solution to the Euclidean loss will be the mean of the set. In color prediction, this averaging effect favors grayish, desaturated results. Additionally, if the set of plausible colorizations is non-convex, the solution will in fact be out of the set, giving implausible results.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Design colorization based pretext task to get a good image semantic representations: <strong>produce a plausible colorization that could potentially fool a human observer</strong></p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li><strong>Produce vibrant colorization</strong>: Predict a distribution of possible colors for each pixel. Then, re-weight the loss at training time to emphasize rare colors. This encourages the model to exploit the full diversity of the large-scale data on which it is trained. Lastly, produce a final colorization by taking the annealed mean of the distribution.</li><li>Evaluate synthesized images: set up a “colorization Turing test”.</li></ul><h3 id="data-preparation">Data Preparation</h3><ul><li>Quantize the <span class="math inline">\(ab\)</span> output space into bins with grid size <span class="math inline">\(10\)</span> and keep the <span class="math inline">\(Q = 313\)</span> values which are in-gamut. Then this is the label <span class="math inline">\(Z\)</span> of each pixel. Formally, denote the raw label as <span class="math inline">\(Y\)</span>, then <span class="math inline">\(Z = H^{−1}_{gt} (Y)\)</span>, which converts ground truth color <span class="math inline">\(Y\)</span> to vector <span class="math inline">\(Z\)</span>.</li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Model</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110175555.png" alt="image-20210110175554683" style="zoom:30%;" /></p></li><li><p>Loss function</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210112160157.png" alt="image-20210110180438146" style="zoom:33%;" /></p><p>where <span class="math inline">\(v(·)\)</span> is a weighting term that can be used to re-balance the loss based on color-class rarity.</p><ul><li><p>Re-balancing</p><p>The distribution of <span class="math inline">\(ab\)</span> values in natural images is strongly biased towards values with low <span class="math inline">\(ab\)</span> values.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110233506.png" alt="image-20210110233506301" style="zoom:33%;" />, where <span class="math inline">\(\tilde{p}\)</span> the empirical probability of colors in the quantized ab space <span class="math inline">\(p\in \Delta Q\)</span> from the full ImageNet training set and smooth the distribution with a Gaussian kernel <span class="math inline">\(G_{\sigma}\)</span>. <span class="math inline">\(\lambda=\frac{1}{2}, \sigma=5\)</span> worked well.</p></li></ul></li><li><p>Inferring point estimates</p><p>map probability distribution <span class="math inline">\(\hat{Z}\)</span> to color values <span class="math inline">\(\hat{Y}\)</span> with function <span class="math inline">\(\hat{Y} = H(\hat{Z})\)</span></p></li></ul><p>They interpolate by re-adjusting the temperature <span class="math inline">\(T\)</span> of the softmax distribution, and taking the mean of the result. Lowering the temperature <span class="math inline">\(T\)</span> produces a more strongly peaked distribution, and setting <span class="math inline">\(T\rightarrow 0\)</span> results in a 1-hot encoding at the distribution mode. They find that <span class="math inline">\(T=0.38\)</span> captures the vibrancy of the mode while maintaining the spatial coherence of the mean.</p><h2 id="experiments">Experiments</h2><ul><li>Dataset: ImageNet</li><li>Base models</li></ul><table style="width:100%;"><colgroup><col style="width: 11%" /><col style="width: 41%" /><col style="width: 47%" /></colgroup><thead><tr class="header"><th>Model Name</th><th style="text-align: center;">Loss</th><th>Train</th></tr></thead><tbody><tr class="odd"><td>Ours(full)</td><td style="text-align: center;">classification loss</td><td>from scratch with kmeans initialization, ADAM solver for about 450K iterations. <span class="math inline">\(\beta_1 = .9, \beta_2 = .99\)</span>, and weight decay = <span class="math inline">\(10^{−3}\)</span> . Initial learning rate was <span class="math inline">\(3 × 10^{−5}\)</span> and dropped to <span class="math inline">\(10^{−5}\)</span> and <span class="math inline">\(3 × 10^{−6}\)</span> when loss plateaued, at 200k and 375k iterations, respectively.</td></tr><tr class="even"><td>Ours(class)</td><td style="text-align: center;">classification loss withou rebalancing (<span class="math inline">\(\lambda=1\)</span>)</td><td>similar training protocol as Ours(full)</td></tr><tr class="odd"><td>Ours(L2)</td><td style="text-align: center;">L2 regression loss</td><td>same training protocol</td></tr><tr class="even"><td>Ours(L2,ft)</td><td style="text-align: center;">L2 regression loss</td><td>fine tuned from our full classification with rebalancing network</td></tr><tr class="odd"><td>Larsson et al.</td><td style="text-align: center;"></td><td>CNN method</td></tr><tr class="even"><td>Dahl</td><td style="text-align: center;">L2 regression loss</td><td>a Laplacian pyramid on VGG features</td></tr><tr class="odd"><td>Gray</td><td style="text-align: center;">--</td><td>every pixel is gray, with <span class="math inline">\((a, b) = 0\)</span></td></tr><tr class="even"><td>Random</td><td style="text-align: center;">--</td><td>Copies the colors from a random image from the training set</td></tr></tbody></table><h3 id="colorization-quality">Colorization quality</h3><ul><li>AMT: participants confirm their results. They argue that their work produce a more prototypical appearance for those are poorly white balanced</li><li>Semantic interpretability (VGG classification): Are the results realistic enough colorizations to be interpretable to an off-the-shelf object classifier? They check it by by feeding their fake colorized images to a VGG network that was trained to predict ImageNet classes from real color photos.<ul><li>The result is <span class="math inline">\(3.4\%\)</span> lower than Larsson's.</li><li>Without any additional training or fine-tuning, one can improve performance on grayscale image classification, simply by colorizing images with our algorithm and passing them to an off-the-shelf classifier.</li></ul></li><li>Raw accuracy (AuC):<ul><li>L2 metric can achieve accurate colorizations, but has difficulty in optimization from scratch</li><li>class-rebalancing in the training objective achieved its desired effect</li></ul></li><li>Compared with others<ul><li>LEARCH: On SUN dataset, authors have <span class="math inline">\(17.2\%\)</span> on AMT task while LEARCH has <span class="math inline">\(9.8\%\)</span></li></ul></li></ul><h3 id="cross-channel-encoding-as-ssl-feature-learning">Cross-channel encoding as SSL Feature learning</h3><ul><li>Datasets: ImageNet, PASCAL (fine tuned after training on ImageNet)</li><li>Backbone: AlexNet</li><li>Settings<ul><li>ImageNet: fixing the extractor and retrain the classifier (softmax layer) by labels</li><li>PASCAL: : (1) keeping the input grayscale by disregarding color information (Ours (gray)) and (2) modifying conv1 to receive a full 3-channel <span class="math inline">\(Lab\)</span> input, initializing the weights on the <span class="math inline">\(ab\)</span> channels to be zero (Ours (color)).</li></ul></li><li>Summary<ul><li>For ImageNet, there is a <span class="math inline">\(6\%\)</span> performance gap between color and grayscale inputs. Except for the 1st layer, representations from other deeper layers catch and outperform most methods, indicating that <strong>solving the colorization task encourages representations that linearly separate semantic classes in the trained data distribution</strong></li><li>On PASCAL, when conv1 is frozen, the network is effectively only able to interpret grayscale images.</li></ul></li></ul><h3 id="the-properties-of-network">The properties of network</h3><ul><li><p>Is it exploiting low-level cues?</p><p>Given a grayscale Macbeth color chart as input, it was unable to recover its colors. On the other hand, given two recognizable vegetables that are roughly <strong>isoluminant</strong>, <strong>the system is able to recover their color</strong>.</p></li><li><p>Does it learn multimodal color distributions ?</p><p>Take effective dilation ( the spacing at which consecutive elements of the convolutional kernel are evaluated, relative to the input pixels, and is computed by the product of the accumulated stride and the layer dilation) as the measurement. Through each convolutional block from conv1 to conv5, the effective dilation of the convolutional kernel is increased. From conv6 to conv8, the effective dilation is decreased</p></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Designing an appropriate objective function that handles the multimodal uncertainty of the colorization problem and captures a wide diversity of colors</li><li>Introducing a novel framework for testing colorization algorithms, potentially applicable to other image synthesis tasks</li><li>Setting a new high-water mark on the task by training on a million color photos.</li><li>Introduce the colorization task as a competitive and straightforward method for self-supervised representation learning, achieving state-of-the-art results on several benchmarks.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1603.08511.pdf&quot;&gt;Colorful Image Colorization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;http://richzhang.github.io/colorization/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Revisiting Self-Supervised Visual Representation Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html</id>
    <published>2021-01-09T21:21:00.000Z</published>
    <updated>2021-01-12T19:33:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf">Revisiting Self-Supervised Visual Representation Learning</a></p><p>Code <a href="https://github.com/google/revisiting-self-supervised">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>robotics: the result of interacting with the world, and the fact that multiple perception modalities simultaneously get sensory inputs are strong signals for pretext</li><li>videos: the synchronized cross-modality stream of audio, video, and potentially subtitles, or of the consistency in the temporal dimension</li><li>image datasets:<ul><li>Patch-based methods: E.g.: predicting the relative location of image patches; "jigsaw puzzle"</li><li>Image-level classification tasks:<ul><li>RotNet, create class labels by clustering images, image inpaiting, image colorization, split-brain and motion segmentation prediction;</li><li>Enforce structural constraints on the representation space: an equivariance relation to match the sum of multiple tiled representations to a single scaled representation; predict future patches in via autoregressive predictive coding</li><li>Combining multiple pretext task: E.g. extend the “jigsaw puzzle” task by combining it with colorization and inpainting; Combining the jigsaw puzzle task with clustering-based pseudo labels ( Jigsaw++) ; make one single neural network learn all of four different SSL methdos in a multi-task setting; combined the selfsupervised loss GANs objective</li></ul></li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Expensive labeled data for supervised task</li></ul></li><li>Limitations:<ul><li>Previous works mostly concentrate on pretext task, but didn't pay much attention to the choice of backbones etc.</li></ul></li></ul><h2 id="goals">Goals</h2><p><strong>An optimal CNN architecture for pretext task</strong>, investigating the influence of architecture design on the representation quality.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>a comparison of different self-supervision methods using a unified neural network architecture, but with the goal of combining all these tasks into a single self-supervision task</li></ul><h3 id="implementation">Implementation</h3><h4 id="family-of-cnns">Family of CNNs</h4><ul><li><p>variants of ResNet:</p></li><li><p><strong>ResNet50</strong>, the output before task-specific logits layer is named as <span class="math inline">\(pre-logits\)</span>. explore <span class="math inline">\(k \in \{4, 8, 12, 16\}\)</span>, resulting in pre-logits of size <span class="math inline">\(2048, 4096, 6144\)</span> and <span class="math inline">\(8192\)</span> respectively. <span class="math inline">\(k\)</span> is the widening factor.</p></li><li><p><strong>ResNet v1</strong>: ???batch normalization (BN) right after each convolution and before activation???</p></li><li><p><strong>ResNet v2</strong>: ?</p></li><li><p><strong>ResNet (-)</strong>: without ReLU preceding the global average pooling</p></li><li><p>a batch-normalized <strong>VGG</strong> architecture since VGG is structurally close to AlexNet. BN between CNN and activation, VGG19.</p></li><li><p><strong>RevNets</strong>: stronger invertibility guarantees so as to compare with ResNets. The residual unit used here is equivalent to double application of the residual unit.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110133429.png" alt="image-20210110133429332" style="zoom:50%;" />, check <a href="https://papers.nips.cc/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf">here</a> for details. Apart from this slightly complex residual unit, others are the same as ResNet.</p></li></ul><h4 id="family-of-pretext-tasks">Family of pretext tasks</h4><ul><li><strong>Rotation</strong>: same as RotNet, <span class="math inline">\(\{0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}\}\)</span></li><li><strong>Exemplar</strong>: triplet loss</li><li><strong>Jigsaw</strong>: recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches was performed. Patches are sampled with a random gap between them. Each patch is then independently converted to grayscale with probability <span class="math inline">\(\frac{2}{3}\)</span> and normalized to zero mean and unit standard deviation. Extract final image representations by averaging representations of 9 cropped patches.</li><li><strong>Relative patch location</strong>: predicting the relative location of two given patches of an image. Extract final image representations by averaging representations of 9 cropped patches.</li></ul><h4 id="evaluation-of-the-quality-of-learned-representations">Evaluation of the quality of learned representations</h4><ul><li>Idea: <strong>Using learned representations for training a linear logistic regression model to solve multiclass image classification tasks</strong> (downstream tasks). All representations come from pre-logits level.</li><li>Details: the linear logistic regression model is trained by L-BFGS. But for comparison, using SGD with momentum and use data augmentation during training.</li></ul><h2 id="experiments">Experiments</h2><ul><li>Datasets</li></ul><table><colgroup><col style="width: 12%" /><col style="width: 15%" /><col style="width: 72%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Train</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>ImageNet</td><td style="text-align: center;">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr><tr class="even"><td>Places 205</td><td style="text-align: center;">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr></tbody></table><h3 id="pretext-cnns-downstream">Pretext? CNNs? Downstream?</h3><ul><li><p>Pretext and its preferred CNN architecture: <strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110151545.png" alt="image-20210110151545316" style="zoom:30%;" /></p></li><li><p>The generalization of representations from pretext tasks: each pretext task can be generalized to other dataset. Check the trendings in figure 2.</p></li><li><p>Optimal CNNs for Pretext and downstream tasks: not consistent. But after selecting the right architecture for each self-supervision and increasing the widening factor, models significantly outperform previously reported results.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155158.png" alt="image-20210110155157874" style="zoom:45%;" /></p></li><li><p><strong>Better performance on the pretext task does not always translate to better representations</strong>: Performance on pretext cannot be used to reliably select the model architecture.</p></li></ul><h3 id="cnns-architecture">CNNs architecture</h3><ul><li>Skip-connection: For VGG, representations deteriorate towards the end of the network cause models specialize to the pretext task in the later layers. ResNet prevent this deterioration. They argue that this is because <strong>ResNet’s residual units being invertible under some conditions</strong> and confirm this by RevNet.</li><li>Depth of CNNs: For residual architectures, the pre-logits are always best.</li><li>Model-width and representation size:<ul><li>whether the increase in performance is due to increased network capacity or to the use of higher-dimensional representations, or to the interplay of both? To answer it, authors disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer.</li><li><strong>Model-width and representation size both matter independently, and larger is always better.</strong></li><li>SSL techniques are likely to <strong>benefit from using CNNs with increased number of channels</strong> across wide range of scenarios, even under low-data regime.</li></ul></li></ul><h3 id="evaluate-the-quality-of-representations">Evaluate the quality of representations</h3><ul><li><strong>A linear model is adequate</strong>: MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged</li></ul><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155830.png" alt="image-20210110155829820" style="zoom:50%;" /></p><ul><li>To train <strong>linear model, SGD optimization hyperparameters:</strong> very long training (≈ 500 epochs) results in higher accuracy</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Most affect performance in the fully labeled setting, may significantly affect performance in the selfsupervised setting.</li><li>the quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</li><li>Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations</li><li>The evaluation procedure, where a linear model is trained on a fixed visual representation using stochastic gradient descent, is sensitive to the learning rate schedule and may take many epochs to converge</li><li><strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong>--&gt;<strong>pretext tasks for self-supervised learning</strong> should not <strong>be considered</strong> in isolation, but <strong>in conjunction with underlying architectures</strong>.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&quot;&gt;Revisiting Self-Supervised Visual Representation Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/google/revisiting-self-supervised&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Representation Learning by Predicting Image Rotations</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html</id>
    <published>2021-01-09T04:11:12.000Z</published>
    <updated>2021-01-12T20:56:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1803.07728.pdf">Unsupervised Representation Learning by Predicting Image Rotations</a></p><p>Codes <a href="https://github.com/gidariss/FeatureLearningRotNet">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>How to get <strong>a high-level image semantic representation using unlabeled data</strong></p><ul><li>SSL: defines an annotation free pretext task, has been proved as good alternatives for transferring on other vision tasks. E.g.: colorize gray scale images, predict the relative position of image patches, predict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames.</li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li><strong>the attention maps are equivariant w.r.t. the image rotations, check appendix A.</strong></li></ul></li><li>Limitations<ul><li>supervised feature learning has the main limitation of requiring intensive manual labeling effort</li></ul></li></ul><h2 id="goals">Goals</h2><p>Provide a "self-supervised" formulation for image data, a <strong>self defined supervised task involving predicting the transformations used for image.</strong> The model won't have access to the initial image.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>Concretely, define the geometric transformations as the image rotations by 0, 90, 180, and 270 degrees. Thus, the ConvNet model is trained on the 4-way image classification task of recognizing one of the four image rotations.</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li><p>2D image Rotation</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109124726.png" alt="image-20210109124726420" style="zoom:40%;" /></p><table><thead><tr class="header"><th>Operations</th><th style="text-align: center;">Implementation</th></tr></thead><tbody><tr class="odd"><td>+90</td><td style="text-align: center;">transpose then flip vertically</td></tr><tr class="even"><td>+180</td><td style="text-align: center;">flip vertically then flip horizontally</td></tr><tr class="odd"><td>+270</td><td style="text-align: center;">flip vertically then transpose</td></tr></tbody></table></li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109123815.png" alt="image-20210109123815880" style="zoom:45%;" />, where <span class="math inline">\(F(\cdot)\)</span> is the predicted probability of the geometric transformation with label <span class="math inline">\(y\)</span> and <span class="math inline">\(\theta\)</span> are the learnable parameters of model <span class="math inline">\(F(\cdot)\)</span>, and <span class="math inline">\(g(X_i|y)\)</span> is the transformed image with transformation <span class="math inline">\(y\)</span>.</p></li><li><p><strong>Why does it works?</strong></p><ul><li>To work for this pretext task, extractor has to <strong>understand the concept of the objects depicted in the image</strong>. Models must learn to localize salient objects in the image, recognize their orientation and object type, and then relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images.</li><li>Easy to be implemented by flipping and transpose, no chance for importing low-level visual artifacts so as to avoid trivial features (which have no practical value)</li><li>Operations are easy to be recognized manually.</li></ul></li></ul><h2 id="experiments">Experiments</h2><h3 id="cifar-object-recognition">CIFAR: object recognition</h3><ul><li>Dataset:</li></ul><table><thead><tr class="header"><th>Datasets</th><th>Preprocess</th></tr></thead><tbody><tr class="odd"><td>CIFAR-10</td><td>Rotations</td></tr></tbody></table><ul><li>Training: SGD with batch size 128, momentum 0.9, weight decay <span class="math inline">\(5e−4\)</span> and <span class="math inline">\(lr\)</span> of 0.1. We drop the learning rates by a factor of 5 after epochs 30, 60, and 80. 100 epochs. Each time feeding with all 4 images.</li><li>Summary<ul><li><em>The learned feature hierarchies</em>: convnet with different number of layers. <strong>Representations from the 2nd block</strong> are good, and <strong>increasing the total depth</strong> of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers.</li><li><em>The quality of the learned features w.r.t. the number of recognized rotations</em>: 4 discrete rotations outperform.</li><li><em>Compared with previous work</em> : almost the same as the NIN supervised model. Fine-tuned the unsupervised learned features further improves the classification performance.</li><li><em>Correlation between object classification task and rotation prediction task</em>: The representations from pretext make classifier converge faster compared with the classifier trained from scratch.</li><li><em>Semi-supervised setting</em>: pretrained on the whole dataset without labels, then fine-tuned on a small labeled subset. It exceeds the supervised model when the number of examples per category drops below 1000.</li></ul></li></ul><h3 id="others-classification-object-detection-segmentation">Others: classification, object detection , segmentation</h3><ul><li><p>Dataset: ImageNet, Places, and PASCAL VOC.</p><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th>Task</th><th>Datasets</th></tr></thead><tbody><tr class="odd"><td>Classification</td><td>Pretrained on ImageNet, then test on ImageNet, Places, and PASCAL VOC.</td></tr><tr class="even"><td>Object detection</td><td>PASCAL VOC</td></tr><tr class="odd"><td>Object segmentation</td><td>PASCAL VOC</td></tr></tbody></table></li><li><p>Backbones: AlexNet without local response normalization units, dropout units, or groups in the colvolutional layers while it includes batch normalization units after each linear layer</p></li><li><p>Pretrained: on ImageNet, SGD with batch size 192, momentum 0.9, weight decay <span class="math inline">\(5e − 4\)</span> and <span class="math inline">\(lr\)</span> of 0.01. Learning rates are dropped by a factor of 10 after epochs 10, and 20 epochs. Trained in total for 30 epochs.</p></li><li><p>Summary:</p><ul><li><p>ImageNet classification task: surpasses all the other unsupervised methods by a significant margin, narrows the performance gap between unsupervised features and supervised features.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154626.png" alt="image-20210109154626599" style="zoom:50%;" /></p></li><li><p>Transfer learning evaluation on PASCAL VOC: fine tuning, used weight rescaling proposed by Krahenbuhl et al.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154831.png" alt="image-20210109154831162" style="zoom:60%;" /></p></li><li><p>Places classification task: the learnt features are evaluated w.r.t. their generalization on classes that were “unseen” during the unsupervised training phase</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154953.png" alt="image-20210109154952935" style="zoom:50%;" /></p></li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Propose a new self-supervised task that is very simple and at the same time.</li><li>Rotationsod under various settings (e.g. semi-supervised or transfer learning settings) and in various vision tasks (i.e., CIFAR-10, ImageNet, Places, and PASCAL classification, detection, or segmentation tasks).</li><li>They argue this self-supervised formulation demonstrates state-of-the-art results with dramatic improvements w.r.t. prior unsupervised approaches, and narrows the gap between unsupervised and supervised feature learning.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1803.07728.pdf&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Codes &lt;a href=&quot;https://github.com/gidariss/FeatureLearningRotNet&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Visual Representation Learning by Context Prediction</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html</id>
    <published>2021-01-08T21:22:12.000Z</published>
    <updated>2021-01-12T19:33:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf">Unsupervised Visual Representation Learning by Context Prediction</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>How to get <strong>a good image representation</strong></p><ul><li>The latent variables of an appropriate generative model. --&gt; generative models<ul><li>But given an image, inferring the latent structure is <strong>intractable</strong> for even relatively simple models --&gt; to fix, use <strong>sampling</strong> to perform approximate inference.</li></ul></li><li>An embedding that can discriminate the semantics in images by distances of them. -- create a supervised "pretext" task. But hard to tell whether the predictions themselves are correct.<ul><li>Reconstruction-based: E.g., denoising autoencoders (reconstruction ), sparse autoencoders (reconstruction + sparsity penalty )</li><li>Context prediction: "skip-gram" to "filling the blank" task, and convert the prediction task to discriminate task like discriminating between real images vs. images where one patch has been replaced by a random patch from elsewhere in the dataset. But not hard enough for high-level representations</li><li>Discover object categories using hand-crafted features and various forms of clustering. But they will lose shape information. To keep more shape information, some take contour extraction or defining similarity metrics.</li><li>Video-based: since the identity of objects remains unchanged -</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>the difficulties for generalizing CNNs models on Internet -scale datasets</li><li>context has proven to be a powerful source of automatic supervisory signal for learning representations --&gt; context can be regarded as a 'pretext' task to force the model to learn a good word embedding</li><li>current reconstruction-based algorithms struggle with low-level phenomena, like stochastic textures, making it hard to even measure whether a model is generating well.</li></ul></li><li>Limitations:<ul><li>generative models are rather efficiently on smaller datasets but burden on high-resolution natural images</li><li>Some are too simple for extracting high-level representations</li><li>Hard to tell whether the model has obtained good representations.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Provide a "self-supervised" formulation for image data, a supervised task involving predicting the context for a patch.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>Hypothesis: Doing well on predicting patches' positions requires understanding scenes and objects--&gt; a good visual representation</li><li>Concretely, sample random pairs of patches in one of eight spatial configurations, and present each pair to a machine learner. The algorithm must then guess the position of one patch relative to the other.</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li><p>Two patches are fed into network</p></li><li><p>Given an image, one patch will be sampled uniformly, then according to the position of this sampled patch, then 2nd patch will be sampled randomly from the eight possible neighboring locations.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108181759.png" alt="image-20210108181757911" style="zoom:33%;" /></p></li><li><p>including a gap between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</p></li><li><p>For some images ( chromatic aberration), after solving the relative location task (like by detecting the separation between green and magenta (red + blue). ), this problem will be relaxed.</p><ul><li>Shift green and magenta toward gray</li><li>Color dropping : randomly drop 2 of the 3 color channels from each patch and replace them by gaussian noise.</li></ul></li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Siamese network based on AlexNet. But not all layers share weights, LRN (local response normalization ) layers won't.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108183110.png" alt="image-20210108183110662" style="zoom:33%;" /></p></li><li><p><strong>Why does it works?</strong></p><ul><li>Avoid 'trivial' shortcuts (like boundary patterns or textures continuing between patches)--&gt; including a gap (up to 48 pixels) between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</li><li>Enhancing performance on images with chromatic aberration.</li></ul></li></ul><h2 id="experiments">Experiments</h2><p>Pre-Training: SGD+BN+high momentum, 4 weeks on K40 GPU.</p><table><colgroup><col style="width: 8%" /><col style="width: 28%" /><col style="width: 63%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Resizing</th><th>Preprocess</th></tr></thead><tbody><tr class="odd"><td>ImageNet</td><td style="text-align: center;"><span class="math inline">\(150K\sim450K\)</span> total pixels</td><td>1. sample patches at resolution <span class="math inline">\(96\times 96\)</span><br />2. mean subtraction, projecting or dropping colors, and randomly downsampling some patches to as little as 100 total pixels, and then upsampling it.</td></tr></tbody></table><h3 id="ability-on-semantic">Ability on semantic</h3><p>Does it get similar representations for patches with similar semantics?</p><ul><li><p>check nearest neighbors by normalized correlation of <span class="math inline">\(fc6\)</span>'s output. Compared with results from random initialized model and ImageNet AlexNet.</p></li><li><p>Summary</p><ul><li>in a few cases, random (untrained) ConvNet also does reasonably well</li><li>the representations from proposed model often capture the semantic information</li></ul></li></ul><h3 id="learnability-of-chromatic-aberration">Learnability of Chromatic Aberration</h3><ul><li>Patches displayed similar aberration tend to be predicted at the same location.</li><li>The effect of color projection operation is canceled for this kind of images.</li></ul><h3 id="object-detection">Object detection</h3><ul><li>Dataset : VOC 2007</li><li>Train: fine-tune the pretrained model (model is slight different with the previous one considering the image size in VOC) on VOC 2007.</li><li>Test: output from <span class="math inline">\(fc7\)</span> is taken.</li><li>Summary<ul><li>Pre-trained model outperforms the one trained from scratch</li><li>Obtained the best result on VOC 2007 without using labels</li><li>Robustness of the representations for one object in different datasets: acceptable</li></ul></li></ul><h3 id="visual-data-mining">Visual data mining</h3><ul><li>Task : aims to use a large image collection to discover image fragments which happen to depict the same semantic objects</li><li>Specification for this task: sample a constellation of four adjacent patches from an image, after finding the top 100 images which have the strongest matches for all four patches, then use a type of geometric verification to filter away the images where the four matches are not geometrically consistent. Finally, rank the different constellations by counting the number of times the top 100 matches geometrically verify.</li><li>To define the geometric verification: first compute the best-fitting square <span class="math inline">\(S\)</span> to the patch centers (via least-squares), while constraining that side of <span class="math inline">\(S\)</span> be between 2/3 and 4/3 of the average side of the patches. Then compute the squared error of the patch centers relative to <span class="math inline">\(S\)</span> (normalized by dividing the sum-of-squared-errors by the square of the side of <span class="math inline">\(S\)</span>). The patch is geometrically verified if this normalized squared error is less than 1.</li><li>Test: VOC 2011, Street View images from Paris</li><li>Summary<ul><li>The discovery of birds and torsos is good</li><li>The gains in terms of coverage, suggesting increased invariance for learned features</li><li>The pretext task is difficult: for a large fraction of patches within each image, the task is almost impossible</li><li>Limitations: some loss of purity, and cannot currently determine an object mask automatically (although one could imagine dynamically adding more sub-patches to each proposed object).</li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>instance-level supervision appears to improve performance on category-level tasks</li><li>The proposed model is sensitive to objects and the layout of the rest of the image</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf&quot;&gt;Unsupervised Visual Representation Learning by Context Prediction&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
    <link href="http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html</id>
    <published>2021-01-06T16:17:12.000Z</published>
    <updated>2021-01-12T20:58:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1406.6909.pdf">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Supervised : labeled data with a specific CNN<ul><li>directly penalizing the derivative of the output with respect to the magnitude of the transformations, but will be sensitive to the magnitude of the applied transformation.</li></ul></li><li>Unsupervised: learning invariant representations<ul><li>Directly modeling the input distribution and are hard for jointly training multiple layers of a CNN<ul><li><strong>autoencoders</strong>: denoising auto encoders, say reconstruct data from randomly perturbed input samples; or learn representations from videos by enforcing a temporal slowness constraint on the feature representation learned by a linear autoencoder.</li><li>invariant to local transformations</li></ul></li><li>most aims at regularization of the latent representation</li></ul></li><li>Semi-supervised<ul><li>Regularization supervised algorithms by unlabeled data: self-training, entropy regularization</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>the features learned by one network often generalize to new datasets</li><li>a network can be adapted to a new task by replacing the loss function and possibly the last few layers of the network and fine-tuning it to the new problem</li></ul></li><li>Limitations:<ul><li>the need for huge labeled datasets to be used for the initial supervised training</li><li>the transfer becomes less efficient the more the new task differs from the original training task</li></ul></li></ul><h2 id="goals">Goals</h2><p>a more general extractor using unlabeled data. The extractor should satisfy two requirements:</p><ul><li>there must be at least one feature that is similar for images of the same category <span class="math inline">\(y\)</span> (invariance);</li><li>there must be at least one feature that is sufficiently different for images of different categories (ability to discriminate)</li></ul><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>creating an auxiliary task + invariant features to transformations</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li>Do random selected transformation (from a predefined family of transformations) for sampled patches (regions containing considerable gradients so that sample a patch with probability proportional to mean squared gradient magnitude within the patch )</li><li>The family of transformations<ul><li>translation</li><li>scaling</li><li>rotation</li><li>contrast: PCA and HSV</li><li>color: works on HSV space</li><li>blur etc.</li></ul></li><li>Before feeding into model, do normalization (subtract the mean of each pixel over the whole resulting dataset)</li><li>Labeling: all transformed patches from the same seed patch are labeled by the same index</li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function :</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155621.png" alt="image-20210107155618997" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155911.png" alt="image-20210107155854389" style="zoom:30%;" /></p><ul><li><p>After transformations, the loss for a whole class (augmented by the same seed patch ) can be taken as</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107162118.png" alt="image-20210107162118193" style="zoom:50%;" />, notice the 2nd and the 4th can be canceled.</p><ul><li>The 1st term: enforces correct classification of the average representation <span class="math inline">\(\mathbb{E}_\alpha[g(T_\alpha x_i)]\)</span> for a given input sample</li><li>The 2nd term: a regularizer enforcing all $ h(T_x_i)$ to be close to their average value, i.e., the feature representation is sought to be approximately invariant to the transformations $ T_$, note the convergence to global minimum is listed at appendix.</li></ul></li></ul></li><li><p><strong>Why does it works?</strong></p><ul><li>Previous works mostly focus on modeling the input distribution <span class="math inline">\(p(x)\)</span>, based on the assumption that a good model of <span class="math inline">\(p(x)\)</span> contains information about the category distribution <span class="math inline">\(p(y|x)\)</span>. Therefore, to get the invariance, one will do regularization of the latent representation and obtain representation by reconstruction .</li><li>Their work does not directly model the input distribution <span class="math inline">\(p(x)\)</span> but learns a representation that discriminates between input samples. They argue that this <strong>allows more DOF to model the desired variability of a sample and avoid task-unnecessary reconstruction.</strong></li><li>However, their work will <strong>fail on color-relied task</strong></li></ul></li></ul><h2 id="experiments">Experiments</h2><h3 id="classification">Classification</h3><ul><li><p>Datasets: <strong>STL-10</strong>, CIFAR-10, Caltech-101 and Caltech-256. report mean and standard deviation</p><table style="width:100%;"><colgroup><col style="width: 6%" /><col style="width: 24%" /><col style="width: 35%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Resizing</th><th>Train</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>STL-10</td><td style="text-align: center;">64c5-64c5-128f</td><td>10 pre-defined folds of the training data</td><td>fixed test set</td></tr><tr class="even"><td>CIFAR-10</td><td style="text-align: center;">resize from <span class="math inline">\(32\times 32\)</span> to <span class="math inline">\(64\times 64\)</span></td><td>1. whole training set<br />2. 10 random selections of 400 training samples per class</td><td>1. results on CIFAR-10<br />2. average results on 10 sets.</td></tr><tr class="odd"><td>Caltech-101</td><td style="text-align: center;">to <span class="math inline">\(150\times 150\)</span></td><td>30 random samples per class</td><td>not more than 50 samples per class</td></tr><tr class="even"><td>Caltech-256</td><td style="text-align: center;"><span class="math inline">\(256\times 256\)</span></td><td>randomly selected 30 samples per class</td><td>those except for training</td></tr></tbody></table></li><li><p>backbones of network</p><table><thead><tr class="header"><th>Network</th><th style="text-align: center;">Structure</th><th>Training</th></tr></thead><tbody><tr class="odd"><td>small</td><td style="text-align: center;">64c5-64c5-128f</td><td>1.5 days, SGD with fixed momentum of 0.9</td></tr><tr class="even"><td>medium</td><td style="text-align: center;">64c5-128c5-256c5-512f</td><td>4 days, SGD with fixed momentum of 0.9</td></tr><tr class="odd"><td>large</td><td style="text-align: center;">92c5-256c5-512c5-1024f</td><td>9 days, SGD with fixed momentum of 0.9</td></tr></tbody></table></li><li><p>Training: learning rate starts at 0.01， then when there was no improvement in validation error, decreased the learning rate by a factor of 3. All networks are trained on one Titan</p></li><li><p>Test features: one-vs-all linear SVM.</p></li><li><p>Summary</p><ul><li>with increasing feature vector dimensionality and number of labeled samples, training an SVM becomes less dependent on the quality of the features</li><li>Relation of <strong>the number of surrogate classes</strong> : <strong>sampling too many, too similar images for training can even decrease the performance of the learned features</strong>. ( the discriminative loss is no longer reasonable with too many similar surrogate classes.) --&gt; fix: e.g. clustering the output features then do augmentation for clusters and feed these augmented classes as surrogate data</li><li>Relation of <strong>the number of samples per surrogate class</strong> : around 100 samples is sufficient</li><li>Relation of <strong>types of transformations </strong> : each time remove a group of transformations and check how the performance is decreased , e.g. scaling, rotation etc. Translations, color variations and contrast variations are significantly more important. For the matching task, using blur as an additional transformation improves the performance.</li><li>Relation of <strong>Influence of the dataset </strong>: the learned features generalize well to other datasets</li><li>Relation of <strong>Influence of the Network Architecture on Classification Performance</strong>: Classification accuracy generally improves with the network size</li></ul></li></ul><h3 id="descriptor-matching">Descriptor matching</h3><ul><li>Task: Matching of interest points</li><li>Datasets: by Mikolajczyk et al., augmented by applying 6 different types of transformations with varying strengths to 16 base images from Flickr. In addition to the transformations used before, also change the lighting and blur .</li><li>Backbones: 64c7s2-128c5-256c5-512f, named as Exemplar-CNN-blur</li><li>Training: use unlabeled images from Flickr for training</li><li>Test and measurements: prediction is <span class="math inline">\(TP\)</span> if <span class="math inline">\(IOU\ge 0.5\)</span>. Compared with SIFT and Alexnet</li><li>Summary<ul><li>Optimum patch size (or layer in CNNs): SIFT is based on normalized finite differences, and thus very robust to blurred edges caused by interpolation. In contrast, for the networks, especially for their lower layers, there is an optimal patch size. They argue that features from higher layers have access to larger receptive fields and, thus, can again benefit from larger patch sizes.</li><li>A loss function that focuses on the invariance properties (rather than class-specific features) required for descriptor matching yields better results.</li><li>Features obtained with the unsupervised training procedure outperform the features from AlexNet on both datasets</li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Pretty good on tasks: object classification , descriptor matching</li><li>emphasizes the value of data augmentation in general and suggests the use of more diverse transformations.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1406.6909.pdf&quot;&gt;Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>notes-DAPPER</title>
    <link href="http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html"/>
    <id>http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html</id>
    <published>2019-10-05T16:52:21.000Z</published>
    <updated>2021-01-12T19:34:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see <a href="https://github.com/nansencenter/DAPPER">here</a>.</p><p>This notes keep records of the problems encountered while using DAPPER.</p><a id="more"></a><ol type="1"><li>Pre-setting <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.</span><br></pre></td></tr></table></figure> Copy mkl_*.dll, libiomp5md.dll and <em>libiomp5md.pdb</em> from directory "./Library/bin" to the root directory of python exe.</li></ol><p>Reference from <a href="https://blog.csdn.net/supertangcugu/article/details/89790617">here</a>.</p><ol start="2" type="1"><li>Manual</li></ol><p>Manual <a href="https://dapper.readthedocs.io/en/latest/implementation.html">online</a></p><p>For EnKF: Nx-by-N (维度数<em>样本数) For ndarrays: N-by-Nx (样本数</em>维度数)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see &lt;a href=&quot;https://github.com/nansencenter/DAPPER&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This notes keep records of the problems encountered while using DAPPER.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="da" scheme="http://yoursite.com/tags/da/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks and Remote Sensing, July 26th, 2019.</title>
    <link href="http://yoursite.com/posts/talks/2019-07-26-talk-gan-rs.html"/>
    <id>http://yoursite.com/posts/talks/2019-07-26-talk-gan-rs.html</id>
    <published>2019-07-26T17:33:39.000Z</published>
    <updated>2021-02-16T18:17:05.386Z</updated>
    
    <content type="html"><![CDATA[<p>Some works using GANs handle the problems in remote sensing.</p><p>Check <a href="/assets/slides/GAN/GANRS.pdf">slide</a> for more details.``</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Some works using GANs handle the problems in remote sensing.&lt;/p&gt;
&lt;p&gt;Check &lt;a href=&quot;/assets/slides/GAN/GANRS.pdf&quot;&gt;slide&lt;/a&gt; for more deta
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="remote sensing images" scheme="http://yoursite.com/tags/remote-sensing-images/"/>
    
  </entry>
  
  <entry>
    <title>Notes of Mathematics</title>
    <link href="http://yoursite.com/posts/notes/2019-06-19-notes-math.html"/>
    <id>http://yoursite.com/posts/notes/2019-06-19-notes-math.html</id>
    <published>2019-06-19T13:40:09.000Z</published>
    <updated>2021-04-28T23:26:01.555Z</updated>
    
    <content type="html"><![CDATA[<h2 id="泛函">泛函</h2><center><img src="/assets/img/Rules/lines.png" width=500"></center><h3 id="不动点定理">不动点定理</h3><ol type="1"><li><p>不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。</p><p><a href="https://zhuanlan.zhihu.com/p/33885648">链接</a></p></li></ol><h3 id="紧性的利用">紧性的利用</h3><ul><li>证明存在性</li><li>在无限维空间中“模仿”有限维的欧式空间</li></ul><p>紧集是为了模仿描述欧式空间中的有界闭集合么？ 紧=相对紧+闭</p><h3 id="流形manifolds">流形(Manifolds)</h3><ol type="1"><li>概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。</li></ol><h3 id="支撑集support">支撑集(Support)</h3><ol type="1"><li>概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合</li></ol><a id="more"></a><h2 id="数学分析">数学分析</h2><h3 id="lipschitz连续">Lipschitz连续</h3><ol type="1"><li>若存在一个常数K，使得定义域内的任意两点x1,x2满足： $ |f(x_1)-f(x_2)|=K|x_1-x_2|$ 则称函数为Lipschitz连续函数。此性质限定了f的导函数的绝对值不超过K，规定了函数的最大局部变动幅度。</li></ol><h2 id="statistical-learning-theory">Statistical learning theory</h2><h3 id="introduction">Introduction</h3><p>Determine how well a model performs on unseen data</p><h3 id="preliminary">Preliminary</h3><ul><li><p>Markov Inequality</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210155914107.png" alt="image-20210210155914107" style="zoom:67%;" />, in the order of <span class="math inline">\(\mathcal{O}(\frac{1}{deviation})\)</span></p></li><li><p>Chebyshev's Inequality</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210155723575.png" alt="image-20210210155723575" style="zoom:67%;" />, in the order of <span class="math inline">\(\mathcal{O}(\frac{1}{deviation^2})\)</span></p></li><li><p>Generic Chernoff's Bound</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210160010328.png" alt="image-20210210160010328" style="zoom:67%;" /></p></li><li><p>Hoeffding's Inequality</p><ul><li><p>Hoeffding's lemma</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210160051227.png" alt="image-20210210160051227" style="zoom:67%;" /></p></li><li><p>Hoeffding's Inequality</p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210160116965.png" alt="image-20210210160116965" /><figcaption aria-hidden="true">image-20210210160116965</figcaption></figure><p>Hoeffding's inequality is useful to bound the probability of the gap between an empirical value and the true expectation of an average of <strong>bounded</strong> random variables.</p></li></ul></li><li><p>McDiarmid's Inequality</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210181355241.png" alt="image-20210210181355241" style="zoom:50%;" /></p><ul><li>A concentration inequality.</li><li>This bound is useful because if we prove that an algorithm is β stable then we will have this property on a specific function.</li></ul></li></ul><h3 id="pac">PAC</h3><p>Check <a href="http://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-7-notes.pdf">here</a> for PAC, VC, uniform bound and others.</p><ul><li><p><strong>PAC Learning (agnostic PAC learnable)</strong>--finite hypothesis class</p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210155242200.png" alt="image-20210210155242200" /><figcaption aria-hidden="true">image-20210210155242200</figcaption></figure><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210090621996.png" alt="image-20210210090621996" style="zoom:70%;" /></p><ul><li>Consider a simple linear classifier with 2 weights <span class="math inline">\(\vec{w} = (w_1;w_2)\)</span>, which are stored using a 32 bit floats. This implies that the hypothesis class is finite with $|| = 2^{32} $.</li><li>This theorem <strong>works on finite hypothesis class</strong>, and answers that for a hypothesis class <span class="math inline">\(\mathcal{H}\)</span>, to make the generation gap is smaller than <span class="math inline">\(\epsilon\)</span> with at least <span class="math inline">\(1-\delta\)</span>, the required samples complexity.</li><li><em>Once a hypothesis class is PAC learnable, with high probability the training set is <span class="math inline">\(\epsilon\)</span>-representative.</em></li><li>Note Suppose <span class="math inline">\(\mathcal{H}\)</span> is PAC learnable, there <strong>is not a unique function</strong> <span class="math inline">\(m_{\mathcal{H}}\)</span> that satisfies the requirements given in the definition of PAC learnability.</li><li>Finite classes are PAC Learnable, also agnostic PAC Learnable.</li></ul></li><li><p><strong>Uniform convergence</strong></p><ul><li>Formalize that over all hypothesis in <span class="math inline">\(\mathcal{H}\)</span>, the empirical risk is close to the true risk. This will <strong>make sure the ERM to work.</strong></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210161725993.png" alt="image-20210210161725993" style="zoom:80%;" /></li></ul></li><li><p>VC dimension</p><ul><li><p>measure the complexity of hypothesis class other than cardinality.</p></li><li><p><strong>Shattering</strong></p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210163136908.png" alt="image-20210210163136908" /><figcaption aria-hidden="true">image-20210210163136908</figcaption></figure></li><li><p><strong>VC Dimension</strong></p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210163157194.png" alt="image-20210210163157194" /><figcaption aria-hidden="true">image-20210210163157194</figcaption></figure><p>Informally VC dimension is the maximum number of distinct points that a hypothesis in <span class="math inline">\(\mathcal{H}\)</span> can correctly classify every possible labeling with zero error.</p><ul><li>With infinite VC dimension, the hypothesis class won't be PAC learnable.</li><li>There exist hypothesis classes with uncountable cardinality but finite VC dimension.</li><li>For every two hypothesis classes if <span class="math inline">\(\mathcal{H}_0 \subset \mathcal{H}\)</span> then <span class="math inline">\(VCdim(\mathcal{H}_0) \leq VCdim(\mathcal{H})\)</span>.</li></ul></li></ul></li></ul><h3 id="occams-bound">Occam's bound</h3><ul><li><p>The PAC bound can be treated as Occam's bound with a uniform prior .</p></li><li><p>Occam's bound will put a distribution over the countably infinite hypothesis class <span class="math inline">\(\mathcal{H}\)</span> that is independent of dataset <span class="math inline">\(S\)</span> we will receive. In doing so we will <strong>get bounds on the generalization gap that no longer depend on the size of the hypothesis class</strong>, <span class="math inline">\(|\mathcal{H}|\)</span>. These bounds now become variable depending on how we weigh each individual hypothesis <span class="math inline">\(h\)</span>, i.e. <span class="math inline">\(P(h)\)</span>.</p></li><li><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210090847116.png" alt="image-20210210090847116" style="zoom:80%;" /></p><ul><li><p><strong>Regularizers offers higher probability assigned to <span class="math inline">\(\vec{w}\)</span> near the origin and thus a tighter bound,</strong> it won't influence the algorithm (loss). Specifically , when an <span class="math inline">\(\ell_2\)</span> regularization term is added to the learning algorithm, it adds concavity to the loss function.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210092846655.png" alt="image-20210210092846655" style="zoom:80%;" /></p></li></ul></li><li><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210091000849.png" alt="image-20210210091000849" style="zoom:80%;" />, where <span class="math inline">\(D(Q\|P)\)</span> is the KL divergence which serves as a complexity measure.</p><ul><li><p>It tells that with a good posterior that is close to the prior, then the KL-divergence will become smaller and out bound will be tighter. <strong>Even though it's tight, the bound is tight for hypothesis that we may not care about, e.g. tight on the bound with respect to <span class="math inline">\(P\)</span> prior on hypothesis.</strong></p></li><li><p><em>Note that the posterior is after applying the prior <span class="math inline">\(P\)</span> on hypothesis, and seeing the data, then one can get this posterior <span class="math inline">\(Q\)</span>.</em></p></li><li><p><strong>Why posterior?</strong></p><p>different choices of prior and posterior hypotheses can be made, each resulting in a new bound without us touching the algorithm.</p></li><li><p>The dropout PAC-Bayes is a lower bound on the PAC-Bayes bound that becomes tight when the dropout factor is 0.</p></li></ul></li></ul><h3 id="stability-generalization">Stability, Generalization</h3><ul><li><p>PAC learning and Occam's bound work as algorithm-agnostic bounds.</p></li><li><p>Stability</p><ul><li><p>Hint: a change in data distribution does not change the predictions.</p></li><li><p>Definition: <strong>uniform stability</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210183949427.png" alt="image-20210210183949427" style="zoom:67%;" /></p><p>An algorithm with this property can be understood as one that produces a hypothesis such that the loss function <span class="math inline">\(\ell\)</span> is not drastically affected by perturbing the dataset in this manner.</p></li><li><p><font color='blue'>EMR with regularization is <span class="math inline">\(\beta\)</span>-stable.</font></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210211826259.png" alt="image-20210210211826259" style="zoom:50%;" /></p><ul><li><em>If we perturb the data by a single element, we learn <span class="math inline">\(\mathcal{A}\)</span> that can become arbitrarily close for large <span class="math inline">\(n\)</span>.</em></li></ul></li><li><p><font color='blue'>SGD is stable</font></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210214207908.png" alt="image-20210210214207908" style="zoom:57%;" /></p><ul><li><strong>It holds for a finite number of steps <span class="math inline">\(T\)</span></strong></li><li></li></ul></li></ul></li><li><p><strong>Defect</strong>:</p><p><span class="math inline">\(D[h_S]=R[h_S]-\hat{R}_S[h_S]\)</span></p><ul><li><p>Defect <span class="math inline">\(D[h_S]\)</span> for a hypothesis <span class="math inline">\(h_S\)</span> derived from an algorithm after seeing the dataset <span class="math inline">\(S\)</span> is defined as the difference between the population risk and the empirical risk.</p></li><li><p><strong>It's expectation is not zero.</strong></p></li><li><p><strong>The expectation value of defect can be bounded under certain conditions.</strong></p><p><font color='blue'>If $ $ is a <span class="math inline">\(\beta\)</span>-uniformly stable algorithm, then <span class="math inline">\(-\beta\le \mathbb{E}[D[h_S]]\le\beta\)</span>. </font></p></li><li><p><font color='blue'>Let <span class="math inline">\(\mathcal{A}\)</span> be a <span class="math inline">\(\beta\)</span>-uniformly stable learning algorithm with respect to a loss function <span class="math inline">\(\ell:\mathcal{Y\times Y}\rightarrow [0，M]\)</span>. <strong>The absolute difference of the defect calculated on a dataset <span class="math inline">\(S\)</span> and on a perturbed version of the dataset</strong> <span class="math inline">\(S^{i,z}\)</span> is bounded by <span class="math inline">\(|D[h_S]-D[h_{S^{i,z}}]|\le 2\beta +\frac{M}{n}\)</span>. </font></p></li></ul></li><li><p><font color='blue'>For a <span class="math inline">\(\beta\)</span>-uniformly stable algorithm, the relationship between the empirical and the population risk is <span class="math inline">\(\mathbb{E}[R[h_S]]\le\mathbb{E}_S[\hat{R}_S[h_S]]+\beta\)</span>. </font></p><ul><li>It's a bound on the expectation value of the population risk. But this bound does not hold for all possible <span class="math inline">\(h_S\)</span>.</li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210205331186.png" alt="image-20210210205331186" style="zoom:40%;" /></li></ul></li><li><p><font color='blue'>For a <span class="math inline">\(\beta\)</span>-uniformly stable algorithm <span class="math inline">\(\mathcal{A}\)</span> with respect to a loss function <span class="math inline">\(\ell:\mathcal{Y\times Y}\rightarrow [0，M]\)</span> and a hypothesis <span class="math inline">\(h_S\)</span> with <span class="math inline">\(|S|=n\)</span>. <strong>The relationship between the empirical and the population risk holds with probability <span class="math inline">\(1-\delta\)</span></strong>: <span class="math inline">\(R[h_S]\le \hat{R}_S[h_S]+\beta +(n\beta+\frac{M}{2})\sqrt{\frac{2\log\frac{2}{\delta}}{n}}\)</span>. </font></p><ul><li>The last term is a concentration Inequality (McDiramid's)</li><li>For this bound, as <span class="math inline">\(n\)</span> goes up, it becomes less tight.</li></ul></li></ul><h2 id="convex-optimization">Convex Optimization</h2><p>Book <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>, <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></p><table><colgroup><col style="width: 45%" /><col style="width: 54%" /></colgroup><thead><tr class="header"><th>Terminology</th><th>Definition</th></tr></thead><tbody><tr class="odd"><td>Convex function: origin</td><td><span class="math inline">\(f(\theta x+(1-\theta)y)\le \theta f(x)+(1-\theta)f(y)\)</span></td></tr><tr class="even"><td>Convex function: 1st order (once differential )</td><td>$f(y)f(x)+f(x)^T (y-x) $</td></tr><tr class="odd"><td>Convex function: 2nd order (twice differential )</td><td><span class="math inline">\(\nabla^2f(x)\succeq0\)</span>, the eigenvalues won't be negative</td></tr><tr class="even"><td><span class="math inline">\(L\)</span>-Lipschitz</td><td><span class="math inline">\(\|f(x)-f(y)\|\le L\|x-y\|\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\beta\)</span>-smooth: <span class="math inline">\(\beta\)</span>-Lipschitz on gradient</td><td><span class="math inline">\(\|\nabla f(x)-\nabla f(y)\|\le \beta\|x-y\|\Rightarrow \nabla^2f(x)\preceq\beta\mathrm{I}\)</span></td></tr><tr class="even"><td><span class="math inline">\(\alpha\)</span>-strong convex, limited the domain mostly.</td><td><span class="math inline">\(f(y)-f(x)\le\nabla f(x)^T (y-x)-\frac{\alpha}{2}\|y-x\|^2\Rightarrow \nabla^2f(x)\succeq\alpha\mathrm{I}\)</span></td></tr></tbody></table><center><strong>Table: Optimizer in different conditions</strong></center></br> For each optimizer, from the top line downwarding, the rate of convergence is increasing. The optimal step size is gotten by minimizing the bound.</center><table><colgroup><col style="width: 9%" /><col style="width: 18%" /><col style="width: 18%" /><col style="width: 18%" /><col style="width: 18%" /><col style="width: 18%" /></colgroup><thead><tr class="header"><th>Optimizer</th><th>Condition</th><th>Converge rate</th><th>Optimal step size</th><th>Sub-optimal gap</th><th>Bounds of the gap</th></tr></thead><tbody><tr class="odd"><td>GD after <span class="math inline">\(T\)</span> steps</td><td>L-Lipschitz convex</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{\sqrt{T}})\)</span></td><td><span class="math inline">\(\gamma=\frac{\|x_1-x^*\|_2}{L\sqrt{T}}\)</span></td><td><span class="math inline">\(f(\frac{1}{T}\sum\limits_{k=1}^{T}x_k)-f(x^*)\)</span></td><td><span class="math inline">\(\le\frac{\|x_1-x^*\|L}{\sqrt{T}}\)</span>, <br />the initial point matters</td></tr><tr class="even"><td>GD</td><td><span class="math inline">\(\beta\)</span>-smooth<br />+convex</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{T})\)</span></td><td><span class="math inline">\(\gamma=\frac{1}{\beta}\)</span>, <br />constant and independent of <span class="math inline">\(T\)</span></td><td><span class="math inline">\(f(x_k)-f(x^*)\)</span><br />Notice the average on all samples can be erased</td><td><span class="math inline">\(\le\frac{2\beta\|x_1-x^*\|^2}{k-1}\)</span>, <br /><span class="math inline">\(k\)</span> means the <span class="math inline">\(k\)</span> step. <br />Bound depends on initial point</td></tr><tr class="odd"><td>Projected subGD after <span class="math inline">\(T\)</span> steps</td><td><span class="math inline">\(\alpha\)</span>-strong<br /> + <span class="math inline">\(L\)</span>-Lipschitz</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{T})\)</span></td><td><span class="math inline">\(\gamma_k=\frac{2}{\alpha (k+1)}\)</span>,<br /> diminish at every step</td><td><span class="math inline">\(f(\sum\limits_{k=1}^T\frac{2k}{T(T+1)}x_k)-f(x^*)\)</span></td><td><span class="math inline">\(\le\frac{2L^2}{\alpha (T+1)}\)</span></td></tr><tr class="even"><td>GD</td><td><span class="math inline">\(\lambda\)</span>-strong <br />+ <span class="math inline">\(\beta\)</span>-smooth</td><td><span class="math inline">\(\mathcal{O}(\exp{(-T)})\)</span></td><td><span class="math inline">\(\gamma=\frac{2}{\lambda+\beta}\)</span></td><td><span class="math inline">\(f(x_{t+1})-f(x^*)\)</span><br />Notice the average on all samples can be erased</td><td><span class="math inline">\(\le\frac{\beta}{2}\exp{(-\frac{4t}{\kappa+1})}\|x_1-x^*\|^2\)</span>,<br />$ =<span class="math inline">\(. &lt;br /&gt;\)</span>k$ is the same meaning of <span class="math inline">\(t\)</span>,<br /> rather than the <span class="math inline">\(\kappa\)</span> in denominator.</td></tr><tr class="odd"><td>Polyak (heavy ball)</td><td>Quadratic loss</td><td><span class="math inline">\(\mathcal{O} ((\frac{\sqrt\kappa-1}{\sqrt\kappa+1})^t)\\\approx\exp(-\frac{C}{\sqrt\kappa})\)</span>,<br /><span class="math inline">\(\kappa=\frac{h_{max}}{h_{min}}\)</span></td><td><span class="math inline">\(\gamma^*=\frac{(1+\sqrt\mu)^2}{h_{max}}\\=\frac{(1-\sqrt\mu)^2}{h_{min}}\)</span></td><td><span class="math inline">\(\left\|\left[ \begin{matrix} x_{t+1}-x*\\ x_t-x^* \end{matrix} \right]\right\|_2\)</span></td><td><span class="math inline">\(\le\mathcal{O} (\rho(A)^T)=\mathcal{O} (\sqrt{\mu}^T)\)</span>.<br /> <span class="math inline">\(\mu\)</span> is the curvature</td></tr><tr class="even"><td>Nesterov NAG</td><td><span class="math inline">\(\beta\)</span>-smooth</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{T^2})\)</span></td><td></td><td><span class="math inline">\(f(y_t)-f(x^*)\)</span></td><td></td></tr><tr class="odd"><td>Nesterov NAG</td><td><span class="math inline">\(\alpha\)</span>-strong <br />+<span class="math inline">\(\beta\)</span>-smooth</td><td><span class="math inline">\(\mathcal{O}(\exp(-\frac{T}{\sqrt{\kappa}}))\)</span></td><td></td><td><span class="math inline">\(f(y_t)-f(x^*)\)</span></td><td><span class="math inline">\(\le\frac{\alpha+\beta}{2}\|x_k -x^*\|^2\exp(-\frac{t-1}{\sqrt\kappa})\)</span></td></tr><tr class="even"><td>SGD</td><td><span class="math inline">\(L\)</span>-Lipschitz by<br /> <span class="math inline">\(\|\tilde{g}(x)\|\le L\)</span> with prob. 1</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{\sqrt T})\)</span><br />If wants a <span class="math inline">\(\epsilon\)</span>-tolerance, <span class="math inline">\(T\ge\frac{B^2L^2}{\epsilon^2}\)</span></td><td><span class="math inline">\(\gamma=\frac{B}{L\sqrt{T}}\)</span></td><td><span class="math inline">\(\mathbb{E}[f(\bar{x})]-f(x^*)\)</span>, where <br /><span class="math inline">\(x^*\in\arg\min_{x:\|x\|&lt;B}f(x)\)</span></td><td><span class="math inline">\(\le\frac{BL}{\sqrt T}\)</span></td></tr><tr class="odd"><td>SGD</td><td><span class="math inline">\(\alpha\)</span>-strong+ <span class="math inline">\(\mathbb{E}\|\tilde{g}(x)\|_*^2\le B^2\)</span> (kind of <span class="math inline">\(B\)</span>-smooth)</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{T})\)</span></td><td><span class="math inline">\(\gamma=\frac{2}{\alpha(s+1)}\)</span></td><td><span class="math inline">\(f(\sum\limits_{s=1}^t\frac{2s}{t(t+1)}x_s)-f(x^*)\)</span></td><td><span class="math inline">\(\le\frac{2B^2}{\alpha(t+1)}\)</span></td></tr></tbody></table><h3 id="estimate-sequence"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&amp;rep=rep1&amp;type=pdf">Estimate sequence</a></h3><ul><li><p>Definition</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210129143844.png" alt="image-20210129143844647" style="zoom:67%;" /></p></li><li><p>Properties</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131115340.png" alt="image-20210131115338221" style="zoom:50%;" /></p><p>for any sequence <span class="math inline">\(\{\lambda_k\}\)</span>, satisfying <span class="math inline">\((2.2.2)\)</span> we can derive <strong>the rate of convergence of the minimization process directly from the rate of convergence of the sequence <span class="math inline">\(\{\lambda_k\}\)</span>.</strong></p><p>Note: below in estimate sequence, all <span class="math inline">\(L\)</span> means <span class="math inline">\(L\)</span>-smooth function</p></li><li><p>How to form an estimate sequence?</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131150937.png" alt="image-20210131150935371" style="zoom:40%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131151028.png" alt="image-20210131151028446" style="zoom:40%;" /></p></li><li><p>How to ensure <span class="math inline">\((2.2.2)\)</span>?</p><ul><li><p>Method 1: Do scheme (2.2.6), it will generate a sequence <span class="math inline">\(\{x_k\}_{k=0}^\infin\)</span> such that <span class="math inline">\(f(x_k)-f^*\le \lambda_k[f(x_0)-f^*+\frac{\gamma_0}{2}\|x_0-x^*\|^2]\)</span>. It will make sequence satisfy <span class="math inline">\((2.2.2)\)</span></p></li><li><p>Method 2: Using gradient step</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210201094020.png" alt="image-20210201094020408" style="zoom:50%;" /></p></li></ul></li><li><p>Some cases</p><ul><li><font color='orange'>If in the scheme (2.2.6) we choose <span class="math inline">\(\gamma_0\ge\mu\)</span>, then <span class="math inline">\(\lambda_k\le\min\{(1-\sqrt{\frac{\mu}{L}})^k,\frac{4L}{(2\sqrt{L}+k\sqrt{\gamma})^2}\}\)</span>.</font></li><li><font color='blue'>If in the scheme (2.2.6) we choose <span class="math inline">\(\gamma_0=L\)</span>, then this scheme generates a sequence <span class="math inline">\(\{x_k\}_{k=0}^{\infin}\)</span> such that <span class="math inline">\(f(x_k)-f^*\le L\min \{(1-\sqrt{\frac{\mu}{L}})^k,\frac{4}{(k+2)^2}\}\|x_0-x^*\|^2\)</span>. This  means that it's optimal for the class <span class="math inline">\(\mathcal{S}_{\mu,L}^{1，1}(R^n)\)</span> with <span class="math inline">\(\mu\ge0\)</span>. </font></li><li><font color='blue'>If in the scheme <span class="math inline">\((2.2.8)\)</span> we choose <span class="math inline">\(\alpha_0\ge \sqrt{\frac{\mu}{L}}\)</span>, then this scheme generates a sequence <span class="math inline">\(\{x_k\}_{k=0}^{\infin}\)</span> such that <span class="math inline">\(f(x_k)-f^*\le \min \{(1-\sqrt{\frac{\mu}{L}})^k,\frac{4L}{(2\sqrt{L}+k\sqrt{\gamma_0})^2}\}[f(x_0)-f^*+\frac{\gamma_0}{2}\|x_0-x^*\|^2]\)</span>, where <span class="math inline">\(\gamma_0=\frac{\alpha_0(\alpha_0L-\mu)}{1-\alpha_0}\)</span>. Here <span class="math inline">\(\alpha_0\ge \sqrt{\frac{\mu}{L}}\)</span> is equivalent to <span class="math inline">\(\gamma_0\ge\mu\)</span>. </font></li><li></li></ul></li></ul><h3 id="convex-sets">Convex sets</h3><h4 id="affine-sets">Affine sets</h4><ul><li>Affine sets:<ul><li>Definition : A set <span class="math inline">\(C\subseteq R^n\)</span> is affine if the line through any two distinct points in <span class="math inline">\(C\)</span> lies in <span class="math inline">\(C\)</span>, aka for any <span class="math inline">\(x_1 ,x_2\in C\)</span> and <span class="math inline">\(\theta\in R\)</span>, one has <span class="math inline">\(\theta x_1+(1-\theta)x_2\in C\)</span>. It indicates that the <span class="math inline">\(C\)</span> contains the linear combination of any two points in <span class="math inline">\(C\)</span>.</li><li>Induction: If <span class="math inline">\(C\)</span> is an affine set, <span class="math inline">\(x_1，\cdots,x_k\in C\)</span> and <span class="math inline">\(\theta_1+\cdots+\theta_k=1\)</span>, then the point <span class="math inline">\(\theta_1 x_1+\cdots+\theta_kx_k\)</span> also belongs to <span class="math inline">\(C\)</span>.</li></ul></li><li>Affine hull<ul><li>Definition: the set of all affine combinations of points in some set <span class="math inline">\(C\subseteq R^n\)</span>, denoted as <span class="math inline">\(\mathrm{aff}C\)</span></li><li>It's the smallest affine set that contains <span class="math inline">\(C\)</span></li></ul></li><li>Affine dimension<ul><li>Definition: as the dimension of its affine hull.</li><li>E.g.: <span class="math inline">\(\{x\in R^2|x_1 ^2+x_2^2=1\}\)</span>, the affine dimension is 2.</li></ul></li></ul><h4 id="convex-sets-1">Convex sets</h4><ul><li>Definition: If every point in the set can be seen by every other point, along an unobstructed straight path between them, where unobstructed means lying in the set.</li><li>Every affine set is also convex.</li><li>Convex hull, denotes as <span class="math inline">\(\mathrm{conv} C\)</span>, is the set of all convex combinations of points in <span class="math inline">\(C\)</span>. It is always convex, and it's the smallest convex set that contains <span class="math inline">\(C\)</span></li><li>More generally, suppose <span class="math inline">\(p: R^n \rightarrow R\)</span> satisfies <span class="math inline">\(p(x)\ge0\)</span> for all <span class="math inline">\(x\in C\)</span> and <span class="math inline">\(\int_Cp(x)dx=1\)</span>, where <span class="math inline">\(C\subseteq R^n\)</span> is convex, then <span class="math inline">\(\int_Cp(x)x dx \in C\)</span>, if the integral exists.</li></ul><h3 id="convex-functions">Convex functions</h3><h4 id="convex-functions-1">Convex functions</h4><ul><li><p>Definition</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117165609.png" alt="image-20210117165609361" style="zoom:50%;" /></p></li><li><p>All affine function are both convex and concave.</p></li><li><p><span class="math inline">\(f\)</span> is convex if and only if for all <span class="math inline">\(x\in \mathrm{dom}f\)</span> and all <span class="math inline">\(v\)</span>, the function <span class="math inline">\(g(t)=f(x+tv)\)</span> is convex.</p></li></ul><h4 id="extended-value-extensions">Extended-value extensions</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117170246.png" alt="image-20210117170246166" style="zoom:50%;" /></p><h4 id="first-order-conditions">First-order conditions</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117171410.png" alt="image-20210117171410378" style="zoom:50%;" /></p><p>The inequality (3.2) states that for a convex function, the first-order Taylor approximation is in fact a global <em>underestimator</em> of the function. Conversely, if the first-order Taylor approximation of a function is always a global <em>underestimator</em> of the function, then the function is convex.</p><ul><li>The inequality (3.2) shows that if <span class="math inline">\(\nabla f(x) = 0\)</span>, then for all <span class="math inline">\(y \in \mathrm{dom} f, f(y) ≥ f(x)\)</span>, i.e., <span class="math inline">\(x\)</span> is a global minimizer of the function <span class="math inline">\(f\)</span>.</li></ul><h4 id="second-order-conditions">Second-order conditions</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117173919.png" alt="image-20210117173919136" style="zoom:67%;" /></p><h4 id="more-constraints-on-convex-function">More constraints on convex function</h4><h5 id="beta-smooth"><span class="math inline">\(\beta\)</span>-smooth</h5><ul><li>Definition: a continuously function <span class="math inline">\(f\)</span> is <span class="math inline">\(\beta\)</span>-smooth if the gradient <span class="math inline">\(\nabla f\)</span> is <span class="math inline">\(\beta\)</span>-Lipschitz, that is <span class="math inline">\(\|\nabla f(x)-\nabla f(y)\|\le\beta\|x-y\|\)</span>.</li><li><strong><em>If <span class="math inline">\(f\)</span> is twice differentiable then this is equivalent to the eigenvalues of the Hessians being smaller than <span class="math inline">\(\beta\)</span> at any point.</em></strong> , <span class="math inline">\(\nabla^2f(x)\preceq\beta \mathrm{I}_n,\forall x\)</span>.</li><li><font color='green'>smoothness removes dependency from the averaging scheme</font><br /></li><li><font color='cyan'>If extend the <span class="math inline">\(\beta\)</span>-smooth to multi power, it's called <a href="https://en.wikipedia.org/wiki/H">Holder condition</a></font><br /></li><li>The bigger your function changes in gradients, the upper you have to explore.</li></ul><h5 id="alpha-strong-convexity"><span class="math inline">\(\alpha\)</span>-strong convexity</h5><p>Strong convexity can significantly speed-up the convergence of first order methods.</p><ul><li><p>Definition</p><p>We say that <span class="math inline">\(f:\mathcal{X}\rightarrow\mathbb{R}\)</span> is a <span class="math inline">\(\alpha\)</span><em>-strongly convex</em> if it satisfies the following improved subgradient inequality:</p><p><span class="math inline">\(f(x)-f(y)\le\nabla f(x)^T(x-y)-\frac{\alpha}{2}\|x-y\|^2\)</span>. A large value of <span class="math inline">\(\alpha\)</span> will lead to a faster rate. A <span class="math inline">\(\alpha\)</span><strong>-strong convexity for twice differential function <span class="math inline">\(f\)</span> can also be interpreted as <span class="math inline">\(\nabla^2f(x)\succeq\alpha \mathrm{I}_n,\forall x\)</span></strong>.</p></li><li><p>Strong convexity plus <span class="math inline">\(\beta\)</span>-smoothness will lead to the gradient descent with a constant step-size achieves a linear rate of convergence, precisely the oracle complexity will be <span class="math inline">\(O(\frac{\beta}{\alpha}\log(1/\varepsilon)), \beta\ge\alpha\)</span>. In some sense strong convexity is a dual assumption to smoothness, and in fact this can be made precise within the framework of Fenchel duality.</p></li><li><p><span class="math inline">\(\alpha\)</span> can often be reviewed as large as the sample size. Thus reducing the number of step from <strong>sample size</strong> to <span class="math inline">\(\sqrt{\mathrm{sample \quad size}}\)</span> (cause <span class="math inline">\(\kappa=\frac{\beta}{\alpha}\)</span> for <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strong function, and in basic gradient descent algorithm, to reach <span class="math inline">\(\epsilon\)</span>-accuracy, it requires <span class="math inline">\(\mathcal{O}(\kappa\log(\frac{1}{\epsilon}))\)</span>, and for Nesterov's Accelerated Gradient Descent attains the improved oracle complexity of <span class="math inline">\(\mathcal{O}(\sqrt{\kappa}\log(\frac{1}{\epsilon}))\)</span>) can be a huge deal, especially in large scale applications.</p></li></ul><h4 id="examples-of-convex-functions">Examples of Convex functions</h4><ul><li><em>Norms</em></li><li><em>Max function</em></li><li><em>Quadratic-over-linear function</em> <span class="math inline">\(\frac{x^2}{y}\)</span></li><li><em>Log-sum-exp</em>: <span class="math inline">\(\log(e^{x_1}+\cdots+e^{x_n})\)</span>, which is regarded as a differentiable approximation of the max function</li><li><em>Geometric mean</em>: <span class="math inline">\((\prod\limits_{i=1}^{n}x_i)^{1/n}\)</span>, concave</li><li><em>Log-determinant</em>: <span class="math inline">\(\log\det X\)</span>, concave</li></ul><p>For proofs, check Chapter <span class="math inline">\(3.1.5\)</span> of <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>.</p><h4 id="sublevel-sets">Sublevel sets</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181258.png" alt="image-20210117181258728" style="zoom:50%;" /></p><h4 id="epigraph">Epigraph</h4><p>A function is convex if and only if its epigraph is a convex set.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181415.png" alt="image-20210117181415039" style="zoom:50%;" /></p><h4 id="jensens-inequality-and-extensions">Jensen's inequality and extensions</h4><p>Once a function is convex, then you can get</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117185801.png" alt="image-20210117185801678" style="zoom:50%;" /> the simplest version of it is <span class="math inline">\(f(\frac{x+y}{2})\le\frac{f(x)+f(y)}{2}\)</span>.</p><h4 id="holders-inequality">Holder's inequality</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190336.png" alt="image-20210117190336823" style="zoom:50%;" /></p><h4 id="operations-that-preserve-convexity">Operations that preserve convexity</h4><ul><li><p>Nonnegative weighted sums: <span class="math inline">\(f=w_1f_1+\cdots+w_mf_m\)</span></p></li><li><p>Composition with an affine mapping: <span class="math inline">\(g(x)=f(Ax+b)\)</span>. If <span class="math inline">\(f\)</span> is convex, so is <span class="math inline">\(g\)</span>; if <span class="math inline">\(f\)</span> is concave, so is <span class="math inline">\(g\)</span>.</p></li><li><p>Pointwise maximum and suprenum: <span class="math inline">\(f(x)=\max\{f_1(x),f_2(x)\}\)</span> and <span class="math inline">\(f(x)=\max\{f_1(x),\cdots,f_m(x)\}\)</span></p></li><li><p>Composition: <span class="math inline">\(f(x)=h(g(x))\)</span></p><ul><li><p>Scalar composition</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190943.png" alt="image-20210117190943043" style="zoom:50%;" /></p></li><li><p>Vector composition <span class="math inline">\(f(x)=h(g(x))=h(g_1(x),\cdots,g_k(x))\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117191112.png" alt="image-20210117191112754" style="zoom:50%;" /></p></li><li><p>Minimization</p></li></ul></li></ul><h3 id="typical-numerical-optimization">Typical Numerical optimization</h3><h4 id="gradient-descent">Gradient descent</h4><p>The basic principle behind <strong>gradient descent</strong> is to make a small step in the direction that minimizes the local first order Taylor approximation of <span class="math inline">\(f\)</span> (also known as the steepest descent direction). This kind of methods will <strong>obtain an oracle complexity <em>independent of the dimension</em>.</strong></p><p><span class="math inline">\(x_{t+1}=x_t-\eta\nabla f(x_t)\)</span></p><p>Taking <span class="math inline">\(f(w)=\frac{1}{2}w^TAw-b^Tw,w\in\mathbb{R}^n\)</span> into consideration, suppose <span class="math inline">\(A\)</span> is symmetric and invertible, then <span class="math inline">\(A=Q\Lambda Q^T,\Lambda=(\lambda_1,\cdots,\lambda_n),\lambda_1\le\lambda_2\le\cdots\le\lambda_{n-1}\le\lambda_n\)</span>.</p><ul><li><p>All errors are not made equal. Indeed, there are different kinds of errors, <span class="math inline">\(n\)</span> to be exact, one for each of the eigenvectors of <span class="math inline">\(A\)</span>.</p><p><span class="math inline">\(f(w^k)-f(w^*)=\sum(1-\alpha\lambda_i)^{2k}\lambda_i[x_i^0]^2\)</span></p></li><li><p><strong>Denote the condition number <span class="math inline">\(\kappa=\frac{\lambda_n}{\lambda_1}\)</span>, then the bigger the <span class="math inline">\(\kappa\)</span> is, the slower gradient descent will be</strong>, cause the condition number is a direct description of pathological curvature.</p></li><li><p><strong>The optimal step-size causes the first and last eigenvectors to converge at the same rate.</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124212843.png" alt="image-20210124212843050" style="zoom:50%;" /></p></li></ul><h4 id="projected-gradient-descent">Projected gradient descent</h4><ul><li><p>Subgradient</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222307.png" alt="image-20210117222306873" style="zoom:40%;" /></p></li><li><p>Projected subgradient descent</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222725.png" alt="image-20210117222725673" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117223015.png" alt="image-20210117222937543" style="zoom:40%;" /></p></li></ul><h4 id="gradient-descent-with-momentum-polyaks-momentum"><a href="https://distill.pub/2017/momentum/">Gradient descent with momentum</a> : Polyak's Momentum</h4><ul><li><p>Sometimes SGD fail with a reason of pathological curvature of objective. (like valley, trench)</p></li><li><p>Momentum modify gradient descent by adding a short-term memory</p><p><span class="math inline">\(y_{t+1}=\beta y_t+\nabla f(x_t)\\x_{t+1}=x_{t}-\alpha y_{t+1}\)</span>.</p><p>When <span class="math inline">\(\beta=0\)</span>, it's gradient descent.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124224747.png" alt="image-20210124224747333" style="zoom:50%;" /></p></li><li><p>Momentum allows us to crank up the step-size up by a factor of 2 before diverging.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124225628.png" alt="image-20210124225628085" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124225657.png" alt="image-20210124225656929" style="zoom:50%;" /></p></li><li><p>Optimize over <span class="math inline">\(\beta\)</span>: The critical value of <span class="math inline">\(\beta = (1 - \sqrt{\alpha \lambda_i})^2\)</span> gives us a convergence rate (in eigenspace <span class="math inline">\(i\)</span>) of <span class="math inline">\(1 - \sqrt{\alpha\lambda_i}\)</span>. A square root improvement over gradient descent, <span class="math inline">\(1-\alpha\lambda_i\)</span>! Alas, this only applies to the error in the <span class="math inline">\(i^{th}\)</span> eigenspace, with <span class="math inline">\(\alpha\)</span> fixed.</p></li><li><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124231402.png" alt="image-20210124231402777" style="zoom:50%;" /></p></li><li><p><strong>Failing</strong>: there exist strongly-convex and smooth functions for which, by choosing carefully the hyperparameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> and the initial condition <span class="math inline">\(x_0\)</span>, the heavy-ball method fails to converge.</p></li></ul><h4 id="nesterovs-accelerated-gradient-descent">Nesterov’s Accelerated Gradient Descent</h4><ul><li><p><a href="https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/">Some notes of it</a></p></li><li><p>Iterations: starting at an arbitrary initial point <span class="math inline">\(x_1=y_1\)</span></p><p><span class="math display">\[y_{s+1}=x_s-\frac{1}{\beta}\nabla f(x_s),\\x_{s+1}=(1+\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})y_{s+1}-\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}y_s\]</span></p></li><li><p><font color='blue'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex, then Nesterov's gradient descent satisfies for <span class="math inline">\(t\ge0\)</span>, <span class="math inline">\(f(y_t)-f(x^*)\le\frac{\alpha+\beta}{2}\|x_1-x^*\|^2\exp{(-\frac{t-1}{\sqrt{\kappa}})}\)</span> .</font></p></li><li><p><strong>Converge in <span class="math inline">\(\mathcal{O}(\frac{1}{T^2})\)</span> for smooth case, and <span class="math inline">\(\mathcal{O}(\exp(-\frac{T}{\sqrt{\kappa}}))\)</span>, it guaranteed convergence for quadratic functions (and not piece-wise quadractic)</strong></p></li></ul><h4 id="stochastic-gradient-descent">Stochastic gradient descent</h4><ul><li><p>Cases: one is <span class="math inline">\(\mathbb{E}_{\xi}\nabla_x\ell(x,\xi)\in\part f(x)\)</span>, where <span class="math inline">\(\xi\)</span> is sampled. This method cannot be reproduced; the other is directly minimize <span class="math inline">\(f(x)=\frac{1}{m}\sum\limits_{i=1}^{m}f_i(x)\)</span>, here gradient is reported as <span class="math inline">\(\nabla f_I(x)\)</span>, where <span class="math inline">\(I\in[m]\)</span>, this method can be reproduced.</p></li><li><p>Non-smooth stochastic optimization</p><ul><li>Definition: there exists <span class="math inline">\(B&gt;0\)</span> such that <span class="math inline">\(\mathbb{E}\|\tilde{g}(x)\|_*^2\le B^2\)</span> for all <span class="math inline">\(x\in\mathcal{X}\)</span></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202153046.png" alt="image-20210202153043826" style="zoom:67%;" /></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202153117.png" alt="image-20210202153116696" style="zoom:67%;" /></li></ul></li><li><p>Smooth stochastic optimization and mini-batch SGD</p><ul><li><p>Definition: there exists <span class="math inline">\(\sigma&gt;0\)</span> such that <span class="math inline">\(\mathbb{E}\|\tilde{g}(x)-\nabla f(x)\|_*^2\le \sigma^2\)</span> for all <span class="math inline">\(x\in\mathcal{X}\)</span>.</p></li><li><p><font color='orange'>smoothness does not bring any acceleration for a general stochastic oracle , while in exact orale case it does.</font></p></li><li><p><font color='blue'>Stochastic smooth optimization converge in <span class="math inline">\(\frac{1}{\sqrt{t}}\)</span>. Deterministic smooth optimization converge in <span class="math inline">\(\frac{1}{t}\)</span></font></p></li><li><p><font color='blue'>Mini-batch SGD converges between <span class="math inline">\(\frac{1}{\sqrt{t}}\)</span> and  <span class="math inline">\(\frac{1}{t}\)</span>.</font></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202154653.png" alt="image-20210202154653243" style="zoom:67%;" /></p></li><li></li></ul></li></ul><h4 id="relations">Relations</h4><h5 id="generalization">Generalization</h5><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124232634.png" alt="image-20210124232634072" style="zoom:50%;" /></p><h5 id="the-momentum-sgd-and-adaptive-optimizers">The momentum SGD and Adaptive optimizers</h5><p><a href="https://arxiv.org/pdf/1706.03471.pdf">YellowFin and the Art of Momentum Tuning</a></p><ul><li>Summary: hand-tuning a single learning rate and momentum makes it competitive with Adam. The proposed YellowFin (an automatic fine tuner for momentum and learning rate in SGD), can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing.</li><li>Adaptive optimizers: like Adam, AdaGrad, RmsProp</li><li>For details of paper check <a href="">here</a></li></ul><h3 id="dimension-free-convex-optimization">Dimension-free convex optimization</h3><h4 id="projected-subgradient-descent-for-lipschitz-functions">Projected subgradient descent for Lipschitz functions</h4><ul><li><p><strong>Theorem</strong></p><p>Assume that <span class="math inline">\(\mathcal{X}\)</span> is contained in an Euclidean ball centered at <span class="math inline">\(x_1\in \mathcal{X}\)</span> and of radius <span class="math inline">\(R\)</span>. Assume that <span class="math inline">\(f\)</span> is such that for any <span class="math inline">\(x\in \mathcal{X}\)</span> and of any <span class="math inline">\(g\in\part f(x)\)</span> (assume <span class="math inline">\(\part f(x)\ne \emptyset\)</span>) one has <span class="math inline">\(\|g\|\le L\)</span>. (This implies that <span class="math inline">\(f\)</span> is L-Lipschitz on <span class="math inline">\(\mathcal{X}\)</span>, that is <span class="math inline">\(\|f(x)-f(y)\|\le L\|x-y\|\)</span>)</p><p><font color='blue'><strong>The projected subgradient descent method with <span class="math inline">\(\eta=\frac{R}{L\sqrt{t}}\)</span> satisfies</strong> <span class="math inline">\(f(\frac{1}{t}\sum\limits_{s=1}^{t}x_s)-f(x^*)\le\frac{RL}{\sqrt{t}}\)</span></font></p><ul><li><p>Proof</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117225400.png" alt="image-20210117225359857" style="zoom:40%;" /></p></li><li><p><font color='blue'><strong>The rate is unimprovable from a black-box perspective.</strong></font></p></li></ul></li></ul><h4 id="gradient-descent-for-smooth-functions">Gradient descent for smooth functions</h4><h5 id="theorems-under-unconstrained-cases">Theorems under unconstrained cases</h5><p>In this section all <span class="math inline">\(f\)</span> is a convex and <span class="math inline">\(\beta\)</span>-smooth function on <span class="math inline">\(\mathbb{R}^n\)</span>.</p><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let <span class="math inline">\(f\)</span> be convex and <span class="math inline">\(\beta\)</span>-smooth function on <span class="math inline">\(\mathbb{R}^n\)</span>. Then gradient descent with <span class="math inline">\(\eta=\frac{1}{\beta}\)</span> satisfies <span class="math inline">\(f(x_t)-f(x^*)\le\frac{2\beta\|x_1-x^*\|^2}{t-1}\)</span> .</font></p><p>For the proof check <span class="math inline">\(3.2\)</span> in <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a>.</p></li><li><p>Gradient descent attains a much faster rate in <span class="math inline">\(\beta\)</span>-smooth situation than in the non-smooth case of the previous section.</p></li><li><p>The Definition of smooth convex functions</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117233245.png" alt="image-20210117233244773" style="zoom:50%;" /></p></li></ul><h5 id="the-constrained-cases">The constrained cases</h5><p>This time <em>consider the projected gradient descent algorithm <span class="math inline">\(x_{t+1}=\prod_{\mathcal{X}}(x_t-\eta\nabla f(x_t))\)</span></em></p><ul><li><strong>Lemma</strong></li></ul><p><font color='orange'>Let <span class="math inline">\(x,y\in \mathcal{X},x^+=\prod_{\mathcal{X}}(x-\frac{1}{\beta}\nabla f(x))\)</span> and <span class="math inline">\(g_{\mathcal{X}}(x)=\beta (x-x^+)\)</span> , then the following holds true: <span class="math inline">\(f(x^+)-f(y)\le g_{\mathcal{X}}(x)^T(x-y)-\frac{1}{2\beta}\|g_{\mathcal{X}}(x)\|^2\)</span>.</font></p><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let <span class="math inline">\(f\)</span> be convex and <span class="math inline">\(\beta\)</span>-smooth function on <span class="math inline">\(\mathcal{X}\)</span>. Then projected gradient descent with <span class="math inline">\(\eta=\frac{1}{\beta}\)</span> satisfies <span class="math inline">\(f(x_t)-f(x^*)\le\frac{3\beta\|x_1-x^*\|^2+f(x_1)-f(x^*)}{t}\)</span> .</font></p><h4 id="strong-convexity">Strong convexity</h4><h5 id="strongly-convex-and-lipschitz-functions"><strong>Strongly convex and Lipschitz functions</strong></h5><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210118152328.png" alt="image-20210118152328268" style="zoom:50%;" /></p><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(L\)</span>-Lipschitz and <span class="math inline">\(\alpha\)</span>-strongly convex  on <span class="math inline">\(\mathcal{X}\)</span>. Then projected gradient descent with <span class="math inline">\(\eta_s=\frac{2}{\alpha(s+1)}\)</span> satisfies <span class="math inline">\(f(\sum\limits_{s=1}^{t}\frac{2s}{t(t+1)}x_s)-f(x^*)\le\frac{2L^2}{\alpha(t+1)}\)</span> .</font></p><p><em>The combination of <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(L\)</span>-Lipschitz means that function has to be constrained in a bounded domain.</em></p><h5 id="strongly-convex-and-smooth-functions"><strong>Strongly convex and smooth functions</strong></h5><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex on <span class="math inline">\(\mathcal{X}\)</span>, then projected gradient descent with <span class="math inline">\(\eta=\frac{1}{\beta}\)</span> satisfies for <span class="math inline">\(t\ge0\)</span>, <span class="math inline">\(\|x_{t+1}-x^*\|^2\le\exp(-\frac{t}{\kappa})\|x_a -x^*\|^2\)</span> .</font></p><p>The intuition of changing <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>: If increasing <span class="math inline">\(\beta\)</span>, the upper bound will be decreased, and if increasing <span class="math inline">\(\alpha\)</span>, the lower bound will be increased. <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121112619.png" alt="image-20210121112605020" style="zoom:33%;" /></p></li><li><p><strong>Lemma</strong></p><p><font color='orange'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex on <span class="math inline">\(\mathbb{R}^n\)</span>, then for all <span class="math inline">\(x,y\in \mathbb{R}^n\)</span>, one has <span class="math inline">\((\nabla f(x)-\nabla f(y))^T(x-y)\ge\frac{\alpha\beta}{\alpha+\beta}\|x-y\|^2+\frac{1}{\beta+\alpha}\|\nabla f(x)-\nabla f(y)\|^2\)</span> .</font></p></li><li><p><strong>Theorem</strong></p><p><font color='blue'> Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex  on <span class="math inline">\(\mathbb{R}^n\)</span>, <span class="math inline">\(\kappa=\frac{\beta}{\alpha}\)</span> as the condition number. Then gradient descent with <span class="math inline">\(\eta=\frac{2}{\beta+\alpha}\)</span> satisfies <span class="math inline">\(f(x_{t+1})-f(x^*)\le\frac{\beta}{2}\exp(-\frac{4t}{\kappa+1}\|x_1-x^*\|^2)\)</span>   </font></p></li></ul><h4 id="lower-bound----black-box">Lower bound -- black box</h4><ul><li><p>A black-box procedure is a mapping from "history" to the next query point, that is it maps (<span class="math inline">\(x_a ,g_1,\cdots,x_t,g_t\)</span>) (with <span class="math inline">\(g_s\in\part f(x_s)\)</span>) to <span class="math inline">\(x_{t+1}\)</span>. To simplify, make the following assumption on the black-box procedure: <span class="math inline">\(x_1=0\)</span> and for any <span class="math inline">\(t\ge0，x_{t+1}\)</span> is in the linear span of <span class="math inline">\(g_1,\cdots,g_t\)</span>, that is <span class="math display">\[x_{t+1}\in\mathrm{Span}(g_t ,\cdots,g_t)\tag{3.15}\label{eq315}\]</span></p></li><li><p><strong>Theorem</strong></p><ul><li><font color='blue'> Let <span class="math inline">\(t\le n,L,R&gt;0\)</span>. There exists a convex and <span class="math inline">\(L\)</span>-Lipschitz function <span class="math inline">\(f\)</span> such that for any black-box procedure satisfying  <span class="math inline">\(\eqref{eq315}\)</span>, <span class="math inline">\(\min\limits_{1\le s\le t}f(x_s)-\min\limits_{x\in B_2(R)}f(x_s)\ge\frac{RL}{2(1+\sqrt{t})}\)</span>, where <span class="math inline">\(B_2(R)=\{x\in\mathbb{R}^n:\|x\|\le R\}\)</span> . There also exists an <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(L\)</span>-Lipschitz function <span class="math inline">\(f\)</span> such that for any black-box procedure satisfying <span class="math inline">\(\eqref{eq315}\)</span>, <span class="math inline">\(\min\limits_{1\le s\le t}f(x_s)-\min\limits_{x\in B_2(\frac{L}{2\alpha})}f(x_s)\ge\frac{L^2}{8\alpha t}\)</span></font></li><li><font color='blue'> Let <span class="math inline">\(t\le (n-1)/2,\beta&gt;0\)</span>. There exists a <span class="math inline">\(\beta\)</span>-smooth convex function <span class="math inline">\(f\)</span> such that for any black-box procedure satisfying <span class="math inline">\(\eqref{eq315}\)</span>, <span class="math inline">\(\min\limits_{1\le s\le t}f(x_s)-f(x^*)\ge\frac{3\beta}{32}\frac{\|x_1-x^*\|^2}{(t+1)^2}\)</span>.</font></li><li><font color='blue'> Let <span class="math inline">\(\kappa\ge 1\)</span>. There exists a <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex function <span class="math inline">\(f:\ell_2\rightarrow \mathbb{R}\)</span> with <span class="math inline">\(\kappa=\frac{\beta}{\alpha}\)</span> such that for any <span class="math inline">\(t\ge1\)</span> and black-box procedure satisfying <span class="math inline">\(\eqref{eq315}\)</span> one has <span class="math inline">\(f(x_t)-f(x^*)\ge\frac{\alpha}{2}(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})^{2(t-1)}\|x_1-x^*\|^2\)</span>. Note that for large values of the condition number <span class="math inline">\(\kappa\)</span> one has <span class="math inline">\((\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})^{2(t-1)}\approx\exp(-\frac{4(t-1)}{\sqrt{\kappa}})\)</span></font></li></ul></li></ul><h2 id="references">References</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/103359560">【凸优化笔记5】-次梯度方法（Subgradient method）</a></li><li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></li><li><a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></li><li>'Understanding Analysis' by Stephen Abbott. It's a nice and light intro to analysis</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;泛函&quot;&gt;泛函&lt;/h2&gt;
&lt;center&gt;
&lt;img src=&quot;/assets/img/Rules/lines.png&quot; width=500&quot;&gt;
&lt;/center&gt;
&lt;h3 id=&quot;不动点定理&quot;&gt;不动点定理&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33885648&quot;&gt;链接&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;紧性的利用&quot;&gt;紧性的利用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;证明存在性&lt;/li&gt;
&lt;li&gt;在无限维空间中“模仿”有限维的欧式空间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;紧集是为了模仿描述欧式空间中的有界闭集合么？ 紧=相对紧+闭&lt;/p&gt;
&lt;h3 id=&quot;流形manifolds&quot;&gt;流形(Manifolds)&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;支撑集support&quot;&gt;支撑集(Support)&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>About</title>
    <link href="http://yoursite.com/posts/uncategorized/2019-06-13-about.html"/>
    <id>http://yoursite.com/posts/uncategorized/2019-06-13-about.html</id>
    <published>2019-06-13T17:02:18.000Z</published>
    <updated>2021-04-28T23:27:52.930Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to Mia Feng's Blog!</p><p>I'm doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in Montreal, Canada. I'm interested in data analysis, GCNs, transfer learning, and anomaly detection.</p><p>I got the bachelor degree from Wuhan University, China, and then I did my master at the National University of Defense Technology, China. During my studies, I've processed spatial-temporal data, and financial data. I also worked as an internship of fintech in Meituan-Dianping. The main areas I have learned including machine learning, geographical information system, software engineering, data assimilation, and finance. Currently I am learning something about computational neuroscience. I want to learn more about transfer learning and visualization and explanation of neural networks.</p><p>Have a look at my <a href="https://github.com/skaudrey/cv/blob/master/cv.pdf">resume</a> for more information.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to Mia Feng&#39;s Blog!&lt;/p&gt;
&lt;p&gt;I&#39;m doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in M
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>rl-intro</title>
    <link href="http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html</id>
    <published>2019-05-26T17:02:18.000Z</published>
    <updated>2021-01-12T20:56:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver. <a id="more"></a> ## Why RL?</p><h3 id="the-difference-between-supervised-learning-and-rl">The difference between supervised learning and RL</h3><ul><li><p>task directed from fixed data sets vs goal directed learning from interaction</p></li><li><p>characteristics:</p><ul><li>RL<ul><li>trial-and-error search</li><li>delayed reward</li></ul></li><li>ML<ul><li>generalization methods: regularization, data augmentation</li><li>real-time loss</li></ul></li></ul></li><li><p>methods:</p><ul><li>RL<ul><li>exploit: get reward</li><li>exploration: make better action selection in the future.</li></ul></li><li>ML<ul><li>discriminative and generative: parameterized and semi-parameterized</li><li>supervised and unsupervised: depends on whether have labeled data</li></ul></li></ul></li></ul><center><img src="/assets/img/RLIntro/RLML.png" width=400"></center><h3 id="the-capacity-of-rl">The capacity of RL</h3><ul><li>sequential decision maker facing unknown or known environment</li><li>works for non i.i.d. data</li></ul><center><img src="/assets/img/RLIntro/RLCapacity.png" width=300"></center><h2 id="whats-rl">What's RL?</h2><h3 id="the-agent-environment-interaction">The agent-environment interaction</h3><center><img src="/assets/img/RLIntro/RLEA.png" width=300"></center><ul><li>At each step t the agent:<ul><li>Executes action at</li><li>Transform to state St</li><li>Receives scalar reward rt</li></ul></li><li>The environment:<ul><li>Receives action at</li><li>Transform to state St</li><li>Emits scalar reward rt+1</li></ul></li><li>t increments at env. step</li></ul><h3 id="the-elements-of-rl">The elements of RL</h3><ul><li><p>Policy: agent's behaviour function, mostly is a PDF mapping state to action</p><ul><li><p>Deterministic policy</p><p><span class="math display">\[      a = \pi\left(S\right)  \]</span></p></li><li><p>Stochastic policy</p><p><span class="math display">\[      \pi\left(a|S\right)=\mathbb{P}\left(A_t=a|S_t=s\right)  \]</span></p></li></ul></li><li><p>Value function: how good is each state and/or action, the scalar value is also named reward.</p><p><span class="math display">\[      v_{\pi}\left(s\right)=\mathbb{E}\left(R_{t+1}+\gamma R_{t+1}+{\gamma}^2 R_{t+2}+\cdots|S_t=s\right)  \]</span></p><p>Mostly, to make algorithm converge, a final state will be rewarded 0, and other non-final states are rewarded as a minus value.</p></li><li><p>Model: agent's representation of the environment, they can be modeled by TKinter, gym etc.</p><ul><li><p>e.g.: models in assimilation, maze</p><p><span class="math display">\[      \mathcal{P}_{ss&#39;}^{a}=\mathbb{P}\left(S_{t+1}=s&#39;|S_t=s,A_t=a\right)\\      \mathcal{R}_{s}^{a}=\mathbb{E}\left(R_{t+1}=s&#39;|S_t=s,A_t=a\right)  \]</span></p></li><li><p>unknown environment can be stimulated by sampling ### Classification of RL #### What you want</p></li><li><p>Value Based: No Policy (Implicit), Value Function</p></li><li><p>Policy Based: Policy, No Value Function</p></li><li><p>Actor Critic: Policy, Value Function #### What you knew</p></li><li><p>Model-free: Policy and/or Value Function, No Model</p></li><li><p>Model-based: Policy and/or Value Function, Model</p></li></ul></li></ul><h2 id="how-to-rl">How to RL?</h2><h3 id="markov-process----to-simplify">Markov process -- to simplify</h3><h4 id="markov-process">Markov Process</h4><p><span class="math display">\[    \mathbb{P}\left(S_{t+1}\right)=\mathbb{P}\left(S_{t+1}|S_1,\cdots,S_t\right)\\    \mathcal{P}_{ss&#39;}=\mathbb{P}\left(S_{t+1}=s&#39;|S_t=s\right)\]</span></p><h4 id="markov-rewarded-process">Markov Rewarded Process</h4><p><span class="math display">\[    \langle S,\mathcal{P},\mathcal{R},\gamma\rangle\]</span> solve the reward from state at time t to the final state, which can be also solved by adding immediate reward and discounted value of successor state.</p><p><span class="math display">\[    v\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)=\mathbb{E}\left(R_{t+1}+\gamma v\left(S_{t+1}|S_t=s\right)\right)\]</span></p><h4 id="markov-decision-process">Markov Decision Process</h4><p><span class="math display">\[    \langle S,\mathcal{A},\mathcal{P},\mathcal{R},\gamma\rangle\]</span></p><p>Sequential decision making. * state-value function <span class="math display">\[    v_{\pi}\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)\]</span></p><ul><li>action-value function <span class="math display">\[  q_{\pi}\left(s,a\right)=\mathbb{E}_{\pi}\left(G_t|S_t=s,A_t=a\right)\]</span></li></ul><h3 id="valuepolicy">Value？Policy？</h3><h4 id="policy-iteration">Policy Iteration</h4><center><img src="/assets/img/RLIntro/policyitr.png" width=300"></center><h4 id="value-iteration">Value Iteration</h4><center><img src="/assets/img/RLIntro/valueitr.png" width=300"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>gnn</title>
    <link href="http://yoursite.com/posts/notes/2019-05-24-gnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-24-gnn.html</id>
    <published>2019-05-24T22:17:30.000Z</published>
    <updated>2019-09-23T09:27:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list the generation and improvement of graph neural networks. # Why Use * Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,</p><h1 id="basic-lines">Basic lines</h1><p>Contents in this block mainly comes from paper <a href="https://arxiv.org/pdf/1812.08434.pdf">Graph Neural Networks: A Review of Methods and Applications</a> and</p><a id="more"></a><h2 id="history">History</h2><h3 id="the-proposal-of-gnns">The proposal of GNNs</h3><p>learn a target node’s representation by propagating neighbor information via recurrent neural architectures in an iterative manner until a stable fixed point is reached Computation expensive * <a href="https://www.researchgate.net/publication/4202380_A_new_model_for_earning_in_raph_domains">A new model for learning in graph domains</a> * Big Question: processing the graph without losing topological information * reason<br />Traditional preprocessing methods for graphs dropped topological information, and thus leads to poor performance and generalization. * background RNN can only handle graph-level problems; Traditional methods dropped topological information.</p><ul><li>the approximation capability of GNN, <a href="https://www.researchgate.net/publication/23763868_Computational_Capabilities_of_Graph_Neural_Networks">Computational Capabilities of Graph Neural Networks</a> under mild generic conditions, most of the practically useful functions on graphs can be approximated in probability by GNNs up to any prescribed degree of accuracy.</li><li><a href="https://ieeexplore.ieee.org/document/4773279">Neural network for graphs: A contextual constructive approach</a></li><li><a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a> ### Go to GNNs #### Spectral-based Graph difficult to parallel or scale to large graphs,cause they need to load the whole graph into the memory. relies on eigen-decomposition of the Laplacian matrix.</li><li>Spectral GNN, <a href="https://arxiv.org/pdf/1312.6203v3.pdf">Spectral networks and locally connected networks on graphs</a></li><li>ChebNet, <a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a>，codes <a href="https://github.com/mdeff/cnn%20graph">here</a>.</li><li>1st ChebNet, <a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a>, code <a href="https://github.com/tkipf/gcn">here</a>. localized in space, but the computation requirement grow exponentially, to reduce it, sampling methods are proposed. See <a href="https://arxiv.org/pdf/1801.10247.pdf">FASTGCN</a>, <a href="https://arxiv.org/pdf/1710.10568.pdf">reduce variance</a> and <a href="https://arxiv.org/pdf/1809.05343v1.pdf">adaptive sampling</a> for details.</li><li>AGCN, calculate a pairwise distance of nodes to construct a residual graph, see <a href="https://arxiv.org/pdf/1801.03226.pdf">Adaptive Graph Convolutional Neural Networks</a> for details.</li></ul><h4 id="spatial-based-graph-convolution">Spatial-based Graph Convolution</h4><p>has gained more attention * <a href="https://arxiv.org/pdf/1706.02216.pdf">Inductive representation learning on large graphs</a> * <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model cnns</a> * <a href="https://arxiv.org/pdf/1605.05273.pdf">Learning convolutional neural networks for graphs</a> * <a href="https://arxiv.org/abs/1808.03965">Large-scale learnable graph convolutional networks</a> [1],[4] used sampling strategy the common way is to stack multiple graph convolution layer together. ##### Recurrent-based Spatial GCNs update a node's representation recursively until a stable fixed point is reached * GNNs, <a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a> * GGNNs, used GRU, <a href="https://www.aclweb.org/anthology/D14-1179">Learning phrase representations using rnn encoder-decoder for statistical machine translation</a>, codes <a href="https://github.com/yujiali/ggnn">here</a>. * Stochastic Steady-state Embedding (SSE), updates the node latent representations stochastically in an asynchronous fashion, <a href="http://proceedings.mlr.press/v80/dai18a/dai18a.pdf">Learning steady-states of iterative algorithms over graphs</a>, codes <a href="https://github.com/Hanjun-Dai/steady%20state%20embedding">here</a>.</p><h5 id="composition-based-spatial-gcns">Composition-based Spatial GCNs</h5><ul><li>Message Passing Neural Networks (MPNNs), <a href="https://arxiv.org/pdf/1704.01212.pdf">Neural Message Passing for Quantum Chemistry</a></li><li>GraphSage, <a href="https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf">Inductive representation learning on large graphs</a>, codes <a href="https://github.com/williamleif/GraphSAGE">here</a>. ###### Miscellaneous Variants of Spatial GCNs</li><li>Diffusion Convolution Neural Networks (DCNN), the hidden node representation is get by independently convolving inputs with power series or transition probability matrix, <a href="https://arxiv.org/pdf/1511.02136.pdf">Diffusion-convolutional neural networks</a></li><li>Build GCN into a standard grid to do CNN, <a href="http://proceedings.mlr.press/v48/niepert16.pdf">Learning convolutional neural networks for graphs</a>, but it ignored the node information.</li><li>Large-scale Graph Convolution Networks (LGCN), <a href="https://arxiv.org/pdf/1808.03965.pdf">Large-scale learnable graph convolutional networks</a>, still using standard grid, but it also collects nodes' information and draw subgraph for mini-batch training. Codes <a href="https://github.com/divelab/lgcn/">here</a>.</li><li>Mixture Model Network (MoNet), <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model CNNs</a>, introduce pseudo-coordinates and weight functions to let the weight of a node’s neighbor be determined by the relative position (pseudo-coordinates) between the node and its neighbor.</li><li><a href="https://arxiv.org/abs/1802.00910">Geniepath: Graph neural networks with adaptive receptive paths</a>, everages gating mechanisms to control the depth and breadth of a node's neighborhood.</li><li><a href="https://persagen.com/files/misc/zhuang2018dual.pdf">Dual graph convolutional networks for graph-based semi-supervised classification</a>, one for global representation the other for local representation.</li><li><a href="https://arxiv.org/pdf/1811.10435.pdf">On filter size in graph convolutional networks</a>, introduce a hyperparameter to influence the receptive field size of a node.</li></ul><h4 id="pooling-module">Pooling module</h4><ul><li><a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a></li><li>pooling by rearranging vertices into meaningful order, <a href="https://arxiv.org/pdf/1506.05163.pdf">Deep convolutional networks on graph-structured data</a></li><li><a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf">An end-to-end deep learning architecture for graph classification</a></li><li>DIFFPOOL pools nodes hierarchically by learning a cluster assignment matrix in each layer to get a cluster embedding, which can be combined with any standard GCN module, <a href="https://arxiv.org/pdf/1806.08804.pdf">Hierarchical graph representation learning with differentiable pooling</a></li></ul><h3 id="graph-attention-networks">Graph attention networks</h3><p>For sequence-based tasks, in total, assigning attention weights to different neighbors when aggregating feature information, ensembling multiple models according to attention weights, and using attention weights to guide random walks. * <a href="https://arxiv.org/pdf/1710.10903.pdf">Graph attention networks</a>, (GAT), multi-head weights. Codes <a href="https://github.com/PetarV-/GAT">here</a>. * <a href="https://arxiv.org/pdf/1803.07294.pdf">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</a>， also use multi-head, but it use a self attention mechanism to compute a different head for each head. * <a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">Graph classification using structural attention</a>, Graph Attention Model (GAM), adaptively visiting a sequence of important nodes. * <a href="https://arxiv.org/pdf/1710.09599.pdf">Watch your step: Learning node embeddings via graph attention</a>, factorize the co-occurrence matrix with differentiable attention weights. ### Graph Autoencoders had to handle the problem caused by the sparsity of adjacency matrix. * <a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf">Deep neural networks for learning graph representations</a>, uses the stacked denoising auto-encoders to reconstruct PPMI matrix. Codes <a href="https://github.com/ShelsonCao/DNGR">here</a> * <a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">Structural deep network embedding</a>, preserve nodes first-order proximity (drive representations of adjacent nodes close to each other) and second-order proximity (a node's neighbourhood information) jointly. Codes <a href="https://github.com/suanrong/SDNE">here</a> * <a href="https://arxiv.org/pdf/1611.07308.pdf">Variational graph auto-encoders</a>, Graph Auto-Encoder (GAE), combined with GCN firstly. Codes <a href="https://github.com/limaosen0/Variational-Graph-Auto-Encoders">here</a> * <a href="https://shiruipan.github.io/pdf/CIKM-17-Wang.pdf">Mgae: Marginalized graph autoencoder for graph clustering</a>, reconstruct node's hidden state. * <a href="https://www.ijcai.org/proceedings/2018/0362.pdf">Adversarially regularized graph autoencoder for graph embedding</a>, using GANs to regularize the graph auto-encoders, recover adjacency matrix. Codes <a href="https://github.com/Ruiqi-Hu/ARGA">here</a> * <a href="https://www.kdd.org/kdd2018/accepted-papers/view/learning-deep-network-representations-with-adversarially-regularized-autoen">Learning deep network representations with adversarially regularized autoencoders</a>, recover node sequences rather than adjacency matrix. * <a href="http://pengcui.thumedialab.com/papers/NE-RegularEquivalence.pdf">Deep recursive network embedding with regular equivalence</a>, codes <a href="https://github.com/tadpole/DRNE">here</a> ### Graph Generative Networks Not scalable to large graphs. * <a href="https://arxiv.org/pdf/1802.08773.pdf">Graphrnn: A deep generative model for graphs</a>, graph-level RNN + node-level RNN, use breadth-first-search (BFS) to sequence the nodes and Bernoulli assumption for edge generation. Codes <a href="https://github.com/snap-stanford/GraphRNN">here</a>. * <a href="https://arxiv.org/pdf/1803.03324.pdf">Learning deep generative models of graphs</a>, utilize spatial-based GCNs to obtain a hidden representation of an existing graph. * <a href="https://arxiv.org/pdf/1805.11973.pdf">Molgan: An implicit generative model for small molecular graphs</a>, RL+GAN+GCN * <a href="https://arxiv.org/pdf/1803.00816.pdf">Net-gan: Generating graphs via random walks</a>, combines LSTM with Wasserstein GAN to generate graphs from a random-walk-based approach. As for random walk, see <a href="http://leogrady.net/wp-content/uploads/2017/01/grady2004multilabel.pdf">here</a>. * <a href="https://arxiv.org/pdf/1809.02630.pdf">Constrained generation of semantically valid graphs via regularizing variational autoencoders</a> ### Graph Spatial-Temporal Networks</p><ul><li><a href="https://arxiv.org/pdf/1612.07659.pdf">Structured sequence modeling with graph convolutional recurrent networks</a></li><li><a href="https://arxiv.org/pdf/1707.01926.pdf">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</a>, can work forwardly or backwardly. Codes <a href="https://github.com/liyaguang/DCRNN">here</a>.</li><li><a href="https://arxiv.org/pdf/1709.04875.pdf">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</a>, codes <a href="https://github.com/VeritasYin/STGCN_IJCAI-18">here</a>.</li><li><a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial temporal graph convolutional networks for skeleton-based action recognition</a>, extend the temporal flow as graph edges, and then assign each a label to each edge. Codes <a href="https://github.com/yysijie/st-gcn">here</a>.</li><li><a href="https://arxiv.org/pdf/1511.05298.pdf">Structural-rnn: Deep learning on spatio-temporal graphs</a>, aims at predicting nodes' labels at each time, has nodeRNN and edgeRNN, and split nodes and edges into semantic groups. Codes <a href="https://github.com/asheshjain399/RNNexp">here</a>.</li></ul><h2 id="main-methodologies----graph-embedding">Main Methodologies -- Graph Embedding</h2><h3 id="matrix-factorization">Matrix Factorization</h3><ul><li><a href="https://www.ijcai.org/proceedings/2018/0493.pdf">Discrete network embedding</a></li><li><a href="https://shiruipan.github.io/pdf/ICDM-18-Yang.pdf">Binarized attributed network embedding</a> ### Random Walks</li><li><a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf">Deepwalk: Online learning of social representations</a></li></ul><h2 id="problems">Problems</h2><ul><li>Does going deeper always work in GNNs?</li><li>How to select representative receptive field for a node?</li><li>How to work on large graphs?</li><li>How to handle dynamic and heterogeneous graph structures?</li></ul><h1 id="papers">Papers</h1><h2 id="introduction">Introduction</h2><ul><li>M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, "Geometric deep learning: going beyond euclidean data,"IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 18–42, 2017 ## others ### <a href="https://arxiv.org/pdf/1905.02296v1.pdf">Are Graph Neural Networks Miscalibrated?</a></li></ul><h3 id="estimating-node-importance-in-knowledge-graphs-using-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.08865.pdf">Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks</a></h3><p>GENI, a GNN-based method designed to deal with distinctive challenges involved with predicting node importance in KGs.</p><h3 id="understanding-attention-in-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.02850.pdf">Understanding attention in graph neural networks</a></h3><p>We aim to better understand attention over nodes in graph neural networks and identify factors influencing its effectiveness. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 40% in some of our classification tasks</p><h3 id="are-graph-neural-networks-miscalibrated"><a href="https://arxiv.org/pdf/1905.02296.pdf">Are Graph Neural Networks Miscalibrated?</a></h3><p>Graph Neural Networks (GNNs) have proven to be successful in many classification tasks, outperforming previous state-of-the-art methods in terms of accuracy</p><h3 id="graph-convolutional-networks-with-eigenpooling"><a href="https://arxiv.org/pdf/1904.13107.pdf">Graph Convolutional Networks with EigenPooling</a></h3><p>To apply graph neural networks for the graph classification task, approaches to generate the  from node representations are demanded. Experimental results of the graph classification task on <span class="math inline">\(6\)</span> commonly used benchmarks demonstrate the effectiveness of the proposed framework.</p><h3 id="pan-path-integral-based-convolution-for-deep-graph-neural-networks"><a href="https://arxiv.org/pdf/1904.10996.pdf">PAN: Path Integral Based Convolution for Deep Graph Neural Networks</a></h3><p>Experimental results show that the path integral based graph neural networks have great learnability and fast convergence rate, and achieve state-of-the-art performance on benchmark tasks.</p><h3 id="attacking-graph-based-classification-via-manipulating-the-graph-structure"><a href="https://arxiv.org/pdf/1903.00553.pdf">Attacking Graph-based Classification via Manipulating the Graph Structure</a></h3><p>We evaluate our attacks and compare them with a recent attack designed for graph neural networks. Results show that our attacks 1) can effectively evade graph-based classification methods; 2) do not require access to the true parameters, true training dataset, and/or complete graph; and 3) outperform the existing attack for evading collective classification methods and some graph neural network methods</p><h3 id="deep-learning-in-bioinformatics-introduction-application-and-perspective-in-big-data-era"><a href="https://arxiv.org/abs/1903.00342">Deep learning in bioinformatics: introduction, application, and perspective in big data era</a></h3><p>After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures</p><h3 id="constant-time-graph-neural-networks"><a href="https://arxiv.org/pdf/1901.07868.pdf">Constant Time Graph Neural Networks</a></h3><p>Recent advancements in graph neural networks (GNN) have led to state-of-the-art performance in various applications including chemo-informatics, question answering systems, and recommendation systems, to name a few</p><h3 id="a-comprehensive-survey-on-graph-neural-networks"><a href="https://arxiv.org/pdf/1901.00596.pdf">A Comprehensive Survey on Graph Neural Networks</a></h3><p>We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories</p><h3 id="graph-transformation-policy-network-for-chemical-reaction-prediction"><a href="https://openreview.net/pdf?id=r1f78iAcFm">Graph Transformation Policy Network for Chemical Reaction Prediction</a></h3><p>To this end, we propose Graph Transformation Policy Network (GTPN) -- a novel generic method that combines the strengths of graph neural networks and reinforcement learning to learn the reactions directly from data with minimal chemical knowledge. Evaluation results show that GTPN improves the top-1 accuracy over the current state-of-the-art method by about 3% on the large USPTO dataset</p><h3 id="contextualized-non-local-neural-networks-for-sequence-learning"><a href="https://arxiv.org/pdf/1811.08600.pdf">Contextualized Non-local Neural Networks for Sequence Learning</a></h3><p>Recently, a large number of neural mechanisms and models have been proposed for sequence learning, of which self-attention, as exemplified by the Transformer model, and graph neural networks (GNNs) have attracted much attention. Specifically, we propose contextualized non-local neural networks (CN<span class="math inline">\(^{\textbf{3}}\)</span>), which can both dynamically construct a task-specific structure of a sentence and leverage rich local dependencies within a particular neighborhood.</p><h3 id="automated-theorem-proving-in-intuitionistic-propositional-logic-by-deep-reinforcement-learning"><a href="https://arxiv.org/pdf/1811.00796.pdf">Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning</a></h3><p>Using the large volume of augmented data, we train highly accurate graph neural networks that approximate the value function for the set of the syntactic structures of formulas. Within the specified time limit, our prover solved 84% of the theorems in a benchmark library, while <span class="math inline">\(\texttt{tauto}\)</span> was able to solve only 52%.</p><h3 id="pileup-mitigation-at-the-large-hadron-collider-with-graph-neural-networks"><a href="https://arxiv.org/pdf/1810.07988.pdf">Pileup mitigation at the Large Hadron Collider with Graph Neural Networks</a></h3><p>We present a classifier based on Graph Neural Networks, trained to retain particles coming from high-transverse-momentum collisions, while rejecting those coming from pileup collisions. This model is designed as a refinement of the PUPPI algorithm, employed in many LHC data analyses since 2015</p><h3 id="weisfeiler-and-leman-go-neural-higher-order-graph-neural-networks"><a href="https://arxiv.org/pdf/1810.02244.pdf">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</a></h3><p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. The following work investigates GNNs from a theoretical point of view and relates them to the <span class="math inline">\(1\)</span>-dimensional Weisfeiler-Leman graph isomorphism heuristic (<span class="math inline">\(1\)</span>-WL). We show that GNNs have the same expressiveness as the <span class="math inline">\(1\)</span>-WL in terms of distinguishing non-isomorphic (sub-)graphs</p><h3 id="multitask-learning-on-graph-neural-networks---learning-multiple-graph-centrality-measures-with-a-unified-network"><a href="https://arxiv.org/pdf/1809.07695.pdf">Multitask Learning on Graph Neural Networks - Learning Multiple Graph Centrality Measures with a Unified Network</a></h3><p>Graph neural networks (GNN), consisting of trained neural modules which can be arranged in different topologies at run time, are sound alternatives to tackle relational problems which lend themselves to graph representations. The proposed model achieves <span class="math inline">\(89\%\)</span> accuracy on a test dataset of random instances with up to 128 vertices and is shown to generalise to larger problem sizes</p><h3 id="meta-gnn-on-few-shot-node-classification-in-graph-meta-learning"><a href="https://arxiv.org/pdf/1905.09718.pdf">Meta-GNN: On Few-shot Node Classification in Graph Meta-learning</a></h3><p>However, there are very few works applying meta-learning to non-Euclidean domains, and the recently proposed graph neural networks (GNNs) models do not perform effectively on graph few-shot learning problems. Additionally, Meta-GNN is a general model that can be straightforwardly incorporated into any existing state-of-the-art GNN</p><h3 id="mr-gnn-multi-resolution-and-dual-graph-neural-network-for-predicting-structured-entity-interactions"><a href="https://arxiv.org/pdf/1905.09558.pdf">MR-GNN: Multi-Resolution and Dual Graph Neural Network for Predicting Structured Entity Interactions</a></h3><p>In recent years, graph neural networks have become attractive. Experiments conducted on real-world datasets show that MR-GNN improves the prediction of state-of-the-art methods.</p><h3 id="revisiting-graph-neural-networks-all-we-have-is-low-pass-filters"><a href="https://arxiv.org/pdf/1905.09550.pdf">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</a></h3><p>In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property</p><h3 id="multi-hop-reading-comprehension-across-multiple-documents-by-reasoning-over-heterogeneous-graphs"><a href="https://arxiv.org/pdf/1905.07374.pdf">Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs</a></h3><p>We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based model (single model) achieves state-of-the-art result.</p><h3 id="ipc-a-benchmark-data-set-for-learning-with-graph-structured-data"><a href="https://arxiv.org/pdf/1905.06393.pdf">IPC: A Benchmark Data Set for Learning with Graph-Structured Data</a></h3><p>The data set, named IPC, consists of two self-contained versions, grounded and lifted, both including graphs of large and skewedly distributed sizes, posing substantial challenges for the computation of graph models such as graph kernels and graph neural networks</p><h1 id="datasets">Datasets</h1><ul><li><a href="https://www.aminer.cn/citation">Citation Networks</a>: <a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/cora_raw.zip">Cora</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/citeseer.zip">Citeseer</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/pubmed.zip">Pubmed</a>,<a href="https://www.aminer.cn/citation">DBLP</a></li><li><a href="http://networkrepository.com/soc_BlogCatalog.php">Social Networks</a>: <a href="http://socialcomputing.asu.edu/datasets/BlogCatalog">BlogCatalog</a>,<a href="https://github.com/linanqiu/reddit-dataset">Reddit</a>,<a href="http://www.trustlet.org/downloaded_epinions.html">Epinions</a></li><li>Chemical/Biological Graphs: <a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI1.zip">NCI-1</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI109.zip">NCI-9</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/MUTAG.zip">MUTAG</a>, D&amp;D,<a href="https://github.com/bigdata-ustc/QM9nano4USTC">QM9</a>,<a href="https://tripod.nih.gov/tox21/challenge/data.jsp">Tox21</a>,<a href="http://snap.stanford.edu/graphsage/ppi.zip">PPI</a>.</li><li>Unstructured Graphs: convert <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, <a href="http://www.mattmahoney.net/dc/textdata.html">Wikipedia</a> or News Groups into graphs.</li><li>Others: <a href="https://pan.baidu.com/s/14Yy9isAIZYdU__OYEQGa_g#list/path=%2F">METR-LA</a>, <a href="https://grouplens.org/datasets/movielens/1m/">Movies-Lens1M</a>, NELL.</li></ul><p>Thanks for the links given <a href="https://www.jianshu.com/p/67137451b67f">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list the generation and improvement of graph neural networks. # Why Use * Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,&lt;/p&gt;
&lt;h1 id=&quot;basic-lines&quot;&gt;Basic lines&lt;/h1&gt;
&lt;p&gt;Contents in this block mainly comes from paper &lt;a href=&quot;https://arxiv.org/pdf/1812.08434.pdf&quot;&gt;Graph Neural Networks: A Review of Methods and Applications&lt;/a&gt; and&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>UNet proposed by Olaf Ronneberger etc.</title>
    <link href="http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html</id>
    <published>2019-05-23T21:18:21.000Z</published>
    <updated>2021-01-12T19:33:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>The notes of paper <a href="https://arxiv.org/abs/1505.04597v1">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>.</p><a id="more"></a><h2 id="big-question-classification-in-pixel-level-and-thus-image-segmentation">Big Question: classification in pixel level and thus image segmentation</h2><h3 id="reason">reason</h3><ul><li>single label for a image is not enough to support segmentation. ### background</li><li>why focus on it<ul><li>biomedical images: like cells segmentation. Add: also appropriate for other entity segmentation</li></ul></li><li>how have been done:<ul><li>the development of deeper CNNs</li><li>using CNNs segmenting electron microscopy images</li></ul></li><li>what have been missed:<ul><li>computation efficiency and redundancy: slow cause every patch require a running of network; patch overlapping</li><li>difficult trade-off for localization and the usage of context.</li></ul></li></ul><h2 id="methods">Methods</h2><h3 id="for-what">For what?</h3><p>Cells segmentation.</p><h3 id="framework-of-methods">Framework of Methods</h3><p>Convolution2D + deconvolution (upsampling 2D). The output of one downsampling layer is contracted as part of the input of the corresponding symmetric upsampling layer.</p><h3 id="novelty">Novelty</h3><ul><li>data augmentation randomly elastic deformations: shift, rotation, gray value, random elastic deformations are the most important</li><li>replace pooling by upsampling.</li><li>No fully connection layers.</li><li>weighted the loss of touching objects (cells).</li></ul><h2 id="details">Details</h2><h3 id="weighted-map-to-segment-overlapped-cells">weighted map to segment overlapped cells</h3><p>According to the paper, they pre-compute the weight map for each ground truth segmentation to compensate the different frequency of pixels.</p><h3 id="data-augmentation">data augmentation</h3><p>Smooth deformations using random displacements vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation.</p><h2 id="abstract">Abstract</h2><p>The main idea in abstract are contracted NNs and data augmentation so that the new NNs can get reasonable results by fewer images.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The notes of paper &lt;a href=&quot;https://arxiv.org/abs/1505.04597v1&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>paper--Faster RCNN</title>
    <link href="http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster%20rcnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster rcnn.html</id>
    <published>2019-05-19T15:17:12.000Z</published>
    <updated>2021-01-12T20:48:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Some understanding about the details in Faster RCNN, based on the codes in tensorflow.</p><a id="more"></a><h2 id="big-question">Big Question</h2><h3 id="reason">reason</h3><h3 id="background">background</h3><h2 id="region-proposal-network">Region Proposal Network</h2><h3 id="some-numbers">Some numbers</h3><ul><li>The number of anchor boxes for one anchor target <span class="math inline">\(k = scale \times ratios\)</span>,</li><li>The number of anchor boxes for one feature layer (which has <span class="math inline">\(W \times H\)</span> grids), will get $ W H k $ anchor boxes. Every grid in the feature map (the output of a popular CNN without FC layers) will have <span class="math inline">\(k\)</span> anchor boxes.</li><li>Not like the ROI method, the size of features are fixed, but anchor boxes are rescaled by <span class="math inline">\(k\)</span> regressors.</li></ul><h2 id="experiments">Experiments</h2><h3 id="prove">prove</h3><ul><li>The top-ranked RPN proposals are accurate.</li><li>NMS does not harm the detection mAP and may reduce false alarms.</li></ul><h2 id="construct">Construct</h2><h3 id="add-loss">Add loss</h3><h3 id="problems-using-it-processing-typhoon-data">Problems using it processing typhoon data</h3><ul><li>Does NMS lead to loss of typhoon? Not really, the texture of typhoon is obvious in image.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Some understanding about the details in Faster RCNN, based on the codes in tensorflow.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Positioning Data of FY4 AGRI.</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html</id>
    <published>2019-04-26T22:53:05.000Z</published>
    <updated>2019-04-26T15:31:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.</p><p>There are two ways for positioning, one is querying the lookup table given by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">NSMC</a>, the other is calculating by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">formulas</a>.</p><p>However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days. <a id="more"></a></p><h2 id="querying-the-lookup-table.">Querying the lookup table.</h2><p>There are two errors in the files.</p><ul><li>The first 8 bytes denote latitude, and the next 8 bytes are reserved for longitude.</li><li>The data are stored as little-endian data.<center><img src="/assets/img/FY4-AGRI/lookup.png" width="400"></center></li></ul><h2 id="calculating-by-formulas">Calculating by formulas</h2><p>The formulas are OK, but the constant variable <span class="math inline">\(\lambda_D\)</span> should be measured in rad before being used.</p><center><img src="/assets/img/FY4-AGRI/formulas.png" width="400"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.&lt;/p&gt;
&lt;p&gt;There are two ways for positioning, one is querying the lookup table given by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;NSMC&lt;/a&gt;, the other is calculating by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;formulas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="Sensor" scheme="http://yoursite.com/tags/Sensor/"/>
    
  </entry>
  
  <entry>
    <title>A Tex Template of Cornell Notes</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2019-04-26T15:34:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these. <a id="more"></a></p><h2 id="preliminaries">Preliminaries</h2><p>Before starting, you need install * CTex</p><h2 id="install-missed-packages-of-ctex">Install missed packages of CTex</h2><h3 id="download-the-required-packages">Download the required packages</h3><p>Search <a href="https://www.ctan.org/pkg">here</a>.</p><h3 id="unzip-and-compile-manually-if-needed.">Unzip and compile manually if needed.</h3><p>Unzip the downloaded file and jump to the directory after unzip. If need compile, run <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  pdflatex ***.ins</span><br></pre></td></tr></table></figure> ### Install Copy the compiled file folder to the CTex path: ~/CTex/CTex/tex/latex</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ texhash --admin</span><br></pre></td></tr></table></figure>The notes will be generated like this:<center><img src="/assets/img/CornellNotes.png" width="400"></center><h1 id="acknowledgement">Acknowledgement</h1><p>Thank <a href="https://blog.csdn.net/Myriad_Dreamin/article/details/83384110">this blog</a>.</p><h1 id="resources">Resources</h1><p>Tex file can be found <a href="https://github.com/skaudrey/skaudrey.github.io/tree/master/assets/notes/Cornell">here</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>Understand and debug the codes of GCN proposed by Thomas N. Kipf</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2021-01-27T14:45:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1609.02907.pdf">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>.</p><a id="more"></a><h2 id="big-question-semi-supervised-classification-of-graph-data">Big Question: semi-supervised classification of graph data</h2><ul><li>reason<ul><li>computation effective: semi-supervision</li><li>the complex of graphs, the information of nodes and edges are not structural information.</li></ul></li><li>background<ul><li>the improvement of GCNs: spectral GCNs</li></ul></li></ul><h2 id="key-points">Key points</h2><h3 id="the-approximation-of-spectral-graph-convolution">The approximation of spectral graph convolution</h3><p>The lines in paper had confused me at first before I ran the codes.</p><p>The difference of graph convolution and valina convolution is the input, as the input is a graph rather than data in same dimension, the key point is how to convert data represented by node and graph to a tensor in fixed dimension.</p><p>To solve it, Thomas maps the graph into a spectral space and also, to be computational efficient, approximate the infinite coefficients by second-order Chebyshev polynomial formulas.</p><p>After those approximation, it is input into the whole network with features.</p><h3 id="build-model">Build model</h3><p>Actually, except the complicated preprocess to represent graph G into a sparse tensor, the other step are not that complex, just the similar as what a convolution layer do. <span class="math display">\[Z = f\left(\mathbf{X},A\right)=softmax\left(\hat{A}ReLU\left({\hat{A}XW^{\left(0\right)}}\right)W^{\left(1\right)}\right)\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
  </entry>
  
  <entry>
    <title>Build up personal blog</title>
    <link href="http://yoursite.com/posts/notes/2019-01-28-hexo+killy+github%20pages=blog.html"/>
    <id>http://yoursite.com/posts/notes/2019-01-28-hexo+killy+github pages=blog.html</id>
    <published>2019-01-28T18:33:39.000Z</published>
    <updated>2021-02-07T15:48:06.557Z</updated>
    
    <content type="html"><![CDATA[<p>This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github. <a id="more"></a></p><h2 id="preliminaries">Preliminaries</h2><p>Before starting, you need: * node.js+npm</p><pre><code>Get node.js from [here](https://pan.baidu.com/s/1kU5OCOB#list/path=%2Fpub%2Fnodejs). Check [here](https://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000/00143450141843488beddae2a1044cab5acb5125baf0882000) for more info of node.</code></pre><ul><li><p>hexo Install by npm:</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure></p></li><li><p>git,</p></li><li><p>an account of Github,</p></li></ul><p>and configure the ssh-key on your device.</p><h2 id="build-blog">Build blog</h2><h3 id="initialize-hexo-with-hexo">Initialize hexo with hexo</h3><p>Create a local folder as your root directory, such as "blog", and go to the directory in your terminal and initialize it by hexo. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br><span class="line">$ <span class="built_in">cd</span> blog</span><br></pre></td></tr></table></figure> Then initialize this directory with npm.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="link-hexo-with-github">Link hexo with Github</h3><p>Set deployment tool,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>and initialize the remote repository for your blog on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br><span class="line">$ git add *</span><br><span class="line">$ git commit -m <span class="string">&quot;init commit&quot;</span></span><br></pre></td></tr></table></figure><p>Change the deployment in file "_config.yml" like:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line">deploy: </span><br><span class="line">    <span class="built_in">type</span>: git </span><br><span class="line">    repo: git@github.com:jack/jack.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure><p>Tips: The name of your hosting repository should be "[githubname].github.io", such as "jack.github.io". And mind the blankspaces while rewriting file "_config.yml". ### Generate static files Do it before you push it on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><h1 id="see-your-pages">see your pages</h1><p>Click https://[githubname].github.io/, such as https://jack.github.io/.</p><h2 id="customization">Customization</h2><h3 id="change-theme">change theme</h3><p>I picked theme yilia. Configuration should be done as bellow:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ git <span class="built_in">clone</span> https://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br></pre></td></tr></table></figure><p>Change the default theme defined in "_config.yml" under root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: yilia</span><br></pre></td></tr></table></figure><h3 id="upload-your-avatar">upload your avatar</h3><p>New a folder under the "source" directory, I named it assets. I also new the "img" folder for pictures. Put you avatar picture here. Then reconfigure the _config.yml file beneath theme "yilia"'s folder, which is:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avatar: /assets/img/avatar.jpg</span><br></pre></td></tr></table></figure><h3 id="classify-your-posts-by-categories-rather-than-tags-in-default">classify your posts by categories rather than tags in default</h3><p>Now take your eye away from file "_config.yml" under theme yilia, open the file "_config.yml" under the root directory of your blog. You need to configure category_map, for instance,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  about: about</span><br><span class="line">  notes: notes</span><br><span class="line">  projects: projects</span><br><span class="line">  papers: papers</span><br><span class="line">  talks: talks</span><br><span class="line">  meetings: meetings</span><br></pre></td></tr></table></figure><p>Each pair of it can be different, it is just a mapping, such as:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  parole: talks</span><br><span class="line">  关于我: about</span><br></pre></td></tr></table></figure><h3 id="change-the-naming-rule-of-a-new-post">change the naming rule of a new post</h3><p>The default naming rule of hexo is YYYY/MM/DD/[post name], which leads to a hyper-link without html suffix. I change it as html. It can be accomplished by configure the _config.yml in root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permalink: posts/:category/:year-:month-:day-:title.html</span><br></pre></td></tr></table></figure><h3 id="truncate-the-post-in-home-list-when-it-is-too-long.">Truncate the post in home list when it is too long.</h3><p>You need to add</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure><p>after where you want to truncate in a post. And configure the __config.yml_ which is under themes' folder.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The truncate signal while post is too long.</span></span><br><span class="line">excerpt_link: <span class="string">&quot;more&quot;</span></span><br></pre></td></tr></table></figure><h3 id="support-latex">Support Latex</h3><p>Check <a href="https://www.jianshu.com/p/5623c5e35c93">here</a> for details.</p><p>Tips: if things don't work try restart, regenerate and redeploy.</p><h1 id="tips">Tips</h1><p>You can debug pages locally by</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>, which is convenient before deployment.</p><h1 id="acknowledgement">Acknowledgement</h1><p>Thank <a href="https://www.cnblogs.com/wumz/p/8030244.html">Mauger</a>, and <a href="https://github.com/litten/hexo-theme-yilia">litten</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>Clouds detection of infrared hyperspectral data based on logistic.</title>
    <link href="http://yoursite.com/posts/projects/2019-01-28-lr.html"/>
    <id>http://yoursite.com/posts/projects/2019-01-28-lr.html</id>
    <published>2019-01-28T18:33:39.000Z</published>
    <updated>2021-02-16T18:18:55.095Z</updated>
    
    <content type="html"><![CDATA[<p>This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed with what clouds release, and they exist in more than 90% IFOVs, cloudy IFOVs have to be kicked off in order to get clean data.</p><p>Therefore, a new feature construction method is proposed for infrared hyperspectral data, such as what IASI releases. Concretely, four channels of IASI are picked, namely channel 921, channel 386, channel 306 and channel 241. They are picked because of physical characteristics. And then, cloudy IFOVs are detected by logistic regression.</p><p>The recall, auc and accuracy of this new method carried on IASI data was more than 0.95 when detecting IFOVs of sea, while the result of land's IFOVs was less than it. After adding surface emissivity features, the auc of it increased by aroud 5%, and recall of it grew by 10% approximately.</p><p>Codes are available <a href="https://github.com/skaudrey/cloud">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed 
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="logistic" scheme="http://yoursite.com/tags/logistic/"/>
    
  </entry>
  
  <entry>
    <title>HCR--Compress and Resonstruct Hyperspectral Data.</title>
    <link href="http://yoursite.com/posts/projects/2018-12-28-hcr.html"/>
    <id>http://yoursite.com/posts/projects/2018-12-28-hcr.html</id>
    <published>2018-12-28T18:33:39.000Z</published>
    <updated>2021-02-16T18:19:17.158Z</updated>
    
    <content type="html"><![CDATA[<p>This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression and reconstruction. The numerous infrared hyperspectral data are overloaden for computing resources currently. Taking IASI, an atmosphere detector on satellite Metop launched by European Organization for the Exploitation of Meteorological Satellites (EUMETSAT), as an example, it has 8461 channels, which can detect atmosphere vertically in details. To process these data more efficiently, compressing them and then reconstructing is required.</p><p>Considering their high correlation in spectral and spatial dimension, a new compressing and reconstructing network HCR is proposed. Concretely, the radiation brightness values are gridded so that one value at specific location is recongnized as a color value at this pixel. After normalizing by batch normalization, HCR compresses by convolution and reconstructs by deconvlution.</p><p>Carrying on IASI data, the RMSE of this new method was decreased by 5% at least compared with the result of principle component analysis (PCA) in the same compression ratio. The compression kernels encode tempetature information and reconstruct it. In reconstruction, the kernels' weights for likewise data are similar.</p><p>Codes are available <a href="https://github.com/skaudrey/hyp">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression an
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="cnn" scheme="http://yoursite.com/tags/cnn/"/>
    
      <category term="compress" scheme="http://yoursite.com/tags/compress/"/>
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
  </entry>
  
  <entry>
    <title>What Can Artificial Intelligence Do in Data Assimilation? Dec. 9th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2018-12-09-talk-mlutility.html"/>
    <id>http://yoursite.com/posts/talks/2018-12-09-talk-mlutility.html</id>
    <published>2018-12-09T18:33:39.000Z</published>
    <updated>2021-02-16T18:16:19.620Z</updated>
    
    <content type="html"><![CDATA[<p>This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my lab, and most of them haven't learn much about AI. The talk would like to show them what can AI do these days, and help them figure out what else can AI do in data assimilation.</p><p>Slides are avaliable <a href="/assets/slides/mlDo/mlUtility.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my l
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="Data assimilation" scheme="http://yoursite.com/tags/Data-assimilation/"/>
    
  </entry>
  
  <entry>
    <title>Computing in the 21th Century &amp; Asia Faculty Summit held by Microsoft, Nov., 2018.</title>
    <link href="http://yoursite.com/posts/meetings/2018-11-23-microsoft.html"/>
    <id>http://yoursite.com/posts/meetings/2018-11-23-microsoft.html</id>
    <published>2018-11-23T18:33:39.000Z</published>
    <updated>2021-02-16T18:18:38.701Z</updated>
    
    <content type="html"><![CDATA[<p>This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is <a href="https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda">here</a>.</p><p>Babysitting AI and computational neuroscience impressed me a lot. <a id="more"></a> # computing neuroscience: robots with feelings.</p><p>Prof. Lenore Blue's keynote is about computational neuroscience. They try to let robots feel pain, and to simulate the long and short term encoding happening in our brains.</p><p>What she talked reminds me of something I had read before.</p><p>According to the book <em>Psychology</em> written by Daniel Schacter, our brains do encode information into long and short codes, and the short form is possibly trasformed into a long one. Even though we don't mean to encode or memorize something sometimes, encoding still occurs unconsciously.</p><p>Reviewing is one useful way for recalling these information. Moreover, if you are in the similar environment in which you encoded the codes before, you will have higher possibility to recall it. However, the <strong>encoding error</strong> happend during reviewing is more, and that's why a detective should try to get full information while inquiring evidences from witnesses at the 1st time.</p><p>Also, our brains are more sensible to pictorial information compared with text information. So, if you try to make each thing you want to keep code as a photo, you can boost the capability of memorizing.</p><h1 id="yoshu-bengio-beyond-i.i.d.-and-babysitting-ai">Yoshu Bengio: Beyond i.i.d. and babysitting AI</h1><p>One of the basic assumption that makes generation possible is independent identically distributed assumption. However, influenced by observing equipments, imbalanced samples and others, the distributions of training data and test data are not always the same. Hence, Prof. Bengio's team proposed that all data are sampled from the same system rather than same distribution. As for weather of two different seasons, data desciping them are sampled from the same atmospheric circulation system, but they don't distribute identically. Bengio said they tend to initialize this system with diversed initial conditions, and the result of this distribution will be taken as what the data set follows. It makes sense.</p><p><em>One thing that troubles me is, how can I model the system and figure out the initial conditions? For things with obvious physical rules, it is easy, and even the model's codes are open-acssessed online. What if the one I don't know? How to make this idea works in common situations?</em></p><p>CNN is renowned as its power in representation learning, which encodes a variety of information into vectors. Our brains also work like this. Nontheless, what they learn are supervised, and the utility of binary network, the simplification of network structure all demostrate that the captured information are redundant. The model learned is fragile, too. After adding some noises into an image, even though the image dosen't change visually for us humans, model can not tell what it is as before. It all comes from the uncontrolling unsupervision. Babysitting AI aims at modeling with environmental information and other information, so that leading AI models.</p><h1 id="andrew-c.-yao-the-advent-of-quantum-computing">Andrew C. Yao: The Advent of Quantum Computing</h1><p>The quantum computing will offer exponential speedup for crypto-code breaking, simulation of quantum physical systems, simulation of materials, chemistry, and biology, nonlinear optimization, ML and AI. It will break through the bottleneck of computing.</p><p>Its implementation is like crystallography. In terms of crystallography, you take an X-ray photo for a crystal and then compute its structure. For quantum computing, instead of taking a real photo, you just need to collect a polynomial number of sample points. By wave-particle duality, this single photo can recreate the raw image probabilistically.</p><p>According to Andrew, dimond qubits are in the highest possibility to be used in our laptops.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is &lt;a href=&quot;https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Babysitting AI and computational neuroscience impressed me a lot.
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="applications" scheme="http://yoursite.com/tags/applications/"/>
    
      <category term="tendency" scheme="http://yoursite.com/tags/tendency/"/>
    
  </entry>
  
  <entry>
    <title>The Introduction of Infrared Hyperspectral Data and Kernel PCA, June 5th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2018-06-05-talk-hyp.html"/>
    <id>http://yoursite.com/posts/talks/2018-06-05-talk-hyp.html</id>
    <published>2018-06-05T17:33:39.000Z</published>
    <updated>2021-02-16T18:16:35.981Z</updated>
    
    <content type="html"><![CDATA[<p>Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:</p><ul><li>high spectral correlation,</li><li>high spatial correlation,</li><li>and sparsity,</li></ul><p>and they casue a trouble during processing. <a id="more"></a> This talk explained why they are highly correlated but also sparse. Kernel PCA for compressing was also tested.</p><p>Check <a href="/assets/slides/hyp/hypCompression.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;high spectral correlation,&lt;/li&gt;
&lt;li&gt;high spatial correlation,&lt;/li&gt;
&lt;li&gt;and sparsity,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and they casue a trouble during processing.
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="compression" scheme="http://yoursite.com/tags/compression/"/>
    
      <category term="reconstruction" scheme="http://yoursite.com/tags/reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Weather processes interpolation based on GPR</title>
    <link href="http://yoursite.com/posts/projects/2018-05-17-gpr.html"/>
    <id>http://yoursite.com/posts/projects/2018-05-17-gpr.html</id>
    <published>2018-05-17T17:33:39.000Z</published>
    <updated>2021-02-16T18:19:54.655Z</updated>
    
    <content type="html"><![CDATA[<p>This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale dependencies of weather processes. Weather processes with and without cyclones are discussed, and two interpolation methods are proposed. Check <a href="http://www.mdpi.com/2073-4433/9/5/194/pdf">paper</a> for more information. Codes are available <a href="https://github.com/skaudrey/gpml">here</a>.</p><h1 id="reference">Reference</h1><pre><code>Carl Edward Rasmussen. Gaussian process for Machine Learning.</code></pre><h1 id="acknowledgement">Acknowledgement</h1><pre><code>Thanks for the opening source toolbox GAUSSIAN PROCESS REGRESSION AND CLASSIFICATION Toolbox version 4.0, programmed by Carl et al.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale depen
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
  </entry>
  
  <entry>
    <title>Multivariate Interpolation of Wind Fields Based on Gaussian Process Regression, Jan. 24th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2018-01-24-talk-gpr.html"/>
    <id>http://yoursite.com/posts/talks/2018-01-24-talk-gpr.html</id>
    <published>2018-01-24T18:33:39.000Z</published>
    <updated>2021-02-16T18:16:51.349Z</updated>
    
    <content type="html"><![CDATA[<p>This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check the <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html">projects' introduction</a> and <a href="https://github.com/skaudrey/gpml/">github</a> for more details.</p><p>Slides are avaliable <a href="/assets/slides/gpr/windInterpolation.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check th
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
      <category term="interpolation" scheme="http://yoursite.com/tags/interpolation/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2017-09-11-notes-fourier-GCN.html"/>
    <id>http://yoursite.com/posts/talks/2017-09-11-notes-fourier-GCN.html</id>
    <published>2017-09-11T17:33:39.000Z</published>
    <updated>2021-02-16T18:18:02.480Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.</p><p>Check <a href="/assets/slides/notes/GCN/fourier-GCN.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.&lt;/p&gt;
&lt;p&gt;Check &lt;a hre
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2017-09-11-talk-da.html"/>
    <id>http://yoursite.com/posts/talks/2017-09-11-talk-da.html</id>
    <published>2017-09-11T17:33:39.000Z</published>
    <updated>2021-02-16T18:17:17.766Z</updated>
    
    <content type="html"><![CDATA[<p>Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it from other forms of machine learning, image analysis, and statistical methods. This talk discussed the basic ideas of machine leaning, and compared it with machine learning. It is given after I came back from Harbin's summer school in Agust, 2017. After this talk, I began to throw myself into studying machine learning.</p><p>Check <a href="/assets/slides/D.A/pres.pdf">slide</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>The summer school held in Harbin, Aug. 2017.</title>
    <link href="http://yoursite.com/posts/meetings/2017-08-22-harbin.html"/>
    <id>http://yoursite.com/posts/meetings/2017-08-22-harbin.html</id>
    <published>2017-08-22T17:33:39.000Z</published>
    <updated>2021-02-16T18:19:33.454Z</updated>
    
    <content type="html"><![CDATA[<p>I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check <a href="http://mss2017.hit.edu.cn/showSubjectDominWebSite.do">here</a> for more info. I gave a talk to the students in my lab after returning, <a href="https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html">here</a> are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation. <a id="more"></a> The themes given by those professors are listed below.</p><ul><li><p>Prof. Francois</p><p>Research Area：Variational data assimilation (VAR), especially 4DVAR.</p><p>Keynotes：The direvation of adjoint models, sensitivity analysis and the introduction of image assimilation. See the <a href="/assets/notes/harbin/François%20meeting%20minutes.pdf">minutes file</a> for details.</p></li><li><p>Prof. Jordan</p><p>Research Area：Statistical Learning</p><p>Keynotes: Summarize popular machine learning algorithms, and prove the convergence etc. See the <a href="/assets/notes/harbin/Jordan%20meeting%20minutes.pdf">minutes file</a> for details.</p></li><li><p>Prof. Jurgen</p><p>Research Area：AI, DL</p><p>Keynotes: The introduction of utilizing AI. Check more from his <a href="http://people.idsia.ch/~juergen/">home page</a>.</p></li><li><p>Prof. Ma</p><p>Research Area：Compression sensing.</p><p>Keynotes: The introduction of compression sensing and its applications.</p></li><li><p>Prof. Cai</p><p>Research Area：Statistical inference.</p><p>Keynotes: Statistical inference in high-dimensions. No slides, the minutes file is <a href="/assets/notes/harbin/Tony%20meeting%20minutes.pdf">here</a>.</p></li></ul><p>Slides and records are available in the <a href="https://pan.baidu.com/s/1jGj07koiMIV-MOf17N_jeg">baidu network disk</a> with password 6o2x. Enjoy yourself.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check &lt;a href=&quot;http://mss2017.hit.edu.cn/showSubjectDominWebSite.do&quot;&gt;here&lt;/a&gt; for more info. I gave a talk to the students in my lab after returning, &lt;a href=&quot;https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html&quot;&gt;here&lt;/a&gt; are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation.
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="summer school" scheme="http://yoursite.com/tags/summer-school/"/>
    
      <category term="mathematics" scheme="http://yoursite.com/tags/mathematics/"/>
    
  </entry>
  
  <entry>
    <title>The naive implementation of some popular machine learning algorithms.</title>
    <link href="http://yoursite.com/posts/projects/2017-06-28-ml-implement.html"/>
    <id>http://yoursite.com/posts/projects/2017-06-28-ml-implement.html</id>
    <published>2017-06-28T17:33:39.000Z</published>
    <updated>2021-02-16T18:18:21.140Z</updated>
    
    <content type="html"><![CDATA[<p>Naive implementations of some M.L. algorithms, which are updated continuously.</p><p>The algorithms that have been implemented are listed as follows:</p><ul><li>Logistic Regression,</li><li>SVM solved by SMO,</li><li>K-Means，</li><li>GMM solved by EM,</li><li>Perceptron，</li><li>Naive Bayes,</li><li>LeNet-Keras，</li><li>MLP-Numpy solved with BP,</li><li>MCMC sampling.</li></ul><p>Codes are available <a href="https://github.com/skaudrey/ml_algorithm">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Naive implementations of some M.L. algorithms, which are updated continuously.&lt;/p&gt;
&lt;p&gt;The algorithms that have been implemented are list
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="naive" scheme="http://yoursite.com/tags/naive/"/>
    
  </entry>
  
</feed>
