<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mia&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/2d5354ebc5a8c2413323ef55a6c6d252</icon>
  <subtitle>Je marche lentement, mais je ne recule jamais.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-01-21T22:51:22.874Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Mia Feng</name>
    <email>skaudreymia@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Paper--Graph Embedded Pose Clustering for Anomaly Detection</title>
    <link href="http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html</id>
    <published>2021-01-15T20:17:13.000Z</published>
    <updated>2021-01-21T22:51:22.874Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1912.11850.pdf">Graph Embedded Pose Clustering for Anomaly Detection</a></p><p>Code <a href="https://github.com/amirmk89/gepc">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li><p>Anomaly detection task</p><ul><li>Fine-grained anomaly detection: Detecting abnormal variations of an action: e.g. an abnormal type of walking</li><li>Coarse-grained anomaly detection: Defining normal actions and regard other action as abnormal. Aka there are multiple poses regarded as normal actions, rather than a single normal action.</li></ul></li><li><p>Video anomaly detection</p><ul><li>Reconstructive models: learn a feature representation for each sample and attempt to reconstruct a sample based on that embedding, often using <strong>Autoencoder</strong>. Samples poorly reconstructed are considered anomalous.</li><li>Predictive models: model the current frame based on a set of previous frames, often relying on recurrent neural networks or 3D convolutions. Samples poorly predicted are considered anomalous.</li><li>Reconstructive + predictive models</li><li>Generative models: used to reconstruct, predict or model the distribution of the data, often using Variational Autoencoders (VAEs) or GANs. E.g. the differences in gradient-based features and optical flow.</li></ul></li><li><p>GNNs</p><p>The point is the weighted adjacency matrix.</p><ul><li>Temporal and multiple adjacency extensions. (ST-GCN)</li><li>Graph attention networks. (2s-AGCN)</li></ul></li><li><p>Deep clustering models</p><p>Provide useful cluster assignments by optimizing a deep model under a cluster inducing objective.</p></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Skeleton-based methods make the analysis independent of nuisance parameters such as viewpoint or illumination.</li></ul></li><li>Limitations<ul><li>Traditional RGB-based anomaly detection methods have to consider many trivial information (viewing direction, illumination, background clutter etc.), and those data are sparse in human pose.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Generating action words from skeleton-based graphs and then classify actions into normal and abnormal (anomaly detection). With an aim at it can work both on fine-grained and coarse-grained task.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Map graphs into representation space and then cluster them so as to get action words. At last, Dirichlet process based mixture is used for classifying normal and abnormal.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115162120.png" alt="image-20210115162120660" style="zoom:50%;" /></p><h3 id="data-preparation">Data Preparation</h3><ul><li>Similar skeleton graph as what used in ST-GCN.</li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Backbone: ST-GCN</p></li><li><p>ST-GCAE network</p><ul><li><p>GCN block</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115185527.png" alt="image-20210115185526918" style="zoom:50%;" /></p><p>The block will be used in SAGC</p></li><li><p><strong>SAGC</strong> block</p><p>Each adjacency type is applied with its own GCN, using separate weights.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115190041.png" alt="image-20210115190041541" style="zoom:50%;" /></p><ul><li><p>Adjacency matrices</p><table><colgroup><col style="width: 8%" /><col style="width: 38%" /><col style="width: 28%" /><col style="width: 24%" /></colgroup><thead><tr class="header"><th>matrix</th><th>sharing</th><th>level</th><th>Dimension</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\mathrm{A}\)</span></td><td>fixed and shared by all layers</td><td>body-part connectivity over node relations</td><td><span class="math inline">\([V,V]\)</span>, <span class="math inline">\(V\)</span> is the number of nodes</td></tr><tr class="even"><td><span class="math inline">\(\mathrm{B}\)</span></td><td>individual at each layer, applied equally to all samples</td><td>dataset level keypoint relations</td><td><span class="math inline">\([V,V]\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\mathrm{C}\)</span></td><td>is different for different sample</td><td>sample specific relations</td><td><span class="math inline">\([N,V,V]\)</span>, <span class="math inline">\(N\)</span> is the batch size</td></tr></tbody></table></li></ul></li><li><p>ST-GCAE</p><p>The encoder uses large temporal strides with an increasing channel number to compress an input sequence to a latent vector. The decoder uses temporal up-sampling layers and additional graph convolutional blocks.</p></li></ul></li><li><p><strong>Deep embedded cluster</strong></p><ul><li><p>The input is the embedding from ST-GCAE, denoted as <span class="math inline">\(\mathrm{z}_i\)</span> for sample <span class="math inline">\(i\)</span></p></li><li><p>Soft-assignment --clustering layer</p><p>The probability <span class="math inline">\(p_{ik}\)</span> for the <span class="math inline">\(i\)</span>-th sample to be assigned to the <span class="math inline">\(k\)</span>-th cluster is:</p><p><span class="math inline">\(p_{ik}=Pr(y_i=k|\mathrm{z}_i,\Theta)=\frac{exp(\theta^T_k\mathrm{z}_i)}{\sum\limits_{k&#39;=1}^{K}exp(\theta^T_{k&#39;}\mathrm{z}_i)}\)</span>, where <span class="math inline">\(\Theta\)</span> is the clustering layer’s parameters. (Simple softmax)</p></li><li><p><strong>Optimize clustering layer</strong></p><ul><li>Objective: Minimize the KL-divergence between the current model probability clustering prediction <span class="math inline">\(P\)</span> and a target distribution <span class="math inline">\(Q\)</span>. The target distribution aims to strengthen current cluster assignments by normalizing and pushing each value closer to a value of either 0 or 1.</li><li><strong>EM</strong> style. In expectation step, the entire model is fixed and the target distribution <span class="math inline">\(Q\)</span> is updated. In maximization stage, the model is optimized to minimize the clustering loss <span class="math inline">\(L_{cluster}\)</span>.</li></ul></li></ul></li><li><p>Anomaly classifier--<strong>normality scoring??</strong></p><ul><li>Two types of multimodal distributions. One is at the cluster assignment level; the other is at the soft-assignment vector level.</li><li>DPMM based. Classifier is fitted by soft-assignment vector (e.g., for class <span class="math inline">\(i\)</span> the softmax result) and then it can do inference.</li></ul></li><li><p>Model</p><p>Feeding the embedding from ST-GCAE to clustering layer, then fixing decoder, fine-tune the encoder in ST-GCAE and clustering layer by combined loss. After fine tuning, using DPMM-based classifier for final inference.</p><ul><li><p>Loss function</p><ul><li><p><strong>Reconstruction loss</strong> <span class="math inline">\(L_{rec}\)</span>: <span class="math inline">\(\ell_2\)</span> loss between the original temporal pose graphs and those reconstructed by ST-GCAE, <strong>used in pre-training stage</strong>, for training the whole ST-GCAE.</p></li><li><p><strong>Clustering loss</strong> <span class="math inline">\(L_{cluster}\)</span>, combined with reconstruction loss and used for fine-tuning encoder of ST-GCAE+clustering layer</p><p><span class="math inline">\(L_{cluster}=KL(Q||P)=\sum\limits_i\sum\limits_kq_{ik}\log\frac{q_{ik}}{p_{ik}},\\ q_{ik}=\frac{p_{ik}/(\sum_{i&#39;}p_{i&#39;k})^{\frac{1}{2}}}{\sum_{k&#39;}p_{ik&#39;}/(\sum_{i&#39;}p_{i&#39;k&#39;})^{\frac{1}{2}}}\)</span></p></li><li><p>Combined loss</p><p><span class="math inline">\(L_{combined}=L_{rec}+\lambda\cdot L_{cluster}\)</span>, this loss is for training encoder and clustering layer, which means the decoder is fixed while using it.</p></li></ul></li><li><p>Optimization</p><ul><li>encoder: reconstruction loss + cluster loss</li><li>decoder: reconstruction loss</li><li>clustering layer: cluster loss</li></ul></li></ul></li></ul><h2 id="experiments">Experiments</h2><ul><li><p>Dataset</p><ul><li>ShanghaiTech: 130 abnormal events captured in 13 different scenes with complex lighting conditions and camera angles.<ul><li>training set contains only normal examples</li><li>test set contains both normal and abnormal examples</li><li>2D pose</li></ul></li><li>Kinetics-based: Kinetics-250 and NTU-RGBD. Actions in each set are sampled randomly or meaningfully. In Kinetics dataset, remove actions that focus only on slightly part joints' movements, like hair braiding.<ul><li><em>Few vs. Many</em>: few normal actions (<span class="math inline">\(3\sim5\)</span>) in the training set and many abnormal (<span class="math inline">\(10\sim 11\)</span> hundreds) actions in the test set</li><li><em>Many vs. Few</em>: switch the training set and test set in experiment above.</li></ul></li></ul></li><li><p>Preprocessing</p><ul><li>Pre-extracting 2D pose from ShanghaiTech Campus</li></ul></li><li><p>Input features</p><ul><li>The coordinates of joints</li><li>For ShanghaiTech: The embeddings of the patch around each joint (from one of the pose estimation model's hidden layers)</li></ul></li><li><p>Test Algorithms on coarse-grained (Kinetics and NTU-RGBD)</p><ul><li><p>Autoencoder reconstruction loss: ST-GCAE reached convergence prior to the deep clustering fine-tuning stage.</p></li><li><p>Autoencoder based one-class SVM: fit a one-class SVM using the encoded pose sequence representation</p></li><li><p>Video anomaly detection methods: Train Future frame prediction model and the skeleton trajectory model. Anomaly scores for each video are obtained by averaging the per-frame scores.</p></li><li><p>Classifier softmax scores: supervised baseline. Anomaly score is by either using the softmax vector's max value or by using the Dirichlet normality score</p></li><li><p>Test video in fixed size but with sliding-window if the test video with unknown frames</p></li></ul></li><li><p>Evaluation metrics</p><ul><li>Frame-level score: the maximal score over all the people in the frame</li><li>AUC as the combined score over all frames of one test</li></ul></li><li><p>Summary</p><ul><li><p>On ShanghaiTech (fine-grained): Patches ST-GCAE outstands.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121170448.png" alt="image-20210121170448502" style="zoom:40%;" /></p></li><li><p>On coarse grained dataset, ST-GCAE outperforms, but better on meaningful actions. <em>A good skeleton help ST-GCAE</em> (NTU-RGBD has better detection on skeletons cause the depth data is known.)</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121171357.png" alt="image-20210121171357539" style="zoom:30%;" /></p></li><li><p><em>Failed cases</em>: occlusions, high-speed action like cycling, non-person related abnormal like bursting into a vehicle.</p></li><li><p><strong>Ablation study:</strong> adding some abnormal actions into normal actions</p><ul><li>ST-GACE on NTU-RGBD (<strong><em>only dropping, touching and Rand8 dataset are tested</em></strong>): ST-GCAE loses on average less than <span class="math inline">\(10\%\)</span> of performance when trained with <span class="math inline">\(5\%\)</span> abnormal actions added as noises.</li></ul></li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>The use of embedded pose graphs and a Dirichlet process mixture for video anomaly detection;</li><li>A new coarse-grained setting for exploring broader aspects of video anomaly detection;</li><li>State-of-the-art AUC of 0.761 for the ShanghaiTech Campus anomaly detection benchmark.</li></ul><h2 id="remarks"><font color='blue'>Remarks</font></h2><ul><li>The reconstruction (learning representations of graph) is mixed with clustering in the final loss, will this be good? Won't the trivial information of clustering influence the reconstruction?</li><li>The training set for clustering layer is initialized by the K-Means centroids, won't the initialization methods matter?</li><li><strong>The embeddings of patches around each joint outperforms the simple joint coordinates</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1912.11850.pdf&quot;&gt;Graph Embedded Pose Clustering for Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/amirmk89/gepc&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html</id>
    <published>2021-01-15T01:51:32.000Z</published>
    <updated>2021-01-15T20:20:14.803Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/lshiwjx/2s-AGCN">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Convolutional DL based methods manually structure the skeleton as a sequence of joint-coordinate vectors or as a pseudo-image, which is fed into RNNs or CNNs to generate the prediction.</li><li>Skeleton-based action recognition<ul><li>Design handcrafted features to model human body, but they are barely satisfactory.</li><li>DL-based: CNN-based methods are generally more popular than RNN-based methods. But both fail to fully represent the structure of the skeleton data.</li><li>GCN-based: ST-GCN, eliminates the meed for designing handcrafted part assignment or traversal rules.</li></ul></li><li>GNNs<ul><li><strong>Spatial perspective</strong>: directly perform the convolution filters on the graph vertexes and their neighbors, which are extracted and normalized based on manually designed rules.</li><li>Spectral perspective: use the eigenvalues and eigenvectors of the graph Laplace matrices. They perform the graph convolution in the frequency domain with the help of graph Fourier transform.</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>The second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods.</li></ul></li><li>Limitations:<ul><li>Representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependency between correlated joints</li><li>In GCN-based skeleton action recognition, the topology of the graph is set manually and thus may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks.</li><li>ST-GCN: 1) The skeleton graph is heuristically predefined and represents only the physical structure of the human body. 2) The fixed topology of graph limiting the flexibility and capacity to model the multilevel semantic information. 3) One fixed graph structure may not be optimal for all the samples of different action classes. Like hands-related actions and legs-related actions</li></ul></li></ul><h2 id="goals">Goals</h2><p>Propose a improved ST-GCN (graph convolutional based model), so as to use 2nd order information and improve the accuracy of action recognition based on skeletons.</p><p>Make the graph is unique for different layers and samples.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Modification on ST-GCN</p><ul><li>Two types of graphs:<ul><li>Global graph: represents the common pattern for all the data</li><li>Individual graph: represents the unique pattern for each data</li></ul></li><li>Second-order information: the length and directions of bones are formulated as a vector pointing from its source joint to its target joint.</li></ul><h3 id="data-preparation">Data Preparation</h3><ul><li>The structure of the graph follows the work of ST-GCN.</li></ul><h3 id="a-look-at-st-gcn">A look at ST-GCN</h3><p>Graph convolution in ST-GCN: <span class="math inline">\(f_{out}(v_{ti})=\sum\limits_{v_{tj}\in \mathcal{B}_i}\frac{1}{Z_{ti}(v_{tj})}f_{in}(v_{j})\cdot \mathrm{w}(l_{ti}(v_{tj}))\)</span>, follows spatial configuration partitioning.</p><ul><li>Graph convolution in spatial dimension</li></ul><p><span class="math inline">\(f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}_k(f_{in}\mathrm{A}_k)\odot\mathrm{M}_k\)</span>, where <span class="math inline">\(\mathrm{M}_k\)</span> is an <span class="math inline">\(N\times N\)</span> attention map that indicates the importance of each vertex. <span class="math inline">\(\mathrm{A}_k\)</span> <strong>determines whether there are connections between two vertexes and <span class="math inline">\(\mathrm{M}_k\)</span> determines the strength of the connections.</strong></p><ul><li>Graph convolution in temporal dimension: <span class="math inline">\(K_t\times 1\)</span>convolution on the output feature map</li></ul><p><em>The model is calculated based on a predefined graph, which may not be a good choice.</em></p><h3 id="implementation">Implementation</h3><ul><li><p>Adaptive graph convolutional network (<strong>AGCN</strong>)</p><p><strong>BN+9 of adaptive graph convolutional blocks + global average pooling + softmax classifier</strong></p><ul><li><p>Adaptive graph convolutional layer:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142649.png" alt="image-20210115142649202" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115133332.png" alt="image-20210115133332722" style="zoom:50%;" /></p><p><span class="math inline">\(f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}_kf_{in}(\mathrm{A}_k+\mathrm{B}_k+\mathrm{C}_k)\)</span>. The adjacency matrix is now divided into three parts</p><ul><li><p><span class="math inline">\(\mathrm{A}_k\)</span>: same as <span class="math inline">\(N\times N\)</span> adjacency matrix <span class="math inline">\(\mathrm{A}_k\)</span> in ST-GCN, it <em>represents the physical structure of the human body</em>.</p></li><li><p><span class="math inline">\(\mathrm{B}_k\)</span>: An <span class="math inline">\(N\times N\)</span> adjacency matrix. It's trainable. It acts as <span class="math inline">\(\mathrm{M}_k\)</span> (attention mechanism) in ST-GCN, influenced by the connections between two joints and also the strength of the connections.</p></li><li><p><span class="math inline">\(\mathrm{C}_k\)</span>: a similarity matrix calculated by the normalized embedded Gaussian function with vectors embedded by <span class="math inline">\(1\times 1\)</span> convolutional layer.</p><p><span class="math inline">\(\mathrm{C}_k=softmax(\mathrm{f}_{in}^T\mathrm{W}_{\theta k}^{T}\mathrm{W}_{\phi k}\mathrm{f}_{in})\)</span>, where <span class="math inline">\(\mathrm{W}_\theta,\mathrm{W}_\phi\)</span> are the parameters of the embedding functions <span class="math inline">\(\theta,\phi\)</span>, respectively.</p></li></ul></li><li><p>Adaptive graph convolutional block</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115132928.png" alt="image-20210115132928144" style="zoom:33%;" /></p></li></ul></li><li><p>Model: two stream networks</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142748.png" alt="image-20210115142748418" style="zoom:50%;" /></p><ul><li>J-stream: the input data are joints, as what's depicted in AGCN.</li><li>B-stream: the input data are bones.<ul><li>A bone is the vector pointing from its source joint to its target joint. This vector will contain both length and direction of a bone. An empty bone is added so as to make sure the B-stream has similar quantity of input as J-stream.</li></ul></li></ul><p>Finally, the <em>softmax</em> scores of the two streams are added to obtain the fused score and do prediction.</p></li><li><p>Loss function</p><p>Cross-entropy</p></li><li><p>Why does it work?</p><ul><li>Considering bones (2nd information)</li><li>Offers trainable attention matrix <span class="math inline">\(\mathrm{B}_k\)</span> and the similarity evaluation of <span class="math inline">\(\mathrm{C}_k\)</span> to estimate the strength of connection. Both of them offer more possible connections and provide more flexibility.</li></ul></li></ul><h2 id="experiments">Experiments</h2><ul><li>Dataset: Kinetics and NTU-RGBD<ul><li>NTU-RGBD: If the number of bodies in the sample is less than 2, the second body is padded with 0.</li><li>Kinetics: Same data augmentation as done in ST-GCN.</li></ul></li><li>Training: SGD with Nesterov momentum (0.9), batch size is 64. The weight decay is set to 0.0001.<ul><li>NTU-RGBD: learning rate is set as 0.1 and is divided by 10 at the 30th epoch and 40th epoch. The training process is ended at the 50th epoch.</li><li>Kinetics: The learning rate is set as 0.1 and is divided by 10 at the 45th epoch and 55th epoch. Training ends at the 65th epoch.</li></ul></li><li>Evaluation metrics</li><li>NTU-RGBD: top-1 accuracy<ul><li>Kinetics : top-1 and top-5 accuracy</li></ul></li></ul><h3 id="ablation-study">Ablation study</h3><ul><li><p>Adaptive graph convolutional block</p><p>Manually delete one of the graphs and estimate.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115150909.png" alt="image-20210115150909820" style="zoom:33%;" /></p><ul><li>Given each connection, a weight parameter is important, which also proves the importance of the adaptive graph structure</li></ul></li><li><p>Visualization of the learned graphs</p><p>Denote the strength of joints by dot size, the bigger the stronger connection.</p><ul><li><p>A higher layer in AGCN contains higher-level information, comparing the dot size in different layers</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115151421.png" alt="image-20210115151421543" style="zoom:50%;" /></p></li><li><p>The diversity for different sample in the same layer is proved.</p></li></ul></li><li><p>Two-stream framework</p><p>The two-stream method outperforms the one-stream-based methods either the J-stream or the B-stream.</p></li></ul><h3 id="compared-with-sota">Compared with SOTA</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115144443.png" alt="image-20210115144443887" style="zoom:50%;" /></p><ul><li>Question 1: <font color='red'> ResNet helps?</font></li><li>Question 2: <font color='red'>How about compared with methods based on RGB or optical flow ?</font> In paper ST-GCN their model fails to those models.</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>An adaptive graph convolutional network is proposed.</li><li>The second-order information of the skeleton data is explicitly formulated and combined with the first-order information using a two-stream framework, which brings notable improvement for the recognition performance.</li><li>On two large-scale datasets for skeleton-based action recognition, the proposed 2s-AGCN exceeds the SOTA by a significant margin.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf&quot;&gt;Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/lshiwjx/2s-AGCN&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html</id>
    <published>2021-01-13T02:58:39.000Z</published>
    <updated>2021-01-15T18:26:08.271Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/yysijie/st-gcn">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Human action recognition: can be solved by appearance, depth, optical flows, and body skeletons.</li><li>Models for graph<ul><li>Recurrent neural networks</li><li>GNNs:<ul><li>Spectral perspective: the locality of the graph convolution is considered in the form of spectral analysis.</li><li><strong>Spatial perspective</strong>: the convolution filters are applied directly on the graph nodes and their neighbors.</li></ul></li></ul></li><li>Skeleton Based Action Recognition<ul><li>Handcrafted feature based methods: design several handcrafted features to capture the dynamics of joint motion. E.g., covariance matrices of joint trajectories, relative positions of joints, rotations and translations between body parts.</li><li>Deep learning methods: recurrent neural networks and temporal CNNs. Many emphasize <strong>the importance of modeling the joints within parts of human bodies.</strong></li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Earlier methods of using skeletons for action recognition simply employ the joint coordinates at individual time steps to form feature vectors, and apply temporal analysis thereon. <strong>They do not explicitly exploit the spatial relationships among the joint</strong>.</li><li>Most existing methods which explore spatial relationship rely on hand-crafted parts or rules to analyze the spatial patterns.</li><li>Traditional CNNs are not suitable for 2D or 3D skeletons (graphs rather than data in grids).</li></ul></li><li>Limitations:<ul><li>Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization.</li><li>Models using hand-crafted parts are difficult to be generalized to others</li><li>These parts used in DL based methods are usually explicitly assigned using domain knowledge, which is not automatic and practical.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Build a better model for dynamics of human body skeletons. Specifically, a new method that can automatically capture the patterns embedded in the <strong>spatial configuration of the joints as well as the temporal dynamics</strong> is required.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Skeletons in frames are connected as natural spatial-temporal graph, then using GCN.</p><h3 id="data-preparation">Data Preparation</h3><ul><li>The feature vector on a node <span class="math inline">\(F(v_{ti})\)</span> consists of coordinate vectors, as well as estimation confidence, of the i-th joint on frame t.</li><li>Construct the spatial temporal graph on the skeleton sequences in two steps. First, the joints within one frame are connected with edges according to the connectivity of human body structure. Then each joint will be connected to the same joint in the consecutive frame.</li><li>Both 18 joints skeleton model or 25 joints skeleton model work fine.</li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Model</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113115442.png" alt="image-20210113115429152" style="zoom:50%;" /></p><p>ResNet mechanism is applied on each ST-GCN unit.</p><table><thead><tr class="header"><th>Layer name</th><th>configuration</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(1\sim3\)</span></td><td>64 channels</td></tr><tr class="even"><td><span class="math inline">\(4\sim6\)</span></td><td>128 channels</td></tr><tr class="odd"><td><span class="math inline">\(7\sim9\)</span></td><td>256 channels, 9 temporal kernel size</td></tr><tr class="even"><td>Global pooling+softmax</td><td></td></tr></tbody></table></li><li><p>Loss function</p><ul><li><p><strong>Sampling function</strong> <span class="math inline">\(\mathbb{p}\)</span> enumerates the neighbors of location <span class="math inline">\(x\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113155452.png" alt="image-20210113155452481" style="zoom:40%;" /></p><p>In this paper, <strong>the 1-neighbor set</strong> of joint nodes are used.</p></li><li><p><strong>The filter weights</strong> <span class="math inline">\(\mathrm{w} (v_{ti},v_{tj})\)</span> are shared everywhere on the input image.</p><p>Build a mapping <span class="math inline">\(l_{ti}:B(v_{ti})\rightarrow\{0,\cdots,K−1\}\)</span> which maps a node in the neighborhood to its subset label. The weight function <span class="math inline">\(\mathrm{w}(v_{ti}, v_{tj}):B(v_{ti})\rightarrow R^c\)</span> can be implemented by indexing a tensor of <span class="math inline">\((c,K)\)</span> dimension or</p><p><span class="math inline">\(\mathrm{w}(v_{ti},v_{tj})=\mathrm{w}&#39;(l_{ti}(v_{tj})\)</span></p><ul><li><p>Labeling strategies (the definition of <span class="math inline">\(l_{ti}\)</span>)</p><p><img src="C:%5CUsers%5C10457%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210113195834069.png" alt="image-20210113195834069" style="zoom:50%;" /></p><ul><li><p>Uni-labeling</p><p>Make the whole neighbor set itself as subset. Then feature vectors on every neighboring node will have a inner product with the same weight vector. Formally, <span class="math inline">\(K=1,l_{ti}(v_{tj})=0,\forall i,j\in V\)</span></p></li><li><p>Distance partitioning</p><p><span class="math inline">\(d=0\)</span> refers to the root node itself and remaining neighbor nodes are in the <span class="math inline">\(d =1\)</span> subset. Formally , <span class="math inline">\(K=2,l_{ti}(v_{tj})=d(v_{tj},v_{ti})\)</span></p></li><li><p>Spatial configuration partitioning</p><p>Three subsets: 1) the root node itself; 2)centripetal group and 3) centrifugal group.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113200600.png" alt="image-20210113200600480" style="zoom:33%;" />, where <span class="math inline">\(r_i\)</span> is the average distance from gravity center to joint <span class="math inline">\(i\)</span> over all frames in the training set.</p></li></ul></li><li><p>Learnable edge importance weighting</p><p>One joint appears in multiple body parts should have different importance in modeling the dynamics of these parts. A learnable mask M is added on every layer of spatial temporal graph convolution.</p></li></ul></li><li><p>The <strong>spatial graph convolution</strong></p><p><span class="math inline">\(f_{out}(v_{ti})=\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(\mathbb{p}(v_{ti},v_{tj}))\cdot \mathrm{w}(v_{ti},v_{tj})=\\\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(v_{tj})\cdot \mathrm{w}(l_{ti}(v_{tj}))\)</span>, <span class="math inline">\(Z_{ti}(v_{tj})=|\{v_{tk}|l_{ti}(v_{tk})=l_{ti}(v_{tj})\}|\)</span> is the cardinality of the corresponding subset. It's for balancing the contributions of different subsets to the output.</p></li><li><p><strong>Spatial temporal modeling</strong></p><p>Extend neighbors so as to include temporally connected joints</p><p><span class="math inline">\(B(v_{ti})={v_{qj}|d(v_{tj},v_{ti})\le K,|q-t|\le \lfloor\Gamma/2\rfloor}\)</span>, where <span class="math inline">\(\Gamma\)</span> is temporal kernel size (controls the temporal range to be included in the neighbor graph). Then , the sampling function is <span class="math inline">\(l_{ST}(v_{qj})=l_{ti}(v_{tj})+(q-t+\lfloor\Sigma/2\rfloor)\times K\)</span>.</p></li><li><p>The final convolution formula:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113222927.png" alt="image-20210113222926988" style="zoom:40%;" />, where <span class="math inline">\(\Lambda_j^{ii}=\sum_k(A_j^{ik})+\alpha\)</span>, <span class="math inline">\(\alpha=0.001\)</span> is to avoid empty rows in <span class="math inline">\(A_j\)</span>. To add learnable mask <span class="math inline">\(M\)</span>, displace <span class="math inline">\(A_j\)</span> as <span class="math inline">\(A_j\otimes M\)</span>.</p></li></ul></li><li><p>Why does it work?</p><ul><li>Parts restrict the modeling of joints trajectories within “local regions” compared with the whole skeleton, thus forming a hierarchical representation of the skeleton sequences.</li></ul></li></ul><h2 id="experiments">Experiments</h2><ul><li><p>Dataset:</p><ul><li><p>Kinetics (unconstrained action recognition dataset), provides only raw video clips without skeleton data.</p><ul><li><p>Augmentation: To avoid overfitting,two kinds of augmentation are used to replace dropout layers when training on the Kinetics dataset. 1) affine transformations, 2) sampling part of the frames from the whole frame and testing by a whole frame. <font color=Blue>May that's a way for avoiding the two consecutive frame are too similar?</font></p></li><li><p>Videos to skeletons</p><p>To work on skeletons, openpose is used for extracting. Concretely, resize all videos to the resolution of 340 × 256 and convert the frame rate to 30 FPS. Then OpenPose toolbox is used to estimate the location of 18 joints on every frame of the clips.</p></li><li><p>Final features:</p><p>Finally <strong>the clips are represented by a tensor in shape</strong> <span class="math inline">\((3,T,18,2)\)</span>, where 18 is the number of joints, 2 is the number of people and 3 is the number of features (X,Y,C), C is the confidence.</p></li></ul></li><li><p>NTU-RGBD ( in-house captured action recognition dataset)</p><ul><li>Already annotated with 25 3D joints</li><li>Each clip is guaranteed to have at most 2 subjects</li></ul></li></ul></li><li><p>Training: SGD with a learning rate of 0.01. <span class="math inline">\(lr\)</span> decay by 0.1 after every 10 epochs.</p></li><li><p>Evaluation metrics</p><ul><li><p>Kinetics</p><p>Test on validation set. Using <strong>top-1 and top-5 classification accuracy</strong></p></li><li><p>NTU-RGBD</p><p>Report top-1 recognition accuracy.</p><ul><li><strong>Cross-subject</strong>: Train on one subset of actors and test on the remaining actors.</li><li><strong>Cross-view</strong>: Train on skeletons from camera views 2 and 3, and test on those from camera view 1.</li></ul></li></ul></li></ul><h3 id="ablation-study">Ablation study</h3><p>Applied on Kinetics dataset.</p><ul><li><p>Spatial temporal graph convolution</p><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th>Model</th><th>configuration</th></tr></thead><tbody><tr class="odd"><td><em>baseline-TCN</em></td><td>squeeze Spatial dimension, concatenate all input joint locations to form the input features at each frame <span class="math inline">\(t\)</span>.</td></tr><tr class="even"><td><em>local convolution</em></td><td>The input data are the same format, but with unshared convolution filters.</td></tr></tbody></table></li><li><p>Partition strategies: same as what described before. <em>Distance partitioning*</em> is as intermediate between the distance partitioning and uni-labeling. The filters in this setting only differs with a scaling factor -1, or to say <span class="math inline">\(\mathrm{w}_0=-\mathrm{w}_1\)</span>.</p></li><li><p>Learnable edge importance weighting</p><p>This setting is named as <em>ST-GCN+Imp</em>.</p></li><li><p>Results</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114194501.png" alt="image-20210114194452831" style="zoom:50%;" /></p><ul><li>Better performance of ST-GCN based models could justify the power of the spatial temporal graph convolution in skeleton based action recognition</li><li>Distance partitioning* achieves better performance than uni-labeling, which again demonstrate <strong>the importance of the partitioning with multiple subsets</strong>.</li><li>ST-GCN model with learnable edge importance weights can learn to express the joint importance.</li></ul></li></ul><h3 id="compared-with-sota">Compared with SOTA</h3><p><strong>Model setting: ST-GCN+Learnable weights+Spatial configuration partitioning</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204800.png" alt="image-20210114204800283" style="zoom:30%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204722.png" alt="image-20210114204722008" style="zoom:30%;" /></p><ul><li>Kinetics: ST-GCN is able to outperform previous representative approaches, but under-perform methods in RGB or optical flow.</li><li>NTU-RGBD: No data augmentation before training. It outperforms all other selected candidates.</li><li>The skeleton based model ST-GCN can provide complementary information to RGB and optical flow models.</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Propose ST-GCN, a generic graph-based formulation for modeling dynamic skeletons, which is the first that applies graph-based neural networks for action recognition.</li><li>Propose several principles in designing convolution kernels in ST-GCN to meet the specific demands in skeleton modeling.</li><li>On two large scale datasets for skeleton-based action recognition, the proposed model achieves superior performance as compared to previous methods using hand-crafted parts or traversal rules, with considerably less effort in manual design.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1801.07455.pdf&quot;&gt;Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/yysijie/st-gcn&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Self-supervised Learning on Graphs, Deep Insights and New Directions</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html</id>
    <published>2021-01-12T03:00:00.000Z</published>
    <updated>2021-01-22T01:11:05.259Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2006.10141.pdf">Self-supervised Learning on Graphs: Deep Insights and New Directions</a></p><p><a href="https://github.com/ChandlerBang/SelfTask-GNN">Codes</a></p><a id="more"></a><h2 id="why">Why?</h2><ul><li>Nodes in graphs present unique structure information and they are inherently linked indicating not independent and identically distributed (or i.i.d.).</li><li>(SSL) has been introduced in both the image and text domains to alleviate the need of large labeled data by deriving labels for the significantly more unlabeled data.</li><li>To fully exploit the unlabeled nodes for GNNs, SSL can be naturally harnessed for providing additional supervision.</li><li>The challenges of graph to use SSL:<ul><li>graphs are not restricted to these rigid structures.</li><li>each node in a graph is an individual instance and has its own associated attributes and topological structures</li><li>instances (or nodes) are inherently linked and dependent of each other.</li></ul></li></ul><h2 id="goals">Goals</h2><ul><li>Focus on advancing GNNs for node classification where GNNs leverage both labeled and unlabeled nodes on a graph to jointly learn node representations and a classifier that can predict the labels of unlabeled nodes on the graph. Aims at gain insights on when and why SSL works for GNNs and which strategy can better integrate SSL for GNNs.</li></ul><h2 id="problem">Problem</h2><p>semi-supervised node classification task</p><p><span class="math inline">\(\min\limits_{\theta}\mathcal{L}_{task}(\theta,\mathrm{A,X},\mathcal{D}_L)=\sum\limits_{(v_i,y_i)\in\mathcal{D}_L}\ell(f_{\theta}(\mathcal{G})_{v_i},y_i)\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121192249.png" alt="image-20210121192249653" style="zoom:40%;" /></p><h2 id="previous-work">Previous work</h2><h3 id="basic-pretext-task-on-graphs">Basic pretext task on graphs</h3><h4 id="structure-information-adjacency-matrix-mathrma">Structure information (Adjacency matrix <span class="math inline">\(\mathrm{A}\)</span>)</h4><p>Construct self-supervision information for the unlabeled nodes based on their local structure information, or how they relate to the rest of the graph</p><h5 id="local-structure-information"><strong>Local structure information</strong></h5><ul><li><p>Node property</p><ul><li><p>use node degree as a representative local node property for self-supervision while leaving other node properties (or the combination) as one future work</p></li><li><p>Formally, let <span class="math inline">\(d_i=\sum\limits_{j=1}^{N}\mathrm{A}_{ij}\)</span> denote the degree of <span class="math inline">\(v_i\)</span> and construct the associated loss of the SSL pretext task as</p><p><span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}(f_{\theta&#39;}(\mathcal{G})_{v_i}-d_i)^2\)</span>, where <span class="math inline">\(\mathcal{D}_U\)</span> denote the set of unlabeled nodes and associated pretext task labels in the graph.</p></li><li><p>Assumption: The node property information is related to the specific task of interest.</p></li></ul></li><li><p><strong>EdgeMask</strong></p><ul><li><p>Build pretext task based on the connections between two nodes in the graph. Specifically, <strong>one can first randomly mask some edges and then the model is asked to reconstruct the masked edges</strong>.</p></li><li><p>Formally, first mask <span class="math inline">\(m_e\)</span> edges denotes as the set <span class="math inline">\(\mathcal{M}_e\subset\varepsilon\)</span> and also sample the set <span class="math inline">\(\bar{\mathcal{M}_e}=\{(v_i,v_j)|v_i,v_j\in\mathcal{V},(v_i,v_j)\notin\varepsilon\}\)</span>, <span class="math inline">\(|\bar{\mathcal{M}_e}|=|\mathcal{M}_e|=m_e\)</span> . Then the SSL pretext task is to predict whether there exist a link between a given node pair.</p><p><span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\\\frac{1}{|\mathcal{M}_e|}\sum\limits_{(v_i,v_j)\in\mathcal{M}_e}\ell(f_w(|f_{\theta&#39;}(\mathcal{G})_{v_i}-f_{\theta&#39;}(\mathcal{G})_{v_j}|),1)+\frac{1}{|\bar{\mathcal{M}_e}|}\sum\limits_{(v_i,v_j)\in\bar{\mathcal{M}_e}}\ell(f_w(|f_{\theta&#39;}(\mathcal{G})_{v_i}-f_{\theta&#39;}(\mathcal{G})_{v_j}|),0)\)</span>, where <span class="math inline">\(\ell(\cdot,\cdot)\)</span> is the cross entropy loss, <span class="math inline">\(f_w\)</span> linearly maps to 1-dimension.</p></li></ul></li></ul><h5 id="global-structure-information"><strong>Global structure information</strong></h5><ul><li><p>PairwiseDistance</p></li><li><p>Distance2Clusters</p></li></ul><h4 id="attribute-information-nodes-matrix-mathrmx">Attribute information (Nodes matrix <span class="math inline">\(\mathrm{X}\)</span>)</h4><ul><li>AttributeMask</li><li>PairewiseAttrSim</li></ul><h3 id="analysis">Analysis</h3><h2 id="advanced-pretext-task-on-graphs">Advanced pretext task on graphs</h2><h2 id="conclusion">Conclusion</h2><ul><li></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2006.10141.pdf&quot;&gt;Self-supervised Learning on Graphs: Deep Insights and New Directions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ChandlerBang/SelfTask-GNN&quot;&gt;Codes&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Predicting What You Already Know Helps, Provable Self-Supervised Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html</id>
    <published>2021-01-11T20:00:00.000Z</published>
    <updated>2021-01-12T19:38:37.955Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2008.01064.pdf">Predicting What You Already Know Helps: Provable Self-Supervised Learning</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li><p>Pretext tasks</p><ul><li><strong>Reconstruct images from corrupted versions or just part it</strong>: including denoising auto-encoders, image inpainting, and split-brain autoencoder</li><li><strong>Using visual common sense</strong>, including predicting rotation angle, relative patch position, recovering color channels, solving jigsaw puzzle games, and discriminating images created from distortion.</li><li><strong>Contrastive learning</strong>: learn representations that <strong>bring similar data points closer</strong> while pushing randomly selected points further away or <strong>maximize a contrastive-based mutual information lower bound</strong> between different views</li><li><strong>Create auxiliary tasks</strong>: The natural ordering or topology of data is also exploited in video-based, graph-based or map-based self-supervised learning. For instance, the pretext task is to determine the correct temporal order for video frames.</li></ul></li><li><p>Theory for self-supervised learning: contrastive learning</p><ul><li>Contrastive learning may not work when conditional independence holds only with additional latent variables</li></ul><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th style="text-align: left;">Theory</th><th style="text-align: left;">Limitations</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Shows shows guarantees for contrastive learning representations on linear classification tasks using a class conditional independence assumption</td><td style="text-align: left;">Not handle approximate conditional independence</td></tr><tr class="even"><td style="text-align: left;">Contrastive learning representations can linearly recover any continuous functions of the underlying topic posterior under a topic modeling assumption for text</td><td style="text-align: left;">The <strong>assumption of independent sampling of words</strong> that they exploit is <strong>strong</strong> and <strong>not generalizable to other domains</strong> like images</td></tr><tr class="odd"><td style="text-align: left;">Studies contrastive learning on the hypersphere through intuitive properties like alignment and uniformity of representations</td><td style="text-align: left;">No connection made to downstream tasks</td></tr><tr class="even"><td style="text-align: left;">A mutual information maximization view of contrastive learning</td><td style="text-align: left;">Some issues point by paper [45]</td></tr><tr class="odd"><td style="text-align: left;">Explain negative sampling based methods use the theory of noise contrastive estimation</td><td style="text-align: left;"><strong>guarantees are only asymptotic and not for downstream tasks.</strong></td></tr><tr class="even"><td style="text-align: left;">Conditional independence assumptions and redundancy assumptions on multiple views are used to analyze co-training</td><td style="text-align: left;">not for downstream task</td></tr></tbody></table></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Forming the pretext tasks:<ul><li>Colorization: can be interpreted as <span class="math inline">\(p(X_1,X_2|Y)=p(X_1|Y)\times p(X_2|Y)\)</span>, aka <span class="math inline">\(X_1,X_2\)</span> are independently conditioned on <span class="math inline">\(Y\)</span></li><li>Inpainting: <span class="math inline">\(p(X_1,X_2|Y,Z)=p(X_1|Y,Z)\times p(X_2|Y,Z)\)</span>,aka the inpainted <span class="math inline">\(X_2\)</span> is conditionally independent of <span class="math inline">\(X_2\)</span> (the remainder) given <span class="math inline">\(Y,Z\)</span>.</li></ul></li><li>The only way to solve the pretext task is to first implicitly predict <span class="math inline">\(Y\)</span> and then predict <span class="math inline">\(X_2\)</span> from <span class="math inline">\(Y\)</span></li></ul></li><li>Limitations:<ul><li>The underlying principles of self-supervised learning are still mysterious since it is a-priori unclear why predicting what we already know should help.</li></ul></li></ul><h2 id="goals">Goals</h2><p><strong><em>What conceptual connection between pretext and downstream tasks ensures good representations?</em></strong></p><p><strong><em>What is a good way to quantify this?</em></strong></p><h2 id="how">How?</h2><h3 id="notations">Notations</h3><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th style="text-align: left;">Symbol</th><th style="text-align: left;">Meaning</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><span class="math inline">\(\mathbb{E}^L[Y|X]\)</span></td><td style="text-align: left;">the best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span></td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(\Sigma_{XY|Z}\)</span></td><td style="text-align: left;">partial covariance matrix between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z\)</span></td></tr><tr class="odd"><td style="text-align: left;"><span class="math inline">\(X_1,X_2\)</span></td><td style="text-align: left;">the input variable and the target random variable for the pretext tasks</td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(Y\)</span></td><td style="text-align: left;">label for the downstream task</td></tr><tr class="odd"><td style="text-align: left;"><span class="math inline">\(P_{X_1X_2Y}\)</span></td><td style="text-align: left;">the joint distribution over <span class="math inline">\(\mathcal{X}_1 \times \mathcal{X}_2 \times \mathcal{Y}\)</span></td></tr></tbody></table><h3 id="idea">Idea</h3><ul><li>Under approximate condition independence (CI) (quantified by the norm of a certain partial covariance matrix), show similar sample complexity improvements.</li><li>Testify pretext task helps when CI is approximately satisfied in text domain.</li><li>Demonstrate on a real-world image dataset that a pretext task-based linear model outperforms or is comparable to many baselines.</li></ul><h3 id="formalize-ssl-with-pretext-task">Formalize SSL with pretext task</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111171426.png" alt="image-20210111171415185" style="zoom:50%;" /></p><p>It will be estimated by:</p><ul><li><strong>approximation erro</strong>r:<img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173318.png" alt="image-20210111173318840" style="zoom:33%;" />, where <span class="math inline">\(f^*=\mathbb{E}[Y|X_1]\)</span> is the optimal predictor for the task</li><li><strong>estimation error</strong>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173520.png" alt="image-20210111173520930" style="zoom:25%;" />, it's the difference between Predicting <span class="math inline">\(Y\)</span> directly by <span class="math inline">\(X_1\)</span> and Predicting by the representations from pretext task</li></ul><h2 id="experiments">Experiments</h2><ul><li></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks</li><li>Quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) <strong>allows us to learn representations that can solve the downstream task with drastically reduced sample complexity</strong> by just training a linear layer on top of the learned representation.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2008.01064.pdf&quot;&gt;Predicting What You Already Know Helps: Provable Self-Supervised Learning&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Colorful Image Colorization</title>
    <link href="http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html</id>
    <published>2021-01-10T22:15:39.000Z</published>
    <updated>2021-01-12T21:19:21.259Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1603.08511.pdf">Colorful Image Colorization</a></p><p>Code <a href="http://richzhang.github.io/colorization/">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>Predicting colors in free way: taking the image’s <span class="math inline">\(L\)</span> channel as input and its <span class="math inline">\(ab\)</span> channels as the supervisory signal--&gt; but tend to look desaturated, one explanation is using loss functions that encourage conservative predictions</p><ul><li><p>Non-parametric methods: given an input grayscale image, first define one or more color reference images. Then, transfer colors onto the input image from analogous regions of the reference image(s).</p></li><li><p>Parametric methods: learn prediction functions from large datasets of color images at training time, posing the problem as either regression onto continuous color space or classification of quantized color values. --&gt; Work in this paper is also classification task.</p></li><li><p>Concurrent work on colorization</p><table><colgroup><col style="width: 10%" /><col style="width: 37%" /><col style="width: 45%" /><col style="width: 6%" /></colgroup><thead><tr class="header"><th>Paper</th><th style="text-align: center;">loss</th><th>CNNs</th><th>Dataset</th></tr></thead><tbody><tr class="odd"><td>Larsson et al.</td><td style="text-align: center;">un-rebalanced classification loss</td><td>hypercolumns on a VGG</td><td>ImageNet</td></tr><tr class="even"><td>Iizuka et al.</td><td style="text-align: center;">regression loss</td><td>two-stream architecture in which fuse global and local features</td><td>Places</td></tr><tr class="odd"><td>This paper</td><td style="text-align: center;">classification loss, with rebalanced rare classes,</td><td>a single-stream, VGG-styled network with added depth and dilated convolutions</td><td>ImageNet</td></tr></tbody></table></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Even in gray images, the semantics of the scene and its surface texture provide ample cues for many regions in each image</li><li>Color prediction is inherently multimodal --&gt; sparks for a loss tailored to their work</li></ul></li><li>Limitations:<ul><li>Loss only cares Euclidean distance: If an object can take on a set of distinct ab values, the optimal solution to the Euclidean loss will be the mean of the set. In color prediction, this averaging effect favors grayish, desaturated results. Additionally, if the set of plausible colorizations is non-convex, the solution will in fact be out of the set, giving implausible results.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Design colorization based pretext task to get a good image semantic representations: <strong>produce a plausible colorization that could potentially fool a human observer</strong></p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li><strong>Produce vibrant colorization</strong>: Predict a distribution of possible colors for each pixel. Then, re-weight the loss at training time to emphasize rare colors. This encourages the model to exploit the full diversity of the large-scale data on which it is trained. Lastly, produce a final colorization by taking the annealed mean of the distribution.</li><li>Evaluate synthesized images: set up a “colorization Turing test”.</li></ul><h3 id="data-preparation">Data Preparation</h3><ul><li>Quantize the <span class="math inline">\(ab\)</span> output space into bins with grid size <span class="math inline">\(10\)</span> and keep the <span class="math inline">\(Q = 313\)</span> values which are in-gamut. Then this is the label <span class="math inline">\(Z\)</span> of each pixel. Formally, denote the raw label as <span class="math inline">\(Y\)</span>, then <span class="math inline">\(Z = H^{−1}_{gt} (Y)\)</span>, which converts ground truth color <span class="math inline">\(Y\)</span> to vector <span class="math inline">\(Z\)</span>.</li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Model</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110175555.png" alt="image-20210110175554683" style="zoom:30%;" /></p></li><li><p>Loss function</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210112160157.png" alt="image-20210110180438146" style="zoom:33%;" /></p><p>where <span class="math inline">\(v(·)\)</span> is a weighting term that can be used to re-balance the loss based on color-class rarity.</p><ul><li><p>Re-balancing</p><p>The distribution of <span class="math inline">\(ab\)</span> values in natural images is strongly biased towards values with low <span class="math inline">\(ab\)</span> values.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110233506.png" alt="image-20210110233506301" style="zoom:33%;" />, where <span class="math inline">\(\tilde{p}\)</span> the empirical probability of colors in the quantized ab space <span class="math inline">\(p\in \Delta Q\)</span> from the full ImageNet training set and smooth the distribution with a Gaussian kernel <span class="math inline">\(G_{\sigma}\)</span>. <span class="math inline">\(\lambda=\frac{1}{2}, \sigma=5\)</span> worked well.</p></li></ul></li><li><p>Inferring point estimates</p><p>map probability distribution <span class="math inline">\(\hat{Z}\)</span> to color values <span class="math inline">\(\hat{Y}\)</span> with function <span class="math inline">\(\hat{Y} = H(\hat{Z})\)</span></p></li></ul><p>They interpolate by re-adjusting the temperature <span class="math inline">\(T\)</span> of the softmax distribution, and taking the mean of the result. Lowering the temperature <span class="math inline">\(T\)</span> produces a more strongly peaked distribution, and setting <span class="math inline">\(T\rightarrow 0\)</span> results in a 1-hot encoding at the distribution mode. They find that <span class="math inline">\(T=0.38\)</span> captures the vibrancy of the mode while maintaining the spatial coherence of the mean.</p><h2 id="experiments">Experiments</h2><ul><li>Dataset: ImageNet</li><li>Base models</li></ul><table style="width:100%;"><colgroup><col style="width: 11%" /><col style="width: 41%" /><col style="width: 47%" /></colgroup><thead><tr class="header"><th>Model Name</th><th style="text-align: center;">Loss</th><th>Train</th></tr></thead><tbody><tr class="odd"><td>Ours(full)</td><td style="text-align: center;">classification loss</td><td>from scratch with kmeans initialization, ADAM solver for about 450K iterations. <span class="math inline">\(\beta_1 = .9, \beta_2 = .99\)</span>, and weight decay = <span class="math inline">\(10^{−3}\)</span> . Initial learning rate was <span class="math inline">\(3 × 10^{−5}\)</span> and dropped to <span class="math inline">\(10^{−5}\)</span> and <span class="math inline">\(3 × 10^{−6}\)</span> when loss plateaued, at 200k and 375k iterations, respectively.</td></tr><tr class="even"><td>Ours(class)</td><td style="text-align: center;">classification loss withou rebalancing (<span class="math inline">\(\lambda=1\)</span>)</td><td>similar training protocol as Ours(full)</td></tr><tr class="odd"><td>Ours(L2)</td><td style="text-align: center;">L2 regression loss</td><td>same training protocol</td></tr><tr class="even"><td>Ours(L2,ft)</td><td style="text-align: center;">L2 regression loss</td><td>fine tuned from our full classification with rebalancing network</td></tr><tr class="odd"><td>Larsson et al.</td><td style="text-align: center;"></td><td>CNN method</td></tr><tr class="even"><td>Dahl</td><td style="text-align: center;">L2 regression loss</td><td>a Laplacian pyramid on VGG features</td></tr><tr class="odd"><td>Gray</td><td style="text-align: center;">--</td><td>every pixel is gray, with <span class="math inline">\((a, b) = 0\)</span></td></tr><tr class="even"><td>Random</td><td style="text-align: center;">--</td><td>Copies the colors from a random image from the training set</td></tr></tbody></table><h3 id="colorization-quality">Colorization quality</h3><ul><li>AMT: participants confirm their results. They argue that their work produce a more prototypical appearance for those are poorly white balanced</li><li>Semantic interpretability (VGG classification): Are the results realistic enough colorizations to be interpretable to an off-the-shelf object classifier? They check it by by feeding their fake colorized images to a VGG network that was trained to predict ImageNet classes from real color photos.<ul><li>The result is <span class="math inline">\(3.4\%\)</span> lower than Larsson's.</li><li>Without any additional training or fine-tuning, one can improve performance on grayscale image classification, simply by colorizing images with our algorithm and passing them to an off-the-shelf classifier.</li></ul></li><li>Raw accuracy (AuC):<ul><li>L2 metric can achieve accurate colorizations, but has difficulty in optimization from scratch</li><li>class-rebalancing in the training objective achieved its desired effect</li></ul></li><li>Compared with others<ul><li>LEARCH: On SUN dataset, authors have <span class="math inline">\(17.2\%\)</span> on AMT task while LEARCH has <span class="math inline">\(9.8\%\)</span></li></ul></li></ul><h3 id="cross-channel-encoding-as-ssl-feature-learning">Cross-channel encoding as SSL Feature learning</h3><ul><li>Datasets: ImageNet, PASCAL (fine tuned after training on ImageNet)</li><li>Backbone: AlexNet</li><li>Settings<ul><li>ImageNet: fixing the extractor and retrain the classifier (softmax layer) by labels</li><li>PASCAL: : (1) keeping the input grayscale by disregarding color information (Ours (gray)) and (2) modifying conv1 to receive a full 3-channel <span class="math inline">\(Lab\)</span> input, initializing the weights on the <span class="math inline">\(ab\)</span> channels to be zero (Ours (color)).</li></ul></li><li>Summary<ul><li>For ImageNet, there is a <span class="math inline">\(6\%\)</span> performance gap between color and grayscale inputs. Except for the 1st layer, representations from other deeper layers catch and outperform most methods, indicating that <strong>solving the colorization task encourages representations that linearly separate semantic classes in the trained data distribution</strong></li><li>On PASCAL, when conv1 is frozen, the network is effectively only able to interpret grayscale images.</li></ul></li></ul><h3 id="the-properties-of-network">The properties of network</h3><ul><li><p>Is it exploiting low-level cues?</p><p>Given a grayscale Macbeth color chart as input, it was unable to recover its colors. On the other hand, given two recognizable vegetables that are roughly <strong>isoluminant</strong>, <strong>the system is able to recover their color</strong>.</p></li><li><p>Does it learn multimodal color distributions ?</p><p>Take effective dilation ( the spacing at which consecutive elements of the convolutional kernel are evaluated, relative to the input pixels, and is computed by the product of the accumulated stride and the layer dilation) as the measurement. Through each convolutional block from conv1 to conv5, the effective dilation of the convolutional kernel is increased. From conv6 to conv8, the effective dilation is decreased</p></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Designing an appropriate objective function that handles the multimodal uncertainty of the colorization problem and captures a wide diversity of colors</li><li>Introducing a novel framework for testing colorization algorithms, potentially applicable to other image synthesis tasks</li><li>Setting a new high-water mark on the task by training on a million color photos.</li><li>Introduce the colorization task as a competitive and straightforward method for self-supervised representation learning, achieving state-of-the-art results on several benchmarks.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1603.08511.pdf&quot;&gt;Colorful Image Colorization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;http://richzhang.github.io/colorization/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Revisiting Self-Supervised Visual Representation Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html</id>
    <published>2021-01-09T21:21:00.000Z</published>
    <updated>2021-01-12T19:33:22.637Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf">Revisiting Self-Supervised Visual Representation Learning</a></p><p>Code <a href="https://github.com/google/revisiting-self-supervised">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>robotics: the result of interacting with the world, and the fact that multiple perception modalities simultaneously get sensory inputs are strong signals for pretext</li><li>videos: the synchronized cross-modality stream of audio, video, and potentially subtitles, or of the consistency in the temporal dimension</li><li>image datasets:<ul><li>Patch-based methods: E.g.: predicting the relative location of image patches; "jigsaw puzzle"</li><li>Image-level classification tasks:<ul><li>RotNet, create class labels by clustering images, image inpaiting, image colorization, split-brain and motion segmentation prediction;</li><li>Enforce structural constraints on the representation space: an equivariance relation to match the sum of multiple tiled representations to a single scaled representation; predict future patches in via autoregressive predictive coding</li><li>Combining multiple pretext task: E.g. extend the “jigsaw puzzle” task by combining it with colorization and inpainting; Combining the jigsaw puzzle task with clustering-based pseudo labels ( Jigsaw++) ; make one single neural network learn all of four different SSL methdos in a multi-task setting; combined the selfsupervised loss GANs objective</li></ul></li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Expensive labeled data for supervised task</li></ul></li><li>Limitations:<ul><li>Previous works mostly concentrate on pretext task, but didn't pay much attention to the choice of backbones etc.</li></ul></li></ul><h2 id="goals">Goals</h2><p><strong>An optimal CNN architecture for pretext task</strong>, investigating the influence of architecture design on the representation quality.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>a comparison of different self-supervision methods using a unified neural network architecture, but with the goal of combining all these tasks into a single self-supervision task</li></ul><h3 id="implementation">Implementation</h3><h4 id="family-of-cnns">Family of CNNs</h4><ul><li><p>variants of ResNet:</p></li><li><p><strong>ResNet50</strong>, the output before task-specific logits layer is named as <span class="math inline">\(pre-logits\)</span>. explore <span class="math inline">\(k \in \{4, 8, 12, 16\}\)</span>, resulting in pre-logits of size <span class="math inline">\(2048, 4096, 6144\)</span> and <span class="math inline">\(8192\)</span> respectively. <span class="math inline">\(k\)</span> is the widening factor.</p></li><li><p><strong>ResNet v1</strong>: ???batch normalization (BN) right after each convolution and before activation???</p></li><li><p><strong>ResNet v2</strong>: ?</p></li><li><p><strong>ResNet (-)</strong>: without ReLU preceding the global average pooling</p></li><li><p>a batch-normalized <strong>VGG</strong> architecture since VGG is structurally close to AlexNet. BN between CNN and activation, VGG19.</p></li><li><p><strong>RevNets</strong>: stronger invertibility guarantees so as to compare with ResNets. The residual unit used here is equivalent to double application of the residual unit.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110133429.png" alt="image-20210110133429332" style="zoom:50%;" />, check <a href="https://papers.nips.cc/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf">here</a> for details. Apart from this slightly complex residual unit, others are the same as ResNet.</p></li></ul><h4 id="family-of-pretext-tasks">Family of pretext tasks</h4><ul><li><strong>Rotation</strong>: same as RotNet, <span class="math inline">\(\{0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}\}\)</span></li><li><strong>Exemplar</strong>: triplet loss</li><li><strong>Jigsaw</strong>: recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches was performed. Patches are sampled with a random gap between them. Each patch is then independently converted to grayscale with probability <span class="math inline">\(\frac{2}{3}\)</span> and normalized to zero mean and unit standard deviation. Extract final image representations by averaging representations of 9 cropped patches.</li><li><strong>Relative patch location</strong>: predicting the relative location of two given patches of an image. Extract final image representations by averaging representations of 9 cropped patches.</li></ul><h4 id="evaluation-of-the-quality-of-learned-representations">Evaluation of the quality of learned representations</h4><ul><li>Idea: <strong>Using learned representations for training a linear logistic regression model to solve multiclass image classification tasks</strong> (downstream tasks). All representations come from pre-logits level.</li><li>Details: the linear logistic regression model is trained by L-BFGS. But for comparison, using SGD with momentum and use data augmentation during training.</li></ul><h2 id="experiments">Experiments</h2><ul><li>Datasets</li></ul><table><colgroup><col style="width: 12%" /><col style="width: 15%" /><col style="width: 72%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Train</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>ImageNet</td><td style="text-align: center;">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr><tr class="even"><td>Places 205</td><td style="text-align: center;">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr></tbody></table><h3 id="pretext-cnns-downstream">Pretext? CNNs? Downstream?</h3><ul><li><p>Pretext and its preferred CNN architecture: <strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110151545.png" alt="image-20210110151545316" style="zoom:30%;" /></p></li><li><p>The generalization of representations from pretext tasks: each pretext task can be generalized to other dataset. Check the trendings in figure 2.</p></li><li><p>Optimal CNNs for Pretext and downstream tasks: not consistent. But after selecting the right architecture for each self-supervision and increasing the widening factor, models significantly outperform previously reported results.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155158.png" alt="image-20210110155157874" style="zoom:45%;" /></p></li><li><p><strong>Better performance on the pretext task does not always translate to better representations</strong>: Performance on pretext cannot be used to reliably select the model architecture.</p></li></ul><h3 id="cnns-architecture">CNNs architecture</h3><ul><li>Skip-connection: For VGG, representations deteriorate towards the end of the network cause models specialize to the pretext task in the later layers. ResNet prevent this deterioration. They argue that this is because <strong>ResNet’s residual units being invertible under some conditions</strong> and confirm this by RevNet.</li><li>Depth of CNNs: For residual architectures, the pre-logits are always best.</li><li>Model-width and representation size:<ul><li>whether the increase in performance is due to increased network capacity or to the use of higher-dimensional representations, or to the interplay of both? To answer it, authors disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer.</li><li><strong>Model-width and representation size both matter independently, and larger is always better.</strong></li><li>SSL techniques are likely to <strong>benefit from using CNNs with increased number of channels</strong> across wide range of scenarios, even under low-data regime.</li></ul></li></ul><h3 id="evaluate-the-quality-of-representations">Evaluate the quality of representations</h3><ul><li><strong>A linear model is adequate</strong>: MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged</li></ul><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155830.png" alt="image-20210110155829820" style="zoom:50%;" /></p><ul><li>To train <strong>linear model, SGD optimization hyperparameters:</strong> very long training (≈ 500 epochs) results in higher accuracy</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Most affect performance in the fully labeled setting, may significantly affect performance in the selfsupervised setting.</li><li>the quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</li><li>Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations</li><li>The evaluation procedure, where a linear model is trained on a fixed visual representation using stochastic gradient descent, is sensitive to the learning rate schedule and may take many epochs to converge</li><li><strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong>--&gt;<strong>pretext tasks for self-supervised learning</strong> should not <strong>be considered</strong> in isolation, but <strong>in conjunction with underlying architectures</strong>.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&quot;&gt;Revisiting Self-Supervised Visual Representation Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/google/revisiting-self-supervised&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Representation Learning by Predicting Image Rotations</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html</id>
    <published>2021-01-09T04:11:12.000Z</published>
    <updated>2021-01-12T20:56:55.126Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1803.07728.pdf">Unsupervised Representation Learning by Predicting Image Rotations</a></p><p>Codes <a href="https://github.com/gidariss/FeatureLearningRotNet">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>How to get <strong>a high-level image semantic representation using unlabeled data</strong></p><ul><li>SSL: defines an annotation free pretext task, has been proved as good alternatives for transferring on other vision tasks. E.g.: colorize gray scale images, predict the relative position of image patches, predict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames.</li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li><strong>the attention maps are equivariant w.r.t. the image rotations, check appendix A.</strong></li></ul></li><li>Limitations<ul><li>supervised feature learning has the main limitation of requiring intensive manual labeling effort</li></ul></li></ul><h2 id="goals">Goals</h2><p>Provide a "self-supervised" formulation for image data, a <strong>self defined supervised task involving predicting the transformations used for image.</strong> The model won't have access to the initial image.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>Concretely, define the geometric transformations as the image rotations by 0, 90, 180, and 270 degrees. Thus, the ConvNet model is trained on the 4-way image classification task of recognizing one of the four image rotations.</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li><p>2D image Rotation</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109124726.png" alt="image-20210109124726420" style="zoom:40%;" /></p><table><thead><tr class="header"><th>Operations</th><th style="text-align: center;">Implementation</th></tr></thead><tbody><tr class="odd"><td>+90</td><td style="text-align: center;">transpose then flip vertically</td></tr><tr class="even"><td>+180</td><td style="text-align: center;">flip vertically then flip horizontally</td></tr><tr class="odd"><td>+270</td><td style="text-align: center;">flip vertically then transpose</td></tr></tbody></table></li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109123815.png" alt="image-20210109123815880" style="zoom:45%;" />, where <span class="math inline">\(F(\cdot)\)</span> is the predicted probability of the geometric transformation with label <span class="math inline">\(y\)</span> and <span class="math inline">\(\theta\)</span> are the learnable parameters of model <span class="math inline">\(F(\cdot)\)</span>, and <span class="math inline">\(g(X_i|y)\)</span> is the transformed image with transformation <span class="math inline">\(y\)</span>.</p></li><li><p><strong>Why does it works?</strong></p><ul><li>To work for this pretext task, extractor has to <strong>understand the concept of the objects depicted in the image</strong>. Models must learn to localize salient objects in the image, recognize their orientation and object type, and then relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images.</li><li>Easy to be implemented by flipping and transpose, no chance for importing low-level visual artifacts so as to avoid trivial features (which have no practical value)</li><li>Operations are easy to be recognized manually.</li></ul></li></ul><h2 id="experiments">Experiments</h2><h3 id="cifar-object-recognition">CIFAR: object recognition</h3><ul><li>Dataset:</li></ul><table><thead><tr class="header"><th>Datasets</th><th>Preprocess</th></tr></thead><tbody><tr class="odd"><td>CIFAR-10</td><td>Rotations</td></tr></tbody></table><ul><li>Training: SGD with batch size 128, momentum 0.9, weight decay <span class="math inline">\(5e−4\)</span> and <span class="math inline">\(lr\)</span> of 0.1. We drop the learning rates by a factor of 5 after epochs 30, 60, and 80. 100 epochs. Each time feeding with all 4 images.</li><li>Summary<ul><li><em>The learned feature hierarchies</em>: convnet with different number of layers. <strong>Representations from the 2nd block</strong> are good, and <strong>increasing the total depth</strong> of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers.</li><li><em>The quality of the learned features w.r.t. the number of recognized rotations</em>: 4 discrete rotations outperform.</li><li><em>Compared with previous work</em> : almost the same as the NIN supervised model. Fine-tuned the unsupervised learned features further improves the classification performance.</li><li><em>Correlation between object classification task and rotation prediction task</em>: The representations from pretext make classifier converge faster compared with the classifier trained from scratch.</li><li><em>Semi-supervised setting</em>: pretrained on the whole dataset without labels, then fine-tuned on a small labeled subset. It exceeds the supervised model when the number of examples per category drops below 1000.</li></ul></li></ul><h3 id="others-classification-object-detection-segmentation">Others: classification, object detection , segmentation</h3><ul><li><p>Dataset: ImageNet, Places, and PASCAL VOC.</p><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th>Task</th><th>Datasets</th></tr></thead><tbody><tr class="odd"><td>Classification</td><td>Pretrained on ImageNet, then test on ImageNet, Places, and PASCAL VOC.</td></tr><tr class="even"><td>Object detection</td><td>PASCAL VOC</td></tr><tr class="odd"><td>Object segmentation</td><td>PASCAL VOC</td></tr></tbody></table></li><li><p>Backbones: AlexNet without local response normalization units, dropout units, or groups in the colvolutional layers while it includes batch normalization units after each linear layer</p></li><li><p>Pretrained: on ImageNet, SGD with batch size 192, momentum 0.9, weight decay <span class="math inline">\(5e − 4\)</span> and <span class="math inline">\(lr\)</span> of 0.01. Learning rates are dropped by a factor of 10 after epochs 10, and 20 epochs. Trained in total for 30 epochs.</p></li><li><p>Summary:</p><ul><li><p>ImageNet classification task: surpasses all the other unsupervised methods by a significant margin, narrows the performance gap between unsupervised features and supervised features.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154626.png" alt="image-20210109154626599" style="zoom:50%;" /></p></li><li><p>Transfer learning evaluation on PASCAL VOC: fine tuning, used weight rescaling proposed by Krahenbuhl et al.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154831.png" alt="image-20210109154831162" style="zoom:60%;" /></p></li><li><p>Places classification task: the learnt features are evaluated w.r.t. their generalization on classes that were “unseen” during the unsupervised training phase</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154953.png" alt="image-20210109154952935" style="zoom:50%;" /></p></li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Propose a new self-supervised task that is very simple and at the same time.</li><li>Rotationsod under various settings (e.g. semi-supervised or transfer learning settings) and in various vision tasks (i.e., CIFAR-10, ImageNet, Places, and PASCAL classification, detection, or segmentation tasks).</li><li>They argue this self-supervised formulation demonstrates state-of-the-art results with dramatic improvements w.r.t. prior unsupervised approaches, and narrows the gap between unsupervised and supervised feature learning.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1803.07728.pdf&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Codes &lt;a href=&quot;https://github.com/gidariss/FeatureLearningRotNet&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Visual Representation Learning by Context Prediction</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html</id>
    <published>2021-01-08T21:22:12.000Z</published>
    <updated>2021-01-12T19:33:36.144Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf">Unsupervised Visual Representation Learning by Context Prediction</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>How to get <strong>a good image representation</strong></p><ul><li>The latent variables of an appropriate generative model. --&gt; generative models<ul><li>But given an image, inferring the latent structure is <strong>intractable</strong> for even relatively simple models --&gt; to fix, use <strong>sampling</strong> to perform approximate inference.</li></ul></li><li>An embedding that can discriminate the semantics in images by distances of them. -- create a supervised "pretext" task. But hard to tell whether the predictions themselves are correct.<ul><li>Reconstruction-based: E.g., denoising autoencoders (reconstruction ), sparse autoencoders (reconstruction + sparsity penalty )</li><li>Context prediction: "skip-gram" to "filling the blank" task, and convert the prediction task to discriminate task like discriminating between real images vs. images where one patch has been replaced by a random patch from elsewhere in the dataset. But not hard enough for high-level representations</li><li>Discover object categories using hand-crafted features and various forms of clustering. But they will lose shape information. To keep more shape information, some take contour extraction or defining similarity metrics.</li><li>Video-based: since the identity of objects remains unchanged -</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>the difficulties for generalizing CNNs models on Internet -scale datasets</li><li>context has proven to be a powerful source of automatic supervisory signal for learning representations --&gt; context can be regarded as a 'pretext' task to force the model to learn a good word embedding</li><li>current reconstruction-based algorithms struggle with low-level phenomena, like stochastic textures, making it hard to even measure whether a model is generating well.</li></ul></li><li>Limitations:<ul><li>generative models are rather efficiently on smaller datasets but burden on high-resolution natural images</li><li>Some are too simple for extracting high-level representations</li><li>Hard to tell whether the model has obtained good representations.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Provide a "self-supervised" formulation for image data, a supervised task involving predicting the context for a patch.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>Hypothesis: Doing well on predicting patches' positions requires understanding scenes and objects--&gt; a good visual representation</li><li>Concretely, sample random pairs of patches in one of eight spatial configurations, and present each pair to a machine learner. The algorithm must then guess the position of one patch relative to the other.</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li><p>Two patches are fed into network</p></li><li><p>Given an image, one patch will be sampled uniformly, then according to the position of this sampled patch, then 2nd patch will be sampled randomly from the eight possible neighboring locations.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108181759.png" alt="image-20210108181757911" style="zoom:33%;" /></p></li><li><p>including a gap between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</p></li><li><p>For some images ( chromatic aberration), after solving the relative location task (like by detecting the separation between green and magenta (red + blue). ), this problem will be relaxed.</p><ul><li>Shift green and magenta toward gray</li><li>Color dropping : randomly drop 2 of the 3 color channels from each patch and replace them by gaussian noise.</li></ul></li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Siamese network based on AlexNet. But not all layers share weights, LRN (local response normalization ) layers won't.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108183110.png" alt="image-20210108183110662" style="zoom:33%;" /></p></li><li><p><strong>Why does it works?</strong></p><ul><li>Avoid 'trivial' shortcuts (like boundary patterns or textures continuing between patches)--&gt; including a gap (up to 48 pixels) between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</li><li>Enhancing performance on images with chromatic aberration.</li></ul></li></ul><h2 id="experiments">Experiments</h2><p>Pre-Training: SGD+BN+high momentum, 4 weeks on K40 GPU.</p><table><colgroup><col style="width: 8%" /><col style="width: 28%" /><col style="width: 63%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Resizing</th><th>Preprocess</th></tr></thead><tbody><tr class="odd"><td>ImageNet</td><td style="text-align: center;"><span class="math inline">\(150K\sim450K\)</span> total pixels</td><td>1. sample patches at resolution <span class="math inline">\(96\times 96\)</span><br />2. mean subtraction, projecting or dropping colors, and randomly downsampling some patches to as little as 100 total pixels, and then upsampling it.</td></tr></tbody></table><h3 id="ability-on-semantic">Ability on semantic</h3><p>Does it get similar representations for patches with similar semantics?</p><ul><li><p>check nearest neighbors by normalized correlation of <span class="math inline">\(fc6\)</span>'s output. Compared with results from random initialized model and ImageNet AlexNet.</p></li><li><p>Summary</p><ul><li>in a few cases, random (untrained) ConvNet also does reasonably well</li><li>the representations from proposed model often capture the semantic information</li></ul></li></ul><h3 id="learnability-of-chromatic-aberration">Learnability of Chromatic Aberration</h3><ul><li>Patches displayed similar aberration tend to be predicted at the same location.</li><li>The effect of color projection operation is canceled for this kind of images.</li></ul><h3 id="object-detection">Object detection</h3><ul><li>Dataset : VOC 2007</li><li>Train: fine-tune the pretrained model (model is slight different with the previous one considering the image size in VOC) on VOC 2007.</li><li>Test: output from <span class="math inline">\(fc7\)</span> is taken.</li><li>Summary<ul><li>Pre-trained model outperforms the one trained from scratch</li><li>Obtained the best result on VOC 2007 without using labels</li><li>Robustness of the representations for one object in different datasets: acceptable</li></ul></li></ul><h3 id="visual-data-mining">Visual data mining</h3><ul><li>Task : aims to use a large image collection to discover image fragments which happen to depict the same semantic objects</li><li>Specification for this task: sample a constellation of four adjacent patches from an image, after finding the top 100 images which have the strongest matches for all four patches, then use a type of geometric verification to filter away the images where the four matches are not geometrically consistent. Finally, rank the different constellations by counting the number of times the top 100 matches geometrically verify.</li><li>To define the geometric verification: first compute the best-fitting square <span class="math inline">\(S\)</span> to the patch centers (via least-squares), while constraining that side of <span class="math inline">\(S\)</span> be between 2/3 and 4/3 of the average side of the patches. Then compute the squared error of the patch centers relative to <span class="math inline">\(S\)</span> (normalized by dividing the sum-of-squared-errors by the square of the side of <span class="math inline">\(S\)</span>). The patch is geometrically verified if this normalized squared error is less than 1.</li><li>Test: VOC 2011, Street View images from Paris</li><li>Summary<ul><li>The discovery of birds and torsos is good</li><li>The gains in terms of coverage, suggesting increased invariance for learned features</li><li>The pretext task is difficult: for a large fraction of patches within each image, the task is almost impossible</li><li>Limitations: some loss of purity, and cannot currently determine an object mask automatically (although one could imagine dynamically adding more sub-patches to each proposed object).</li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>instance-level supervision appears to improve performance on category-level tasks</li><li>The proposed model is sensitive to objects and the layout of the rest of the image</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf&quot;&gt;Unsupervised Visual Representation Learning by Context Prediction&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
    <link href="http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html</id>
    <published>2021-01-06T16:17:12.000Z</published>
    <updated>2021-01-12T20:58:39.413Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1406.6909.pdf">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Supervised : labeled data with a specific CNN<ul><li>directly penalizing the derivative of the output with respect to the magnitude of the transformations, but will be sensitive to the magnitude of the applied transformation.</li></ul></li><li>Unsupervised: learning invariant representations<ul><li>Directly modeling the input distribution and are hard for jointly training multiple layers of a CNN<ul><li><strong>autoencoders</strong>: denoising auto encoders, say reconstruct data from randomly perturbed input samples; or learn representations from videos by enforcing a temporal slowness constraint on the feature representation learned by a linear autoencoder.</li><li>invariant to local transformations</li></ul></li><li>most aims at regularization of the latent representation</li></ul></li><li>Semi-supervised<ul><li>Regularization supervised algorithms by unlabeled data: self-training, entropy regularization</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>the features learned by one network often generalize to new datasets</li><li>a network can be adapted to a new task by replacing the loss function and possibly the last few layers of the network and fine-tuning it to the new problem</li></ul></li><li>Limitations:<ul><li>the need for huge labeled datasets to be used for the initial supervised training</li><li>the transfer becomes less efficient the more the new task differs from the original training task</li></ul></li></ul><h2 id="goals">Goals</h2><p>a more general extractor using unlabeled data. The extractor should satisfy two requirements:</p><ul><li>there must be at least one feature that is similar for images of the same category <span class="math inline">\(y\)</span> (invariance);</li><li>there must be at least one feature that is sufficiently different for images of different categories (ability to discriminate)</li></ul><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>creating an auxiliary task + invariant features to transformations</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li>Do random selected transformation (from a predefined family of transformations) for sampled patches (regions containing considerable gradients so that sample a patch with probability proportional to mean squared gradient magnitude within the patch )</li><li>The family of transformations<ul><li>translation</li><li>scaling</li><li>rotation</li><li>contrast: PCA and HSV</li><li>color: works on HSV space</li><li>blur etc.</li></ul></li><li>Before feeding into model, do normalization (subtract the mean of each pixel over the whole resulting dataset)</li><li>Labeling: all transformed patches from the same seed patch are labeled by the same index</li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function :</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155621.png" alt="image-20210107155618997" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155911.png" alt="image-20210107155854389" style="zoom:30%;" /></p><ul><li><p>After transformations, the loss for a whole class (augmented by the same seed patch ) can be taken as</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107162118.png" alt="image-20210107162118193" style="zoom:50%;" />, notice the 2nd and the 4th can be canceled.</p><ul><li>The 1st term: enforces correct classification of the average representation <span class="math inline">\(\mathbb{E}_\alpha[g(T_\alpha x_i)]\)</span> for a given input sample</li><li>The 2nd term: a regularizer enforcing all $ h(T_x_i)$ to be close to their average value, i.e., the feature representation is sought to be approximately invariant to the transformations $ T_$, note the convergence to global minimum is listed at appendix.</li></ul></li></ul></li><li><p><strong>Why does it works?</strong></p><ul><li>Previous works mostly focus on modeling the input distribution <span class="math inline">\(p(x)\)</span>, based on the assumption that a good model of <span class="math inline">\(p(x)\)</span> contains information about the category distribution <span class="math inline">\(p(y|x)\)</span>. Therefore, to get the invariance, one will do regularization of the latent representation and obtain representation by reconstruction .</li><li>Their work does not directly model the input distribution <span class="math inline">\(p(x)\)</span> but learns a representation that discriminates between input samples. They argue that this <strong>allows more DOF to model the desired variability of a sample and avoid task-unnecessary reconstruction.</strong></li><li>However, their work will <strong>fail on color-relied task</strong></li></ul></li></ul><h2 id="experiments">Experiments</h2><h3 id="classification">Classification</h3><ul><li><p>Datasets: <strong>STL-10</strong>, CIFAR-10, Caltech-101 and Caltech-256. report mean and standard deviation</p><table style="width:100%;"><colgroup><col style="width: 6%" /><col style="width: 24%" /><col style="width: 35%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Resizing</th><th>Train</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>STL-10</td><td style="text-align: center;">64c5-64c5-128f</td><td>10 pre-defined folds of the training data</td><td>fixed test set</td></tr><tr class="even"><td>CIFAR-10</td><td style="text-align: center;">resize from <span class="math inline">\(32\times 32\)</span> to <span class="math inline">\(64\times 64\)</span></td><td>1. whole training set<br />2. 10 random selections of 400 training samples per class</td><td>1. results on CIFAR-10<br />2. average results on 10 sets.</td></tr><tr class="odd"><td>Caltech-101</td><td style="text-align: center;">to <span class="math inline">\(150\times 150\)</span></td><td>30 random samples per class</td><td>not more than 50 samples per class</td></tr><tr class="even"><td>Caltech-256</td><td style="text-align: center;"><span class="math inline">\(256\times 256\)</span></td><td>randomly selected 30 samples per class</td><td>those except for training</td></tr></tbody></table></li><li><p>backbones of network</p><table><thead><tr class="header"><th>Network</th><th style="text-align: center;">Structure</th><th>Training</th></tr></thead><tbody><tr class="odd"><td>small</td><td style="text-align: center;">64c5-64c5-128f</td><td>1.5 days, SGD with fixed momentum of 0.9</td></tr><tr class="even"><td>medium</td><td style="text-align: center;">64c5-128c5-256c5-512f</td><td>4 days, SGD with fixed momentum of 0.9</td></tr><tr class="odd"><td>large</td><td style="text-align: center;">92c5-256c5-512c5-1024f</td><td>9 days, SGD with fixed momentum of 0.9</td></tr></tbody></table></li><li><p>Training: learning rate starts at 0.01， then when there was no improvement in validation error, decreased the learning rate by a factor of 3. All networks are trained on one Titan</p></li><li><p>Test features: one-vs-all linear SVM.</p></li><li><p>Summary</p><ul><li>with increasing feature vector dimensionality and number of labeled samples, training an SVM becomes less dependent on the quality of the features</li><li>Relation of <strong>the number of surrogate classes</strong> : <strong>sampling too many, too similar images for training can even decrease the performance of the learned features</strong>. ( the discriminative loss is no longer reasonable with too many similar surrogate classes.) --&gt; fix: e.g. clustering the output features then do augmentation for clusters and feed these augmented classes as surrogate data</li><li>Relation of <strong>the number of samples per surrogate class</strong> : around 100 samples is sufficient</li><li>Relation of <strong>types of transformations </strong> : each time remove a group of transformations and check how the performance is decreased , e.g. scaling, rotation etc. Translations, color variations and contrast variations are significantly more important. For the matching task, using blur as an additional transformation improves the performance.</li><li>Relation of <strong>Influence of the dataset </strong>: the learned features generalize well to other datasets</li><li>Relation of <strong>Influence of the Network Architecture on Classification Performance</strong>: Classification accuracy generally improves with the network size</li></ul></li></ul><h3 id="descriptor-matching">Descriptor matching</h3><ul><li>Task: Matching of interest points</li><li>Datasets: by Mikolajczyk et al., augmented by applying 6 different types of transformations with varying strengths to 16 base images from Flickr. In addition to the transformations used before, also change the lighting and blur .</li><li>Backbones: 64c7s2-128c5-256c5-512f, named as Exemplar-CNN-blur</li><li>Training: use unlabeled images from Flickr for training</li><li>Test and measurements: prediction is <span class="math inline">\(TP\)</span> if <span class="math inline">\(IOU\ge 0.5\)</span>. Compared with SIFT and Alexnet</li><li>Summary<ul><li>Optimum patch size (or layer in CNNs): SIFT is based on normalized finite differences, and thus very robust to blurred edges caused by interpolation. In contrast, for the networks, especially for their lower layers, there is an optimal patch size. They argue that features from higher layers have access to larger receptive fields and, thus, can again benefit from larger patch sizes.</li><li>A loss function that focuses on the invariance properties (rather than class-specific features) required for descriptor matching yields better results.</li><li>Features obtained with the unsupervised training procedure outperform the features from AlexNet on both datasets</li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Pretty good on tasks: object classification , descriptor matching</li><li>emphasizes the value of data augmentation in general and suggests the use of more diverse transformations.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1406.6909.pdf&quot;&gt;Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-notes-fourier-GCN.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-notes-fourier-GCN.html</id>
    <published>2019-12-19T11:46:44.411Z</published>
    <updated>2021-01-12T20:59:12.788Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.</p><p>Check <a href="/assets/slides/notes/GCN/fourier-GCN.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.&lt;/p&gt;
&lt;p&gt;Check &lt;a hre
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>What Can Artificial Intelligence Do in Data Assimilation? Dec. 9th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-mlutility.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-mlutility.html</id>
    <published>2019-12-19T11:46:44.394Z</published>
    <updated>2018-12-10T02:06:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my lab, and most of them haven't learn much about AI. The talk would like to show them what can AI do these days, and help them figure out what else can AI do in data assimilation.</p><p>Slides are avaliable <a href="/assets/slides/mlDo/mlUtility.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my l
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="Data assimilation" scheme="http://yoursite.com/tags/Data-assimilation/"/>
    
  </entry>
  
  <entry>
    <title>The naive implementation of some popular machine learning algorithms.</title>
    <link href="http://yoursite.com/posts/projects/2019-12-19-ml-implement.html"/>
    <id>http://yoursite.com/posts/projects/2019-12-19-ml-implement.html</id>
    <published>2019-12-19T11:46:44.387Z</published>
    <updated>2018-12-10T12:57:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>Naive implementations of some M.L. algorithms, which are updated continuously.</p><p>The algorithms that have been implemented are listed as follows:</p><ul><li>Logistic Regression,</li><li>SVM solved by SMO,</li><li>K-Means，</li><li>GMM solved by EM,</li><li>Perceptron，</li><li>Naive Bayes,</li><li>LeNet-Keras，</li><li>MLP-Numpy solved with BP,</li><li>MCMC sampling.</li></ul><p>Codes are available <a href="https://github.com/skaudrey/ml_algorithm">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Naive implementations of some M.L. algorithms, which are updated continuously.&lt;/p&gt;
&lt;p&gt;The algorithms that have been implemented are list
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="naive" scheme="http://yoursite.com/tags/naive/"/>
    
  </entry>
  
  <entry>
    <title>Computing in the 21th Century &amp; Asia Faculty Summit held by Microsoft, Nov., 2018.</title>
    <link href="http://yoursite.com/posts/meetings/2019-12-19-microsoft.html"/>
    <id>http://yoursite.com/posts/meetings/2019-12-19-microsoft.html</id>
    <published>2019-12-19T11:46:44.380Z</published>
    <updated>2018-12-10T08:53:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is <a href="https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda">here</a>.</p><p>Babysitting AI and computational neuroscience impressed me a lot. <a id="more"></a> # computing neuroscience: robots with feelings.</p><p>Prof. Lenore Blue's keynote is about computational neuroscience. They try to let robots feel pain, and to simulate the long and short term encoding happening in our brains.</p><p>What she talked reminds me of something I had read before.</p><p>According to the book <em>Psychology</em> written by Daniel Schacter, our brains do encode information into long and short codes, and the short form is possibly trasformed into a long one. Even though we don't mean to encode or memorize something sometimes, encoding still occurs unconsciously.</p><p>Reviewing is one useful way for recalling these information. Moreover, if you are in the similar environment in which you encoded the codes before, you will have higher possibility to recall it. However, the <strong>encoding error</strong> happend during reviewing is more, and that's why a detective should try to get full information while inquiring evidences from witnesses at the 1st time.</p><p>Also, our brains are more sensible to pictorial information compared with text information. So, if you try to make each thing you want to keep code as a photo, you can boost the capability of memorizing.</p><h1 id="yoshu-bengio-beyond-i.i.d.-and-babysitting-ai">Yoshu Bengio: Beyond i.i.d. and babysitting AI</h1><p>One of the basic assumption that makes generation possible is independent identically distributed assumption. However, influenced by observing equipments, imbalanced samples and others, the distributions of training data and test data are not always the same. Hence, Prof. Bengio's team proposed that all data are sampled from the same system rather than same distribution. As for weather of two different seasons, data desciping them are sampled from the same atmospheric circulation system, but they don't distribute identically. Bengio said they tend to initialize this system with diversed initial conditions, and the result of this distribution will be taken as what the data set follows. It makes sense.</p><p><em>One thing that troubles me is, how can I model the system and figure out the initial conditions? For things with obvious physical rules, it is easy, and even the model's codes are open-acssessed online. What if the one I don't know? How to make this idea works in common situations?</em></p><p>CNN is renowned as its power in representation learning, which encodes a variety of information into vectors. Our brains also work like this. Nontheless, what they learn are supervised, and the utility of binary network, the simplification of network structure all demostrate that the captured information are redundant. The model learned is fragile, too. After adding some noises into an image, even though the image dosen't change visually for us humans, model can not tell what it is as before. It all comes from the uncontrolling unsupervision. Babysitting AI aims at modeling with environmental information and other information, so that leading AI models.</p><h1 id="andrew-c.-yao-the-advent-of-quantum-computing">Andrew C. Yao: The Advent of Quantum Computing</h1><p>The quantum computing will offer exponential speedup for crypto-code breaking, simulation of quantum physical systems, simulation of materials, chemistry, and biology, nonlinear optimization, ML and AI. It will break through the bottleneck of computing.</p><p>Its implementation is like crystallography. In terms of crystallography, you take an X-ray photo for a crystal and then compute its structure. For quantum computing, instead of taking a real photo, you just need to collect a polynomial number of sample points. By wave-particle duality, this single photo can recreate the raw image probabilistically.</p><p>According to Andrew, dimond qubits are in the highest possibility to be used in our laptops.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is &lt;a href=&quot;https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Babysitting AI and computational neuroscience impressed me a lot.
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="applications" scheme="http://yoursite.com/tags/applications/"/>
    
      <category term="tendency" scheme="http://yoursite.com/tags/tendency/"/>
    
  </entry>
  
  <entry>
    <title>Clouds detection of infrared hyperspectral data based on logistic.</title>
    <link href="http://yoursite.com/posts/projects/2019-12-19-lr.html"/>
    <id>http://yoursite.com/posts/projects/2019-12-19-lr.html</id>
    <published>2019-12-19T11:46:44.375Z</published>
    <updated>2018-12-10T13:17:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed with what clouds release, and they exist in more than 90% IFOVs, cloudy IFOVs have to be kicked off in order to get clean data.</p><p>Therefore, a new feature construction method is proposed for infrared hyperspectral data, such as what IASI releases. Concretely, four channels of IASI are picked, namely channel 921, channel 386, channel 306 and channel 241. They are picked because of physical characteristics. And then, cloudy IFOVs are detected by logistic regression.</p><p>The recall, auc and accuracy of this new method carried on IASI data was more than 0.95 when detecting IFOVs of sea, while the result of land's IFOVs was less than it. After adding surface emissivity features, the auc of it increased by aroud 5%, and recall of it grew by 10% approximately.</p><p>Codes are available <a href="https://github.com/skaudrey/cloud">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed 
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="logistic" scheme="http://yoursite.com/tags/logistic/"/>
    
  </entry>
  
  <entry>
    <title>The Introduction of Infrared Hyperspectral Data and Kernel PCA, June 5th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-hyp.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-hyp.html</id>
    <published>2019-12-19T11:46:44.346Z</published>
    <updated>2018-12-10T08:01:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:</p><ul><li>high spectral correlation,</li><li>high spatial correlation,</li><li>and sparsity,</li></ul><p>and they casue a trouble during processing. <a id="more"></a> This talk explained why they are highly correlated but also sparse. Kernel PCA for compressing was also tested.</p><p>Check <a href="/assets/slides/hyp/hypCompression.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;high spectral correlation,&lt;/li&gt;
&lt;li&gt;high spatial correlation,&lt;/li&gt;
&lt;li&gt;and sparsity,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and they casue a trouble during processing.
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="compression" scheme="http://yoursite.com/tags/compression/"/>
    
      <category term="reconstruction" scheme="http://yoursite.com/tags/reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Build up personal blog</title>
    <link href="http://yoursite.com/posts/notes/2019-12-19-hexo+killy+github%20pages=blog.html"/>
    <id>http://yoursite.com/posts/notes/2019-12-19-hexo+killy+github pages=blog.html</id>
    <published>2019-12-19T11:46:44.340Z</published>
    <updated>2021-01-12T21:00:55.975Z</updated>
    
    <content type="html"><![CDATA[<p>This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github. <a id="more"></a></p><h2 id="preliminaries">Preliminaries</h2><p>Before starting, you need: * node.js+npm,</p><pre><code>Get node.js from [here](https://pan.baidu.com/s/1kU5OCOB#list/path=%2Fpub%2Fnodejs). Check [here](https://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000/00143450141843488beddae2a1044cab5acb5125baf0882000) for more info about node.</code></pre><ul><li>hexo, Install by npm: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li><li>git,</li><li>an account of Github,</li></ul><p>and configure the ssh-key on your device.</p><h2 id="build-blog">Build blog</h2><h3 id="initialize-hexo-with-hexo">Initialize hexo with hexo</h3><p>Create a local folder as your root directory, such as "blog", and go to the directory in your terminal and initialize it by hexo. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br><span class="line">$ <span class="built_in">cd</span> blog</span><br></pre></td></tr></table></figure> Then initialize this directory with npm.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="link-hexo-with-github">Link hexo with Github</h3><p>Set deployment tool,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>and initialize the remote repository for your blog on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br><span class="line">$ git add *</span><br><span class="line">$ git commit -m <span class="string">&quot;init commit&quot;</span></span><br></pre></td></tr></table></figure><p>Change the deployment in file "_config.yml" like:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line">deploy: </span><br><span class="line">    <span class="built_in">type</span>: git </span><br><span class="line">    repo: git@github.com:jack/jack.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure><p>Tips: The name of your hosting repository should be "[githubname].github.io", such as "jack.github.io". And mind the blankspaces while rewriting file "_config.yml". ### Generate static files Do it before you push it on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><h1 id="see-your-pages">see your pages</h1><p>Click https://[githubname].github.io/, such as https://jack.github.io/.</p><h2 id="customization">Customization</h2><h3 id="change-theme">change theme</h3><p>I picked theme yilia. Configuration should be done as bellow:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ git <span class="built_in">clone</span> https://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br></pre></td></tr></table></figure><p>Change the default theme defined in "_config.yml" under root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: yilia</span><br></pre></td></tr></table></figure><h3 id="upload-your-avatar">upload your avatar</h3><p>New a folder under the "source" directory, I named it assets. I also new the "img" folder for pictures. Put you avatar picture here. Then reconfigure the _config.yml file beneath theme "yilia"'s folder, which is:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avatar: /assets/img/avatar.jpg</span><br></pre></td></tr></table></figure><h3 id="classify-your-posts-by-categories-rather-than-tags-in-default">classify your posts by categories rather than tags in default</h3><p>Now take your eye away from file "_config.yml" under theme yilia, open the file "_config.yml" under the root directory of your blog. You need to configure category_map, for instance,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  about: about</span><br><span class="line">  notes: notes</span><br><span class="line">  projects: projects</span><br><span class="line">  papers: papers</span><br><span class="line">  talks: talks</span><br><span class="line">  meetings: meetings</span><br></pre></td></tr></table></figure><p>Each pair of it can be different, it is just a mapping, such as:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  parole: talks</span><br><span class="line">  关于我: about</span><br></pre></td></tr></table></figure><h3 id="change-the-naming-rule-of-a-new-post">change the naming rule of a new post</h3><p>The default naming rule of hexo is YYYY/MM/DD/[post name], which leads to a hyper-link without html suffix. I change it as html. It can be accomplished by configure the _config.yml in root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permalink: posts/:category/:year-:month-:day-:title.html</span><br></pre></td></tr></table></figure><h3 id="truncate-the-post-in-home-list-when-it-is-too-long.">Truncate the post in home list when it is too long.</h3><p>You need to add</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure><p>after where you want to trucate in a post. And confiure the __config.yml_ under themes' folder.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The truncate signal while post is too long.</span></span><br><span class="line">excerpt_link: <span class="string">&quot;more&quot;</span></span><br></pre></td></tr></table></figure><h3 id="support-latex">Support Latex</h3><p>Check <a href="https://www.jianshu.com/p/5623c5e35c93">here</a> for details.</p><h1 id="tips">Tips</h1><p>You can debug pages locally by</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>, which is convenient before deployment.</p><h1 id="acknowledgement">Acknowledgement</h1><p>Thank <a href="https://www.cnblogs.com/wumz/p/8030244.html">Mauger</a>, and <a href="https://github.com/litten/hexo-theme-yilia">litten</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>HCR--Compress and Resonstruct Hyperspectral Data.</title>
    <link href="http://yoursite.com/posts/projects/2019-12-19-hcr.html"/>
    <id>http://yoursite.com/posts/projects/2019-12-19-hcr.html</id>
    <published>2019-12-19T11:46:44.333Z</published>
    <updated>2018-12-10T12:45:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression and reconstruction. The numerous infrared hyperspectral data are overloaden for computing resources currently. Taking IASI, an atmosphere detector on satellite Metop launched by European Organization for the Exploitation of Meteorological Satellites (EUMETSAT), as an example, it has 8461 channels, which can detect atmosphere vertically in details. To process these data more efficiently, compressing them and then reconstructing is required.</p><p>Considering their high correlation in spectral and spatial dimension, a new compressing and reconstructing network HCR is proposed. Concretely, the radiation brightness values are gridded so that one value at specific location is recongnized as a color value at this pixel. After normalizing by batch normalization, HCR compresses by convolution and reconstructs by deconvlution.</p><p>Carrying on IASI data, the RMSE of this new method was decreased by 5% at least compared with the result of principle component analysis (PCA) in the same compression ratio. The compression kernels encode tempetature information and reconstruct it. In reconstruction, the kernels' weights for likewise data are similar.</p><p>Codes are available <a href="https://github.com/skaudrey/hyp">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression an
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="cnn" scheme="http://yoursite.com/tags/cnn/"/>
    
      <category term="compress" scheme="http://yoursite.com/tags/compress/"/>
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
  </entry>
  
  <entry>
    <title>The summer school held in Harbin, Aug. 2017.</title>
    <link href="http://yoursite.com/posts/meetings/2019-12-19-harbin.html"/>
    <id>http://yoursite.com/posts/meetings/2019-12-19-harbin.html</id>
    <published>2019-12-19T11:46:44.328Z</published>
    <updated>2018-12-10T07:55:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check <a href="http://mss2017.hit.edu.cn/showSubjectDominWebSite.do">here</a> for more info. I gave a talk to the students in my lab after returning, <a href="https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html">here</a> are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation. <a id="more"></a> The themes given by those professors are listed below.</p><ul><li><p>Prof. Francois</p><p>Research Area：Variational data assimilation (VAR), especially 4DVAR.</p><p>Keynotes：The direvation of adjoint models, sensitivity analysis and the introduction of image assimilation. See the <a href="/assets/notes/harbin/François%20meeting%20minutes.pdf">minutes file</a> for details.</p></li><li><p>Prof. Jordan</p><p>Research Area：Statistical Learning</p><p>Keynotes: Summarize popular machine learning algorithms, and prove the convergence etc. See the <a href="/assets/notes/harbin/Jordan%20meeting%20minutes.pdf">minutes file</a> for details.</p></li><li><p>Prof. Jurgen</p><p>Research Area：AI, DL</p><p>Keynotes: The introduction of utilizing AI. Check more from his <a href="http://people.idsia.ch/~juergen/">home page</a>.</p></li><li><p>Prof. Ma</p><p>Research Area：Compression sensing.</p><p>Keynotes: The introduction of compression sensing and its applications.</p></li><li><p>Prof. Cai</p><p>Research Area：Statistical inference.</p><p>Keynotes: Statistical inference in high-dimensions. No slides, the minutes file is <a href="/assets/notes/harbin/Tony%20meeting%20minutes.pdf">here</a>.</p></li></ul><p>Slides and records are available in the <a href="https://pan.baidu.com/s/1jGj07koiMIV-MOf17N_jeg">baidu network disk</a> with password 6o2x. Enjoy yourself.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check &lt;a href=&quot;http://mss2017.hit.edu.cn/showSubjectDominWebSite.do&quot;&gt;here&lt;/a&gt; for more info. I gave a talk to the students in my lab after returning, &lt;a href=&quot;https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html&quot;&gt;here&lt;/a&gt; are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation.
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="summer school" scheme="http://yoursite.com/tags/summer-school/"/>
    
      <category term="mathematics" scheme="http://yoursite.com/tags/mathematics/"/>
    
  </entry>
  
  <entry>
    <title>Weather processes interpolation based on GPR</title>
    <link href="http://yoursite.com/posts/projects/2019-12-19-gpr.html"/>
    <id>http://yoursite.com/posts/projects/2019-12-19-gpr.html</id>
    <published>2019-12-19T11:46:44.321Z</published>
    <updated>2021-01-12T19:35:17.426Z</updated>
    
    <content type="html"><![CDATA[<p>This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale dependencies of weather processes. Weather processes with and without cyclones are discussed, and two interpolation methods are proposed. Check <a href="http://www.mdpi.com/2073-4433/9/5/194/pdf">paper</a> for more information. Codes are available <a href="https://github.com/skaudrey/gpml">here</a>.</p><h1 id="reference">Reference</h1><pre><code>Carl Edward Rasmussen. Gaussian process for Machine Learning.</code></pre><h1 id="acknowledgement">Acknowledgement</h1><pre><code>Thanks for the opening source toolbox GAUSSIAN PROCESS REGRESSION AND CLASSIFICATION Toolbox version 4.0, programmed by Carl et al.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale depen
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
  </entry>
  
  <entry>
    <title>Multivariate Interpolation of Wind Fields Based on Gaussian Process Regression, Jan. 24th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-gpr.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-gpr.html</id>
    <published>2019-12-19T11:46:44.316Z</published>
    <updated>2018-12-10T02:45:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check the <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html">projects' introduction</a> and <a href="https://github.com/skaudrey/gpml/">github</a> for more details.</p><p>Slides are avaliable <a href="/assets/slides/gpr/windInterpolation.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check th
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
      <category term="interpolation" scheme="http://yoursite.com/tags/interpolation/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks and Remote Sensing, July 26th, 2019.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-gan-rs.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-gan-rs.html</id>
    <published>2019-12-19T11:46:44.306Z</published>
    <updated>2019-08-18T14:16:24.132Z</updated>
    
    <content type="html"><![CDATA[<p>Some works using GANs handle the problems in remote sensing.</p><p>Check <a href="/assets/slides/GAN/GANRS.pdf">slide</a> for more details.``</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Some works using GANs handle the problems in remote sensing.&lt;/p&gt;
&lt;p&gt;Check &lt;a href=&quot;/assets/slides/GAN/GANRS.pdf&quot;&gt;slide&lt;/a&gt; for more deta
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="remote sensing images" scheme="http://yoursite.com/tags/remote-sensing-images/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-da.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-da.html</id>
    <published>2019-12-19T11:46:44.289Z</published>
    <updated>2018-11-17T08:19:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it from other forms of machine learning, image analysis, and statistical methods. This talk discussed the basic ideas of machine leaning, and compared it with machine learning. It is given after I came back from Harbin's summer school in Agust, 2017. After this talk, I began to throw myself into studying machine learning.</p><p>Check <a href="/assets/slides/D.A/pres.pdf">slide</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>notes-DAPPER</title>
    <link href="http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html"/>
    <id>http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html</id>
    <published>2019-10-05T16:52:21.000Z</published>
    <updated>2021-01-12T19:34:37.064Z</updated>
    
    <content type="html"><![CDATA[<p>DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see <a href="https://github.com/nansencenter/DAPPER">here</a>.</p><p>This notes keep records of the problems encountered while using DAPPER.</p><a id="more"></a><ol type="1"><li>Pre-setting <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.</span><br></pre></td></tr></table></figure> Copy mkl_*.dll, libiomp5md.dll and <em>libiomp5md.pdb</em> from directory "./Library/bin" to the root directory of python exe.</li></ol><p>Reference from <a href="https://blog.csdn.net/supertangcugu/article/details/89790617">here</a>.</p><ol start="2" type="1"><li>Manual</li></ol><p>Manual <a href="https://dapper.readthedocs.io/en/latest/implementation.html">online</a></p><p>For EnKF: Nx-by-N (维度数<em>样本数) For ndarrays: N-by-Nx (样本数</em>维度数)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see &lt;a href=&quot;https://github.com/nansencenter/DAPPER&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This notes keep records of the problems encountered while using DAPPER.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="da" scheme="http://yoursite.com/tags/da/"/>
    
  </entry>
  
  <entry>
    <title>Notes of Mathematics</title>
    <link href="http://yoursite.com/posts/notes/2019-06-19-notes-math.html"/>
    <id>http://yoursite.com/posts/notes/2019-06-19-notes-math.html</id>
    <published>2019-06-19T13:40:09.000Z</published>
    <updated>2021-01-21T16:28:25.292Z</updated>
    
    <content type="html"><![CDATA[<h2 id="泛函">泛函</h2><center><img src="/assets/img/Rules/lines.png" width=500"></center><h3 id="不动点定理">不动点定理</h3><ol type="1"><li><p>不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。</p><p><a href="https://zhuanlan.zhihu.com/p/33885648">链接</a></p></li></ol><h3 id="紧性的利用">紧性的利用</h3><ul><li>证明存在性</li><li>在无限维空间中“模仿”有限维的欧式空间</li></ul><p>紧集是为了模仿描述欧式空间中的有界闭集合么？ 紧=相对紧+闭</p><h3 id="流形manifolds">流形(Manifolds)</h3><ol type="1"><li>概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。</li></ol><h3 id="支撑集support">支撑集(Support)</h3><ol type="1"><li>概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合</li></ol><a id="more"></a><h2 id="数学分析">数学分析</h2><h3 id="lipschitz连续">Lipschitz连续</h3><ol type="1"><li>若存在一个常数K，使得定义域内的任意两点x1,x2满足： $ |f(x_1)-f(x_2)|=K|x_1-x_2|$ 则称函数为Lipschitz连续函数。此性质限定了f的导函数的绝对值不超过K，规定了函数的最大局部变动幅度。</li></ol><h2 id="convex-optimization">Convex Optimization</h2><p>Book <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>, <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></p><h3 id="convex-sets">Convex sets</h3><h4 id="affine-sets">Affine sets</h4><ul><li>Affine sets:<ul><li>Definition : A set <span class="math inline">\(C\subseteq R^n\)</span> is affine if the line through any two distinct points in <span class="math inline">\(C\)</span> lies in <span class="math inline">\(C\)</span>, aka for any <span class="math inline">\(x_1 ,x_2\in C\)</span> and <span class="math inline">\(\theta\in R\)</span>, one has <span class="math inline">\(\theta x_1+(1-\theta)x_2\in C\)</span>. It indicates that the <span class="math inline">\(C\)</span> contains the linear combination of any two points in <span class="math inline">\(C\)</span>.</li><li>Induction: If <span class="math inline">\(C\)</span> is an affine set, <span class="math inline">\(x_1，\cdots,x_k\in C\)</span> and <span class="math inline">\(\theta_1+\cdots+\theta_k=1\)</span>, then the point <span class="math inline">\(\theta_1 x_1+\cdots+\theta_kx_k\)</span> also belongs to <span class="math inline">\(C\)</span>.</li></ul></li><li>Affine hull<ul><li>Definition: the set of all affine combinations of points in some set <span class="math inline">\(C\subseteq R^n\)</span>, denoted as <span class="math inline">\(\mathrm{aff}C\)</span></li><li>It's the smallest affine set that contains <span class="math inline">\(C\)</span></li></ul></li><li>Affine dimension<ul><li>Definition: as the dimension of its affine hull.</li><li>E.g.: <span class="math inline">\(\{x\in R^2|x_1 ^2+x_2^2=1\}\)</span>, the affine dimension is 2.</li></ul></li></ul><h4 id="convex-sets-1">Convex sets</h4><ul><li>Definition: If every point in the set can be seen by every other point, along an unobstructed straight path between them, where unobstructed means lying in the set.</li><li>Every affine set is also convex.</li><li>Convex hull, denotes as <span class="math inline">\(\mathrm{conv} C\)</span>, is the set of all convex combinations of points in <span class="math inline">\(C\)</span>. It is always convex, and it's the smallest convex set that contains <span class="math inline">\(C\)</span></li><li>More generally, suppose <span class="math inline">\(p: R^n \rightarrow R\)</span> satisfies <span class="math inline">\(p(x)\ge0\)</span> for all <span class="math inline">\(x\in C\)</span> and <span class="math inline">\(\int_Cp(x)dx=1\)</span>, where <span class="math inline">\(C\subseteq R^n\)</span> is convex, then <span class="math inline">\(\int_Cp(x)x dx \in C\)</span>, if the integral exists.</li></ul><h3 id="convex-functions">Convex functions</h3><h4 id="convex-functions-1">Convex functions</h4><ul><li><p>Definition</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117165609.png" alt="image-20210117165609361" style="zoom:50%;" /></p></li><li><p>All affine function are both convex and concave.</p></li><li><p><span class="math inline">\(f\)</span> is convex if and only if for all <span class="math inline">\(x\in \mathrm{dom}f\)</span> and all <span class="math inline">\(v\)</span>, the function <span class="math inline">\(g(t)=f(x+tv)\)</span> is convex.</p></li></ul><h4 id="extended-value-extensions">Extended-value extensions</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117170246.png" alt="image-20210117170246166" style="zoom:50%;" /></p><h4 id="first-order-conditions">First-order conditions</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117171410.png" alt="image-20210117171410378" style="zoom:50%;" /></p><p>The inequality (3.2) states that for a convex function, the first-order Taylor approximation is in fact a global <em>underestimator</em> of the function. Conversely, if the first-order Taylor approximation of a function is always a global <em>underestimator</em> of the function, then the function is convex.</p><ul><li>The inequality (3.2) shows that if <span class="math inline">\(\nabla f(x) = 0\)</span>, then for all <span class="math inline">\(y \in \mathrm{dom} f, f(y) ≥ f(x)\)</span>, i.e., <span class="math inline">\(x\)</span> is a global minimizer of the function <span class="math inline">\(f\)</span>.</li></ul><h4 id="second-order-conditions">Second-order conditions</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117173919.png" alt="image-20210117173919136" style="zoom:67%;" /></p><h4 id="examples">Examples</h4><ul><li><em>Norms</em></li><li><em>Max function</em></li><li><em>Quadratic-over-linear function</em> <span class="math inline">\(\frac{x^2}{y}\)</span></li><li><em>Log-sum-exp</em>: <span class="math inline">\(\log(e^{x_1}+\cdots+e^{x_n})\)</span>, which is regarded as a differentiable approximation of the max function</li><li><em>Geometric mean</em>: <span class="math inline">\((\prod\limits_{i=1}^{n}x_i)^{1/n}\)</span>, concave</li><li><em>Log-determinant</em>: <span class="math inline">\(\log\det X\)</span>, concave</li></ul><p>For proofs, check Chapter <span class="math inline">\(3.1.5\)</span> of <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>.</p><h4 id="sublevel-sets">Sublevel sets</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181258.png" alt="image-20210117181258728" style="zoom:50%;" /></p><h4 id="epigraph">Epigraph</h4><p>A function is convex if and only if its epigraph is a convex set.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181415.png" alt="image-20210117181415039" style="zoom:50%;" /></p><h4 id="jensens-inequality-and-extensions">Jensen's inequality and extensions</h4><p>Once a function is convex, then you can get</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117185801.png" alt="image-20210117185801678" style="zoom:50%;" /> the simplest version of it is <span class="math inline">\(f(\frac{x+y}{2})\le\frac{f(x)+f(y)}{2}\)</span>.</p><h4 id="holders-inequality">Holder's inequality</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190336.png" alt="image-20210117190336823" style="zoom:50%;" /></p><h3 id="operations-that-preserve-convexity">Operations that preserve convexity</h3><ul><li><p>Nonnegative weighted sums: <span class="math inline">\(f=w_1f_1+\cdots+w_mf_m\)</span></p></li><li><p>Composition with an affine mapping: <span class="math inline">\(g(x)=f(Ax+b)\)</span>. If <span class="math inline">\(f\)</span> is convex, so is <span class="math inline">\(g\)</span>; if <span class="math inline">\(f\)</span> is concave, so is <span class="math inline">\(g\)</span>.</p></li><li><p>Pointwise maximum and suprenum: <span class="math inline">\(f(x)=\max\{f_1(x),f_2(x)\}\)</span> and <span class="math inline">\(f(x)=\max\{f_1(x),\cdots,f_m(x)\}\)</span></p></li><li><p>Composition: <span class="math inline">\(f(x)=h(g(x))\)</span></p><ul><li><p>Scalar composition</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190943.png" alt="image-20210117190943043" style="zoom:50%;" /></p></li><li><p>Vector composition <span class="math inline">\(f(x)=h(g(x))=h(g_1(x),\cdots,g_k(x))\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117191112.png" alt="image-20210117191112754" style="zoom:50%;" /></p></li><li><p>Minimization</p></li></ul></li></ul><h3 id="dimension-free-convex-optimization">Dimension-free convex optimization</h3><p>The basic principle behind <strong>gradient descent</strong> is to make a small step in the direction that minimizes the local first order Taylor approximation of <span class="math inline">\(f\)</span> (also known as the steepest descent direction). This kind of methods will <strong>obtain an oracle complexity <em>independent of the dimension</em>.</strong></p><h4 id="projected-subgradient-descent-for-lipschitz-functions">Projected subgradient descent for Lipschitz functions</h4><ul><li><p>Subgradient</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222307.png" alt="image-20210117222306873" style="zoom:40%;" /></p></li><li><p>Projected subgradient descent</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222725.png" alt="image-20210117222725673" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117223015.png" alt="image-20210117222937543" style="zoom:40%;" /></p></li><li><p><strong>Theorem</strong></p><p>Assume that <span class="math inline">\(\mathcal{X}\)</span> is contained in an Euclidean ball centered at <span class="math inline">\(x_1\in \mathcal{X}\)</span> and of radius <span class="math inline">\(R\)</span>. Assume that <span class="math inline">\(f\)</span> is such that for any <span class="math inline">\(x\in \mathcal{X}\)</span> and of any <span class="math inline">\(g\in\part f(x)\)</span> (assume <span class="math inline">\(\part f(x)\ne \emptyset\)</span>) one has <span class="math inline">\(\|g\|\le L\)</span>. (This implies that <span class="math inline">\(f\)</span> is L-Lipschitz on <span class="math inline">\(\mathcal{X}\)</span>, that is <span class="math inline">\(\|f(x)-f(y)\|\le L\|x-y\|\)</span>)</p><p><font color='blue'><strong>The projected subgradient descent method with <span class="math inline">\(\eta=\frac{R}{L\sqrt{t}}\)</span> satisfies</strong> <span class="math inline">\(f(\frac{1}{t}\sum\limits_{s=1}^{t}x_s)-f(x^*)\le\frac{RL}{\sqrt{t}}\)</span></font></p><ul><li><p>Proof</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117225400.png" alt="image-20210117225359857" style="zoom:40%;" /></p></li><li><p><font color='blue'><strong>The rate is unimprovable from a black-box perspective.</strong></font></p></li></ul></li></ul><h3 id="gradient-descent-for-smooth-functions">Gradient descent for smooth functions</h3><h4 id="beta-smooth"><span class="math inline">\(\beta\)</span>-smooth</h4><ul><li>Definition: a continuously function <span class="math inline">\(f\)</span> is <span class="math inline">\(\beta\)</span>-smooth if the gradient <span class="math inline">\(\nabla f\)</span> is <span class="math inline">\(\beta\)</span>-Lipschitz, that is <span class="math inline">\(\|\nabla f(x)-\nabla f(y)\|\le\beta\|x-y\|\)</span>.</li><li>If <span class="math inline">\(f\)</span> is twice differentiable then this is equivalent to the eigenvalues of the Hessians being smaller than <span class="math inline">\(\beta\)</span>.</li><li><font color='green'>smoothness removes dependency from the averaging scheme</font><br /></li><li><font color='cyan'>If extend the <span class="math inline">\(\beta\)</span>-smooth to multi power, it's called <a href="https://en.wikipedia.org/wiki/H">Holder condition</a></font><br /></li><li>The bigger your function changes in gradients, the upper you have to explore.</li></ul><h4 id="theorems-under-unconstrained-cases">Theorems under unconstrained cases</h4><p>In this section all <span class="math inline">\(f\)</span> is a convex and <span class="math inline">\(\beta\)</span>-smooth function on <span class="math inline">\(\mathbb{R}^n\)</span>.</p><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let <span class="math inline">\(f\)</span> be convex and <span class="math inline">\(\beta\)</span>-smooth function on <span class="math inline">\(\mathbb{R}^n\)</span>. Then gradient descent with <span class="math inline">\(\eta=\frac{1}{\beta}\)</span> satisfies <span class="math inline">\(f(x_t)-f(x^*)\le\frac{2\beta\|x_1-x^*\|^2}{t-1}\)</span> .</font></p><p>For the proof check <span class="math inline">\(3.2\)</span> in <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a>.</p></li><li><p>Gradient descent attains a much faster rate in <span class="math inline">\(\beta\)</span>-smooth situation than in the non-smooth case of the previous section.</p></li><li><p>The Definition of smooth convex functions</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117233245.png" alt="image-20210117233244773" style="zoom:50%;" /></p></li></ul><h4 id="the-constrained-cases">The constrained cases</h4><p>This time <em>consider the projected gradient descent algorithm <span class="math inline">\(x_{t+1}=\prod_{\mathcal{X}}(x_t-\eta\nabla f(x_t))\)</span></em></p><ul><li><strong>Lemma</strong></li></ul><p><font color='orange'>Let <span class="math inline">\(x,y\in \mathcal{X},x^+=\prod_{\mathcal{X}}(x-\frac{1}{\beta}\nabla f(x))\)</span> and <span class="math inline">\(g_{\mathcal{X}}(x)=\beta (x-x^+)\)</span> , then the following holds true: <span class="math inline">\(f(x^+)-f(y)\le g_{\mathcal{X}}(x)^T(x-y)-\frac{1}{2\beta}\|g_{\mathcal{X}}(x)\|^2\)</span>.</font></p><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let <span class="math inline">\(f\)</span> be convex and <span class="math inline">\(\beta\)</span>-smooth function on <span class="math inline">\(\mathcal{X}\)</span>. Then projected gradient descent with <span class="math inline">\(\eta=\frac{1}{\beta}\)</span> satisfies <span class="math inline">\(f(x_t)-f(x^*)\le\frac{3\beta\|x_1-x^*\|^2+f(x_1)-f(x^*)}{t}\)</span> .</font></p><h3 id="strong-convexity">Strong convexity</h3><p>Strong convexity can significantly speed-up the convergence of first order methods.</p><ul><li><p>Definition</p><p>We say that <span class="math inline">\(f:\mathcal{X}\rightarrow\mathbb{R}\)</span> is a <span class="math inline">\(\alpha\)</span><em>-strongly convex</em> if it satisfies the following improved subgradient inequality:</p><p><span class="math inline">\(f(x)-f(y)\le\nabla f(x)^T(x-y)-\frac{\alpha}{2}\|x-y\|^2\)</span>. A large value of <span class="math inline">\(\alpha\)</span> will lead to a faster rate.</p></li><li><p>Strong convexity plus <span class="math inline">\(\beta\)</span>-smoothness will lead to the gradient descent with a constant step-size achieves a linear rate of convergence, precisely the oracle complexity will be <span class="math inline">\(O(\frac{\beta}{\alpha}\log(1/\varepsilon)), \beta\ge\alpha\)</span>. In some sense strong convexity is a dual assumption to smoothness, and in fact this can be made precise within the framework of Fenchel duality.</p></li><li><p><strong>Strongly convex and Lipschitz functions</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210118152328.png" alt="image-20210118152328268" style="zoom:50%;" /></p><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(L\)</span>-Lipschitz and <span class="math inline">\(\alpha\)</span>-strongly convex  on <span class="math inline">\(\mathcal{X}\)</span>. Then projected gradient descent with <span class="math inline">\(\eta_s=\frac{2}{\alpha(s+1)}\)</span> satisfies <span class="math inline">\(f(\sum\limits_{s=1}^{t}\frac{2s}{t(t+1)}x_s)-f(x^*)\le\frac{2L^2}{\alpha(t+1)}\)</span> .</font></p><p><em>The combination of <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(L\)</span>-Lipschitz means that function has to be constrained in a bounded domain.</em></p></li><li><p><strong>Strongly convex and smooth functions</strong></p><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex on <span class="math inline">\(\mathcal{X}\)</span>, then projected gradient descent with <span class="math inline">\(\eta=\frac{1}{\beta}\)</span> satisfies for <span class="math inline">\(t\ge0\)</span>, <span class="math inline">\(\|x_{t+1}-x^*\|^2\le\exp(-\frac{t}{\kappa})\|x_a -x^*\|^2\)</span> .</font></p><p>The intuition of changing <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>: If increasing <span class="math inline">\(\beta\)</span>, the upper bound will be decreased, and if increasing <span class="math inline">\(\alpha\)</span>, the lower bound will be increased. <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121112619.png" alt="image-20210121112605020" style="zoom:33%;" /></p></li><li><p><strong>Lemma</strong></p><p><font color='orange'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex on <span class="math inline">\(\mathbb{R}^n\)</span>, then for all <span class="math inline">\(x,y\in \mathbb{R}^n\)</span>, one has <span class="math inline">\((\nabla f(x)-\nabla f(y))^T(x-y)\ge\frac{\alpha\beta}{\alpha+\beta}\|x-y\|^2+\frac{1}{\beta+\alpha}\|\nabla f(x)-\nabla f(y)\|^2\)</span> .</font></p></li><li><p><strong>Theorem</strong></p><p><font color='blue'> Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex  on <span class="math inline">\(\mathbb{R}^n\)</span>, <span class="math inline">\(\kappa=\frac{\beta}{\alpha}\)</span> as the condition number. Then gradient descent with <span class="math inline">\(\eta=\frac{2}{\beta+\alpha}\)</span> satisfies <span class="math inline">\(f(x_{t+1})-f(x^*)\le\frac{\beta}{2}\exp(-\frac{4t}{\kappa+1}\|x_1-x^*\|^2)\)</span>   </font></p></li></ul></li></ul><h2 id="references">References</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/103359560">【凸优化笔记5】-次梯度方法（Subgradient method）</a></li><li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></li><li><a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></li><li>'Understanding Analysis' by Stephen Abbott. It's a nice and light intro to analysis</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;泛函&quot;&gt;泛函&lt;/h2&gt;
&lt;center&gt;
&lt;img src=&quot;/assets/img/Rules/lines.png&quot; width=500&quot;&gt;
&lt;/center&gt;
&lt;h3 id=&quot;不动点定理&quot;&gt;不动点定理&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33885648&quot;&gt;链接&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;紧性的利用&quot;&gt;紧性的利用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;证明存在性&lt;/li&gt;
&lt;li&gt;在无限维空间中“模仿”有限维的欧式空间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;紧集是为了模仿描述欧式空间中的有界闭集合么？ 紧=相对紧+闭&lt;/p&gt;
&lt;h3 id=&quot;流形manifolds&quot;&gt;流形(Manifolds)&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;支撑集support&quot;&gt;支撑集(Support)&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>About</title>
    <link href="http://yoursite.com/posts/uncategorized/2019-06-13-about.html"/>
    <id>http://yoursite.com/posts/uncategorized/2019-06-13-about.html</id>
    <published>2019-06-13T17:02:18.000Z</published>
    <updated>2020-01-23T22:44:45.908Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to Mia Feng's Blog!</p><p>I'm doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in Montreal, Canada. I'm interested in data analysis, reinforce learning, transfer learning and democratising machine learning and AI.</p><p>I got the bachelor degree from Wuhan University, China, and then I did my master at the National University of Defense Technology, China. During my studies, I've processed spatial-temporal data, and financial data. I also worked as an intership of fintech in Meituan-Dianping. The main areas I have learned including machine learning, geographical information system, software engineering, data assimilation, and finance. Currently I am learning something about computational neuroscience. I want to learn more about transfer learning and visualization and explanation of neural networks.</p><p>Have a look at my <a href="https://github.com/skaudrey/cv/blob/master/cv.pdf">resume</a> for more information.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to Mia Feng&#39;s Blog!&lt;/p&gt;
&lt;p&gt;I&#39;m doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in M
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>rl-intro</title>
    <link href="http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html</id>
    <published>2019-05-26T17:02:18.000Z</published>
    <updated>2021-01-12T20:56:08.594Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver. <a id="more"></a> ## Why RL?</p><h3 id="the-difference-between-supervised-learning-and-rl">The difference between supervised learning and RL</h3><ul><li><p>task directed from fixed data sets vs goal directed learning from interaction</p></li><li><p>characteristics:</p><ul><li>RL<ul><li>trial-and-error search</li><li>delayed reward</li></ul></li><li>ML<ul><li>generalization methods: regularization, data augmentation</li><li>real-time loss</li></ul></li></ul></li><li><p>methods:</p><ul><li>RL<ul><li>exploit: get reward</li><li>exploration: make better action selection in the future.</li></ul></li><li>ML<ul><li>discriminative and generative: parameterized and semi-parameterized</li><li>supervised and unsupervised: depends on whether have labeled data</li></ul></li></ul></li></ul><center><img src="/assets/img/RLIntro/RLML.png" width=400"></center><h3 id="the-capacity-of-rl">The capacity of RL</h3><ul><li>sequential decision maker facing unknown or known environment</li><li>works for non i.i.d. data</li></ul><center><img src="/assets/img/RLIntro/RLCapacity.png" width=300"></center><h2 id="whats-rl">What's RL?</h2><h3 id="the-agent-environment-interaction">The agent-environment interaction</h3><center><img src="/assets/img/RLIntro/RLEA.png" width=300"></center><ul><li>At each step t the agent:<ul><li>Executes action at</li><li>Transform to state St</li><li>Receives scalar reward rt</li></ul></li><li>The environment:<ul><li>Receives action at</li><li>Transform to state St</li><li>Emits scalar reward rt+1</li></ul></li><li>t increments at env. step</li></ul><h3 id="the-elements-of-rl">The elements of RL</h3><ul><li><p>Policy: agent's behaviour function, mostly is a PDF mapping state to action</p><ul><li><p>Deterministic policy</p><p><span class="math display">\[      a = \pi\left(S\right)  \]</span></p></li><li><p>Stochastic policy</p><p><span class="math display">\[      \pi\left(a|S\right)=\mathbb{P}\left(A_t=a|S_t=s\right)  \]</span></p></li></ul></li><li><p>Value function: how good is each state and/or action, the scalar value is also named reward.</p><p><span class="math display">\[      v_{\pi}\left(s\right)=\mathbb{E}\left(R_{t+1}+\gamma R_{t+1}+{\gamma}^2 R_{t+2}+\cdots|S_t=s\right)  \]</span></p><p>Mostly, to make algorithm converge, a final state will be rewarded 0, and other non-final states are rewarded as a minus value.</p></li><li><p>Model: agent's representation of the environment, they can be modeled by TKinter, gym etc.</p><ul><li><p>e.g.: models in assimilation, maze</p><p><span class="math display">\[      \mathcal{P}_{ss&#39;}^{a}=\mathbb{P}\left(S_{t+1}=s&#39;|S_t=s,A_t=a\right)\\      \mathcal{R}_{s}^{a}=\mathbb{E}\left(R_{t+1}=s&#39;|S_t=s,A_t=a\right)  \]</span></p></li><li><p>unknown environment can be stimulated by sampling ### Classification of RL #### What you want</p></li><li><p>Value Based: No Policy (Implicit), Value Function</p></li><li><p>Policy Based: Policy, No Value Function</p></li><li><p>Actor Critic: Policy, Value Function #### What you knew</p></li><li><p>Model-free: Policy and/or Value Function, No Model</p></li><li><p>Model-based: Policy and/or Value Function, Model</p></li></ul></li></ul><h2 id="how-to-rl">How to RL?</h2><h3 id="markov-process----to-simplify">Markov process -- to simplify</h3><h4 id="markov-process">Markov Process</h4><p><span class="math display">\[    \mathbb{P}\left(S_{t+1}\right)=\mathbb{P}\left(S_{t+1}|S_1,\cdots,S_t\right)\\    \mathcal{P}_{ss&#39;}=\mathbb{P}\left(S_{t+1}=s&#39;|S_t=s\right)\]</span></p><h4 id="markov-rewarded-process">Markov Rewarded Process</h4><p><span class="math display">\[    \langle S,\mathcal{P},\mathcal{R},\gamma\rangle\]</span> solve the reward from state at time t to the final state, which can be also solved by adding immediate reward and discounted value of successor state.</p><p><span class="math display">\[    v\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)=\mathbb{E}\left(R_{t+1}+\gamma v\left(S_{t+1}|S_t=s\right)\right)\]</span></p><h4 id="markov-decision-process">Markov Decision Process</h4><p><span class="math display">\[    \langle S,\mathcal{A},\mathcal{P},\mathcal{R},\gamma\rangle\]</span></p><p>Sequential decision making. * state-value function <span class="math display">\[    v_{\pi}\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)\]</span></p><ul><li>action-value function <span class="math display">\[  q_{\pi}\left(s,a\right)=\mathbb{E}_{\pi}\left(G_t|S_t=s,A_t=a\right)\]</span></li></ul><h3 id="valuepolicy">Value？Policy？</h3><h4 id="policy-iteration">Policy Iteration</h4><center><img src="/assets/img/RLIntro/policyitr.png" width=300"></center><h4 id="value-iteration">Value Iteration</h4><center><img src="/assets/img/RLIntro/valueitr.png" width=300"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>gnn</title>
    <link href="http://yoursite.com/posts/notes/2019-05-24-gnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-24-gnn.html</id>
    <published>2019-05-24T22:17:30.000Z</published>
    <updated>2019-09-23T09:27:01.032Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list the generation and improvement of graph neural networks. # Why Use * Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,</p><h1 id="basic-lines">Basic lines</h1><p>Contents in this block mainly comes from paper <a href="https://arxiv.org/pdf/1812.08434.pdf">Graph Neural Networks: A Review of Methods and Applications</a> and</p><a id="more"></a><h2 id="history">History</h2><h3 id="the-proposal-of-gnns">The proposal of GNNs</h3><p>learn a target node’s representation by propagating neighbor information via recurrent neural architectures in an iterative manner until a stable fixed point is reached Computation expensive * <a href="https://www.researchgate.net/publication/4202380_A_new_model_for_earning_in_raph_domains">A new model for learning in graph domains</a> * Big Question: processing the graph without losing topological information * reason<br />Traditional preprocessing methods for graphs dropped topological information, and thus leads to poor performance and generalization. * background RNN can only handle graph-level problems; Traditional methods dropped topological information.</p><ul><li>the approximation capability of GNN, <a href="https://www.researchgate.net/publication/23763868_Computational_Capabilities_of_Graph_Neural_Networks">Computational Capabilities of Graph Neural Networks</a> under mild generic conditions, most of the practically useful functions on graphs can be approximated in probability by GNNs up to any prescribed degree of accuracy.</li><li><a href="https://ieeexplore.ieee.org/document/4773279">Neural network for graphs: A contextual constructive approach</a></li><li><a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a> ### Go to GNNs #### Spectral-based Graph difficult to parallel or scale to large graphs,cause they need to load the whole graph into the memory. relies on eigen-decomposition of the Laplacian matrix.</li><li>Spectral GNN, <a href="https://arxiv.org/pdf/1312.6203v3.pdf">Spectral networks and locally connected networks on graphs</a></li><li>ChebNet, <a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a>，codes <a href="https://github.com/mdeff/cnn%20graph">here</a>.</li><li>1st ChebNet, <a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a>, code <a href="https://github.com/tkipf/gcn">here</a>. localized in space, but the computation requirement grow exponentially, to reduce it, sampling methods are proposed. See <a href="https://arxiv.org/pdf/1801.10247.pdf">FASTGCN</a>, <a href="https://arxiv.org/pdf/1710.10568.pdf">reduce variance</a> and <a href="https://arxiv.org/pdf/1809.05343v1.pdf">adaptive sampling</a> for details.</li><li>AGCN, calculate a pairwise distance of nodes to construct a residual graph, see <a href="https://arxiv.org/pdf/1801.03226.pdf">Adaptive Graph Convolutional Neural Networks</a> for details.</li></ul><h4 id="spatial-based-graph-convolution">Spatial-based Graph Convolution</h4><p>has gained more attention * <a href="https://arxiv.org/pdf/1706.02216.pdf">Inductive representation learning on large graphs</a> * <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model cnns</a> * <a href="https://arxiv.org/pdf/1605.05273.pdf">Learning convolutional neural networks for graphs</a> * <a href="https://arxiv.org/abs/1808.03965">Large-scale learnable graph convolutional networks</a> [1],[4] used sampling strategy the common way is to stack multiple graph convolution layer together. ##### Recurrent-based Spatial GCNs update a node's representation recursively until a stable fixed point is reached * GNNs, <a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a> * GGNNs, used GRU, <a href="https://www.aclweb.org/anthology/D14-1179">Learning phrase representations using rnn encoder-decoder for statistical machine translation</a>, codes <a href="https://github.com/yujiali/ggnn">here</a>. * Stochastic Steady-state Embedding (SSE), updates the node latent representations stochastically in an asynchronous fashion, <a href="http://proceedings.mlr.press/v80/dai18a/dai18a.pdf">Learning steady-states of iterative algorithms over graphs</a>, codes <a href="https://github.com/Hanjun-Dai/steady%20state%20embedding">here</a>.</p><h5 id="composition-based-spatial-gcns">Composition-based Spatial GCNs</h5><ul><li>Message Passing Neural Networks (MPNNs), <a href="https://arxiv.org/pdf/1704.01212.pdf">Neural Message Passing for Quantum Chemistry</a></li><li>GraphSage, <a href="https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf">Inductive representation learning on large graphs</a>, codes <a href="https://github.com/williamleif/GraphSAGE">here</a>. ###### Miscellaneous Variants of Spatial GCNs</li><li>Diffusion Convolution Neural Networks (DCNN), the hidden node representation is get by independently convolving inputs with power series or transition probability matrix, <a href="https://arxiv.org/pdf/1511.02136.pdf">Diffusion-convolutional neural networks</a></li><li>Build GCN into a standard grid to do CNN, <a href="http://proceedings.mlr.press/v48/niepert16.pdf">Learning convolutional neural networks for graphs</a>, but it ignored the node information.</li><li>Large-scale Graph Convolution Networks (LGCN), <a href="https://arxiv.org/pdf/1808.03965.pdf">Large-scale learnable graph convolutional networks</a>, still using standard grid, but it also collects nodes' information and draw subgraph for mini-batch training. Codes <a href="https://github.com/divelab/lgcn/">here</a>.</li><li>Mixture Model Network (MoNet), <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model CNNs</a>, introduce pseudo-coordinates and weight functions to let the weight of a node’s neighbor be determined by the relative position (pseudo-coordinates) between the node and its neighbor.</li><li><a href="https://arxiv.org/abs/1802.00910">Geniepath: Graph neural networks with adaptive receptive paths</a>, everages gating mechanisms to control the depth and breadth of a node's neighborhood.</li><li><a href="https://persagen.com/files/misc/zhuang2018dual.pdf">Dual graph convolutional networks for graph-based semi-supervised classification</a>, one for global representation the other for local representation.</li><li><a href="https://arxiv.org/pdf/1811.10435.pdf">On filter size in graph convolutional networks</a>, introduce a hyperparameter to influence the receptive field size of a node.</li></ul><h4 id="pooling-module">Pooling module</h4><ul><li><a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a></li><li>pooling by rearranging vertices into meaningful order, <a href="https://arxiv.org/pdf/1506.05163.pdf">Deep convolutional networks on graph-structured data</a></li><li><a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf">An end-to-end deep learning architecture for graph classification</a></li><li>DIFFPOOL pools nodes hierarchically by learning a cluster assignment matrix in each layer to get a cluster embedding, which can be combined with any standard GCN module, <a href="https://arxiv.org/pdf/1806.08804.pdf">Hierarchical graph representation learning with differentiable pooling</a></li></ul><h3 id="graph-attention-networks">Graph attention networks</h3><p>For sequence-based tasks, in total, assigning attention weights to different neighbors when aggregating feature information, ensembling multiple models according to attention weights, and using attention weights to guide random walks. * <a href="https://arxiv.org/pdf/1710.10903.pdf">Graph attention networks</a>, (GAT), multi-head weights. Codes <a href="https://github.com/PetarV-/GAT">here</a>. * <a href="https://arxiv.org/pdf/1803.07294.pdf">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</a>， also use multi-head, but it use a self attention mechanism to compute a different head for each head. * <a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">Graph classification using structural attention</a>, Graph Attention Model (GAM), adaptively visiting a sequence of important nodes. * <a href="https://arxiv.org/pdf/1710.09599.pdf">Watch your step: Learning node embeddings via graph attention</a>, factorize the co-occurrence matrix with differentiable attention weights. ### Graph Autoencoders had to handle the problem caused by the sparsity of adjacency matrix. * <a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf">Deep neural networks for learning graph representations</a>, uses the stacked denoising auto-encoders to reconstruct PPMI matrix. Codes <a href="https://github.com/ShelsonCao/DNGR">here</a> * <a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">Structural deep network embedding</a>, preserve nodes first-order proximity (drive representations of adjacent nodes close to each other) and second-order proximity (a node's neighbourhood information) jointly. Codes <a href="https://github.com/suanrong/SDNE">here</a> * <a href="https://arxiv.org/pdf/1611.07308.pdf">Variational graph auto-encoders</a>, Graph Auto-Encoder (GAE), combined with GCN firstly. Codes <a href="https://github.com/limaosen0/Variational-Graph-Auto-Encoders">here</a> * <a href="https://shiruipan.github.io/pdf/CIKM-17-Wang.pdf">Mgae: Marginalized graph autoencoder for graph clustering</a>, reconstruct node's hidden state. * <a href="https://www.ijcai.org/proceedings/2018/0362.pdf">Adversarially regularized graph autoencoder for graph embedding</a>, using GANs to regularize the graph auto-encoders, recover adjacency matrix. Codes <a href="https://github.com/Ruiqi-Hu/ARGA">here</a> * <a href="https://www.kdd.org/kdd2018/accepted-papers/view/learning-deep-network-representations-with-adversarially-regularized-autoen">Learning deep network representations with adversarially regularized autoencoders</a>, recover node sequences rather than adjacency matrix. * <a href="http://pengcui.thumedialab.com/papers/NE-RegularEquivalence.pdf">Deep recursive network embedding with regular equivalence</a>, codes <a href="https://github.com/tadpole/DRNE">here</a> ### Graph Generative Networks Not scalable to large graphs. * <a href="https://arxiv.org/pdf/1802.08773.pdf">Graphrnn: A deep generative model for graphs</a>, graph-level RNN + node-level RNN, use breadth-first-search (BFS) to sequence the nodes and Bernoulli assumption for edge generation. Codes <a href="https://github.com/snap-stanford/GraphRNN">here</a>. * <a href="https://arxiv.org/pdf/1803.03324.pdf">Learning deep generative models of graphs</a>, utilize spatial-based GCNs to obtain a hidden representation of an existing graph. * <a href="https://arxiv.org/pdf/1805.11973.pdf">Molgan: An implicit generative model for small molecular graphs</a>, RL+GAN+GCN * <a href="https://arxiv.org/pdf/1803.00816.pdf">Net-gan: Generating graphs via random walks</a>, combines LSTM with Wasserstein GAN to generate graphs from a random-walk-based approach. As for random walk, see <a href="http://leogrady.net/wp-content/uploads/2017/01/grady2004multilabel.pdf">here</a>. * <a href="https://arxiv.org/pdf/1809.02630.pdf">Constrained generation of semantically valid graphs via regularizing variational autoencoders</a> ### Graph Spatial-Temporal Networks</p><ul><li><a href="https://arxiv.org/pdf/1612.07659.pdf">Structured sequence modeling with graph convolutional recurrent networks</a></li><li><a href="https://arxiv.org/pdf/1707.01926.pdf">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</a>, can work forwardly or backwardly. Codes <a href="https://github.com/liyaguang/DCRNN">here</a>.</li><li><a href="https://arxiv.org/pdf/1709.04875.pdf">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</a>, codes <a href="https://github.com/VeritasYin/STGCN_IJCAI-18">here</a>.</li><li><a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial temporal graph convolutional networks for skeleton-based action recognition</a>, extend the temporal flow as graph edges, and then assign each a label to each edge. Codes <a href="https://github.com/yysijie/st-gcn">here</a>.</li><li><a href="https://arxiv.org/pdf/1511.05298.pdf">Structural-rnn: Deep learning on spatio-temporal graphs</a>, aims at predicting nodes' labels at each time, has nodeRNN and edgeRNN, and split nodes and edges into semantic groups. Codes <a href="https://github.com/asheshjain399/RNNexp">here</a>.</li></ul><h2 id="main-methodologies----graph-embedding">Main Methodologies -- Graph Embedding</h2><h3 id="matrix-factorization">Matrix Factorization</h3><ul><li><a href="https://www.ijcai.org/proceedings/2018/0493.pdf">Discrete network embedding</a></li><li><a href="https://shiruipan.github.io/pdf/ICDM-18-Yang.pdf">Binarized attributed network embedding</a> ### Random Walks</li><li><a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf">Deepwalk: Online learning of social representations</a></li></ul><h2 id="problems">Problems</h2><ul><li>Does going deeper always work in GNNs?</li><li>How to select representative receptive field for a node?</li><li>How to work on large graphs?</li><li>How to handle dynamic and heterogeneous graph structures?</li></ul><h1 id="papers">Papers</h1><h2 id="introduction">Introduction</h2><ul><li>M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, "Geometric deep learning: going beyond euclidean data,"IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 18–42, 2017 ## others ### <a href="https://arxiv.org/pdf/1905.02296v1.pdf">Are Graph Neural Networks Miscalibrated?</a></li></ul><h3 id="estimating-node-importance-in-knowledge-graphs-using-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.08865.pdf">Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks</a></h3><p>GENI, a GNN-based method designed to deal with distinctive challenges involved with predicting node importance in KGs.</p><h3 id="understanding-attention-in-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.02850.pdf">Understanding attention in graph neural networks</a></h3><p>We aim to better understand attention over nodes in graph neural networks and identify factors influencing its effectiveness. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 40% in some of our classification tasks</p><h3 id="are-graph-neural-networks-miscalibrated"><a href="https://arxiv.org/pdf/1905.02296.pdf">Are Graph Neural Networks Miscalibrated?</a></h3><p>Graph Neural Networks (GNNs) have proven to be successful in many classification tasks, outperforming previous state-of-the-art methods in terms of accuracy</p><h3 id="graph-convolutional-networks-with-eigenpooling"><a href="https://arxiv.org/pdf/1904.13107.pdf">Graph Convolutional Networks with EigenPooling</a></h3><p>To apply graph neural networks for the graph classification task, approaches to generate the  from node representations are demanded. Experimental results of the graph classification task on <span class="math inline">\(6\)</span> commonly used benchmarks demonstrate the effectiveness of the proposed framework.</p><h3 id="pan-path-integral-based-convolution-for-deep-graph-neural-networks"><a href="https://arxiv.org/pdf/1904.10996.pdf">PAN: Path Integral Based Convolution for Deep Graph Neural Networks</a></h3><p>Experimental results show that the path integral based graph neural networks have great learnability and fast convergence rate, and achieve state-of-the-art performance on benchmark tasks.</p><h3 id="attacking-graph-based-classification-via-manipulating-the-graph-structure"><a href="https://arxiv.org/pdf/1903.00553.pdf">Attacking Graph-based Classification via Manipulating the Graph Structure</a></h3><p>We evaluate our attacks and compare them with a recent attack designed for graph neural networks. Results show that our attacks 1) can effectively evade graph-based classification methods; 2) do not require access to the true parameters, true training dataset, and/or complete graph; and 3) outperform the existing attack for evading collective classification methods and some graph neural network methods</p><h3 id="deep-learning-in-bioinformatics-introduction-application-and-perspective-in-big-data-era"><a href="https://arxiv.org/abs/1903.00342">Deep learning in bioinformatics: introduction, application, and perspective in big data era</a></h3><p>After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures</p><h3 id="constant-time-graph-neural-networks"><a href="https://arxiv.org/pdf/1901.07868.pdf">Constant Time Graph Neural Networks</a></h3><p>Recent advancements in graph neural networks (GNN) have led to state-of-the-art performance in various applications including chemo-informatics, question answering systems, and recommendation systems, to name a few</p><h3 id="a-comprehensive-survey-on-graph-neural-networks"><a href="https://arxiv.org/pdf/1901.00596.pdf">A Comprehensive Survey on Graph Neural Networks</a></h3><p>We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories</p><h3 id="graph-transformation-policy-network-for-chemical-reaction-prediction"><a href="https://openreview.net/pdf?id=r1f78iAcFm">Graph Transformation Policy Network for Chemical Reaction Prediction</a></h3><p>To this end, we propose Graph Transformation Policy Network (GTPN) -- a novel generic method that combines the strengths of graph neural networks and reinforcement learning to learn the reactions directly from data with minimal chemical knowledge. Evaluation results show that GTPN improves the top-1 accuracy over the current state-of-the-art method by about 3% on the large USPTO dataset</p><h3 id="contextualized-non-local-neural-networks-for-sequence-learning"><a href="https://arxiv.org/pdf/1811.08600.pdf">Contextualized Non-local Neural Networks for Sequence Learning</a></h3><p>Recently, a large number of neural mechanisms and models have been proposed for sequence learning, of which self-attention, as exemplified by the Transformer model, and graph neural networks (GNNs) have attracted much attention. Specifically, we propose contextualized non-local neural networks (CN<span class="math inline">\(^{\textbf{3}}\)</span>), which can both dynamically construct a task-specific structure of a sentence and leverage rich local dependencies within a particular neighborhood.</p><h3 id="automated-theorem-proving-in-intuitionistic-propositional-logic-by-deep-reinforcement-learning"><a href="https://arxiv.org/pdf/1811.00796.pdf">Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning</a></h3><p>Using the large volume of augmented data, we train highly accurate graph neural networks that approximate the value function for the set of the syntactic structures of formulas. Within the specified time limit, our prover solved 84% of the theorems in a benchmark library, while <span class="math inline">\(\texttt{tauto}\)</span> was able to solve only 52%.</p><h3 id="pileup-mitigation-at-the-large-hadron-collider-with-graph-neural-networks"><a href="https://arxiv.org/pdf/1810.07988.pdf">Pileup mitigation at the Large Hadron Collider with Graph Neural Networks</a></h3><p>We present a classifier based on Graph Neural Networks, trained to retain particles coming from high-transverse-momentum collisions, while rejecting those coming from pileup collisions. This model is designed as a refinement of the PUPPI algorithm, employed in many LHC data analyses since 2015</p><h3 id="weisfeiler-and-leman-go-neural-higher-order-graph-neural-networks"><a href="https://arxiv.org/pdf/1810.02244.pdf">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</a></h3><p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. The following work investigates GNNs from a theoretical point of view and relates them to the <span class="math inline">\(1\)</span>-dimensional Weisfeiler-Leman graph isomorphism heuristic (<span class="math inline">\(1\)</span>-WL). We show that GNNs have the same expressiveness as the <span class="math inline">\(1\)</span>-WL in terms of distinguishing non-isomorphic (sub-)graphs</p><h3 id="multitask-learning-on-graph-neural-networks---learning-multiple-graph-centrality-measures-with-a-unified-network"><a href="https://arxiv.org/pdf/1809.07695.pdf">Multitask Learning on Graph Neural Networks - Learning Multiple Graph Centrality Measures with a Unified Network</a></h3><p>Graph neural networks (GNN), consisting of trained neural modules which can be arranged in different topologies at run time, are sound alternatives to tackle relational problems which lend themselves to graph representations. The proposed model achieves <span class="math inline">\(89\%\)</span> accuracy on a test dataset of random instances with up to 128 vertices and is shown to generalise to larger problem sizes</p><h3 id="meta-gnn-on-few-shot-node-classification-in-graph-meta-learning"><a href="https://arxiv.org/pdf/1905.09718.pdf">Meta-GNN: On Few-shot Node Classification in Graph Meta-learning</a></h3><p>However, there are very few works applying meta-learning to non-Euclidean domains, and the recently proposed graph neural networks (GNNs) models do not perform effectively on graph few-shot learning problems. Additionally, Meta-GNN is a general model that can be straightforwardly incorporated into any existing state-of-the-art GNN</p><h3 id="mr-gnn-multi-resolution-and-dual-graph-neural-network-for-predicting-structured-entity-interactions"><a href="https://arxiv.org/pdf/1905.09558.pdf">MR-GNN: Multi-Resolution and Dual Graph Neural Network for Predicting Structured Entity Interactions</a></h3><p>In recent years, graph neural networks have become attractive. Experiments conducted on real-world datasets show that MR-GNN improves the prediction of state-of-the-art methods.</p><h3 id="revisiting-graph-neural-networks-all-we-have-is-low-pass-filters"><a href="https://arxiv.org/pdf/1905.09550.pdf">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</a></h3><p>In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property</p><h3 id="multi-hop-reading-comprehension-across-multiple-documents-by-reasoning-over-heterogeneous-graphs"><a href="https://arxiv.org/pdf/1905.07374.pdf">Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs</a></h3><p>We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based model (single model) achieves state-of-the-art result.</p><h3 id="ipc-a-benchmark-data-set-for-learning-with-graph-structured-data"><a href="https://arxiv.org/pdf/1905.06393.pdf">IPC: A Benchmark Data Set for Learning with Graph-Structured Data</a></h3><p>The data set, named IPC, consists of two self-contained versions, grounded and lifted, both including graphs of large and skewedly distributed sizes, posing substantial challenges for the computation of graph models such as graph kernels and graph neural networks</p><h1 id="datasets">Datasets</h1><ul><li><a href="https://www.aminer.cn/citation">Citation Networks</a>: <a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/cora_raw.zip">Cora</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/citeseer.zip">Citeseer</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/pubmed.zip">Pubmed</a>,<a href="https://www.aminer.cn/citation">DBLP</a></li><li><a href="http://networkrepository.com/soc_BlogCatalog.php">Social Networks</a>: <a href="http://socialcomputing.asu.edu/datasets/BlogCatalog">BlogCatalog</a>,<a href="https://github.com/linanqiu/reddit-dataset">Reddit</a>,<a href="http://www.trustlet.org/downloaded_epinions.html">Epinions</a></li><li>Chemical/Biological Graphs: <a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI1.zip">NCI-1</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI109.zip">NCI-9</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/MUTAG.zip">MUTAG</a>, D&amp;D,<a href="https://github.com/bigdata-ustc/QM9nano4USTC">QM9</a>,<a href="https://tripod.nih.gov/tox21/challenge/data.jsp">Tox21</a>,<a href="http://snap.stanford.edu/graphsage/ppi.zip">PPI</a>.</li><li>Unstructured Graphs: convert <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, <a href="http://www.mattmahoney.net/dc/textdata.html">Wikipedia</a> or News Groups into graphs.</li><li>Others: <a href="https://pan.baidu.com/s/14Yy9isAIZYdU__OYEQGa_g#list/path=%2F">METR-LA</a>, <a href="https://grouplens.org/datasets/movielens/1m/">Movies-Lens1M</a>, NELL.</li></ul><p>Thanks for the links given <a href="https://www.jianshu.com/p/67137451b67f">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list the generation and improvement of graph neural networks. # Why Use * Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,&lt;/p&gt;
&lt;h1 id=&quot;basic-lines&quot;&gt;Basic lines&lt;/h1&gt;
&lt;p&gt;Contents in this block mainly comes from paper &lt;a href=&quot;https://arxiv.org/pdf/1812.08434.pdf&quot;&gt;Graph Neural Networks: A Review of Methods and Applications&lt;/a&gt; and&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>UNet proposed by Olaf Ronneberger etc.</title>
    <link href="http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html</id>
    <published>2019-05-23T21:18:21.000Z</published>
    <updated>2021-01-12T19:33:13.151Z</updated>
    
    <content type="html"><![CDATA[<p>The notes of paper <a href="https://arxiv.org/abs/1505.04597v1">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>.</p><a id="more"></a><h2 id="big-question-classification-in-pixel-level-and-thus-image-segmentation">Big Question: classification in pixel level and thus image segmentation</h2><h3 id="reason">reason</h3><ul><li>single label for a image is not enough to support segmentation. ### background</li><li>why focus on it<ul><li>biomedical images: like cells segmentation. Add: also appropriate for other entity segmentation</li></ul></li><li>how have been done:<ul><li>the development of deeper CNNs</li><li>using CNNs segmenting electron microscopy images</li></ul></li><li>what have been missed:<ul><li>computation efficiency and redundancy: slow cause every patch require a running of network; patch overlapping</li><li>difficult trade-off for localization and the usage of context.</li></ul></li></ul><h2 id="methods">Methods</h2><h3 id="for-what">For what?</h3><p>Cells segmentation.</p><h3 id="framework-of-methods">Framework of Methods</h3><p>Convolution2D + deconvolution (upsampling 2D). The output of one downsampling layer is contracted as part of the input of the corresponding symmetric upsampling layer.</p><h3 id="novelty">Novelty</h3><ul><li>data augmentation randomly elastic deformations: shift, rotation, gray value, random elastic deformations are the most important</li><li>replace pooling by upsampling.</li><li>No fully connection layers.</li><li>weighted the loss of touching objects (cells).</li></ul><h2 id="details">Details</h2><h3 id="weighted-map-to-segment-overlapped-cells">weighted map to segment overlapped cells</h3><p>According to the paper, they pre-compute the weight map for each ground truth segmentation to compensate the different frequency of pixels.</p><h3 id="data-augmentation">data augmentation</h3><p>Smooth deformations using random displacements vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation.</p><h2 id="abstract">Abstract</h2><p>The main idea in abstract are contracted NNs and data augmentation so that the new NNs can get reasonable results by fewer images.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The notes of paper &lt;a href=&quot;https://arxiv.org/abs/1505.04597v1&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>paper--Faster RCNN</title>
    <link href="http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster%20rcnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster rcnn.html</id>
    <published>2019-05-19T15:17:12.000Z</published>
    <updated>2021-01-12T20:48:23.281Z</updated>
    
    <content type="html"><![CDATA[<p>Some understanding about the details in Faster RCNN, based on the codes in tensorflow.</p><a id="more"></a><h2 id="big-question">Big Question</h2><h3 id="reason">reason</h3><h3 id="background">background</h3><h2 id="region-proposal-network">Region Proposal Network</h2><h3 id="some-numbers">Some numbers</h3><ul><li>The number of anchor boxes for one anchor target <span class="math inline">\(k = scale \times ratios\)</span>,</li><li>The number of anchor boxes for one feature layer (which has <span class="math inline">\(W \times H\)</span> grids), will get $ W H k $ anchor boxes. Every grid in the feature map (the output of a popular CNN without FC layers) will have <span class="math inline">\(k\)</span> anchor boxes.</li><li>Not like the ROI method, the size of features are fixed, but anchor boxes are rescaled by <span class="math inline">\(k\)</span> regressors.</li></ul><h2 id="experiments">Experiments</h2><h3 id="prove">prove</h3><ul><li>The top-ranked RPN proposals are accurate.</li><li>NMS does not harm the detection mAP and may reduce false alarms.</li></ul><h2 id="construct">Construct</h2><h3 id="add-loss">Add loss</h3><h3 id="problems-using-it-processing-typhoon-data">Problems using it processing typhoon data</h3><ul><li>Does NMS lead to loss of typhoon? Not really, the texture of typhoon is obvious in image.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Some understanding about the details in Faster RCNN, based on the codes in tensorflow.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Positioning Data of FY4 AGRI.</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html</id>
    <published>2019-04-26T22:53:05.000Z</published>
    <updated>2019-04-26T15:31:57.701Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.</p><p>There are two ways for positioning, one is querying the lookup table given by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">NSMC</a>, the other is calculating by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">formulas</a>.</p><p>However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days. <a id="more"></a></p><h2 id="querying-the-lookup-table.">Querying the lookup table.</h2><p>There are two errors in the files.</p><ul><li>The first 8 bytes denote latitude, and the next 8 bytes are reserved for longitude.</li><li>The data are stored as little-endian data.<center><img src="/assets/img/FY4-AGRI/lookup.png" width="400"></center></li></ul><h2 id="calculating-by-formulas">Calculating by formulas</h2><p>The formulas are OK, but the constant variable <span class="math inline">\(\lambda_D\)</span> should be measured in rad before being used.</p><center><img src="/assets/img/FY4-AGRI/formulas.png" width="400"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.&lt;/p&gt;
&lt;p&gt;There are two ways for positioning, one is querying the lookup table given by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;NSMC&lt;/a&gt;, the other is calculating by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;formulas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="Sensor" scheme="http://yoursite.com/tags/Sensor/"/>
    
  </entry>
  
  <entry>
    <title>A Tex Template of Cornell Notes</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2019-04-26T15:34:08.234Z</updated>
    
    <content type="html"><![CDATA[<p>I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these. <a id="more"></a></p><h2 id="preliminaries">Preliminaries</h2><p>Before starting, you need install * CTex</p><h2 id="install-missed-packages-of-ctex">Install missed packages of CTex</h2><h3 id="download-the-required-packages">Download the required packages</h3><p>Search <a href="https://www.ctan.org/pkg">here</a>.</p><h3 id="unzip-and-compile-manually-if-needed.">Unzip and compile manually if needed.</h3><p>Unzip the downloaded file and jump to the directory after unzip. If need compile, run <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  pdflatex ***.ins</span><br></pre></td></tr></table></figure> ### Install Copy the compiled file folder to the CTex path: ~/CTex/CTex/tex/latex</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ texhash --admin</span><br></pre></td></tr></table></figure>The notes will be generated like this:<center><img src="/assets/img/CornellNotes.png" width="400"></center><h1 id="acknowledgement">Acknowledgement</h1><p>Thank <a href="https://blog.csdn.net/Myriad_Dreamin/article/details/83384110">this blog</a>.</p><h1 id="resources">Resources</h1><p>Tex file can be found <a href="https://github.com/skaudrey/skaudrey.github.io/tree/master/assets/notes/Cornell">here</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>Understand and debug the codes of GCN proposed by Thomas N. Kipf</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2021-01-12T20:59:32.301Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1609.02907.pdf">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>.</p><a id="more"></a><h2 id="big-question-semi-supervised-classification-of-graph-data">Big Question: semi-supervised classification of graph data</h2><ul><li>reason<ul><li>computation effective: semi-supervision</li><li>the complex of graphs, the information of nodes and edges are not structural information.</li></ul></li><li>background<ul><li>the improvement of GCNs: spectral GCNs</li></ul></li></ul><h2 id="key-points">Key points</h2><h3 id="the-approximation-of-spectral-graph-convolution">The approximation of spectral graph convolution</h3><p>The lines in paper had confused me at first before I ran the codes.</p><p>The difference of graph convolution and valina convolution is the input, as the input is a graph rather than data in same dimension, the key point is how to convert data represented by node and graph to a tensor in fixed dimension.</p><p>To solve it, Thomas maps the graph into a spectral space and also, to be computational efficient, approximate the infinite coefficients by second-order Chebyshev polynomial formulas.</p><p>After those approximation, it is input into the whole network with features.</p><h3 id="build-model">Build model</h3><p>Actually, except the complicated preprocess to represent graph G into a sparse tensor, the other step are not that complex, just the similar as what a convolution layer do. <span class="math display">\[Z = f\left(\mathbf{X},A\right)=softmax\left(\hat{A}ReLU\left({\hat{A}XW^{\left(0\right)}}\right)W^{\left(1\right)}\right)\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
  </entry>
  
</feed>
