<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mia&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/2d5354ebc5a8c2413323ef55a6c6d252</icon>
  <subtitle>Je marche lentement, mais je ne recule jamais.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-01-12T19:33:26.981Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Mia Feng</name>
    <email>skaudreymia@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Paper--Self-supervised Learning on Graphs, Deep Insights and New Directions</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html</id>
    <published>2021-01-12T03:00:00.000Z</published>
    <updated>2021-01-12T19:33:26.981Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2006.10141.pdf">Self-supervised Learning on Graphs: Deep Insights and New Directions</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><h3 id="summary">Summary</h3><ul><li>Observations</li><li>Limitations:</li></ul><h2 id="goals">Goals</h2><h2 id="how">How?</h2><h3 id="idea">Idea</h3><h3 id="implementation">Implementation</h3><ul><li></li></ul><h2 id="experiments">Experiments</h2><ul><li></li></ul><h2 id="conclusion">Conclusion</h2><ul><li></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2006.10141.pdf&quot;&gt;Self-supervised Learning on Graphs: Deep Insights and New Directions&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Predicting What You Already Know Helps, Provable Self-Supervised Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html</id>
    <published>2021-01-11T20:00:00.000Z</published>
    <updated>2021-01-12T19:38:37.955Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2008.01064.pdf">Predicting What You Already Know Helps: Provable Self-Supervised Learning</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li><p>Pretext tasks</p><ul><li><strong>Reconstruct images from corrupted versions or just part it</strong>: including denoising auto-encoders, image inpainting, and split-brain autoencoder</li><li><strong>Using visual common sense</strong>, including predicting rotation angle, relative patch position, recovering color channels, solving jigsaw puzzle games, and discriminating images created from distortion.</li><li><strong>Contrastive learning</strong>: learn representations that <strong>bring similar data points closer</strong> while pushing randomly selected points further away or <strong>maximize a contrastive-based mutual information lower bound</strong> between different views</li><li><strong>Create auxiliary tasks</strong>: The natural ordering or topology of data is also exploited in video-based, graph-based or map-based self-supervised learning. For instance, the pretext task is to determine the correct temporal order for video frames.</li></ul></li><li><p>Theory for self-supervised learning: contrastive learning</p><ul><li>Contrastive learning may not work when conditional independence holds only with additional latent variables</li></ul><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th style="text-align: left;">Theory</th><th style="text-align: left;">Limitations</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Shows shows guarantees for contrastive learning representations on linear classification tasks using a class conditional independence assumption</td><td style="text-align: left;">Not handle approximate conditional independence</td></tr><tr class="even"><td style="text-align: left;">Contrastive learning representations can linearly recover any continuous functions of the underlying topic posterior under a topic modeling assumption for text</td><td style="text-align: left;">The <strong>assumption of independent sampling of words</strong> that they exploit is <strong>strong</strong> and <strong>not generalizable to other domains</strong> like images</td></tr><tr class="odd"><td style="text-align: left;">Studies contrastive learning on the hypersphere through intuitive properties like alignment and uniformity of representations</td><td style="text-align: left;">No connection made to downstream tasks</td></tr><tr class="even"><td style="text-align: left;">A mutual information maximization view of contrastive learning</td><td style="text-align: left;">Some issues point by paper [45]</td></tr><tr class="odd"><td style="text-align: left;">Explain negative sampling based methods use the theory of noise contrastive estimation</td><td style="text-align: left;"><strong>guarantees are only asymptotic and not for downstream tasks.</strong></td></tr><tr class="even"><td style="text-align: left;">Conditional independence assumptions and redundancy assumptions on multiple views are used to analyze co-training</td><td style="text-align: left;">not for downstream task</td></tr></tbody></table></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Forming the pretext tasks:<ul><li>Colorization: can be interpreted as <span class="math inline">\(p(X_1,X_2|Y)=p(X_1|Y)\times p(X_2|Y)\)</span>, aka <span class="math inline">\(X_1,X_2\)</span> are independently conditioned on <span class="math inline">\(Y\)</span></li><li>Inpainting: <span class="math inline">\(p(X_1,X_2|Y,Z)=p(X_1|Y,Z)\times p(X_2|Y,Z)\)</span>,aka the inpainted <span class="math inline">\(X_2\)</span> is conditionally independent of <span class="math inline">\(X_2\)</span> (the remainder) given <span class="math inline">\(Y,Z\)</span>.</li></ul></li><li>The only way to solve the pretext task is to first implicitly predict <span class="math inline">\(Y\)</span> and then predict <span class="math inline">\(X_2\)</span> from <span class="math inline">\(Y\)</span></li></ul></li><li>Limitations:<ul><li>The underlying principles of self-supervised learning are still mysterious since it is a-priori unclear why predicting what we already know should help.</li></ul></li></ul><h2 id="goals">Goals</h2><p><strong><em>What conceptual connection between pretext and downstream tasks ensures good representations?</em></strong></p><p><strong><em>What is a good way to quantify this?</em></strong></p><h2 id="how">How?</h2><h3 id="notations">Notations</h3><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th style="text-align: left;">Symbol</th><th style="text-align: left;">Meaning</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><span class="math inline">\(\mathbb{E}^L[Y|X]\)</span></td><td style="text-align: left;">the best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span></td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(\Sigma_{XY|Z}\)</span></td><td style="text-align: left;">partial covariance matrix between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z\)</span></td></tr><tr class="odd"><td style="text-align: left;"><span class="math inline">\(X_1,X_2\)</span></td><td style="text-align: left;">the input variable and the target random variable for the pretext tasks</td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(Y\)</span></td><td style="text-align: left;">label for the downstream task</td></tr><tr class="odd"><td style="text-align: left;"><span class="math inline">\(P_{X_1X_2Y}\)</span></td><td style="text-align: left;">the joint distribution over <span class="math inline">\(\mathcal{X}_1 \times \mathcal{X}_2 \times \mathcal{Y}\)</span></td></tr></tbody></table><h3 id="idea">Idea</h3><ul><li>Under approximate condition independence (CI) (quantified by the norm of a certain partial covariance matrix), show similar sample complexity improvements.</li><li>Testify pretext task helps when CI is approximately satisfied in text domain.</li><li>Demonstrate on a real-world image dataset that a pretext task-based linear model outperforms or is comparable to many baselines.</li></ul><h3 id="formalize-ssl-with-pretext-task">Formalize SSL with pretext task</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111171426.png" alt="image-20210111171415185" style="zoom:50%;" /></p><p>It will be estimated by:</p><ul><li><strong>approximation erro</strong>r:<img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173318.png" alt="image-20210111173318840" style="zoom:33%;" />, where <span class="math inline">\(f^*=\mathbb{E}[Y|X_1]\)</span> is the optimal predictor for the task</li><li><strong>estimation error</strong>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173520.png" alt="image-20210111173520930" style="zoom:25%;" />, it's the difference between Predicting <span class="math inline">\(Y\)</span> directly by <span class="math inline">\(X_1\)</span> and Predicting by the representations from pretext task</li></ul><h2 id="experiments">Experiments</h2><ul><li></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks</li><li>Quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) <strong>allows us to learn representations that can solve the downstream task with drastically reduced sample complexity</strong> by just training a linear layer on top of the learned representation.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2008.01064.pdf&quot;&gt;Predicting What You Already Know Helps: Provable Self-Supervised Learning&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Colorful Image Colorization</title>
    <link href="http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html</id>
    <published>2021-01-10T22:15:39.000Z</published>
    <updated>2021-01-12T19:33:40.144Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1603.08511.pdf">Colorful Image Colorization</a></p><p>Code <a href="http://richzhang.github.io/colorization/">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>Predicting colors in free way: taking the image’s <span class="math inline">\(L\)</span> channel as input and its <span class="math inline">\(ab\)</span> channels as the supervisory signal--&gt; but tend to look desaturated, one explanation is using loss functions that encourage conservative predictions</p><ul><li><p>Non-parametric methods: given an input grayscale image, first define one or more color reference images. Then, transfer colors onto the input image from analogous regions of the reference image(s).</p></li><li><p>Parametric methods: learn prediction functions from large datasets of color images at training time, posing the problem as either regression onto continuous color space or classification of quantized color values. --&gt; Work in this paper is also classification task.</p></li><li><p>Concurrent work on colorization</p><table><colgroup><col style="width: 10%" /><col style="width: 37%" /><col style="width: 45%" /><col style="width: 6%" /></colgroup><thead><tr class="header"><th>Paper</th><th style="text-align: center;">loss</th><th>CNNs</th><th>Dataset</th></tr></thead><tbody><tr class="odd"><td>Larsson et al.</td><td style="text-align: center;">un-rebalanced classification loss</td><td>hypercolumns on a VGG</td><td>ImageNet</td></tr><tr class="even"><td>Iizuka et al.</td><td style="text-align: center;">regression loss</td><td>two-stream architecture in which fuse global and local features</td><td>Places</td></tr><tr class="odd"><td>This paper</td><td style="text-align: center;">classification loss, with rebalanced rare classes,</td><td>a single-stream, VGG-styled network with added depth and dilated convolutions</td><td>ImageNet</td></tr></tbody></table></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Even in gray images, the semantics of the scene and its surface texture provide ample cues for many regions in each image</li><li>Color prediction is inherently multimodal --&gt; sparks for a loss tailored to their work</li></ul></li><li>Limitations:<ul><li>Loss only cares Euclidean distance: If an object can take on a set of distinct ab values, the optimal solution to the Euclidean loss will be the mean of the set. In color prediction, this averaging effect favors grayish, desaturated results. Additionally, if the set of plausible colorizations is non-convex, the solution will in fact be out of the set, giving implausible results.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Design colorization based pretext task to get a good image semantic representations: <strong>produce a plausible colorization that could potentially fool a human observer</strong></p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li><strong>Produce vibrant colorization</strong>: Predict a distribution of possible colors for each pixel. Then, re-weight the loss at training time to emphasize rare colors. This encourages the model to exploit the full diversity of the large-scale data on which it is trained. Lastly, produce a final colorization by taking the annealed mean of the distribution.</li><li>Evaluate synthesized images: set up a “colorization Turing test”.</li></ul><h3 id="data-preparation">Data Preparation</h3><ul><li><p>Quantize the <span class="math inline">\(ab\)</span> output space into bins with grid size <span class="math inline">\(10\)</span> and keep the <span class="math inline">\(Q = 313\)</span> values which are in-gamut. Then this is the label <span class="math inline">\(Z\)</span> of each pixel. Formally, denote the raw label as <span class="math inline">\(Y\)</span></p><p>$ Z = H^{−1}_{gt} (Y)$, which converts ground truth color <span class="math inline">\(Y\)</span> to vector <span class="math inline">\(Z\)</span>s</p></li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Model</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110175555.png" alt="image-20210110175554683" style="zoom:30%;" /></p></li><li><p>Loss function</p><p><img src="C:%5CUsers%5C10457%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210110180438347.png" alt="image-20210110180438146" style="zoom:33%;" /></p><p>where <span class="math inline">\(v(·)\)</span> is a weighting term that can be used to re-balance the loss based on color-class rarity.</p><ul><li><p>Re-balancing</p><p>The distribution of <span class="math inline">\(ab\)</span> values in natural images is strongly biased towards values with low <span class="math inline">\(ab\)</span> values.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110233506.png" alt="image-20210110233506301" style="zoom:33%;" />, where <span class="math inline">\(\tilde{p}\)</span> the empirical probability of colors in the quantized ab space <span class="math inline">\(p\in \Delta Q\)</span> from the full ImageNet training set and smooth the distribution with a Gaussian kernel <span class="math inline">\(G_{\sigma}\)</span>. <span class="math inline">\(\lambda=\frac{1}{2}, \sigma=5\)</span> worked well.</p></li></ul></li><li><p>Inferring point estimates</p><p>map probability distribution <span class="math inline">\(\hat{Z}\)</span> to color values <span class="math inline">\(\hat{Y}\)</span> with function <span class="math inline">\(\hat{Y} = H(\hat{Z})\)</span></p></li></ul><p>They interpolate by re-adjusting the temperature <span class="math inline">\(T\)</span> of the softmax distribution, and taking the mean of the result. Lowering the temperature <span class="math inline">\(T\)</span> produces a more strongly peaked distribution, and setting <span class="math inline">\(T\rightarrow 0\)</span> results in a 1-hot encoding at the distribution mode. They find that <span class="math inline">\(T=0.38\)</span> captures the vibrancy of the mode while maintaining the spatial coherence of the mean.</p><h2 id="experiments">Experiments</h2><ul><li>Dataset: ImageNet</li><li>Base models</li></ul><table style="width:100%;"><colgroup><col style="width: 11%" /><col style="width: 41%" /><col style="width: 47%" /></colgroup><thead><tr class="header"><th>Model Name</th><th style="text-align: center;">Loss</th><th>Train</th></tr></thead><tbody><tr class="odd"><td>Ours(full)</td><td style="text-align: center;">classification loss</td><td>from scratch with kmeans initialization, ADAM solver for about 450K iterations. <span class="math inline">\(\beta_1 = .9, \beta_2 = .99\)</span>, and weight decay = <span class="math inline">\(10^{−3}\)</span> . Initial learning rate was <span class="math inline">\(3 × 10^{−5}\)</span> and dropped to <span class="math inline">\(10^{−5}\)</span> and <span class="math inline">\(3 × 10^{−6}\)</span> when loss plateaued, at 200k and 375k iterations, respectively.</td></tr><tr class="even"><td>Ours(class)</td><td style="text-align: center;">classification loss withou rebalancing (<span class="math inline">\(\lambda=1\)</span>)</td><td>similar training protocol as Ours(full)</td></tr><tr class="odd"><td>Ours(L2)</td><td style="text-align: center;">L2 regression loss</td><td>same training protocol</td></tr><tr class="even"><td>Ours(L2,ft)</td><td style="text-align: center;">L2 regression loss</td><td>fine tuned from our full classification with rebalancing network</td></tr><tr class="odd"><td>Larsson et al.</td><td style="text-align: center;"></td><td>CNN method</td></tr><tr class="even"><td>Dahl</td><td style="text-align: center;">L2 regression loss</td><td>a Laplacian pyramid on VGG features</td></tr><tr class="odd"><td>Gray</td><td style="text-align: center;">--</td><td>every pixel is gray, with (a, b) = 0</td></tr><tr class="even"><td>Random</td><td style="text-align: center;">--</td><td>Copies the colors from a random image from the training set</td></tr></tbody></table><h3 id="colorization-quality">Colorization quality</h3><ul><li>AMT: participants confirm their results. They argue that their work produce a more prototypical appearance for those are poorly white balanced</li><li>Semantic interpretability (VGG classification): Are the results realistic enough colorizations to be interpretable to an off-the-shelf object classifier? They check it by by feeding their fake colorized images to a VGG network that was trained to predict ImageNet classes from real color photos.<ul><li>The result is <span class="math inline">\(3.4\%\)</span> lower than Larsson's.</li><li>Without any additional training or fine-tuning, one can improve performance on grayscale image classification, simply by colorizing images with our algorithm and passing them to an off-the-shelf classifier.</li></ul></li><li>Raw accuracy (AuC):<ul><li>L2 metric can achieve accurate colorizations, but has difficulty in optimization from scratch</li><li>class-rebalancing in the training objective achieved its desired effect</li></ul></li><li>Compared with others<ul><li>LEARCH: On SUN dataset, authors have <span class="math inline">\(17.2\%\)</span> on AMT task while LEARCH has <span class="math inline">\(9.8\%\)</span></li></ul></li></ul><h3 id="cross-channel-encoding-as-ssl-feature-learning">Cross-channel encoding as SSL Feature learning</h3><ul><li>Datasets: ImageNet, PASCAL (fine tuned after training on ImageNet)</li><li>Backbone: AlexNet</li><li>Settings<ul><li>ImageNet: fixing the extractor and retrain the classifier (softmax layer) by labels</li><li>PASCAL: : (1) keeping the input grayscale by disregarding color information (Ours (gray)) and (2) modifying conv1 to receive a full 3-channel <span class="math inline">\(Lab\)</span> input, initializing the weights on the <span class="math inline">\(ab\)</span> channels to be zero (Ours (color)).</li></ul></li><li>Summary<ul><li>For ImageNet, there is a <span class="math inline">\(6\%\)</span> performance gap between color and grayscale inputs. Except for the 1st layer, representations from other deeper layers catch and outperform most methods, indicating that <strong>solving the colorization task encourages representations that linearly separate semantic classes in the trained data distribution</strong></li><li>On PASCAL, when conv1 is frozen, the network is effectively only able to interpret grayscale images.</li></ul></li></ul><h3 id="the-properties-of-network">The properties of network</h3><ul><li><p>Is it exploiting low-level cues?</p><p>Given a grayscale Macbeth color chart as input, it was unable to recover its colors. On the other hand, given two recognizable vegetables that are roughly <strong>isoluminant</strong>, <strong>the system is able to recover their color</strong>.</p></li><li><p>Does it learn multimodal color distributions ?</p><p>Take effective dilation ( the spacing at which consecutive elements of the convolutional kernel are evaluated, relative to the input pixels, and is computed by the product of the accumulated stride and the layer dilation) as the measurement. Through each convolutional block from conv1 to conv5, the effective dilation of the convolutional kernel is increased. From conv6 to conv8, the effective dilation is decreased</p></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Designing an appropriate objective function that handles the multimodal uncertainty of the colorization problem and captures a wide diversity of colors</li><li>Introducing a novel framework for testing colorization algorithms, potentially applicable to other image synthesis tasks</li><li>Setting a new high-water mark on the task by training on a million color photos.</li><li>Introduce the colorization task as a competitive and straightforward method for self-supervised representation learning, achieving state-of-the-art results on several benchmarks.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1603.08511.pdf&quot;&gt;Colorful Image Colorization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;http://richzhang.github.io/colorization/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Revisiting Self-Supervised Visual Representation Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html</id>
    <published>2021-01-09T21:21:00.000Z</published>
    <updated>2021-01-12T19:33:22.637Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf">Revisiting Self-Supervised Visual Representation Learning</a></p><p>Code <a href="https://github.com/google/revisiting-self-supervised">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>robotics: the result of interacting with the world, and the fact that multiple perception modalities simultaneously get sensory inputs are strong signals for pretext</li><li>videos: the synchronized cross-modality stream of audio, video, and potentially subtitles, or of the consistency in the temporal dimension</li><li>image datasets:<ul><li>Patch-based methods: E.g.: predicting the relative location of image patches; "jigsaw puzzle"</li><li>Image-level classification tasks:<ul><li>RotNet, create class labels by clustering images, image inpaiting, image colorization, split-brain and motion segmentation prediction;</li><li>Enforce structural constraints on the representation space: an equivariance relation to match the sum of multiple tiled representations to a single scaled representation; predict future patches in via autoregressive predictive coding</li><li>Combining multiple pretext task: E.g. extend the “jigsaw puzzle” task by combining it with colorization and inpainting; Combining the jigsaw puzzle task with clustering-based pseudo labels ( Jigsaw++) ; make one single neural network learn all of four different SSL methdos in a multi-task setting; combined the selfsupervised loss GANs objective</li></ul></li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Expensive labeled data for supervised task</li></ul></li><li>Limitations:<ul><li>Previous works mostly concentrate on pretext task, but didn't pay much attention to the choice of backbones etc.</li></ul></li></ul><h2 id="goals">Goals</h2><p><strong>An optimal CNN architecture for pretext task</strong>, investigating the influence of architecture design on the representation quality.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>a comparison of different self-supervision methods using a unified neural network architecture, but with the goal of combining all these tasks into a single self-supervision task</li></ul><h3 id="implementation">Implementation</h3><h4 id="family-of-cnns">Family of CNNs</h4><ul><li><p>variants of ResNet:</p></li><li><p><strong>ResNet50</strong>, the output before task-specific logits layer is named as <span class="math inline">\(pre-logits\)</span>. explore <span class="math inline">\(k \in \{4, 8, 12, 16\}\)</span>, resulting in pre-logits of size <span class="math inline">\(2048, 4096, 6144\)</span> and <span class="math inline">\(8192\)</span> respectively. <span class="math inline">\(k\)</span> is the widening factor.</p></li><li><p><strong>ResNet v1</strong>: ???batch normalization (BN) right after each convolution and before activation???</p></li><li><p><strong>ResNet v2</strong>: ?</p></li><li><p><strong>ResNet (-)</strong>: without ReLU preceding the global average pooling</p></li><li><p>a batch-normalized <strong>VGG</strong> architecture since VGG is structurally close to AlexNet. BN between CNN and activation, VGG19.</p></li><li><p><strong>RevNets</strong>: stronger invertibility guarantees so as to compare with ResNets. The residual unit used here is equivalent to double application of the residual unit.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110133429.png" alt="image-20210110133429332" style="zoom:50%;" />, check <a href="https://papers.nips.cc/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf">here</a> for details. Apart from this slightly complex residual unit, others are the same as ResNet.</p></li></ul><h4 id="family-of-pretext-tasks">Family of pretext tasks</h4><ul><li><strong>Rotation</strong>: same as RotNet, <span class="math inline">\(\{0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}\}\)</span></li><li><strong>Exemplar</strong>: triplet loss</li><li><strong>Jigsaw</strong>: recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches was performed. Patches are sampled with a random gap between them. Each patch is then independently converted to grayscale with probability <span class="math inline">\(\frac{2}{3}\)</span> and normalized to zero mean and unit standard deviation. Extract final image representations by averaging representations of 9 cropped patches.</li><li><strong>Relative patch location</strong>: predicting the relative location of two given patches of an image. Extract final image representations by averaging representations of 9 cropped patches.</li></ul><h4 id="evaluation-of-the-quality-of-learned-representations">Evaluation of the quality of learned representations</h4><ul><li>Idea: <strong>Using learned representations for training a linear logistic regression model to solve multiclass image classification tasks</strong> (downstream tasks). All representations come from pre-logits level.</li><li>Details: the linear logistic regression model is trained by L-BFGS. But for comparison, using SGD with momentum and use data augmentation during training.</li></ul><h2 id="experiments">Experiments</h2><ul><li>Datasets</li></ul><table><colgroup><col style="width: 12%" /><col style="width: 15%" /><col style="width: 72%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Train</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>ImageNet</td><td style="text-align: center;">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr><tr class="even"><td>Places 205</td><td style="text-align: center;">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr></tbody></table><h3 id="pretext-cnns-downstream">Pretext? CNNs? Downstream?</h3><ul><li><p>Pretext and its preferred CNN architecture: <strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110151545.png" alt="image-20210110151545316" style="zoom:30%;" /></p></li><li><p>The generalization of representations from pretext tasks: each pretext task can be generalized to other dataset. Check the trendings in figure 2.</p></li><li><p>Optimal CNNs for Pretext and downstream tasks: not consistent. But after selecting the right architecture for each self-supervision and increasing the widening factor, models significantly outperform previously reported results.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155158.png" alt="image-20210110155157874" style="zoom:45%;" /></p></li><li><p><strong>Better performance on the pretext task does not always translate to better representations</strong>: Performance on pretext cannot be used to reliably select the model architecture.</p></li></ul><h3 id="cnns-architecture">CNNs architecture</h3><ul><li>Skip-connection: For VGG, representations deteriorate towards the end of the network cause models specialize to the pretext task in the later layers. ResNet prevent this deterioration. They argue that this is because <strong>ResNet’s residual units being invertible under some conditions</strong> and confirm this by RevNet.</li><li>Depth of CNNs: For residual architectures, the pre-logits are always best.</li><li>Model-width and representation size:<ul><li>whether the increase in performance is due to increased network capacity or to the use of higher-dimensional representations, or to the interplay of both? To answer it, authors disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer.</li><li><strong>Model-width and representation size both matter independently, and larger is always better.</strong></li><li>SSL techniques are likely to <strong>benefit from using CNNs with increased number of channels</strong> across wide range of scenarios, even under low-data regime.</li></ul></li></ul><h3 id="evaluate-the-quality-of-representations">Evaluate the quality of representations</h3><ul><li><strong>A linear model is adequate</strong>: MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged</li></ul><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155830.png" alt="image-20210110155829820" style="zoom:50%;" /></p><ul><li>To train <strong>linear model, SGD optimization hyperparameters:</strong> very long training (≈ 500 epochs) results in higher accuracy</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Most affect performance in the fully labeled setting, may significantly affect performance in the selfsupervised setting.</li><li>the quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</li><li>Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations</li><li>The evaluation procedure, where a linear model is trained on a fixed visual representation using stochastic gradient descent, is sensitive to the learning rate schedule and may take many epochs to converge</li><li><strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong>--&gt;<strong>pretext tasks for self-supervised learning</strong> should not <strong>be considered</strong> in isolation, but <strong>in conjunction with underlying architectures</strong>.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&quot;&gt;Revisiting Self-Supervised Visual Representation Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/google/revisiting-self-supervised&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Representation Learning by Predicting Image Rotations</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html</id>
    <published>2021-01-09T04:11:12.000Z</published>
    <updated>2021-01-12T19:33:18.355Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1803.07728.pdf">Unsupervised Representation Learning by Predicting Image Rotations</a></p><p>Codes <a href="https://github.com/gidariss/FeatureLearningRotNet">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>How to get <strong>a high-level image semantic representation using unlabeled data</strong></p><ul><li>SSL: defines an annotation free pretext task, has been proved as good alternatives for transferring on other vision tasks. E.g.: colorize gray scale images, predict the relative position of image patches, predict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames.</li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li><strong>the attention maps are equivariant w.r.t. the image rotations, check appendix A.</strong></li></ul></li><li>Limitations<ul><li>supervised feature learning has the main limitation of requiring intensive manual labeling effort</li></ul></li></ul><h2 id="goals">Goals</h2><p>Provide a "self-supervised" formulation for image data, a <strong>self defined supervised task involving predicting the transformations used for image.</strong> The model won't have access to the initial image.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>Concretely, define the geometric transformations as the image rotations by 0, 90, 180, and 270 degrees. Thus, the ConvNet model is trained on the 4-way image classification task of recognizing one of the four image rotations.</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li><p>2D image Rotation</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109124726.png" alt="image-20210109124726420" style="zoom:40%;" /></p><table><thead><tr class="header"><th>Operations</th><th style="text-align: center;">Implementation</th></tr></thead><tbody><tr class="odd"><td>+90</td><td style="text-align: center;">transpose then flip vertically</td></tr><tr class="even"><td>+180</td><td style="text-align: center;">flip vertically then flip horizontally</td></tr><tr class="odd"><td>+270</td><td style="text-align: center;">flip vertically then transpose</td></tr></tbody></table></li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109123815.png" alt="image-20210109123815880" style="zoom:45%;" />, where <span class="math display">\[F(\cdot)\]</span> is the predicted probability of the geometric transformation with label <span class="math inline">\(y\)</span> and <span class="math inline">\(\theta\)</span> are the learnable parameters of model <span class="math display">\[F(\cdot)\]</span>, and <span class="math inline">\(g(X_i|y)\)</span> is the transformed image with transformation <span class="math inline">\(y\)</span>.</p></li><li><p><strong>Why does it works?</strong></p><ul><li>To work for this pretext task, extractor has to <strong>understand the concept of the objects depicted in the image</strong>. Models must learn to localize salient objects in the image, recognize their orientation and object type, and then relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images.</li><li>Easy to be implemented by flipping and transpose, no chance for importing low-level visual artifacts so as to avoid trivial features (which have no practical value)</li><li>Operations are easy to be recognized manually.</li></ul></li></ul><h2 id="experiments">Experiments</h2><h3 id="cifar-object-recognition">CIFAR: object recognition</h3><ul><li>Dataset:</li></ul><table><thead><tr class="header"><th>Datasets</th><th>Preprocess</th></tr></thead><tbody><tr class="odd"><td>CIFAR-10</td><td>Rotations</td></tr></tbody></table><ul><li>Training: SGD with batch size 128, momentum 0.9, weight decay <span class="math inline">\(5e−4\)</span> and <span class="math inline">\(lr\)</span> of 0.1. We drop the learning rates by a factor of 5 after epochs 30, 60, and 80. 100 epochs. Each time feeding with all 4 images.</li><li>Summary<ul><li><em>The learned feature hierarchies</em>: convnet with different number of layers. <strong>Representations from the 2nd block</strong> are good, and <strong>increasing the total depth</strong> of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers.</li><li><em>The quality of the learned features w.r.t. the number of recognized rotations</em>: 4 discrete rotations outperform.</li><li><em>Compared with previous work</em> : almost the same as the NIN supervised model. Fine-tuned the unsupervised learned features further improves the classification performance.</li><li><em>Correlation between object classification task and rotation prediction task</em>: The representations from pretext make classifier converge faster compared with the classifier trained from scratch.</li><li><em>Semi-supervised setting</em>: pretrained on the whole dataset without labels, then fine-tuned on a small labeled subset. It exceeds the supervised model when the number of examples per category drops below 1000.</li></ul></li></ul><h3 id="others-classification-object-detection-segmentation">Others: classification, object detection , segmentation</h3><ul><li><p>Dataset: ImageNet, Places, and PASCAL VOC.</p><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th>Task</th><th>Datasets</th></tr></thead><tbody><tr class="odd"><td>Classification</td><td>Pretrained on ImageNet, then test on ImageNet, Places, and PASCAL VOC.</td></tr><tr class="even"><td>Object detection</td><td>PASCAL VOC</td></tr><tr class="odd"><td>Object segmentation</td><td>PASCAL VOC</td></tr></tbody></table></li><li><p>Backbones: AlexNet without local response normalization units, dropout units, or groups in the colvolutional layers while it includes batch normalization units after each linear layer</p></li><li><p>Pretrained: on ImageNet, SGD with batch size 192, momentum 0.9, weight decay <span class="math inline">\(5e − 4\)</span> and <span class="math inline">\(lr\)</span> of 0.01. Learning rates are dropped by a factor of 10 after epochs 10, and 20 epochs. Trained in total for 30 epochs.</p></li><li><p>Summary:</p><ul><li><p>ImageNet classification task: surpasses all the other unsupervised methods by a significant margin, narrows the performance gap between unsupervised features and supervised features.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154626.png" alt="image-20210109154626599" style="zoom:50%;" /></p></li><li><p>Transfer learning evaluation on PASCAL VOC: fine tuning, used weight rescaling proposed by Krahenbuhl et al.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154831.png" alt="image-20210109154831162" style="zoom:60%;" /></p></li><li><p>Places classification task: the learnt features are evaluated w.r.t. their generalization on classes that were “unseen” during the unsupervised training phase</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154953.png" alt="image-20210109154952935" style="zoom:50%;" /></p></li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Propose a new self-supervised task that is very simple and at the same time.</li><li>Rotationsod under various settings (e.g. semi-supervised or transfer learning settings) and in various vision tasks (i.e., CIFAR-10, ImageNet, Places, and PASCAL classification, detection, or segmentation tasks).</li><li>They argue this self-supervised formulation demonstrates state-of-the-art results with dramatic improvements w.r.t. prior unsupervised approaches, and narrows the gap between unsupervised and supervised feature learning.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1803.07728.pdf&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Codes &lt;a href=&quot;https://github.com/gidariss/FeatureLearningRotNet&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Visual Representation Learning by Context Prediction</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html</id>
    <published>2021-01-08T21:22:12.000Z</published>
    <updated>2021-01-12T19:33:36.144Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf">Unsupervised Visual Representation Learning by Context Prediction</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>How to get <strong>a good image representation</strong></p><ul><li>The latent variables of an appropriate generative model. --&gt; generative models<ul><li>But given an image, inferring the latent structure is <strong>intractable</strong> for even relatively simple models --&gt; to fix, use <strong>sampling</strong> to perform approximate inference.</li></ul></li><li>An embedding that can discriminate the semantics in images by distances of them. -- create a supervised "pretext" task. But hard to tell whether the predictions themselves are correct.<ul><li>Reconstruction-based: E.g., denoising autoencoders (reconstruction ), sparse autoencoders (reconstruction + sparsity penalty )</li><li>Context prediction: "skip-gram" to "filling the blank" task, and convert the prediction task to discriminate task like discriminating between real images vs. images where one patch has been replaced by a random patch from elsewhere in the dataset. But not hard enough for high-level representations</li><li>Discover object categories using hand-crafted features and various forms of clustering. But they will lose shape information. To keep more shape information, some take contour extraction or defining similarity metrics.</li><li>Video-based: since the identity of objects remains unchanged -</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>the difficulties for generalizing CNNs models on Internet -scale datasets</li><li>context has proven to be a powerful source of automatic supervisory signal for learning representations --&gt; context can be regarded as a 'pretext' task to force the model to learn a good word embedding</li><li>current reconstruction-based algorithms struggle with low-level phenomena, like stochastic textures, making it hard to even measure whether a model is generating well.</li></ul></li><li>Limitations:<ul><li>generative models are rather efficiently on smaller datasets but burden on high-resolution natural images</li><li>Some are too simple for extracting high-level representations</li><li>Hard to tell whether the model has obtained good representations.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Provide a "self-supervised" formulation for image data, a supervised task involving predicting the context for a patch.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>Hypothesis: Doing well on predicting patches' positions requires understanding scenes and objects--&gt; a good visual representation</li><li>Concretely, sample random pairs of patches in one of eight spatial configurations, and present each pair to a machine learner. The algorithm must then guess the position of one patch relative to the other.</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li><p>Two patches are fed into network</p></li><li><p>Given an image, one patch will be sampled uniformly, then according to the position of this sampled patch, then 2nd patch will be sampled randomly from the eight possible neighboring locations.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108181759.png" alt="image-20210108181757911" style="zoom:33%;" /></p></li><li><p>including a gap between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</p></li><li><p>For some images ( chromatic aberration), after solving the relative location task (like by detecting the separation between green and magenta (red + blue). ), this problem will be relaxed.</p><ul><li>Shift green and magenta toward gray</li><li>Color dropping : randomly drop 2 of the 3 color channels from each patch and replace them by gaussian noise.</li></ul></li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Siamese network based on AlexNet. But not all layers share weights, LRN (local response normalization ) layers won't.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108183110.png" alt="image-20210108183110662" style="zoom:33%;" /></p></li><li><p><strong>Why does it works?</strong></p><ul><li>Avoid 'trivial' shortcuts (like boundary patterns or textures continuing between patches)--&gt; including a gap (up to 48 pixels) between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</li><li>Enhancing performance on images with chromatic aberration.</li></ul></li></ul><h2 id="experiments">Experiments</h2><p>Pre-Training: SGD+BN+high momentum, 4 weeks on K40 GPU.</p><table><colgroup><col style="width: 8%" /><col style="width: 28%" /><col style="width: 63%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Resizing</th><th>Preprocess</th></tr></thead><tbody><tr class="odd"><td>ImageNet</td><td style="text-align: center;"><span class="math inline">\(150K\sim450K\)</span> total pixels</td><td>1. sample patches at resolution <span class="math inline">\(96\times 96\)</span><br />2. mean subtraction, projecting or dropping colors, and randomly downsampling some patches to as little as 100 total pixels, and then upsampling it.</td></tr></tbody></table><h3 id="ability-on-semantic">Ability on semantic</h3><p>Does it get similar representations for patches with similar semantics?</p><ul><li><p>check nearest neighbors by normalized correlation of <span class="math inline">\(fc6\)</span>'s output. Compared with results from random initialized model and ImageNet AlexNet.</p></li><li><p>Summary</p><ul><li>in a few cases, random (untrained) ConvNet also does reasonably well</li><li>the representations from proposed model often capture the semantic information</li></ul></li></ul><h3 id="learnability-of-chromatic-aberration">Learnability of Chromatic Aberration</h3><ul><li>Patches displayed similar aberration tend to be predicted at the same location.</li><li>The effect of color projection operation is canceled for this kind of images.</li></ul><h3 id="object-detection">Object detection</h3><ul><li>Dataset : VOC 2007</li><li>Train: fine-tune the pretrained model (model is slight different with the previous one considering the image size in VOC) on VOC 2007.</li><li>Test: output from <span class="math inline">\(fc7\)</span> is taken.</li><li>Summary<ul><li>Pre-trained model outperforms the one trained from scratch</li><li>Obtained the best result on VOC 2007 without using labels</li><li>Robustness of the representations for one object in different datasets: acceptable</li></ul></li></ul><h3 id="visual-data-mining">Visual data mining</h3><ul><li>Task : aims to use a large image collection to discover image fragments which happen to depict the same semantic objects</li><li>Specification for this task: sample a constellation of four adjacent patches from an image, after finding the top 100 images which have the strongest matches for all four patches, then use a type of geometric verification to filter away the images where the four matches are not geometrically consistent. Finally, rank the different constellations by counting the number of times the top 100 matches geometrically verify.</li><li>To define the geometric verification: first compute the best-fitting square <span class="math inline">\(S\)</span> to the patch centers (via least-squares), while constraining that side of <span class="math inline">\(S\)</span> be between 2/3 and 4/3 of the average side of the patches. Then compute the squared error of the patch centers relative to <span class="math inline">\(S\)</span> (normalized by dividing the sum-of-squared-errors by the square of the side of <span class="math inline">\(S\)</span>). The patch is geometrically verified if this normalized squared error is less than 1.</li><li>Test: VOC 2011, Street View images from Paris</li><li>Summary<ul><li>The discovery of birds and torsos is good</li><li>The gains in terms of coverage, suggesting increased invariance for learned features</li><li>The pretext task is difficult: for a large fraction of patches within each image, the task is almost impossible</li><li>Limitations: some loss of purity, and cannot currently determine an object mask automatically (although one could imagine dynamically adding more sub-patches to each proposed object).</li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>instance-level supervision appears to improve performance on category-level tasks</li><li>The proposed model is sensitive to objects and the layout of the rest of the image</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf&quot;&gt;Unsupervised Visual Representation Learning by Context Prediction&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
    <link href="http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html</id>
    <published>2021-01-06T16:17:12.000Z</published>
    <updated>2021-01-12T19:33:31.917Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1406.6909.pdf">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Supervised : labeled data with a specific CNN<ul><li>directly penalizing the derivative of the output with respect to the magnitude of the transformations, but will be sensitive to the magnitude of the applied transformation.</li></ul></li><li>Unsupervised: learning invariant representations<ul><li>Directly modeling the input distribution and are hard for jointly training multiple layers of a CNN<ul><li><strong>autoencoders</strong>: denoising auto encoders, say reconstruct data from randomly perturbed input samples; or learn representations from videos by enforcing a temporal slowness constraint on the feature representation learned by a linear autoencoder.</li><li>invariant to local transformations</li></ul></li><li>most aims at regularization of the latent representation</li></ul></li><li>Semi-supervised<ul><li>Regularization supervised algorithms by unlabeled data: self-training, entropy regularization</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>the features learned by one network often generalize to new datasets</li><li>a network can be adapted to a new task by replacing the loss function and possibly the last few layers of the network and fine-tuning it to the new problem</li></ul></li><li>Limitations:<ul><li>the need for huge labeled datasets to be used for the initial supervised training</li><li>the transfer becomes less efficient the more the new task differs from the original training task</li></ul></li></ul><h2 id="goals">Goals</h2><p>a more general extractor using unlabeled data. The extractor should satisfy two requirements:</p><ul><li>there must be at least one feature that is similar for images of the same category <span class="math inline">\(y\)</span> (invariance);</li><li>there must be at least one feature that is sufficiently different for images of different categories (ability to discriminate)</li></ul><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>creating an auxiliary task + invariant features to transformations</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li>Do random selected transformation (from a predefined family of transformations) for sampled patches (regions containing considerable gradients so that sample a patch with probability proportional to mean squared gradient magnitude within the patch )</li><li>The family of transformations<ul><li>translation</li><li>scaling</li><li>rotation</li><li>contrast: PCA and HSV</li><li>color: works on HSV space</li><li>blur etc.</li></ul></li><li>Before feeding into model, do normalization (subtract the mean of each pixel over the whole resulting dataset)</li><li>Labeling: all transformed patches from the same seed patch are labeled by the same index</li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function :</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155621.png" alt="image-20210107155618997" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155911.png" alt="image-20210107155854389" style="zoom:30%;" /></p><ul><li><p>After transformations, the loss for a whole class (augmented by the same seed patch ) can be taken as</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107162118.png" alt="image-20210107162118193" style="zoom:50%;" />, notice the 2nd and the 4th can be canceled.</p><ul><li>The 1st term: enforces correct classification of the average representation <span class="math inline">\(\mathbb{E}_\alpha[g(T_\alpha x_i)]\)</span> for a given input sample</li><li>The 2nd term: a regularizer enforcing all $ h(T_x_i)$ to be close to their average value, i.e., the feature representation is sought to be approximately invariant to the transformations <span class="math display">\[ T_\alpha\]</span>, note the convergence to global minimum is listed at appendix.</li></ul></li></ul></li><li><p><strong>Why does it works?</strong></p><ul><li>Previous works mostly focus on modeling the input distribution <span class="math display">\[p(x)\]</span>, based on the assumption that a good model of <span class="math display">\[p(x)\]</span> contains information about the category distribution <span class="math display">\[p(y|x)\]</span>. Therefore, to get the invariance, one will do regularization of the latent representation and obtain representation by reconstruction .</li><li>Their work does not directly model the input distribution <span class="math display">\[p(x)\]</span> but learns a representation that discriminates between input samples. They argue that this <strong>allows more DOF to model the desired variability of a sample and avoid task-unnecessary reconstruction.</strong></li><li>However, their work will <strong>fail on color-relied task</strong></li></ul></li></ul><h2 id="experiments">Experiments</h2><h3 id="classification">Classification</h3><ul><li><p>Datasets: <strong>STL-10</strong>, CIFAR-10, Caltech-101 and Caltech-256. report mean and standard deviation</p><table><colgroup><col style="width: 6%" /><col style="width: 26%" /><col style="width: 34%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Resizing</th><th>Train</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>STL-10</td><td style="text-align: center;">64c5-64c5-128f</td><td>10 pre-defined folds of the training data</td><td>fixed test set</td></tr><tr class="even"><td>CIFAR-10</td><td style="text-align: center;">resize from <span class="math display">\[32\times 32\]</span> to <span class="math display">\[64\times 64\]</span></td><td>1. whole training set<br />2. 10 random selections of 400 training samples per class</td><td>1. results on CIFAR-10<br />2. average results on 10 sets.</td></tr><tr class="odd"><td>Caltech-101</td><td style="text-align: center;">to <span class="math display">\[150\times 150\]</span></td><td>30 random samples per class</td><td>not more than 50 samples per class</td></tr><tr class="even"><td>Caltech-256</td><td style="text-align: center;"><span class="math display">\[256\times 256\]</span></td><td>randomly selected 30 samples per class</td><td>those except for training</td></tr></tbody></table></li><li><p>backbones of network</p><table><thead><tr class="header"><th>Network</th><th style="text-align: center;">Structure</th><th>Training</th></tr></thead><tbody><tr class="odd"><td>small</td><td style="text-align: center;">64c5-64c5-128f</td><td>1.5 days, SGD with fixed momentum of 0.9</td></tr><tr class="even"><td>medium</td><td style="text-align: center;">64c5-128c5-256c5-512f</td><td>4 days, SGD with fixed momentum of 0.9</td></tr><tr class="odd"><td>large</td><td style="text-align: center;">92c5-256c5-512c5-1024f</td><td>9 days, SGD with fixed momentum of 0.9</td></tr></tbody></table></li><li><p>Training: learning rate starts at 0.01， then when there was no improvement in validation error, decreased the learning rate by a factor of 3. All networks are trained on one Titan</p></li><li><p>Test features: one-vs-all linear SVM.</p></li><li><p>Summary</p><ul><li>with increasing feature vector dimensionality and number of labeled samples, training an SVM becomes less dependent on the quality of the features</li><li>Relation of <strong>the number of surrogate classes</strong> : <strong>sampling too many, too similar images for training can even decrease the performance of the learned features</strong>. ( the discriminative loss is no longer reasonable with too many similar surrogate classes.) --&gt; fix: e.g. clustering the output features then do augmentation for clusters and feed these augmented classes as surrogate data</li><li>Relation of <strong>the number of samples per surrogate class</strong> : around 100 samples is sufficient</li><li>Relation of <strong>types of transformations </strong> : each time remove a group of transformations and check how the performance is decreased , e.g. scaling, rotation etc. Translations, color variations and contrast variations are significantly more important. For the matching task, using blur as an additional transformation improves the performance.</li><li>Relation of <strong>Influence of the dataset </strong>: the learned features generalize well to other datasets</li><li>Relation of <strong>Influence of the Network Architecture on Classification Performance</strong>: Classification accuracy generally improves with the network size</li></ul></li></ul><h3 id="descriptor-matching">Descriptor matching</h3><ul><li>Task: Matching of interest points</li><li>Datasets: by Mikolajczyk et al., augmented by applying 6 different types of transformations with varying strengths to 16 base images from Flickr. In addition to the transformations used before, also change the lighting and blur .</li><li>Backbones: 64c7s2-128c5-256c5-512f, named as Exemplar-CNN-blur</li><li>Training: use unlabeled images from Flickr for training</li><li>Test and measurements: prediction is <span class="math inline">\(TP\)</span> if <span class="math inline">\(IOU\ge 0.5\)</span>. Compared with SIFT and Alexnet</li><li>Summary<ul><li>Optimum patch size (or layer in CNNs): SIFT is based on normalized finite differences, and thus very robust to blurred edges caused by interpolation. In contrast, for the networks, especially for their lower layers, there is an optimal patch size. They argue that features from higher layers have access to larger receptive fields and, thus, can again benefit from larger patch sizes.</li><li>A loss function that focuses on the invariance properties (rather than class-specific features) required for descriptor matching yields better results.</li><li>Features obtained with the unsupervised training procedure outperform the features from AlexNet on both datasets</li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Pretty good on tasks: object classification , descriptor matching</li><li>emphasizes the value of data augmentation in general and suggests the use of more diverse transformations.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1406.6909.pdf&quot;&gt;Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>霍乱时期的爱情</title>
    <link href="http://yoursite.com/posts/feelings/2020-03-06-%E9%9C%8D%E4%B9%B1%E6%97%B6%E6%9C%9F%E7%9A%84%E7%88%B1%E6%83%85.html"/>
    <id>http://yoursite.com/posts/feelings/2020-03-06-霍乱时期的爱情.html</id>
    <published>2020-03-06T20:02:46.660Z</published>
    <updated>2021-01-07T19:00:17.543Z</updated>
    
    <content type="html"><![CDATA[<p>       忘记是在哪里看到对它的推荐，抱着打发时间的态度开卷，竟一时难释。多日用无逻辑的网络小甜文虚度光阴，倒是在这次阅读中重温了那种语句流畅自如之感。读完后再翻开甜腻的网文，难以下咽。题材不是我惯常偏爱的侦探科幻类，却令我警醒。 <a id="more"></a></p><p>       上周期中考试完，终于在心安理得的空闲时间里读完了这部，但一直拖到今天才动笔。可能是发现自己实在有太多需要快速熟悉，兴奋与迷茫一起涌来，竟令我乱了本筹谋好的步调。久未动笔，日记也快成了月记。两个多月，书写中文的时间，甚少。笔记渐渐成长为法文板书的蹩脚英翻版。忙碌但独立的学习生活，让我无暇分心家时常提及的话题，如今父母问的最多的，也成了学习考试，猛然间重温高中生活，单调却也充实。猛然在书中看到关于爱情与婚姻的讨论，心头又犯上疑问。</p><p>       这跨越半个世纪的爱情，是凭着什么滋养？因为阿里萨仍可以看到她？因为阿里萨即使得到过上百个女性却独菲尔米娜是个例外？年少时的一次心动，始终横亘在他的心头，到后来那种衍生的痛苦已成习惯。开始时，他为无法为白月光保留童贞而惭愧；渐渐又沉溺于这种快乐中，并用册子记录自己每一次的所感所悟。他甚至野心期望将它出版，那时，自己的爱人就会意识到自己是多么的招人喜欢，许能让他的白月光追悔呢？到底是个野心，他是没有胆子付诸行动的。内心一面计划着这样的出人头地，又一面在表面行动上巩固他坚持为一人忠心到底的誓言，即使他知道除了他的白月光，其他人都知道他有着怎样的私生活。但，只要白月光不知，只要他最爱的仍为她，这种行为，就可以被他诠释为排解痛苦的手段。这样多的经历倒也不是令他全无所得，他过于旺盛的爱情被分摊到其他人身上，菲尔米娜反而少了压力。加之多个女性展现的爱情婚姻观，也让他有了不同的感悟。后来，得益于这些经历，他极力劝导菲尔米娜让她获得心理上的松快，不累于世人对她孀居生活的评价。他让轮船永远不再着陆，不让爱人接触到那些让她害怕的熟人评论家，爱人就可毫无压力的与他继续下去。</p><p>       三种婚姻爱情观。阿里萨视爱情为人生的信仰，毕生追求让所爱之人与自己厮守，但只敢隐于黑暗中，凭借与其他女人的交往来化解自己的痛苦，靠乌尔比诺会比他先死的信念迎接每个清晨。乌尔比诺冷静自持，选择妻子的条件是美丽动人以称得上自己的身份地位，他的妻子终会被他的家庭逐渐塑造成他需要的模样，只他知道妻子不是令他发狂之人。菲尔米娜少时痴迷于迷惘忧郁的情诗所营造的凄美氛围，旅行中现实的洗礼让她成长为掌家的妇人。再见阿里萨，那一撇彻底打破了她曾为之坚持的幻梦，那样沉郁的少年怎会勾勒出幸福的美梦呢？投身现实，孀居后她对阿里萨由恐惧到憎恶再到需要，担心他对自己悔婚的报复，惧怕他破坏自己孀居的名声。丈夫与父亲的丑闻让她开始怀疑自己孀居的意义，众人的抨击让她需要心理安慰，阿里萨在此时却是极好的选择。她为他积攒半个世纪之久的慰藉之语提供了展示的平台。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;       忘记是在哪里看到对它的推荐，抱着打发时间的态度开卷，竟一时难释。多日用无逻辑的网络小甜文虚度光阴，倒是在这次阅读中重温了那种语句流畅自如之感。读完后再翻开甜腻的网文，难以下咽。题材不是我惯常偏爱的侦探科幻类，却令我警醒。
    
    </summary>
    
      <category term="feelings" scheme="http://yoursite.com/categories/feelings/"/>
    
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
      <category term="digest" scheme="http://yoursite.com/tags/digest/"/>
    
      <category term="feelings" scheme="http://yoursite.com/tags/feelings/"/>
    
  </entry>
  
  <entry>
    <title>苏菲的世界</title>
    <link href="http://yoursite.com/posts/feelings/2019-12-19-%E8%8B%8F%E8%8F%B2%E7%9A%84%E4%B8%96%E7%95%8C.html"/>
    <id>http://yoursite.com/posts/feelings/2019-12-19-苏菲的世界.html</id>
    <published>2019-12-19T11:46:44.496Z</published>
    <updated>2018-12-10T07:40:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>       全书的脑洞还是挺大的。因为一些暗示，我曾一度怀疑结局会不会向类盗梦空间或者多重人格发展，但是没有，最终，苏菲和他的哲学老师逃离到了另一个世界。</p><p>       我是谁？宇宙是什么？存在的意义是什么？小时候看到《星际宝贝》中对蝴蝶效应的演示，以后每一次面临重大的选择时，脑海中总会浮现出那样的画面。会害怕，如果，我选错了怎么办？那会是一步错，步步错了啊。但，又会想，这样无数所谓的机缘巧合拼凑起来，也许就是生命的本质？每一个看起来轻飘飘的选择，都足以令你的人生方向产生重大的转折。生命是流动的水，需要持续不断的导引，才能顺利汇入大海。仰望星空，需与脚踏实地并举。 <a id="more"></a>        我们，因为对未来的不确定性感到恐慌，却也暗暗为这种不确定性而庆幸。一眼望到头的生活，未免无趣。Jack吸引Rose，可能就来自于他对生命中那些未知的热爱吧。因为这些未知与不确定，我们才会竭尽力气去追寻，去体会、去热爱、去证明自己的存在，去追寻存在的意义。意识到生命的短暂，不是要我们放弃挣扎，而是需要我们珍惜，珍惜自由的意志，珍惜独立的感官，最大限度的体悟生命的美好。</p><p>       生命，不该是一世人劳苦奔忙有何益，到头来终究需把眼儿闭。而是一世人欣悦奔忙体悟勤，时时感总是噙笑眉梢喜。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;       全书的脑洞还是挺大的。因为一些暗示，我曾一度怀疑结局会不会向类盗梦空间或者多重人格发展，但是没有，最终，苏菲和他的哲学老师逃离到了另一个世界。&lt;/p&gt;
&lt;p&gt;       我是谁？宇宙是什么？存在的意义是什么？小时候看到《星际宝贝》中对蝴蝶效应的演示，以后每一次面临重大的选择时，脑海中总会浮现出那样的画面。会害怕，如果，我选错了怎么办？那会是一步错，步步错了啊。但，又会想，这样无数所谓的机缘巧合拼凑起来，也许就是生命的本质？每一个看起来轻飘飘的选择，都足以令你的人生方向产生重大的转折。生命是流动的水，需要持续不断的导引，才能顺利汇入大海。仰望星空，需与脚踏实地并举。
    
    </summary>
    
      <category term="feelings" scheme="http://yoursite.com/categories/feelings/"/>
    
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
      <category term="digest" scheme="http://yoursite.com/tags/digest/"/>
    
      <category term="feelings" scheme="http://yoursite.com/tags/feelings/"/>
    
  </entry>
  
  <entry>
    <title>嫌疑人X的献身</title>
    <link href="http://yoursite.com/posts/feelings/2019-12-19-%E5%AB%8C%E7%96%91%E4%BA%BAX%E7%9A%84%E7%8C%AE%E8%BA%AB.html"/>
    <id>http://yoursite.com/posts/feelings/2019-12-19-嫌疑人X的献身.html</id>
    <published>2019-12-19T11:46:44.485Z</published>
    <updated>2018-12-10T13:21:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>       总的来说，电影比小说的矛盾更为突出，最后的爆发点也更合情合理。虽然林心如的声音让人有点跳戏。</p><p>       不过，电影里面,石神将爱情视为最高行为准则，小说里的石神则更真实一些。他也有常人的怯懦，会怕自己坚持不下去，无法保护好静子。杀人，是为了给自己破釜沉舟的勇气。自己从一开始就万劫不复，入狱也是理所应当。没有感情的选择(杀人)，和太过情感充沛的选择(杀人)，都将面对极端结果的考验。要么大获全胜，要么满盘皆输。没有几个人有这样的勇气。石神的破釜沉舟，倒也是在理的行为了。但是，人心，不只是合乎逻辑的选择。理性与感性的平衡点，才是它的基准。静子没有按他所想，幸福的走下去，就是最好的说明。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;       总的来说，电影比小说的矛盾更为突出，最后的爆发点也更合情合理。虽然林心如的声音让人有点跳戏。&lt;/p&gt;
&lt;p&gt;       不过，电影里面,石神将爱情视为最高行为准则，小说里的石神则更真实一些。他也有常人的怯懦，会怕自己坚持不下去，无法保护好静子。杀人，是为了
      
    
    </summary>
    
      <category term="feelings" scheme="http://yoursite.com/categories/feelings/"/>
    
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
      <category term="digest" scheme="http://yoursite.com/tags/digest/"/>
    
      <category term="feelings" scheme="http://yoursite.com/tags/feelings/"/>
    
  </entry>
  
  <entry>
    <title>克里斯多夫</title>
    <link href="http://yoursite.com/posts/feelings/2019-12-19-%E5%85%8B%E9%87%8C%E6%96%AF%E5%A4%9A%E5%A4%AB.html"/>
    <id>http://yoursite.com/posts/feelings/2019-12-19-克里斯多夫.html</id>
    <published>2019-12-19T11:46:44.479Z</published>
    <updated>2018-12-10T07:33:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>       开始以为是贝多芬的传记，读了才知道这只是一个贝多芬式的英雄。他冲动粗暴，却又单纯细腻。他坚强勇敢，却也孤独求爱。他多情，却也专情。他是那样的渴望被爱、被理解，所以总是轻易的爱上对他主动示好的人。但是，开始的他，对友情的了解只止于书本和个人的坚持。他盼着有人爱他如他爱他。而他，一旦爱人。必是要付出所有的。在遇到奥里维和葛拉齐亚之前，他是那样的孤独与骄傲。但是，真正遇到了朋友之后，他选择了妥协，奥里维、葛拉齐亚都有他所不认同的小性格，可他又何尝不是呢。可是，遇到了心爱的朋友、情人，日子必定是平稳乐观，有所寄托的。他漫长的旅途，似乎到达了终点。但是，巨人的旅程，注定是不会止步的。上帝会催促他开始下一段人生经历。 <a id="more"></a></p><p>       奥里维因为他轻易的参与了革命，发生意外而离世。葛拉齐亚虽然爱他，却也因为种种顾虑，一拖再拖，最终抱憾而终。他生命中的每一个人，都教会他一些事情。他从开始的揉不得沙子，到后来的求同存异，即使形式有了变化，但那内蕴的德意志精神、习得的法兰西性格，一直都在。他一直严格的要求自己，即使犯了错，也会勇敢的去纠正，而不是逃避，放任自处。</p><p>       英雄，不是永于不败之地、高高在上的神者。而是挣扎于世俗的洪流，时刻自省、不轻易随波逐流、常怀赤子之心的试错者。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;       开始以为是贝多芬的传记，读了才知道这只是一个贝多芬式的英雄。他冲动粗暴，却又单纯细腻。他坚强勇敢，却也孤独求爱。他多情，却也专情。他是那样的渴望被爱、被理解，所以总是轻易的爱上对他主动示好的人。但是，开始的他，对友情的了解只止于书本和个人的坚持。他盼着有人爱他如他爱他。而他，一旦爱人。必是要付出所有的。在遇到奥里维和葛拉齐亚之前，他是那样的孤独与骄傲。但是，真正遇到了朋友之后，他选择了妥协，奥里维、葛拉齐亚都有他所不认同的小性格，可他又何尝不是呢。可是，遇到了心爱的朋友、情人，日子必定是平稳乐观，有所寄托的。他漫长的旅途，似乎到达了终点。但是，巨人的旅程，注定是不会止步的。上帝会催促他开始下一段人生经历。
    
    </summary>
    
      <category term="feelings" scheme="http://yoursite.com/categories/feelings/"/>
    
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
      <category term="digest" scheme="http://yoursite.com/tags/digest/"/>
    
      <category term="feelings" scheme="http://yoursite.com/tags/feelings/"/>
    
  </entry>
  
  <entry>
    <title>上帝掷骰子吗：量子物理史话</title>
    <link href="http://yoursite.com/posts/feelings/2019-12-19-%E4%B8%8A%E5%B8%9D%E6%8E%B7%E9%AA%B0%E5%AD%90%E5%90%97.html"/>
    <id>http://yoursite.com/posts/feelings/2019-12-19-上帝掷骰子吗.html</id>
    <published>2019-12-19T11:46:44.472Z</published>
    <updated>2019-04-26T16:04:00.592Z</updated>
    
    <content type="html"><![CDATA[<p>       参加完微软亚研院的会议时，有幸听到了姚期智院士的报告。之后就对量子物理是什么产生了一些兴趣。书是去年年末买的，一直拖到半个月前才看完。本来是作为科普读物，闭卷才发现书中不仅有不少大家的八卦，还有发人深省的哲学思辨。 <a id="more"></a>        光的粒子说、波动说、波粒二象性等的发展，促进了物理学的前进。如今，虽弦论已成为主流的物理学理论，但是关于光、物理学的本质的探索从未停息。科学家的成果，很多时候是受限于个人的世界观、价值观的。即使少年时加入激进的改革大军中，老年时也总会因为敏感度的降低、对已有知识的笃定而成为顽固派。唯有时刻更新自己，才不至耄耋之时顽固到令人厌弃。</p><p>       创新，也许很难。可有些时候，只不过是对于旧知识的新应用。诚然，这种应用，一如福尔摩斯所言，是排除了所有的可能之后，保留下的“不可能”的可能。如德布罗意波、测不准原理的提出、量子力学中对矩阵的引进等等，都是在出人意料中给出漂亮的一击。</p><p>       如今，薛定谔的猫已经不再被生死之论折磨，可我们的物理课本中，这个精彩反驳的边角都很少提到。爱学习的好学生，学好课本上的知识就可以了吗？所有刊登在课本上的，都已经是被普遍认为正确的陈词滥调，真正有魅力的事物与激辨却被我们的课本所忽略。科学的魅力不在于它亘古不变的正确，而是它时刻跳出的异常情况，迫使你不断的更新你的方法与理论，促使你一步步接近却又始终不给你一个明朗的结果。折磨么？诚然。开心么？必然。总有些调皮的孩子等着你去看顾，让你伤神，却又为看顾好他而兴奋。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;       参加完微软亚研院的会议时，有幸听到了姚期智院士的报告。之后就对量子物理是什么产生了一些兴趣。书是去年年末买的，一直拖到半个月前才看完。本来是作为科普读物，闭卷才发现书中不仅有不少大家的八卦，还有发人深省的哲学思辨。
    
    </summary>
    
      <category term="feelings" scheme="http://yoursite.com/categories/feelings/"/>
    
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
      <category term="digest" scheme="http://yoursite.com/tags/digest/"/>
    
      <category term="feelings" scheme="http://yoursite.com/tags/feelings/"/>
    
  </entry>
  
  <entry>
    <title>三体</title>
    <link href="http://yoursite.com/posts/feelings/2019-12-19-%E4%B8%89%E4%BD%93.html"/>
    <id>http://yoursite.com/posts/feelings/2019-12-19-三体.html</id>
    <published>2019-12-19T11:46:44.465Z</published>
    <updated>2018-12-10T07:37:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>       三体，一个chaotic system。</p><p>       书的第一部以三体问题求解为切入点，展现了因为chaotic而在恒纪元与乱纪元中艰难求生的异星世界。因为寂静的春天所引发的思考，对人类劣根性的失望，我们，弹星者，天真的将未来交给了异星之神。然而，神之爱，只存在于天堂。宇宙是黑暗森林，文明的延续法则，在于保护自己，排除异己。天真将人类暴露于猎人的视线中。为了自保，人类使用威慑论，以同归于尽为要挟使三体的侵略搁置。但是，因为弹星的尝试，两个文明还是招至了毁灭。清理者的维度攻击，伤敌而自损。没有一个文明可以逃脱。</p><p>       如果说第一部的关注点只在boss，那接下来两部则是对宇宙观，道德观的审视。 <a id="more"></a></p><p>       宇宙观。宇宙的初态是什么？降维至零再从大爆炸中诞生多维是否可行？星际穿越中展示的四维就已经令人瞠目，超维，又会是怎样？大多数人，无暇欣赏那浅淡的光芒。我们，在宇宙尺度下，可能比一粒质子还小。宇宙怎样，在我们太过虚空。也许是饕餮潜伏，开门后的一瞥，就已让人胆寒。也许处处柳暗花明，任我们放肆探索奥义……</p><p>       道德观。不处于相应的环境中，我们永远不知道自己的可怕。想象中的自己，与真实的自己，总是存在着差距。新宇宙观下的新道德观、新物种，与母星物种相去甚远。母星人类对他们的批判，按相应环境来考虑，又似乎只是站着说话。当处于对应的环境中，没有人可以保证能做的比他们好。奥斯维辛、南京、广岛，没有谁可以作为正义的代言者批判他人。杀人，是事实。与其站在道德的制高点挑剔他人，倒不如以最低点为始给予希冀。性善或者性恶，只是起点，后天的引导才是关键。己所不欲勿施于人，自省思齐。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;       三体，一个chaotic system。&lt;/p&gt;
&lt;p&gt;       书的第一部以三体问题求解为切入点，展现了因为chaotic而在恒纪元与乱纪元中艰难求生的异星世界。因为寂静的春天所引发的思考，对人类劣根性的失望，我们，弹星者，天真的将未来交给了异星之神。然而，神之爱，只存在于天堂。宇宙是黑暗森林，文明的延续法则，在于保护自己，排除异己。天真将人类暴露于猎人的视线中。为了自保，人类使用威慑论，以同归于尽为要挟使三体的侵略搁置。但是，因为弹星的尝试，两个文明还是招至了毁灭。清理者的维度攻击，伤敌而自损。没有一个文明可以逃脱。&lt;/p&gt;
&lt;p&gt;       如果说第一部的关注点只在boss，那接下来两部则是对宇宙观，道德观的审视。
    
    </summary>
    
      <category term="feelings" scheme="http://yoursite.com/categories/feelings/"/>
    
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
      <category term="digest" scheme="http://yoursite.com/tags/digest/"/>
    
      <category term="feelings" scheme="http://yoursite.com/tags/feelings/"/>
    
  </entry>
  
  <entry>
    <title>一天</title>
    <link href="http://yoursite.com/posts/feelings/2019-12-19-%E4%B8%80%E5%A4%A9.html"/>
    <id>http://yoursite.com/posts/feelings/2019-12-19-一天.html</id>
    <published>2019-12-19T11:46:44.459Z</published>
    <updated>2018-12-10T07:51:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>The book <em>one day</em> is also adapted for film, which is starred by Anne Hathaway. # 文摘</p><h2 id="大学刚刚毕业的emma的彷徨">大学刚刚毕业的Emma的彷徨</h2><p><em>       天已转凉，她感到丝丝寒意，打起了冷战。那是不期而至的颤抖，带着焦虑穿过整个脊柱，强烈到让她不由得停下脚步。是对未来的恐惧吧，她想，二十二岁的她站在其中，面对那间邋遢的公寓，不知所措。挫败感再次袭来。</em></p><p><em>       “你想过怎样的生活？”总有人用这样或那样的方式问她这个问题，没完没了。老师、父母、凌晨三点时的朋友们。但它从未像现在这版迫切困扰着她，可她依旧在迷茫。未来已在咫尺之遥，那些也许将是空空洞洞虚度的一天又一天，会愈来愈令人生畏，愈发不可知。她要如何去充实。</em></p><p><em>       她再次抬起脚步，往南朝着蒙德走去。“把你的每一天都当做最后一天来过”，虽是陈词滥调，谁又真有精力做得到？要是天下雨，要是你情绪不好呢？太不现实，尽力而为吧，勇敢大胆些，尽我所能的去改变，不是整个世界，而是你周遭的小世界。走出去，带着你的热情，带着打字机，在某个领域……奋斗。用艺术改变人生，珍惜朋友，坚守原则，热情，充实，好好的生活。体验新事物，懂得珍惜，感受爱与被爱。</em></p><p><em>       纵然开端不是很好，但这些基本的道理她懂。刚刚对心爱的人说再见，感觉也不比耸耸肩来得艰难。</em> <a id="more"></a> ## 与Dexter终成眷属后的淡然 <em>       三十八岁了，要是还在指望用一首歌、一本书或是一部电影改变人生，岂不荒唐！不，一切都平淡了，沉静了，生活的背景音乐已变成了安然，知足而又亲昵的轻唱，再也没有刺激的起起落落。朋友圈子则在今后的五年、十年、二十年也不见得会变。如果不出意外，他们想必是不会一夜暴富或破产，健康也该还能享受一阵子。一切都定格了：中年，中间阶层，比上不足比下有余的小幸福。</em></p><p><em>       最后，她爱着一个人，也确信为对方所爱。如果有人问艾玛，她和丈夫怎么相遇的，她会说：“我们一起长大的”。</em></p><h1 id="有感">有感</h1><p>       合卷，又一次发现作者想告诉我们的更多。</p><p>       初看是安妮海瑟薇的电影，俊男美女的搭配，温馨的爱情故事，让我以为它讲述的只是两个人之间互相扶持，最终认识到自己是彼此的归宿的浪漫情节。可，书中更多的，还有青年、中年的彷徨迷茫，对自我认知的日日自省。这不是油纸伞与丁香姑娘的情节，而是对自我的问询。</p><p>       两人都是不甘的，Emma不甘成为一个平庸的人；Dxter不愿走父母设定好的道路。两人都有过迷失，Emma接受了自己并不喜欢的男人的追求指望着日久出亲情，迫于工作的压力和猎奇的补偿心理而与校长不清不楚。Dexter以纸醉金迷的生活来标新立异，从而自命不凡，虽然内心深处怯懦依旧。</p><p>       Dexter是不如Emma坚强的，他欣赏着Emma身上，他不曾有过的执着。Emma曾盲目的崇拜着Dexter，一个帅气多金的美少年，又怎会不抓住自认是丑小鸭的芳心？好的是，两人兜兜转转，终究是没有走散的。他们互相学习，终于到了一个平衡点。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The book &lt;em&gt;one day&lt;/em&gt; is also adapted for film, which is starred by Anne Hathaway. # 文摘&lt;/p&gt;
&lt;h2 id=&quot;大学刚刚毕业的emma的彷徨&quot;&gt;大学刚刚毕业的Emma的彷徨&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;       天已转凉，她感到丝丝寒意，打起了冷战。那是不期而至的颤抖，带着焦虑穿过整个脊柱，强烈到让她不由得停下脚步。是对未来的恐惧吧，她想，二十二岁的她站在其中，面对那间邋遢的公寓，不知所措。挫败感再次袭来。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;       “你想过怎样的生活？”总有人用这样或那样的方式问她这个问题，没完没了。老师、父母、凌晨三点时的朋友们。但它从未像现在这版迫切困扰着她，可她依旧在迷茫。未来已在咫尺之遥，那些也许将是空空洞洞虚度的一天又一天，会愈来愈令人生畏，愈发不可知。她要如何去充实。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;       她再次抬起脚步，往南朝着蒙德走去。“把你的每一天都当做最后一天来过”，虽是陈词滥调，谁又真有精力做得到？要是天下雨，要是你情绪不好呢？太不现实，尽力而为吧，勇敢大胆些，尽我所能的去改变，不是整个世界，而是你周遭的小世界。走出去，带着你的热情，带着打字机，在某个领域……奋斗。用艺术改变人生，珍惜朋友，坚守原则，热情，充实，好好的生活。体验新事物，懂得珍惜，感受爱与被爱。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;       纵然开端不是很好，但这些基本的道理她懂。刚刚对心爱的人说再见，感觉也不比耸耸肩来得艰难。&lt;/em&gt;
    
    </summary>
    
      <category term="feelings" scheme="http://yoursite.com/categories/feelings/"/>
    
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
      <category term="digest" scheme="http://yoursite.com/tags/digest/"/>
    
      <category term="feelings" scheme="http://yoursite.com/tags/feelings/"/>
    
  </entry>
  
  <entry>
    <title>Psychology——Daniel Schacter</title>
    <link href="http://yoursite.com/posts/feelings/2019-12-19-Psychology.html"/>
    <id>http://yoursite.com/posts/feelings/2019-12-19-Psychology.html</id>
    <published>2019-12-19T11:46:44.434Z</published>
    <updated>2018-12-10T07:39:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>       开始读前，以为只是从心理学的基础架构方面进行阐述。但是，收获却超预期： <a id="more"></a></p><ol type="1"><li><p>定量分析方法</p><p>       作者所阐述的定量分析方法、实验中控制组与对照组的严格设定，说明了任何一门学科都需要严谨的分析手段，从而进一步分析相关研究。</p></li><li><p>心理的产生如此复杂，我们对其知之甚少。</p><p>       基于生理学的解释，目前仍停留在较浅的层面。神经心理学、行为心理学等等，都还有很长的路要走。</p></li><li><p>了解了一些心理障碍的成因之后，可以减少对其的偏见。</p><p>       心理障碍其实是一个普遍现象，每个人的一生中，总会有一些心理障碍的时期（比如抑郁），只不过个人的严重程度不同。很多障碍，只要有合适的环境引导，完全是可以恢复的。即使无法恢复，也可能可以找到适合的职业，有所建树。所以，未来关于障碍的治疗，是否不是简单的将其边缘化，而是提供适合的环境，让他们意识到自己的价值？当然，这还有很长远的道路要走。</p></li><li><p>到底什么是疾病？</p><p>       异于常人不应该作为疾病的唯一判据，这也许会抹杀个人的特性。临床上的分析，也许更多的应该关注心理上的疏导，而非简单的确定病症，贴上标签，然后利用手术结果，使得个人同一化（比如切除额叶）。更多的时候，病患康复的压力来源于社交环境而非生理。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;       开始读前，以为只是从心理学的基础架构方面进行阐述。但是，收获却超预期：
    
    </summary>
    
      <category term="feelings" scheme="http://yoursite.com/categories/feelings/"/>
    
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
      <category term="digest" scheme="http://yoursite.com/tags/digest/"/>
    
      <category term="feelings" scheme="http://yoursite.com/tags/feelings/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-notes-fourier-GCN.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-notes-fourier-GCN.html</id>
    <published>2019-12-19T11:46:44.411Z</published>
    <updated>2019-08-18T15:07:05.525Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.</p><p>Check <a href="/assets/slides/notes/GCN/fourier-GCN.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.&lt;/p&gt;
&lt;p&gt;Check &lt;a hre
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>What Can Artificial Intelligence Do in Data Assimilation? Dec. 9th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-mlutility.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-mlutility.html</id>
    <published>2019-12-19T11:46:44.394Z</published>
    <updated>2018-12-10T02:06:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my lab, and most of them haven't learn much about AI. The talk would like to show them what can AI do these days, and help them figure out what else can AI do in data assimilation.</p><p>Slides are avaliable <a href="/assets/slides/mlDo/mlUtility.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my l
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="Data assimilation" scheme="http://yoursite.com/tags/Data-assimilation/"/>
    
  </entry>
  
  <entry>
    <title>The naive implementation of some popular machine learning algorithms.</title>
    <link href="http://yoursite.com/posts/projects/2019-12-19-ml-implement.html"/>
    <id>http://yoursite.com/posts/projects/2019-12-19-ml-implement.html</id>
    <published>2019-12-19T11:46:44.387Z</published>
    <updated>2018-12-10T12:57:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>Naive implementations of some M.L. algorithms, which are updated continuously.</p><p>The algorithms that have been implemented are listed as follows:</p><ul><li>Logistic Regression,</li><li>SVM solved by SMO,</li><li>K-Means，</li><li>GMM solved by EM,</li><li>Perceptron，</li><li>Naive Bayes,</li><li>LeNet-Keras，</li><li>MLP-Numpy solved with BP,</li><li>MCMC sampling.</li></ul><p>Codes are available <a href="https://github.com/skaudrey/ml_algorithm">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Naive implementations of some M.L. algorithms, which are updated continuously.&lt;/p&gt;
&lt;p&gt;The algorithms that have been implemented are list
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="naive" scheme="http://yoursite.com/tags/naive/"/>
    
  </entry>
  
  <entry>
    <title>Computing in the 21th Century &amp; Asia Faculty Summit held by Microsoft, Nov., 2018.</title>
    <link href="http://yoursite.com/posts/meetings/2019-12-19-microsoft.html"/>
    <id>http://yoursite.com/posts/meetings/2019-12-19-microsoft.html</id>
    <published>2019-12-19T11:46:44.380Z</published>
    <updated>2018-12-10T08:53:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is <a href="https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda">here</a>.</p><p>Babysitting AI and computational neuroscience impressed me a lot. <a id="more"></a> # computing neuroscience: robots with feelings.</p><p>Prof. Lenore Blue's keynote is about computational neuroscience. They try to let robots feel pain, and to simulate the long and short term encoding happening in our brains.</p><p>What she talked reminds me of something I had read before.</p><p>According to the book <em>Psychology</em> written by Daniel Schacter, our brains do encode information into long and short codes, and the short form is possibly trasformed into a long one. Even though we don't mean to encode or memorize something sometimes, encoding still occurs unconsciously.</p><p>Reviewing is one useful way for recalling these information. Moreover, if you are in the similar environment in which you encoded the codes before, you will have higher possibility to recall it. However, the <strong>encoding error</strong> happend during reviewing is more, and that's why a detective should try to get full information while inquiring evidences from witnesses at the 1st time.</p><p>Also, our brains are more sensible to pictorial information compared with text information. So, if you try to make each thing you want to keep code as a photo, you can boost the capability of memorizing.</p><h1 id="yoshu-bengio-beyond-i.i.d.-and-babysitting-ai">Yoshu Bengio: Beyond i.i.d. and babysitting AI</h1><p>One of the basic assumption that makes generation possible is independent identically distributed assumption. However, influenced by observing equipments, imbalanced samples and others, the distributions of training data and test data are not always the same. Hence, Prof. Bengio's team proposed that all data are sampled from the same system rather than same distribution. As for weather of two different seasons, data desciping them are sampled from the same atmospheric circulation system, but they don't distribute identically. Bengio said they tend to initialize this system with diversed initial conditions, and the result of this distribution will be taken as what the data set follows. It makes sense.</p><p><em>One thing that troubles me is, how can I model the system and figure out the initial conditions? For things with obvious physical rules, it is easy, and even the model's codes are open-acssessed online. What if the one I don't know? How to make this idea works in common situations?</em></p><p>CNN is renowned as its power in representation learning, which encodes a variety of information into vectors. Our brains also work like this. Nontheless, what they learn are supervised, and the utility of binary network, the simplification of network structure all demostrate that the captured information are redundant. The model learned is fragile, too. After adding some noises into an image, even though the image dosen't change visually for us humans, model can not tell what it is as before. It all comes from the uncontrolling unsupervision. Babysitting AI aims at modeling with environmental information and other information, so that leading AI models.</p><h1 id="andrew-c.-yao-the-advent-of-quantum-computing">Andrew C. Yao: The Advent of Quantum Computing</h1><p>The quantum computing will offer exponential speedup for crypto-code breaking, simulation of quantum physical systems, simulation of materials, chemistry, and biology, nonlinear optimization, ML and AI. It will break through the bottleneck of computing.</p><p>Its implementation is like crystallography. In terms of crystallography, you take an X-ray photo for a crystal and then compute its structure. For quantum computing, instead of taking a real photo, you just need to collect a polynomial number of sample points. By wave-particle duality, this single photo can recreate the raw image probabilistically.</p><p>According to Andrew, dimond qubits are in the highest possibility to be used in our laptops.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is &lt;a href=&quot;https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Babysitting AI and computational neuroscience impressed me a lot.
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="applications" scheme="http://yoursite.com/tags/applications/"/>
    
      <category term="tendency" scheme="http://yoursite.com/tags/tendency/"/>
    
  </entry>
  
  <entry>
    <title>Clouds detection of infrared hyperspectral data based on logistic.</title>
    <link href="http://yoursite.com/posts/projects/2019-12-19-lr.html"/>
    <id>http://yoursite.com/posts/projects/2019-12-19-lr.html</id>
    <published>2019-12-19T11:46:44.375Z</published>
    <updated>2018-12-10T13:17:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed with what clouds release, and they exist in more than 90% IFOVs, cloudy IFOVs have to be kicked off in order to get clean data.</p><p>Therefore, a new feature construction method is proposed for infrared hyperspectral data, such as what IASI releases. Concretely, four channels of IASI are picked, namely channel 921, channel 386, channel 306 and channel 241. They are picked because of physical characteristics. And then, cloudy IFOVs are detected by logistic regression.</p><p>The recall, auc and accuracy of this new method carried on IASI data was more than 0.95 when detecting IFOVs of sea, while the result of land's IFOVs was less than it. After adding surface emissivity features, the auc of it increased by aroud 5%, and recall of it grew by 10% approximately.</p><p>Codes are available <a href="https://github.com/skaudrey/cloud">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed 
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="logistic" scheme="http://yoursite.com/tags/logistic/"/>
    
  </entry>
  
  <entry>
    <title>The Introduction of Infrared Hyperspectral Data and Kernel PCA, June 5th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-hyp.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-hyp.html</id>
    <published>2019-12-19T11:46:44.346Z</published>
    <updated>2018-12-10T08:01:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:</p><ul><li>high spectral correlation,</li><li>high spatial correlation,</li><li>and sparsity,</li></ul><p>and they casue a trouble during processing. <a id="more"></a> This talk explained why they are highly correlated but also sparse. Kernel PCA for compressing was also tested.</p><p>Check <a href="/assets/slides/hyp/hypCompression.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;high spectral correlation,&lt;/li&gt;
&lt;li&gt;high spatial correlation,&lt;/li&gt;
&lt;li&gt;and sparsity,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and they casue a trouble during processing.
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="compression" scheme="http://yoursite.com/tags/compression/"/>
    
      <category term="reconstruction" scheme="http://yoursite.com/tags/reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Build up personal blog</title>
    <link href="http://yoursite.com/posts/notes/2019-12-19-hexo+killy+github%20pages=blog.html"/>
    <id>http://yoursite.com/posts/notes/2019-12-19-hexo+killy+github pages=blog.html</id>
    <published>2019-12-19T11:46:44.340Z</published>
    <updated>2019-04-26T09:57:19.835Z</updated>
    
    <content type="html"><![CDATA[<p>This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github. <a id="more"></a></p><h2 id="preliminaries">Preliminaries</h2><p>Before starting, you need: * node.js+npm,</p><pre><code>Get node.js from [here](https://pan.baidu.com/s/1kU5OCOB#list/path=%2Fpub%2Fnodejs). Check [here](https://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000/00143450141843488beddae2a1044cab5acb5125baf0882000) for more info about node.</code></pre><ul><li>hexo, Install by npm: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li><li>git,</li><li>an account of Github,</li></ul><p>and configure the ssh-key on your device.</p><h2 id="build-blog">Build blog</h2><h3 id="initialize-hexo-with-hexo">Initialize hexo with hexo</h3><p>Create a local folder as your root directory, such as "blog", and go to the directory in your terminal and initialize it by hexo. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br><span class="line">$ <span class="built_in">cd</span> blog</span><br></pre></td></tr></table></figure> Then initialize this directory with npm.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="link-hexo-with-github">Link hexo with Github</h3><p>Set deployment tool,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>and initialize the remote repository for your blog on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br><span class="line">$ git add *</span><br><span class="line">$ git commit -m <span class="string">&quot;init commit&quot;</span></span><br></pre></td></tr></table></figure><p>Change the deployment in file "_config.yml" like:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line">deploy: </span><br><span class="line">    <span class="built_in">type</span>: git </span><br><span class="line">    repo: git@github.com:jack/jack.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure><p>Tips: The name of your hosting repository should be "[githubname].github.io", such as "jack.github.io". And mind the blankspaces while rewriting file "_config.yml". ### Generate static files Do it before you push it on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><h1 id="see-your-pages">see your pages</h1><p>Click https://[githubname].github.io/, such as https://jack.github.io/.</p><h2 id="customization">Customization</h2><h3 id="change-theme">change theme</h3><p>I picked theme yilia. Configuration should be done as bellow:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ git <span class="built_in">clone</span> https://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br></pre></td></tr></table></figure><p>Change the default theme defined in "_config.yml" under root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: yilia</span><br></pre></td></tr></table></figure><h3 id="upload-your-avatar">upload your avatar</h3><p>New a folder under the "source" directory, I named it assets. I also new the "img" folder for pictures. Put you avatar picture here. Then reconfigure the _config.yml file beneath theme "yilia"'s folder, which is:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avatar: /assets/img/avatar.jpg</span><br></pre></td></tr></table></figure><h3 id="classify-your-posts-by-categories-rather-than-tags-in-default">classify your posts by categories rather than tags in default</h3><p>Now take your eye away from file "_config.yml" under theme yilia, open the file "_config.yml" under the root directory of your blog. You need to configure category_map, for instance,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  about: about</span><br><span class="line">  notes: notes</span><br><span class="line">  projects: projects</span><br><span class="line">  papers: papers</span><br><span class="line">  talks: talks</span><br><span class="line">  meetings: meetings</span><br></pre></td></tr></table></figure><p>Each pair of it can be different, it is just a mapping, such as:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  parole: talks</span><br><span class="line">  关于我: about</span><br></pre></td></tr></table></figure><h3 id="change-the-naming-rule-of-a-new-post">change the naming rule of a new post</h3><p>The default naming rule of hexo is YYYY/MM/DD/[post name], which leads to a hyper-link without html suffix. I change it as html. It can be accomplished by configure the _config.yml in root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permalink: posts/:category/:year-:month-:day-:title.html</span><br></pre></td></tr></table></figure><h3 id="truncate-the-post-in-home-list-when-it-is-too-long.">Truncate the post in home list when it is too long.</h3><p>You need to add</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure><p>after where you want to trucate in a post. And confiure the __config.yml_ under themes' folder.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The truncate signal while post is too long.</span></span><br><span class="line">excerpt_link: <span class="string">&quot;more&quot;</span></span><br></pre></td></tr></table></figure><h1 id="tips">Tips</h1><p>You can debug pages locally by</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>, which is convenient before deployment.</p><h1 id="acknowledgement">Acknowledgement</h1><p>Thank <a href="https://www.cnblogs.com/wumz/p/8030244.html">Mauger</a>, and <a href="https://github.com/litten/hexo-theme-yilia">litten</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>HCR--Compress and Resonstruct Hyperspectral Data.</title>
    <link href="http://yoursite.com/posts/projects/2019-12-19-hcr.html"/>
    <id>http://yoursite.com/posts/projects/2019-12-19-hcr.html</id>
    <published>2019-12-19T11:46:44.333Z</published>
    <updated>2018-12-10T12:45:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression and reconstruction. The numerous infrared hyperspectral data are overloaden for computing resources currently. Taking IASI, an atmosphere detector on satellite Metop launched by European Organization for the Exploitation of Meteorological Satellites (EUMETSAT), as an example, it has 8461 channels, which can detect atmosphere vertically in details. To process these data more efficiently, compressing them and then reconstructing is required.</p><p>Considering their high correlation in spectral and spatial dimension, a new compressing and reconstructing network HCR is proposed. Concretely, the radiation brightness values are gridded so that one value at specific location is recongnized as a color value at this pixel. After normalizing by batch normalization, HCR compresses by convolution and reconstructs by deconvlution.</p><p>Carrying on IASI data, the RMSE of this new method was decreased by 5% at least compared with the result of principle component analysis (PCA) in the same compression ratio. The compression kernels encode tempetature information and reconstruct it. In reconstruction, the kernels' weights for likewise data are similar.</p><p>Codes are available <a href="https://github.com/skaudrey/hyp">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression an
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="cnn" scheme="http://yoursite.com/tags/cnn/"/>
    
      <category term="compress" scheme="http://yoursite.com/tags/compress/"/>
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
  </entry>
  
  <entry>
    <title>The summer school held in Harbin, Aug. 2017.</title>
    <link href="http://yoursite.com/posts/meetings/2019-12-19-harbin.html"/>
    <id>http://yoursite.com/posts/meetings/2019-12-19-harbin.html</id>
    <published>2019-12-19T11:46:44.328Z</published>
    <updated>2018-12-10T07:55:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check <a href="http://mss2017.hit.edu.cn/showSubjectDominWebSite.do">here</a> for more info. I gave a talk to the students in my lab after returning, <a href="https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html">here</a> are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation. <a id="more"></a> The themes given by those professors are listed below.</p><ul><li><p>Prof. Francois</p><p>Research Area：Variational data assimilation (VAR), especially 4DVAR.</p><p>Keynotes：The direvation of adjoint models, sensitivity analysis and the introduction of image assimilation. See the <a href="/assets/notes/harbin/François%20meeting%20minutes.pdf">minutes file</a> for details.</p></li><li><p>Prof. Jordan</p><p>Research Area：Statistical Learning</p><p>Keynotes: Summarize popular machine learning algorithms, and prove the convergence etc. See the <a href="/assets/notes/harbin/Jordan%20meeting%20minutes.pdf">minutes file</a> for details.</p></li><li><p>Prof. Jurgen</p><p>Research Area：AI, DL</p><p>Keynotes: The introduction of utilizing AI. Check more from his <a href="http://people.idsia.ch/~juergen/">home page</a>.</p></li><li><p>Prof. Ma</p><p>Research Area：Compression sensing.</p><p>Keynotes: The introduction of compression sensing and its applications.</p></li><li><p>Prof. Cai</p><p>Research Area：Statistical inference.</p><p>Keynotes: Statistical inference in high-dimensions. No slides, the minutes file is <a href="/assets/notes/harbin/Tony%20meeting%20minutes.pdf">here</a>.</p></li></ul><p>Slides and records are available in the <a href="https://pan.baidu.com/s/1jGj07koiMIV-MOf17N_jeg">baidu network disk</a> with password 6o2x. Enjoy yourself.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check &lt;a href=&quot;http://mss2017.hit.edu.cn/showSubjectDominWebSite.do&quot;&gt;here&lt;/a&gt; for more info. I gave a talk to the students in my lab after returning, &lt;a href=&quot;https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html&quot;&gt;here&lt;/a&gt; are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation.
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="summer school" scheme="http://yoursite.com/tags/summer-school/"/>
    
      <category term="mathematics" scheme="http://yoursite.com/tags/mathematics/"/>
    
  </entry>
  
  <entry>
    <title>Weather processes interpolation based on GPR</title>
    <link href="http://yoursite.com/posts/projects/2019-12-19-gpr.html"/>
    <id>http://yoursite.com/posts/projects/2019-12-19-gpr.html</id>
    <published>2019-12-19T11:46:44.321Z</published>
    <updated>2021-01-12T19:35:17.426Z</updated>
    
    <content type="html"><![CDATA[<p>This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale dependencies of weather processes. Weather processes with and without cyclones are discussed, and two interpolation methods are proposed. Check <a href="http://www.mdpi.com/2073-4433/9/5/194/pdf">paper</a> for more information. Codes are available <a href="https://github.com/skaudrey/gpml">here</a>.</p><h1 id="reference">Reference</h1><pre><code>Carl Edward Rasmussen. Gaussian process for Machine Learning.</code></pre><h1 id="acknowledgement">Acknowledgement</h1><pre><code>Thanks for the opening source toolbox GAUSSIAN PROCESS REGRESSION AND CLASSIFICATION Toolbox version 4.0, programmed by Carl et al.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale depen
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
  </entry>
  
  <entry>
    <title>Multivariate Interpolation of Wind Fields Based on Gaussian Process Regression, Jan. 24th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-gpr.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-gpr.html</id>
    <published>2019-12-19T11:46:44.316Z</published>
    <updated>2018-12-10T02:45:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check the <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html">projects' introduction</a> and <a href="https://github.com/skaudrey/gpml/">github</a> for more details.</p><p>Slides are avaliable <a href="/assets/slides/gpr/windInterpolation.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check th
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
      <category term="interpolation" scheme="http://yoursite.com/tags/interpolation/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks and Remote Sensing, July 26th, 2019.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-gan-rs.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-gan-rs.html</id>
    <published>2019-12-19T11:46:44.306Z</published>
    <updated>2019-08-18T14:16:24.132Z</updated>
    
    <content type="html"><![CDATA[<p>Some works using GANs handle the problems in remote sensing.</p><p>Check <a href="/assets/slides/GAN/GANRS.pdf">slide</a> for more details.``</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Some works using GANs handle the problems in remote sensing.&lt;/p&gt;
&lt;p&gt;Check &lt;a href=&quot;/assets/slides/GAN/GANRS.pdf&quot;&gt;slide&lt;/a&gt; for more deta
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="remote sensing images" scheme="http://yoursite.com/tags/remote-sensing-images/"/>
    
  </entry>
  
  <entry>
    <title>English Translation</title>
    <link href="http://yoursite.com/posts/English/2019-12-19-EnglishTranslation.html"/>
    <id>http://yoursite.com/posts/English/2019-12-19-EnglishTranslation.html</id>
    <published>2019-12-19T11:46:44.293Z</published>
    <updated>2021-01-12T19:29:55.777Z</updated>
    
    <content type="html"><![CDATA[<p>A collection of some state of the art popular words. <a id="more"></a></p><ul><li>A box of food：盒饭</li><li>group buying: 团购</li><li>go public: 上市</li><li>e-commerce: 电子商务</li><li>portal: 门户网站</li><li>the commitment of money and time: 投入时间和精力</li><li>cater to: 迎合……的爱好</li><li>send-off: 为某人送别，送行而举办的欢送会</li><li>sum up: wrap up</li><li>hold that thought: 记住……（以便之后可能派的上用场）</li><li>post-credit: 片尾的</li><li>saga: 长篇故事</li><li>appropriation funds: 挪用基金</li><li>cannibalize each other: 互相竞争</li><li>pay homage to sb./sth.: 向某人致敬</li><li>do-over: 失败后再次尝试 He requested a do-over.</li><li>the perils of drug abuse: 吸食毒品的危害</li><li>synthetic biology: 合成生物学</li><li>synthetic material: 合成材料</li><li>plastic face: 整容脸</li><li>an average face: 大众脸</li><li>a Michael Jackson lookalike: ***同款脸</li><li>oval face: 鹅蛋脸</li><li>square face: 方脸</li><li>from scratch: 白手起家 He built his own company from scratch.</li><li>beget: 导致，成为……的父亲 Poverty begets debts.</li><li>coax: 哄骗 His friends coaxed him into talking about his own problems.</li><li>hitherto: so far,迄今为止</li><li>cascade effect: 级联效应</li><li>domino effect: 多米诺骨牌效应</li><li>constitutional reform: 宪法改革</li><li>warm to: 对……越来越感兴趣，对……越来越有热情 I start to warm to this theme.</li><li>make a case for sth.: 为……提供支持 Your friend has been making a case for why we should hire you.</li><li>communal property: 公共财产</li><li>in style: 以令人深刻的方式 He wins in style.</li><li>open house: 接待日，开放日</li><li>of every strip: of all types</li><li>plain-spoken: 直言不讳的，坦率的</li><li>bail sb. out: 把某人保释出来 She keeps running up huge debts and asking friends to bail her out.</li><li>Tory: 英国保守党议员</li><li>MP: 下议院议员</li><li>kick the can down the road: 推迟处理（某事） I don't think we should kick the can down the road and leave it to the next generation.</li><li>indicative vote: 象征性投票</li><li>scrutiny: 仔细审查；under media scrutiny: 媒体审查 Foreign policy has come under close scrutiny recently.</li><li>footage: （尤指表现一个事件的）一段影片，片段 old film footage of the moon landing</li><li>runaway: 机场跑道；（时装表演）T台台道</li><li>pageant: （女子的）选美竞赛</li><li>media outlet: 媒体门户</li><li>corporal punishment: （尤其指对孩子的）体罚</li><li>scale back: 缩减，缩小</li><li>call it quits: 停止做 I'm calling it quits.</li><li>boot sb. off: 迫使……停止，迫使……离开</li><li>turn away: 拒绝；转过脸去 He had nowhere to stay so we couldn't turn him away.</li><li>service: 盛典 It was there in Paris in 1944 was marked in a service of thanksgiving.</li><li>harsh criticism: 过激的评论</li><li>guinea pig: 豚鼠，实验对象 They're asking for students to be guinea pigs for the new teaching methods.</li><li>chromosomal: 染色体的 Humans have 23 pairs of chromosomes in their bodies. chromosomal abnormalities: 染色体异常</li><li>sojourn: 停留，逗留 a week's sojourn in Paris</li><li>go on high alert: 高度警戒，高度戒备 With the war looming, the troops went on high alert.</li><li>epigenetics: 表观遗传学的</li><li>revert to: 回到，恢复 After her divorce she reverted to her maiden name.(未婚时的名字)</li><li>telomere:端粒（在染色体端粒上的着丝点）</li><li>cardiovascular: 心血管的</li><li>kayak: 独木舟，皮艇</li><li>serene: 平静的，安详的 She has a lovely serene face.</li><li>off: 离……不远 She lives just off the company.</li><li>glean: 缓慢艰难的收集（信息） These figures have been gleaned from a number of studies.</li><li>sauna: 桑拿房，桑拿浴</li><li>paid paternity leave: 带薪陪产假 Finland is knows for its top-ranked public schools, cheap universal daycare, long paid paternity leave, and multiple other programs and policies that improve quality of life.</li><li>no word of a lie: 一点不假</li><li>bona fide: 真实的 Only the bona fide fans of the Avengers can use the logo.</li><li>balloon: 增长 "Games of Thrones", which had a budget that started at $5 million an episode but ballooned to as much as $15 million an episode in the final season, helped create the current era of enormous spending.</li><li>pay off: 取得成功，得到好结果；还清 All her hard work paid off in the end, and she finally passed the exam.</li><li>uncharted: 地图上没有标明的；陌生的，未知的</li><li>waters: （某一江、河、湖、海的）水域 The party is sailing in uncharted waters.</li><li>up-ended：翻倒，颠倒 The man's entire life is upended.</li><li>status quo：现状；原来的状况 to defend/maintain the status quo</li><li>by a landslide：以压倒性的多数获胜 He apologized for having behaved badly in his younger years,and went on to win election in a landslide victory.</li><li>incumbent：在职的，现任的 He beat the incumbent.</li><li>rally sb. around sth.: 把……召集在……周围，让……团结在……周围</li><li>bulwark：堡垒，防御工事；保护者，防御者 a bulwark against capitalism</li><li>fictitious: 虚构的 All characters appearing in this work are fictitious. Any resemblance to real persons, living or dead, is purely coincidental.如有雷同，纯属巧合</li><li>poised: 准备就绪的 be poised to do sth.</li><li>on the frontline of ...: 在……的第一线</li><li>standoff: 僵局，僵持局面</li><li>root out: 找到并根除 root out efficiency in this company</li><li>coy: 含糊其辞的 be coy about sth.</li><li>win presidency, win country's presidential election, take over the leadership of a country：当选为总统</li><li>a bitter blow: 沉重的打击 The emphatic victory of Volodymyr Zelenskiy, 41, is a bitter blow fr incumbent Petro Poroshenko who tried to rally Ukrainians around the flag by casting himself as a bulwark against Russia and a champion of Ukrainian identity.</li><li>race identity: 民族认同感</li><li>malaise: 疾病</li><li>toss around: 抛出，提出（想法、建议或说法）</li><li>cynicism: 玩世不恭；愤世嫉俗</li><li>mind-set: 观念；思维模式</li><li>protagonist: 主角</li><li>for all: 尽管 For all her qualifications, she's still useless at the job.</li><li>sensationalist: 耸人听闻的 a sensationalist headline</li><li>cast sb. as: 把某人塑造成</li><li>millennials: 千禧世代的人</li><li>profit from: 从……中获利</li><li>austerity: （经济的）紧缩；严格节制消费</li><li>alive to:意识到，认识到</li><li>gentrification: 中产阶级化</li><li>ripple effect: 连锁反应</li><li>owner-occupied: 业主居住的，自己使用的</li><li>households led by single women：户主为单身女性的住户</li><li>single-use: 一次性的</li><li>per capita: 人均地</li><li>drug cartels, marcos: 毒枭</li><li>infiltrate: 潜入，渗透 Having been used to infiltrate the drug cartels, or narcos, by the Italian state, he now faces debts and death threats. He accuses the authorities of abandoning him.</li><li>police informants: 警方线人</li><li>state witness: 污点证人 Giannino is one of many former police informants and</li><li>witness protection: 证人保护</li><li>information: 情报</li><li>instruments: 某种手段</li><li>mafia clans: 黑手党家族，组织</li><li>interior ministry: 内政部</li><li>collaborators: 线人，中性词</li><li>manage to start over: 真正重新开始</li><li>mobsters: 犯罪集团成员</li><li>at large: 普遍地，全体地；逍遥法外，（罪犯）在逃 What's worse is that there is no psychological support for these men and women who are forced to live like prisoners, while mobsters are still at large. the opinion of the public at large</li><li>Matrix: 黑客帝国</li><li>per se: 本质上，本身 == by itself The drug is not harmful per se, but it is</li><li>dystopian: 反乌托邦的</li><li>mind-expanding: 会导致幻觉的</li><li>sth. wed A with B: 结合 The novel weds mystery and romance.</li><li>balletic cool of Hong Kong action cinema: 香港动作电影中行云流水的炫酷感</li><li>touchstone: 试金石</li><li>Rorschach blot: 罗夏墨迹，比喻仁者见仁智者见智</li><li>ideology: 意识形态</li><li>marvel: 不可思议的事或者人，奇迹 We marvel at the magnificent view of the nature.</li><li>conspiracy: 阴谋，密谋 conspiracy theories: 阴谋论</li><li>technophobic: 反科技的</li><li>dictation: 强令；专断</li><li>the share of: 占比，份额</li><li>the General Social Survey: （美国）综合社会调查 The share of adults aged 18 to 34 reporting that they were "very happy" in life tell to 25 percent--the lowest level that the General Social Survey has ever recorded for the population.</li><li>the ebb and flow: 起伏</li><li>but for: 如果不是</li><li>buffer: 减少</li><li>That's not the end of the story. 事情没那么简单</li><li>exodus: （大批人的）退出，离开；出埃及记</li><li>woo: 争取，努力说服；求爱 The store woos customers by offering low prices.</li><li>smog breaks: 雾霾假</li><li>pollution bonuses: 污染津贴</li><li>put off: 使反感；使对……失去兴趣 Don't be put off by how it looks--it tastes delicious.</li><li>pose: 造成 According to the United Nations Environment Programme, some 92 percent of people in Asia-Pacific region are exposed to levels of air pollution that pose a significant risk to health.</li><li>whole grains: 全谷物</li><li>shave off: 缩减，调低 Too much salt -- and not enough whole grains, fruits and vegetables -- may be shaving years off our lives, a new analysis suggests.</li><li>chic: 时髦的，雅致的 She is always so chic.</li><li>cringe-makingly: 使人尴尬地; close to home; mortifying When the 70s were the recent past, they were cringe-makingly close to home, and therefore easy to poke fun at.</li><li>poke fun at: 嘲弄，开玩笑 He was poked fun at because he could not ride a bike.</li><li>rose-tinted: 玫瑰色的</li><li>consign: 把……置于 The decade is coming to stand for classic chic, in the way that the 50s previously did before being consigned to ancient history.</li><li>catwalk: T台</li><li>plunder: 窃取，剽窃 The rich provinces have been plundered by the invaders.</li><li>trickle-down effect: 滴漏效应</li><li>Cambrian: 寒武纪的 the Cambrian Explosion</li><li>shale: 页岩</li><li>cnidarian: 刺细胞动物</li><li>sedimentary: 沉积的</li><li>be loaded with: 含有大量的……，富含 Most fast foods are loaded with fat.</li><li>comb jelly: 栉水母</li><li>medusa: 水母</li><li>venomous: 有毒的；分泌毒液的</li><li>tentacle: 触手，触角</li><li>worst-case scenario: 最坏的可能，最早的情况</li><li>surreal: 离奇的，荒诞的，超现实的</li><li>collage: （不同事物的）大杂烩；拼贴画 The film is a shoddy collage of various effects.</li><li>frontal lobe: 大脑额叶</li><li>go dark: 进入休眠；关闭</li><li>blind spot: 盲点；无知领域 He has a blind spot about personal finance.</li><li>home in on: 集中注意力于；瞄准，对准 The report homed in on the weakness in the management structure.</li><li>Washington Post: 华盛顿邮报</li><li>New Republic: 新共和</li><li>serotonin: 血清素</li><li>norepinephrine: 去甲肾上腺素<br /></li><li>in that: 因为 Reiwa--Japan's new imperial era--adheres to the established naming custom in that it comprises two kanji and is easy to read and write.</li><li>stanza: （诗歌的）节，段 The characters are taken from a stanza in a poem about plum blossoms that appears in Man'yoshu（万叶集）, the oldest existing collection of Japanese poetry, compiled sometime after 759.</li><li>at that: （用于提供额外信息）而且 She managed to buy a car after all--and a nice one at that.</li><li>news agency: 新闻社</li><li>emeritus professor: 荣誉教授</li><li>flip on one's head: 颠覆 The entire world of computers has been flipped on its head</li><li>a raft of: 大量的</li><li>pull in: 挣钱 In that area, Apple has fallen behind companies like Google, Amazon and Microsoft, which now pull in significant revenue from selling cloud services, which is a fancy term for data center usage and storage.</li><li>break a leg: 成功 The question now is whether Apple can break a leg.</li><li>mile-high barbie: 高空芭比</li><li>sick note: 病假条, a note from the doctor</li><li>flight attendants: 空乘； steward</li><li>false eyelashes: 假睫毛</li><li>Global Gender Gap report: 世界性别差异报告</li><li>flat shoes: 平底鞋</li><li>follow suit: 效仿，模仿 When one store reduces its prices of products, the rest usually follow suit.</li><li>see off: 目送，打败 see off the enemy</li><li>chauvinistic: 沙文主义的 —&gt; chauvinist: 大男子主义的</li><li>grooming: （人的）打扮，装束</li><li>diktats: 勒令，死命令 Air travel isn't alone when it comes to chauvinistic grooming diktats.</li><li>quasi: 类似……，准…… --&gt; quasi-official: 准官方的，半官方的</li><li>gratuitous: 无偿的，免费的</li><li>perilously: 危险地</li><li>totter about in heels: 穿着高跟鞋跌跌撞撞</li><li>figure skating: 花样滑冰</li><li>back-to-back: 接连的</li><li>Grand Prix: 国际大奖赛</li><li>triumph: 取得巨大成功，胜利；triumph over; Good usually triumphs evil in the end.</li><li>black-clad: 黑衣的 A day before the competition starts, a black-clad Hanyu landed several clean quad jumps at practice.</li><li>painkillers: 止痛药</li><li>relish: 期盼，渴望</li><li>world title: 世界冠军</li><li>artefact: 手工艺品</li><li>rafter: 橼</li><li>dilemma: （进退两难的）窘境，困境 the dilemma of adolescence, digital dilemma</li><li>counselling: （职业性的）辅导，咨询</li><li>jettison: 放弃，丢弃 The scheme is jettisoned.</li><li>impersonate: 冒充，假扮 impersonate to police officer</li><li>hoarder: 囤户，贮藏者</li><li>quite a thing: 不得了的事情 Even if one isn't harbouring toxic secrets, that's still quite a thing.</li><li>bare minimum: 最起码 What's the bare minimum you'd advise people to do?</li><li>polling exercise: 选举活动, election</li><li>pin down: 确认</li><li>fray: 热闹的活动，混乱的场面 enter/take part in the fray</li><li>blender: 搅拌机</li><li>rally: 集合，召集</li><li>crisscross: （在……范围内）纵横交错，交叉 Railway lines crisscross the country.</li><li>corral: 把一群人集中起来（尤指控制起来）</li><li>be bombarded with: 被……连续轰炸的；被……围攻的</li><li>rhetoric: 雄辩言辞，煽动性语言 Indian politicians love a good rally: They crisscross the country, corralling locals into large tents where they're bombarded with campaign rhetoric.</li><li>field: 组织参加 England fielded a young team in the World Cup.</li><li>peddle: 兜售，vendor: 商贩</li><li>skewer: 用串肉扦串；讽刺；skewer the mutton 串羊肉</li><li>cranky: 古怪的 cranky ideas, cranky schemes. He is always restless and cranky after school.</li><li>hit TV series: 热播剧</li><li>no mean feat: 了不起的成就 His mother is no mean artist.</li><li>top-ten trending hashtags: 十大热门话题</li><li>premiere: 公演</li><li>on speaking terms: 和睦相处，和好如初</li><li>bickering: 争吵</li><li>unfilial: 不孝的</li><li>hold a grudge against: 对……怀有纪元；怀恨在心</li><li>pamper: 纵容；娇惯</li><li>filial piety: 孝道；孝顺</li><li>unswerving: （信任或信仰）坚定地，始终如一的</li><li>throw tantrums: 大发脾气</li><li>grudgingly: 勉强地，不情愿地</li><li>unlikely hit: 出人意料的走红 "Bohemia Rhapsody" was such an unlikely song that became an even more unlikely hit.</li><li>biopic: 传记片</li><li>single: 单曲</li><li>cameo: 客串</li><li>label: 唱片公司</li><li>walk out on the label: 退出……组织</li><li>sketch: 小品</li><li>beat-up: 破旧的， a 1970s beat-up AMC Pacer</li><li>good call: 好主意</li><li>appreciation: 升值</li><li>joint 120th:并列……名</li><li>behemoth: 庞然大物</li><li>colossus: 极具影响力（或能力）的人物；关系重大的事物 Disney moves from behemoth to colossus with closing of Fox deal.</li><li>conglomerate: 联合大企业，企业集团</li><li>bulk up: 增强肌肉，增重，扩大规模</li><li>cord-cutting: （电视用户）取消订阅</li><li>snowplow: 扫雪机</li><li>affluent: 富裕的，富足的</li><li>chug: 突突地缓慢行驶</li><li>take sth. to extremes: 使……走至极端</li><li>pay off: 贿赂</li><li>go to great lengths: 不遗余力，竭尽全力</li><li>humiliation: 屈辱，丢脸</li><li>unfathomable: 高深莫测的；不可理解的</li><li>balance billing: 差额账单</li><li>kick in: 生效</li><li>loopholes: 漏洞（法律等的） Sometimes, bizarre loopholes kick in at the darkest moments.</li><li>stillbirth: 死胎</li><li>wind up: 以……告终，最终沦落到</li><li>plunge into: 负债，使陷入</li><li>imminent: （尤指令人不快的事）即将发生的，逼近的 Many species of animal are in imminent danger of extinction.</li><li>garnish: 为偿付债务（或理赔）而扣押（钱财，尤指个人薪资）</li><li>tip of the iceberg: 冰山一角</li><li>be looking to do sth.: 试图找到做某事的方法</li><li>wage: 发动（战争）；组织（活动）</li><li>subsidies: 补贴，津贴</li><li>viral: 病毒的；病毒般传播的</li><li>tie-up: 联合，合作</li><li>prospectus: （学校的）招生简章；（企业的）招股说明书</li><li>roll back: （逐渐）减少，削减某物</li><li>stick around: 停留，逗留</li><li>a host of: 许多，一大群 The company has completely rewritten the rules for the coffee business in China and has impacted Starbucks as well as a host of smaller players.</li><li>brand recognition: 品牌知名度</li><li>sweeping: 影响广泛的，规模宏大的 The flu was sweeping the city.</li><li>tantalizing: 吸引人的，撩人的</li><li>direct knowledge: 第一手的资料</li><li>hearing: 听证会</li><li>credentials: 资历，资格证明 academic credentials: 学历证书</li><li>Coalition: 联盟</li><li>contentious: 有争议的 a contentious decision</li><li>startling: 惊人的，令人吃惊的</li><li>dug into: 深入调查</li><li>grant proposal: 资助申请</li><li>rah-rah: 盲目的热情</li><li>women's empowerment: 女权</li><li>on track: 有望成功 She is on track to be promoted.</li><li>title: （书的）一种，一本 The title was the book giant's biggest success of the last year.</li><li>bidding war: 竞标大战</li><li>juggernaut: 不可抗拒的巨大力量；无法控制的机构；重型货车</li><li>global tour: 全球巡回 Obama has been promoting her memoir with a hugely popular global tour.</li><li>lucrative: 赚钱的，赢利的</li><li>strike a deal: 达成协议</li><li>contemptuous: 轻蔑的，鄙视的 She gave him a contemptuous look.</li><li>internal migrants: 外地人</li><li>stereotype: 对……形成模式化的看法；（尤指）对……有成见 negative stereotype: 负面成见</li><li>to see and touch: 看得见摸得着</li><li>census: （官方的）调查，统计；（尤指）人口普查 She was stopped in her car for a traffic census.</li><li>egregious: （错误等）极其严重的，极坏的；令人震惊的 egregious abuses of copyright</li><li>conglomerate: 联合大企业，企业集团</li><li>open position: 空缺职位</li><li>sympathisers: 支持者</li><li>on grounds of: 以……为由 China's employment law prohibits discrimination on grounds of ethnicity, sex, religion, disability, social backgrounds and health.</li><li>be spotted by: 被识别出</li><li>call for: （公开）呼吁，要求</li><li>region discrimination: 地域歧视</li><li>philandering: 花心的，轻薄的</li><li>written all over (one's) face Philandering men have unfaithfulness written all over faces.</li><li>poach: 偷猎，偷捕；挖走 Several employees have been poached by a rival firm.</li><li>cheating: poaching, unfaithful, flaky, infidelity 不忠, seduce: 勾引</li><li>heterosexual: 异性恋的</li><li>stardom: 明星的地位（身份）</li><li>diplomatic chill: 外交寒冰</li><li>put sth. on hold: 暂停某事，put high school in Japan on hold</li><li>wannabe: 梦想成功的人</li><li>shot: 尝试，努力， get a shot，试一把</li><li>a select few: 少数几人</li><li>trainee: 练习生</li><li>acrimony: 怨恨的言辞</li><li>diplomatic ties: 外交关系</li><li>dent: 削弱</li><li>speak to: 证明某事存在或正确</li><li>make it big: 功成名就</li><li>act: 演出团体</li><li>fuel: 激起，加强，煽动； to fuel rumours; to fuel fears</li><li>backlash: （社会或政治方面的）强烈反响；强烈反对</li><li>lactic acid: 乳酸； acid remark: 尖酸刻薄的话语</li><li>olfactory receptor: 嗅觉感受器</li><li>hone in on: 聚焦，锁定在某物上， hone in on our dour</li><li>Aedes aegypti mosquitoes: 埃及伊蚊</li><li>Zika: 寨卡； dengue: 登革热</li><li>process of elimination: 排除法</li><li>mutant: 变体的，变异的</li><li>task sb. with sth.: 给……委派任务</li><li>repellant: 驱蚊剂，驱虫剂</li><li>give voice to sb.: 让某人发声；让某人表达意见或情感</li><li>amyotrophic lateral sclerosis: 肌萎缩性脊髓侧索硬化症</li><li>confine: 限制，把……局限在</li><li>laborious: 耗时费力的；艰巨的，艰难的</li><li>paralysed: 麻痹的，瘫痪的；不知所措的</li><li>vocal-tract: 声道</li><li>epilepsy: 羊角风，癫痫</li><li>implant: （手术）植入，移植</li><li>think tank: 智囊团，智库</li><li>frugal: 节俭的，简朴的；（膳食）少量的</li><li>sway: 影响，使动摇；sway back and forth</li><li>breeders: 饲养员</li><li>hatcheries: 孵化场</li><li>multiply: 成倍增加 Animal breeders, hatcheries, vets and trucking companies multiply.</li><li>shipping containers: 集装箱</li><li>gadgets: 小玩意，小机件</li><li>revere: 崇敬</li><li>win over: 说服</li><li>interior spaces and exterior forms: 室内空间与建筑外观 Winning the fifth Pritzker Architecture Prize in 1983, Pei was cites as giving the 20th century "some of its most beautiful interior spaces and exterior forms ... His versatility and skill in the use of materials and skill in the use of materials approach the level of poetry."</li><li>trapezoid: 不规则四边形</li><li>crowd-pleasing: 受大众欢迎的</li><li>dapper: 衣冠楚楚的 In person, Pei was dapper, good-humoured, charming and usually modest.</li><li>stylistic originality: 形式主义的创新 "Stylistic originality is not my purpose," he said. "I want to find the originality in the time, the place and the problem."</li><li>hoodie: 连帽衫</li><li>wardrobes: （某人的）全部衣服；衣柜，衣橱</li><li>insurers: 保险公司</li><li>automakers: 汽车制造商</li><li>dress code: 着装规范</li><li>morph into sth.: 变形成某物 You can morph into another person.</li><li>dress down: 穿着随意</li><li>dress up: 穿着正式</li><li>effortless: 毫不费力的</li><li>put their finger on sth.: 指出某物的问题所在，指出某事的原因 There was something odd about him, but I couldn't put my finger on it.</li><li>upscale: 高档的；an upscale brand name</li><li>eye-popping: 令人瞠目结舌的，令人大开眼界的</li><li>blazer: 休闲西装外套</li><li>loafers: 乐福鞋</li><li>staples: 主要产品，（某物）主要部分</li><li>stylist: 造型设计师</li><li>a touch of: 一点点</li><li>exorbitantly: 过度地，过高地; exorbitant rates and interest</li><li>execs: 经理，主管；executives;</li><li>CEO: executive men; high-powered men</li><li>the royals, British's royal family, the monarchy, the royal family: 王室</li><li>the press, social media, medium: 媒体</li><li>camp: 盘踞，蹲守</li><li>wing: 建筑的侧翼部分</li><li>Duke and Duchess: 公爵和公爵夫人</li><li>make an entrance: 正式登场，隆重登场</li><li>scoop: 抢在……之前报道 The first world of his arrival came via <span class="citation" data-cites="sussexroyal">@sussexroyal</span>, the couple's official Instagram account, which scooped the press with a message reading: "It's a boy!"</li><li>strained: 关系恶化的，紧张的</li><li>tone down: 使（通常指文章或讲话）缓和，使温和</li><li>muckraking: （尤其指报纸和记者）搜集并揭露丑闻 One is that British newspapers have toned down their royal muckraking.</li><li>libel: 诽谤 Readers can get their royal gossip from foreign publications that are not bound by British privacy or libel laws.</li><li>up: 升级, up one's game</li><li>face-judger: 颜控</li><li>unprecedented job crunch: 史无前例的就业危机</li><li>roll out: 推出（新产品，服务等）</li><li>overseas jobs: 海外工作</li><li>skilled labor: 技术劳工，技术人才</li><li>family-run conglomerates: 家族大型企业</li><li>chaebol: 财阀</li><li>legacy workers: 关系户</li><li>a glut of: 大量的（供过于求的）</li><li>get their hands dirty: 体力活 Even amid a glut of over-educated and under-employed graduates, most refuses to "get their hands dirty".</li><li>degrading: 低贱的，丢脸的 This advertisement is degrading to women.</li><li>rosy: 玫瑰红的，有希望的 The future is looking very rosy for the company.</li><li>menial work: 乏味的（枯燥的）工作</li><li>over-education and under-employed: 学历过剩与就业不充分</li><li>telecommunications giant: 电信巨头</li><li>hold sb. accountable for: 让某人为……负责</li><li>intellectual-property theft: 知识产权窃取</li><li>at stake: 处于危险境地；处于成败关头 What's ultimately at stake here is the future of 5G networks around the world -- the ultrafast connection systems that will soon link smartphones, enable driverless automobiles and, potentially, revolutionize warfare.</li><li>federal approval: 联邦政府的批准</li><li>outright: 直截了当的</li><li>stockpile: 大量储备，囤积</li><li>in anticipation of: 预料，预见</li><li>break new ground: 开辟新天地，创新; groundbreaking</li><li>fashion line: 时尚系列</li><li>gilded: 镀金的；富有的，上层阶级的</li><li>maison: （法语词）公司，品牌；house</li><li>storied: 经常提及的，有名的</li><li>heritage: 遗产；经典的品牌 It joins such storied heritage brands as Dior, Givenchy, Celine and Fendi and positions Rihanna as a breakthrough designer on a number of levels.</li><li>revenue: 收益，收益；first quarter revenue</li><li>insensitivity: 冷漠 cultural insensitivity and discrimination</li><li>make strides: 取得进展</li><li>right: 纠正</li><li>appoint sb. as sth. 任命某人做……</li><li>inclusivity: 包容性</li><li>gender identity: 性别认同</li><li>predispose: 有……的倾向，使倾向于做……；predispose sb. to sth.</li><li>procrastination: 拖延，拖延症; put things off</li><li>hole up: 躲藏，把……监禁</li><li>hand down: 把……传下来 These people survived more because they avoided conflict, and these genes were handed down to future generations.</li><li>the buck stops with sb.: 某人责无旁贷</li><li>innate trait: 天资过人 Talent is made with practice and commitment, not an innate trait.</li><li>dire wolf: 冰原狼</li><li>husky: 哈士奇</li><li>pay the price: 吃苦头，付出代价</li><li>stature: 高度（尤指身高）; a woman of short stature</li><li>lupine: 像狼的，与狼相关的 Enter Siberian huskies. Their shaggy, gray and white fur, pointy ears and lupine facial features make them near-doppelgangers of dire wolves.</li><li>high-maintenance breed: 难饲养的品种</li><li>clash: 冲突，矛盾</li><li>veterinarian: 兽医</li><li>be hooked on: 沉迷于，对……上瘾</li><li>play out: 发生</li><li>accessory: 附属品</li><li>debut: 首次公演，首次亮相；注意t不发音；debuted: -ted发音为te</li><li>pun: 双关语 No pun intended.</li><li>creepy: 怪异的，令人毛骨悚然的；让人起鸡皮疙瘩的</li><li>proliferate: 激增</li><li>snoop: （对别人的私生活）窥探 But as smart speakers from Amazon, Apple and other technology giants proliferate, concerns that they might be digitally snooping have become more widespread.</li><li>utterance: 说话，讲话</li><li>listen out for: 注意听，等着听</li><li>mute button: 静音按钮</li><li>eavesdrop: 偷听</li><li>blithely: 快活的；无忧无虑的</li><li>tech giants: 科技巨头</li><li>trimesters: （妊娠中的）三月期；（一学年三学期制的）一学期</li><li>get one's way: 如愿以偿</li><li>convict of: 宣判有罪</li><li>incest: 乱伦</li><li>"heartbeat" law: 心跳法案</li><li>drive sb. to extremes: In the abortion argument, both sides long ago drove each other to extremes.</li><li>pro-life: 反对堕胎的；pro-choice: 支持堕胎的</li><li>fundamentalist: 原教旨主义的</li><li>fertilised egg: 受精卵</li><li>morning-after pill: 女用口服避孕药</li><li>gridlock: 使陷入僵局</li><li>smoke sb. out: 用烟把人或动物熏出来；使……公之于众</li><li>browbeat: 恐吓；吓唬</li><li>A stay chained to B: A长时间待在B处 But that doesn't stop some managers from demanding that workers stay chained to their desk for long periods. Presenteeism is the curse of the modern office worker.</li><li>self-perpetuating: 自我持续的</li><li>underlings: 下属，部下</li><li>toil: 辛勤劳动； toil away: 忙个不停</li><li>self-delusion: 自我错觉，自欺欺人</li><li>inmates: （机构）入住者，居住者（如监狱犯人）</li><li>sequels: 续集</li><li>revenue: 收入，收益；national revenue; enterprise revenue</li><li>virtually: 几乎地</li><li>whet sb.'s appetite: 刺激某人的胃口</li><li>preawareness: 预先意识，预认识</li><li>juggernaut: 不可抗拒的巨大力量（或组织）</li><li>throttle: 阻挡，压制；扼杀 Inevitably, the sequels juggernaut is throttling fresh ideas, the lifeblood of any creative activity.</li><li>imperil: 危及，使陷入危险</li><li>consoling: 可安慰的，安心的</li><li>filmgoers: 观影人</li><li>Gallup organisation: 盖洛普民意调查组织</li><li>appalling: 使人惊骇的，极为恶劣的</li><li>vent: 发泄，宣泄</li><li>assuage: 缓和，减轻（不快）；assuage fears, assuage one's desire for knowledge</li><li>at large: 整体上，总体上</li><li>by and large: 一般来说</li><li>sprinkle: 随意点缀，在……中随意穿插</li><li>tic: 抽搐，语言习惯</li><li>register: 语言风格</li><li>kidspeak: 宝宝体</li><li>rhetorical: 修辞的，与修辞有关的</li><li>blow: 打击</li><li>couch: 以（某种方式）表达: a contract couched in terms</li><li>-esque: 像……的：toddler-esque</li><li>follow suit: 跟着做，照着做</li><li>Gen Z: Z世代，泛指1996到2010年间出生的人</li><li>compound: 使加重，使恶化</li><li>exorbitant: （价格、要求等）过分的，过高的</li><li>spook: 惊吓，吓唬；spooky：吓人的</li><li>may well do sth.: 很有可能做某事</li><li>lurk: 隐藏着，潜水 Fear lurks beneath the surface.</li><li>ponder: 沉思，考虑，比consider更深沉和严肃一些 She pondered over her next words.</li><li>diameter: 直径</li><li>collide: （尤指移动的物体）相撞，碰撞</li><li>meteorite: 陨星，陨石</li><li>boil down to: 归根结底是</li><li>deflect: （使）转向，（使）偏斜</li><li>consensus: 一致的意见，共识</li><li>bumper car: 碰碰车</li><li>stride: 大步走，阔步行走 She strode toward the door.</li><li>crimson: 深红色的，暗红色的</li><li>Tory: 保守党; factions: 派系，派别，小集团</li><li>insurrection: 叛乱，造反，起义</li><li>step down: 辞职，下台，退位</li><li>bargaining power: 谈判的能力，讨价还价的能力</li><li>be hostage to: 成为人质 Her instincts were right: increasing the Tories' majority would have allowed her to increase her bargaining power with her party and ensure that she was not hostage to pro- or anti- Brexit factions.</li><li>debacle: 垮台，惨败</li><li>grapple: 努力设法解决；grapple with sth.</li><li>be torn over sth.: 被……折磨，被……吞噬；pull apart</li><li>representative and direct democracy: 代议民主制和直接民主制</li><li>bring to tears</li><li>bill（未通过的,measure）, deal, law（通过的）: 法案</li><li>unicorns: 独角兽</li><li>world-beating: 世界一流的</li><li>miraculous: 令人惊奇的，不可思议的</li><li>cumulative: 累计的，渐增的</li><li>ride-hailing: 叫车的；hail a taxi</li><li>doctrine: 信条，学说</li><li>blitzscaling: 闪电式扩张</li><li>winner takes all: 赢者通吃</li><li>contest: 竞争，角逐</li><li>promiscuous: 淫乱的，杂乱的</li><li>in full swing: 处于……顶峰，处于……高潮</li><li>extrapolate: 推断，退知</li><li>satirical: 讽刺的；a well-known satirical magazine</li><li>snatcher: 掠夺者，抢夺者</li><li>suspense: （小说或电影中的）悬疑</li><li>purringly: 猫打呼噜地；（车辆）正常运转时发出呼噜声地</li><li>shiftless: 懒惰的，不思进取的</li><li>stinky: 有强烈气味的；有臭味的</li><li>squalid: 脏的；（尤指地方因贫穷）极其肮脏的</li><li>cynical: 见利忘义的，愤世嫉俗的</li><li>a stroke for fortune: 一件幸运的事</li><li>lucrative: 赚钱的</li><li>distrait: 注意力分散的，心不在焉的</li><li>brazen: 黄铜色的，厚脸皮的</li><li>sucker: 容易受骗的人；傻瓜</li><li>a distorting mirror: 哈哈镜</li><li>tendril: （攀援植物的）卷须；卷须状物</li><li>Mount Everest, Mount Chomolungma, Everest: 珠穆朗玛峰</li><li>conga line: 康加舞</li><li>aspirants: 报复不凡者，有志者</li><li>subtract: 分包，转包</li><li>hubris: 骄傲自大，（希腊悲剧）傲睨神明</li><li>indelibly: 无法忘记地；不可磨灭地</li><li>coin: 创造，杜撰</li><li>acronym: 首字母缩略词</li><li>never smokers: 从不吸烟者</li><li>asbestos: 石棉</li><li>under scrutiny: 可能的原因；在检查中</li><li>noteworthy: 值得注意的</li><li>wok cooking: 炒锅翻炒的烹饪</li><li>aerosols: 气雾剂，喷雾剂</li><li>adamant: 坚决的；固执的</li><li>incriminate: 使（某人）看似有罪 Some LCIN women believe that chemicals in perfume, makeup, aerosols, or cleaning products may be to blame, but Peake is adamant that there is no evidence to incriminate any of them.</li><li>expired food: 过期食品</li><li>best-before dates: 赏味期</li><li>waste mountain: 垃圾山</li><li>inadvertently: 不经意地；无意地</li><li>mould: 霉菌</li><li>take the plunge: （尤指经过长时间考虑后）果断行事，毅然决定</li><li>bin: 丢弃；扔掉</li><li>disorient: 使迷失方向</li><li>be to blame: 对（坏事）有责任 American food goes uneaten each year, and the disorienting effect of the US date labelling system is in large part to blame.</li><li>leaden: 铅灰色的</li><li>carnival: 嘉年华，狂欢节</li><li>hail: 招呼，呼喊</li><li>trophy: 奖杯；猎物身上取下来的纪念品</li><li>miss out on sth.; 错失</li><li>league title: 英超冠军</li><li>agony: 极大的痛苦，苦恼</li><li>ecstatic: 狂喜的，欣喜若狂的</li><li>repertoire: 全部曲目，全部节目</li><li>anthem: 颂歌，赞歌</li><li>bleary-eyed: 睡眼惺忪的</li><li>jubilant: （尤指因成功而）欢欣的，喜气洋洋的</li><li>ripple through: 扩散，涌起</li><li>stake out: 立桩标出，标明领地</li><li>commemorative: 纪念性的</li><li>offside: 越位；goal kick: 球门球；penalty kick: 点球；corner kick: 角球；dive for the ball: 鱼跃；throw in: 界外球</li><li>war of words: 口水战；争论</li><li>parochial: 偏狭的，地方观念的；和教区有关的</li><li>there is a lot to more it: 某事不是那么简单</li><li>serve the interests of sb./sth.: 满足需要，符合利益</li><li>freelance translator: 自由译者</li><li>takes its/ a troll: 产生损失；造成恶果或不良影响</li><li>tinker: 小修小补</li><li>look on: 看待</li><li>conduit: （传递想法、新闻、武器等的）渠道；管道，导管</li><li>handles: （尤指奇怪的）名字，名称；把手；social media handles: 社交媒体账号</li><li>the U.S. Department of State (DOS): 美国国务院</li><li>the State Council of the People's Republic of China: 中华人民共和国国务院</li><li>platform shoes: 坡跟鞋</li><li>screen: 检查，筛选</li><li>set foot on U.S. soil: 踏入美国领土</li><li>the American Civil Liberties Union: 美国公民自由联盟</li><li>infringe on: 侵犯（权利或自由）</li><li>freedom of speech and association: 言论和结社自由</li><li>take off: 起飞，突然开始成功，开始走红</li><li>outlet: 经销点，专卖店</li><li>avid: 极度渴望的；热衷的</li><li>take to: 开始喜欢上 They take to Chinese food soon after they come here.</li><li>rival: 竞争对手</li><li>spout: （容器的）嘴 Many residents like to relax in teahouses, sipping tea served gracefully by waiters from brass pots with long spouts.</li><li>internet viral: wanghong: 网红</li><li>photogenic decor: 上相的装潢</li><li>rip: 拷贝（音频等）；撕开</li><li>tracks: （光盘、录音磁带上的）一首歌</li><li>ephemeral: 短暂的，转瞬即逝的 Fame is largely ephemeral.</li><li>the writing is on the wall: 已有不祥之兆</li><li>treat sb. to sth.: 请客 Let me treat you to a drink. I am going to treat myself to a new pair of sunglasses.</li><li>classified: 机密的： classified documents</li><li>briefings: 简报，简要明示</li><li>Chamber of Commerce: 商会</li><li>sparkiness: 有活力的状态</li><li>edge: 优势，优越之处；刀刃</li><li>takeover: 接管，接受（尤其指收购）</li><li>coagent: 帮手</li><li>all told: 总共，一共 There are 20 people in our team all told.</li><li>spook: 间谍，密探；鬼</li><li>counterintelligence: 反情报行动，反间谍活动</li><li>baptism: 初次体验；考验</li><li>blitzkrieg: 闪电战，闪击战</li><li>come in handy: 派的上用场</li><li>opine: 认为，发表意见</li><li>solicit: 乞求，征求</li><li>courier: （地下或间谍组织的）情报员</li><li>ingratiation: 逢迎，讨好</li><li>guerrilla: 游击队员</li><li>D-Day landings: 诺曼底登陆</li><li>eavesdrop: 偷听，窃听 She eavesdropped on an important conversation.</li><li>recapture: 夺回，收复</li><li>convene: 召集（会议） He has convened a secret meeting of military personnel.</li><li>petticoat: 衬裙</li><li>directorate: （政府中专管某一事项的）司、部、处；（公司的）董事会</li><li>focal point: （注意力的）焦点，重点</li><li>henceforth: 从此，从此之后</li><li>portable:轻便的，可携带的</li><li>veritable: 十足的，名副其实的</li><li>call for A to B: 要求对B的A，A是名词 Railway timetables, time-stamped telegrams and factory disciplines all called for stricter conformity to the time of the clock.</li><li>conformity: 遵从，遵守</li><li>hallmark: 特点，特征</li><li>imposition: （新规则的）实施，颁布</li><li>backwardness: 落后的状况（经济，文化等）</li><li>epidemic: （迅速的）泛滥，蔓延；流行病</li><li>tune into: 收看，收听；tune into next week's show</li><li>eccentric: 古怪的</li><li>breakout: 风靡一时的，突然成功的</li><li>wardrobe tips: 穿搭技巧</li><li>frisson: 兴奋感，震颤，恐惧感</li><li>retro: 复古的，怀旧的；a retro style, a retro music</li><li>leapfrog: 进行跳木马游戏；超越</li><li>apron: 围裙；cami: 贴身小背心；spaghetti strap: 吊带裙；black lace cami: 黑色蕾丝吊带背心</li><li>pull off: 完美驾驭（衣服，风格等）</li><li>seam: 一点儿; a seam of tomboy: 一点儿男孩子气</li><li>tuck: 把（衣服等的末端）塞进</li><li>stay-in-your-lane: 不出格的，中规中矩的</li><li>cndescending: 高人一等的，居高临下的 Will you condescend to visit us?</li><li>geezer: 怪老头</li><li>narcissistic: 自恋的</li><li>overbearing: 专横的，盛气凌人的</li><li>suffocating: 窒息的</li><li>honorific: 敬语</li><li>stricture: 约束，限制；financial stricture; to impose a stricture on sth.</li><li>frown upon: 不赞成，不许</li><li>take hold: 掌握控制权，确立地位</li><li>deference: 尊敬，尊崇</li><li>upstarts: （尤指年轻的）突然发迹的人，暴发户，新贵</li><li>meltdown: 融毁，灾难</li><li>prosthetics: 假体（人造的身体部分）；义肢</li><li>light touch: 轻触（形容做事信手拈来、毫不费力）</li><li>blunt: 直率的；生硬的；直截了当的</li><li>slug: 鼻涕虫</li><li>slip off: 快速脱下（衣服）</li><li>Hiroshima and Nagasaki: 广岛与长崎</li><li>semi-translucent: 半透明的；transparent: 透明的；opaque: 不透明的</li><li>sore: 烂疮</li><li>relegate: 贬值；使降级；降低……的地位</li><li>gurgle: （婴儿）开心地咯咯笑</li><li>atrocious: 残暴的，骇人听闻的</li><li>walls of the veins: 静脉壁</li><li>walk out: 罢工</li><li>suffocate: （使）窒息而死 "The Louvre suffocates," the workers' union said in a statement written in French, citing the "total inadequacy" of the museum's facilities to manage the high volume of visitors.</li><li>confluence: 融合，汇集 The confluence of the Blue Nile and the White Nile</li><li>immiseration: economic impoverishment:经济贫困</li><li>pricing-out: 漫天要价 That has leads to environmental degradation, dangerous conditions, and the immiseration and pricing-out of locals in many places.</li><li>too much of a good thing: 过犹不及</li><li>roll out regulations: 颁布法律</li><li>stand to: be likely</li><li>ubiquitous: 普遍的，无处不在的</li><li>selfie: 自拍照</li><li>upside: advantage: 优点</li><li>sex offenders: 性侵犯者：people convinced of sex offenses/a sex offense</li><li>chemical castration: 化学阉割</li><li>sexual assault, sexual abuse</li><li>testosterone-inhibiting/testosterone-suppressing: 睾酮抑制</li><li>sex drive/libido: 性冲动</li><li>due process: 合法程序</li><li>clinch: 确定（比赛、竞赛）获胜，取胜 clinch victory; clinch the first spot</li><li>be marred by sth.: 被……损坏</li><li>hoist: 举起，抬起</li><li>covet: 梦寐以求的，垂涎的</li><li>scale: 攀登，翻越</li><li>blare out sth.: 发出响而刺耳的声音</li><li>dispatch: 迅速处理，迅速解决： dispatch a bomb</li><li>cognoscenti: 行家，评鉴家</li><li>reigning: (冠军)本届的，现任的</li><li>put sb./sth. on the map: 使……出名，使……名扬四方</li><li>franchise: 职业运动队</li><li>lunar lander: 登月器</li><li>hurdle: 跨栏比赛中的栏架；海关，障碍</li><li>outpost: 前哨基地</li><li>a hub of: ……的中心；It could become a hub of commercial and government activity.</li><li>cislunar: 位于地球和月球之间的；地月的</li><li>behead: 砍……的头</li><li>the death penalty; capital punishment; 死刑</li><li>abduction: 绑架</li><li>opening statement: 开庭陈词</li><li>prosecution: 检方</li><li>downward spiral: （局面的）迅速恶化；螺旋式恶性下降；（价格等的）不断下降</li><li>outlaw: 禁止，取缔</li><li>be obsessed with sth.: 对……着迷</li><li>headsets: 耳机</li><li>anime: （日本）动漫</li><li>soar: 猛增，骤升</li><li>feature film: 正片，长片</li><li>in the running: 参加比赛；有赢的希望</li><li>animation festival in Annecy: 安纳西国际动画电影节</li><li>lack, paucity, shortage: 缺乏</li><li>take on: 接受（工作）；承担（责任） She has taken on too much work.</li><li>stalwart: 忠诚的人，中坚分子</li><li>put in: （在工作中）花费时间</li><li>out of competition: 非竞赛单元，非竞赛影片</li><li>travail: 艰苦出境；艰辛，煎熬</li><li>wake up to sth.: 开始意识到</li><li>export force: 出口力量</li><li>ageless: 永恒的；青春永驻的，永不显老的</li><li>Senegal: 塞内加尔（西非国家）</li><li>life expectancy: （尤指人的）预期寿命</li><li>demographer: 人口统计家</li><li>evangelist: （基督教）布道者；大力提倡者；an evangelist of low-carb diets</li><li>one-size-fits-all: （意图）各方面都顾及的；通用的；one-size-fits-all-across-the-globe</li><li>mortality: 死亡数量，死亡率</li><li>checkups: 体检</li><li>cryptocurrency: 加密电子货币</li><li>pose risk to: 对……构成危险</li><li>shake up: 对……作出重大改变，调整</li><li>trigger: 触发，引起</li><li>policymaker: 决策者</li><li>Bank for International Settlements (BIS): 国际清算银行</li><li>straddle: 跨坐；跨过，横跨（边界）</li><li>regulatory perimeter: 监管范围</li><li>spoiler: 剧透者；spoiler alert: 剧透预警；have someone else spoil it for you: 某人剧透给你</li><li>be obsessed with: 被……痴迷的；受……困扰的</li><li>the biggest opening weekend: 最高的周末票房</li><li>assault: 袭击，攻击</li><li>screening: （电影）放映；（电视节目）播放</li><li>be invested in: 非常关心某事；愿意为某事花费时间和精力</li><li>pander: 迎合，逢迎</li><li>secure: 获得，争取到；保护</li><li>stifle: 阻止，压制</li><li>inflict: 使遭受；inflict sth. on sth./sb.: it inflicts damage on storytelling as a craft</li><li>twist: 剧情反转</li><li>get bent out of shape: （被弄得）奇形怪状的</li><li>predictability: （某事）可被预测</li><li>sensational: 轰动性的，吸引人的；sensational newspaper stories: 夺人眼球的新闻故事</li><li>corrode: （使）腐蚀，侵蚀</li><li>not so much sth. as sth.: 与其说是某物，不如说是某物</li><li>blow minds: 震撼人心</li><li>overrate: 高估，对……评价太高</li><li>hamper: 阻碍，妨碍</li><li>loudly revealing: 大声剧透</li><li>by any measure: 无论以什么标准来衡量</li><li>live-action performance: 真人表演</li><li>thespian: 演员；a distinguished thespian</li><li>marionette: 提线木偶</li><li>contrive: 设计，发明，创造；比invent更强调思路的巧妙</li><li>the better part of: 大部分，大半</li><li>consummate: 完美无缺的，圆满的</li><li>floppy: 松软的，耷拉的</li><li>knock-kneed: x型腿的</li><li>pull-string: 提线的；pull-string doll</li><li>diminutive: 微小的，矮小的</li><li>-phobic: 恐惧的；homophobic恐同，transphobic恐惧跨性别者的</li><li>hate crime: 仇恨犯罪</li><li>surge: 激增，剧烈上升；surge to a record high； spike激增；spark: 引发，触发</li><li>stalking, harassment and violent assault: 非法跟踪，骚扰，暴力袭击</li><li>condemnation: 谴责，声讨</li><li>be down to: 由……引起（造成）</li><li>rightwing populism: 右翼民粹主义</li><li>political climate: 政治形势</li><li>scapegoating: 找人受过；scapegoat: 替罪羊</li><li>physical and verbal violence: 身体和言语暴力</li><li>sartorial: 穿衣的，衣着的；sartorial taste</li><li>don: put on穿戴上</li><li>historic costume, traditional garb: 传统服饰</li><li>period dramas: 电视剧（在某个时间走红的）</li><li>garner: 收集，积累，获得 'The Story of Minglan', a TV series set in the Song Dynasty, garnered more than 400 million viewers in three days when it debuted earlier this year.</li><li>span the gamut: 涵盖各种人群 gamut: 全部音阶，整个范围；College life opened up a whole gamut of new experiences.</li><li>history enthusiasts: 历史爱好者</li><li>anime fans: 动漫迷</li><li>give sth. a boost: 推动……的发展</li><li>the Communist Youth League of China: 中国共青团</li><li>visceral outpouring of emotions: 真情实感的流露</li><li>scavenge: （从废弃物中）觅食；捡破烂，拾荒</li><li>trudge: 费力的走，跋涉；trudge through the snow</li><li>rubble: 瓦砾堆；碎石堆</li><li>cub: 幼崽，幼兽</li><li>raised in captivity: 被养在笼中</li><li>pelt traders: 毛皮商</li><li>trek: 长途跋涉，艰难的旅程</li><li>poacher: 偷猎者</li><li>World Wildlife Fund: 世界自然基金</li><li>tune in to sth.: 收看，收听 Many tune in to our programme every day.</li><li>binge: 狂吃，无节制的吃 binge on chocolate; binge on bubble tea</li><li>capitalize on: 利用…获益；从…中获利；take advantage of She capitalized on her knowledge and experience to get a better job.</li><li>mukbang: 吃播</li><li>scarf down: 狼吞虎咽的吃下去</li><li>grub: 食物</li><li>sitting: 一次（时间）；用餐时间</li><li>king crab: 帝王蟹；mussel: 青口，贻贝；lobster: 龙虾；crayfish: 小龙虾；hard-boiled eggs: 水煮蛋；</li><li>mash-up: （音乐的或视频的）混搭</li><li>align: 对准；校直；align your goal with the reality</li><li>curb: 限制，抑制；curb your bad temper; appetite-curbing: 抑制食欲</li><li>dietitian: 营养学家</li><li>gastrointestinal distress: 胃肠（里）的；与肠胃相关的</li><li>lethargy: 嗜睡</li><li>weight gain: 体重增加</li><li>sitting president: 现任总统</li><li>set foot in: 去，到达</li><li>fortified: 防御加强的；设防的</li><li>Damilitarized Zone: 非军事区</li><li>raise the prospect of sth.: 提出希望…</li><li>a pat on the back: 奖励，鼓励; pat sb. dry: 把某人拍干（如脸上的水分） Give yourself a pat on the back.</li><li>milepost: 里程标</li><li>fraught: 令人极度忧虑的；充满不快的；be fraught with sth.: be full of sth.: 充斥着……（不好的东西） a plan fraught with danger</li><li>territorial disputes: 领域冲突；territorial growls: （动物等）为守护领域发出的嗥叫</li><li>earthlings: 地球人</li><li>launch on: 上映； The film "The Wandering Earth" launched on Netflix in May.</li><li>eschew: 回避，避开</li><li>stupendous: 巨大的，惊人的；a stupendous achievement</li><li>wide-angle shot: 广角镜头</li><li>pincushion: 针垫</li><li>trail off: 减弱，减小</li><li>trilogy: 三部曲</li><li>devotee: （狂热的）崇拜者，爱好者</li><li>trash separation, trash sorting, garbage separation，arrange trash: 垃圾分类</li><li>out of sorts: 心烦意乱的；不太舒服的；易怒的</li><li>household trash: 家庭垃圾；household chores/appliances: 家务/家庭用具</li><li>mandatory: 强制的，法定的，义务的 a government mandate: 政府命令</li><li>be compliant with: 遵守规定的，合规的</li><li>a telephone enquiries: 电话咨询</li><li>plant: 工厂（生产产品或者能源）: a sewage plant</li><li>high-tech "comprehensive utilization bases": 高新技术“综合利用基地</li><li>step up: 加把劲；step up the propaganda: 加大宣传力度</li><li>befriend: 对…友好，以朋友态度对待</li><li>domesticated: （动物）驯化的；热爱家庭生活的 He is very domesticated.他擅长家务</li><li>petulant: 任性的，耍脾气的 a petulant flash of puppy-dog eyes: 小狗一闪而过的任性眼神</li><li>pull on the heartstrings: 触动心弦</li><li>split off: 分离</li><li>crack open: 打开，砸开</li><li>time capsule: 时间胶囊</li><li>Proceedings of the National Academy of Sciences: 美国皇家科学院院刊</li><li>droopy: 下垂的</li><li>fortuitous: （好事）偶然发生的，碰巧的</li><li>where sb./sth. comes in: 某人/某事需要发挥什么作用 Where do I come in? 需要我做什么</li><li>nurturing instincts: 抚养本能</li><li>all but: 差点儿，几乎</li><li>Silicon Valley giant: 硅谷巨头</li><li>futuristic: 极新潮的；未来主义的</li><li>flying-saucer: 飞碟 He also helped design Apple's new headquarters, a futuristic, flying-saucer-like glass building that has become one of the most distinctive structures in Silicon Valley.</li><li>structure: 建筑物</li><li>be at low ebb: 低谷期</li><li>make a difference: 产生重大的影响</li><li>come to do sth.: 逐渐成为，变成（某种状态）</li><li>make a point of doing sth.: 强调某事；重视，特别注意</li><li>whale hunting, whaling: 捕鲸</li><li>set sail: 起航，开船</li><li>brush aside: 对…置之不理，漠视…</li><li>resume, resumption: 重启，重新开始</li><li>conservationist: 环境保护主义者</li><li>slam: 批评，抨击；（使）砰地关上 The door slammed.</li><li>flashpoint: 爆发点；（液体）燃点</li><li>be subject to: 遭受，承受</li><li>sound the death knell for sth: 敲响…的丧钟</li><li>dwindling, shrinking: 减少的</li><li>teaser, tailor: 预告片，tailer专指影视作品或广播，teaser不限于此范围，如广告片之类</li><li>animated film: 动画电影</li><li>miss a trick: 错失良机</li><li>theatrical: 喜剧的，电影的</li><li>montage: 蒙太奇</li><li>be down to: 由…引起或造成</li><li>epilogue: 收场白；后记</li><li>brake run: 刹车，制动器</li><li>Space Mountain: 飞越太空山</li><li>set pieces: 具有众所周知的固定形式的文学，艺术作品的片段</li><li>feat: 英勇事迹；功绩，壮举</li><li>cash grab: 捞金的产品</li><li>land in: 达到…的状态</li><li>reasoning: 推理，论证; the line of reasoning: 思路</li><li>uncharitable: 苛刻的，刻薄的；charitable: 宽容的，仁慈的</li><li>next to: 几乎，差不多 I knew next to nothing about cosmetics.</li><li>deflate: 削弱，（气球、轮胎）泄气，放气；挫某人的锐气 The balloon deflated.</li><li>unravel: 瓦解，破坏；解开，拆散 unravel threads/the wrapping paper</li><li>disentangle: 理清，分清 disentangle fact from fiction</li><li>at hand: 手头的，需要马上处理的；某件事可能很快发生的</li><li>accommodate: 适应</li><li>receptive: （对建议、看法）愿接受的；易接受的</li><li>detain: 拘留，扣押；（短时间地）耽搁 detention: 留堂；拘留: 12 months' detention in prison</li><li>on suspicion of: 对…怀疑；嫌疑 Suspicion is the cancer of friendship.</li><li>molest: 调戏；猥亵；对…进行性骚扰</li><li>minor: 小的；未成年人 The film contains material unsuitable for minors.</li><li>de facto: 事实上的；实际上的</li><li>open letter: 公开信</li><li>alleged: 被指控的；被说成的;the alleged criminal</li><li>stock market crash: 股市暴跌</li><li>wipe sth. off sth.: 从…中出去（或抹去）</li><li>credit ratings agencies: 信用评级机构</li><li>standing: 声誉；级别；地位</li><li>lineup: 队伍；阵容；management team lineup</li><li>fallout: 不良影响，恶果</li><li>Bloomberg Billionaires Index: 彭博全球亿万富豪实时排行榜</li><li>kick off: （足球比赛）开球，开始；kick-off: 开幕</li><li>franchise: 特许经销权；特许经销企业；电影系列；a fast-food franchise</li><li>blockbusters: 大片</li><li>dichotomy: 一分为二，对立；the moral dichotomy of good and evil</li><li>spooky, creepy: 可怕的</li><li>loopy: 怪异的，傻的</li><li>subvert: 破坏，动摇，颠覆</li><li>over the top: 太过头，太过分</li><li>the joker: 小丑</li><li>portrayal: 描述，描绘，描写</li><li>definitive: 最佳的，最完整可靠的</li><li>villain: 反派</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A collection of some state of the art popular words.
    
    </summary>
    
      <category term="English" scheme="http://yoursite.com/categories/English/"/>
    
    
      <category term="English" scheme="http://yoursite.com/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2019-12-19-talk-da.html"/>
    <id>http://yoursite.com/posts/talks/2019-12-19-talk-da.html</id>
    <published>2019-12-19T11:46:44.289Z</published>
    <updated>2018-11-17T08:19:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it from other forms of machine learning, image analysis, and statistical methods. This talk discussed the basic ideas of machine leaning, and compared it with machine learning. It is given after I came back from Harbin's summer school in Agust, 2017. After this talk, I began to throw myself into studying machine learning.</p><p>Check <a href="/assets/slides/D.A/pres.pdf">slide</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>notes-DAPPER</title>
    <link href="http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html"/>
    <id>http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html</id>
    <published>2019-10-05T16:52:21.000Z</published>
    <updated>2021-01-12T19:34:37.064Z</updated>
    
    <content type="html"><![CDATA[<p>DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see <a href="https://github.com/nansencenter/DAPPER">here</a>.</p><p>This notes keep records of the problems encountered while using DAPPER.</p><a id="more"></a><ol type="1"><li>Pre-setting <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.</span><br></pre></td></tr></table></figure> Copy mkl_*.dll, libiomp5md.dll and <em>libiomp5md.pdb</em> from directory "./Library/bin" to the root directory of python exe.</li></ol><p>Reference from <a href="https://blog.csdn.net/supertangcugu/article/details/89790617">here</a>.</p><ol start="2" type="1"><li>Manual</li></ol><p>Manual <a href="https://dapper.readthedocs.io/en/latest/implementation.html">online</a></p><p>For EnKF: Nx-by-N (维度数<em>样本数) For ndarrays: N-by-Nx (样本数</em>维度数)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see &lt;a href=&quot;https://github.com/nansencenter/DAPPER&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This notes keep records of the problems encountered while using DAPPER.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="da" scheme="http://yoursite.com/tags/da/"/>
    
  </entry>
  
  <entry>
    <title>数学，内容方法意义</title>
    <link href="http://yoursite.com/posts/uncategorized/2019-06-20-%E6%95%B0%E5%AD%A6%EF%BC%8C%E5%86%85%E5%AE%B9%E6%96%B9%E6%B3%95%E6%84%8F%E4%B9%89.html"/>
    <id>http://yoursite.com/posts/uncategorized/2019-06-20-数学，内容方法意义.html</id>
    <published>2019-06-20T21:28:48.000Z</published>
    <updated>2019-06-20T09:33:10.547Z</updated>
    
    <content type="html"><![CDATA[<p>《数学，它的内容，方法与意义》 <a id="more"></a> ## 卷二 ### 2019.6.20, p1-p11 1. 微分方程的概念与分类 * 每一个微分方程都有无限多函数满足其，所以每一个微分方程确定一整族的满足它的函数 * 分为常微分（未知函数仅与一个变量有关）与偏微分（未知函数与几个变量有关）。</p><ol start="2" type="1"><li>微分方程的具体物理应用例子。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《数学，它的内容，方法与意义》
    
    </summary>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
  </entry>
  
  <entry>
    <title>Digest of News</title>
    <link href="http://yoursite.com/posts/English/2019-06-20-readingdigest.html"/>
    <id>http://yoursite.com/posts/English/2019-06-20-readingdigest.html</id>
    <published>2019-06-20T19:31:00.000Z</published>
    <updated>2021-01-12T19:29:39.422Z</updated>
    
    <content type="html"><![CDATA[<p>A collection of some latest news. <a id="more"></a></p><h2 id="digest">Digest</h2><h3 id="the-guardian-view-on-the-notre-dame-fire-we-share-frances-terrible-loss">The Guardian View on the Notre Dame fire: we share France's terrible loss</h3><p>It feels as though the very heart of France and the soul of Europe have been suddenly and viciously ripped out. The fire that coursed through large sections of Notre Dame Cathedral in Paris on Monday evening was an act of blind and terrible destruction that causes a great stab of emotional pain to us all.</p><p>The fire struck quickly and seemingly uncontrollably. It gathered force with immense power and ferocity, engulfing much of the roof and the cathedral spire as it caught hold. After an hour or so, the Paris fire brigade was able to begin to get part of the fire under some degree of control and to protect the parts of the cathedral that may survive. As night fell, the great west towers still stood against the sky, proud but vulnerable.</p><p>Dame is the embodiment of the French capital and of France itself. It has been in its place since the 12th century. It was here that the liberation of Paris in 1944 was marked in service of thanksgiving.</p><p>It is world famous not simply as an iconic Parisian building but through the writing of Victor Hugo, who in his novel Hunchback of Notre Dame made the building itself come alive.</p><p>The cathedral will rise again in time. This terrible fire is not an event that should be trivialised or banalised. We stand with France in its hour of heartbreak. We will never, ever, turn away.</p><h3 id="no-word-of-a-lie-scientists-rate-the-worlds-biggest-peddlers-of-bull">No word of a lie: scientists rate the world's biggest peddlers of bull</h3><p>In new research, scientists claim to have identified the most common practitioners of the ignoble art--bullshit. Their study of 40,000 teenagers reveals that boys; those from privileged backgrounds; and North Americans in particular, top the charts as the worst offenders.</p><h3 id="why-urban-millennials-love-uniqlo">Why urban millennials love Uniqlo</h3><p>A pair of Uniqlo slacks is never going to look like a $200 pair from a high -end competitor. But becasue Uniqlo offers free tailoring, the pants are probably not going to look like you got them for $40, either.</p><h3 id="not-so-fantastic-can-japan-end-its-love-affair-with-plastic">Not so fantastic: can Japan end its love affair with plastic?</h3><p>From bento boxes to individually wrapped bananas, plastic reigns supreme in Japan. But amid global concern about single-use waste, new legislation could help end the country's love affair with plastic.</p><h3 id="the-matrix-at-20-how-the-sci-fi-gamechangeer-remains-influential">The Matrix at 20: how the sci-fi gamechangeer remains influential</h3><p>The Matrix wasn't about the internet per se -- it takes place in a dystopian future, approximately 200 years from the present -- but it understood where things were header far better than any film did at the time. Back in 1999, the mind-expanding adventures of Neo, Trinity and Morpheus in a world ruled by machines felt more like a technical revolution that a cultural one, an ultra-stylish Hollywood thriller that wedded the balletic cool of Hong Kong action movie with the stunning elasticity of CGI.</p><p>Yet objects tend to shift during flight, and in the year 2019, The Matrix has endured as both touchstone and Rorschach blot, a way for people of vastly different ideologies to make sense of the world around them. The effects are still a marvel, but the film's ideas have taken a root in a destabilized culture where conspiracy theories flourish and individuals are defining for themselves what is and isn't real.</p><p>For a film of such immense scale and complexity, The Matrix now seems remarkably hopeful about the future, which separates it from the technophobic films of the day. Neo may be shaken by learning the dystopia and falsehood of the world as he once understood it, but other paths open up where he can become fully himself, unbound by the dictation of others. The internet has helped make the positive transformation possible, and the Wachowskis, the film's directors, saw it long before other film-makers did.</p><h3 id="that-70s-shows-why-the-disco-decade-is-back-in-fashion">That 70s shows: why the disco decade is back in fashion</h3><p>There is a logical, mathematical explanation for why the 70s have been pronounced chic by Paris fashion week. The 90s are the new 70s, you see, and the 70s are the new 50s. It goes like this. When the 70s were the recent past, they were cringe-makingly close to home, and therefore easy to poke fun at.</p><p>Now, the 90s are the relatively past--and so 90s revivalism has become fashion's go-to gag. The 70s, meanwhile, have slipped into rose-tinted memory, the embarrassing parts forgotten along the way. The decade is coming to stand for classic chic, in the way what the 50s previously did before being consigned to ancient history.</p><p>The 70s became headline news in Paris this week because of Hedi Slimane at Celine. Instead of going back to Old Celine, he went to Old Old Celine--an untra-bourgeois late--70s version of what ladylike looks like, with attitude and great wedge boots. And the fashion industry looked at the collection and thought: 'That's exactly what I want to wear.'</p><p>It is hard to avoid plundering the 70s when its influence is everywhere on the catwalk, which means the trickle-down effect from show to pavement was already well under way on the streets of Paris.</p><p>So how did they wear it? Layered, mostly, and in a 70s palette of browns, greens and yellows. The art director Sofia Sanchez de Betak probably won the look. The thrown-together layering of tie, tweed blazer, trench and scarf in various shades of brown and grey was a direct reference to Annie Hall--bourgeois Manhattanite is something to aim for.</p><h3 id="when-we-dream-we-have-the-perfect-chemical-canvas-for-intense-visions">'When we dream, we have the perfect chemical canvas for intense visions'</h3><p>When we dream, the logic centres of our brain -- the frontal lobes -- go dark, and chemicals associated with self-control, like serotonin and norepinephrine, drop.</p><h3 id="why-indias-election-is-among-the-worlds-most-expensive">Why India's election is among the world's most expensive?</h3><p>India will soon hold what's likely to be one of the world's costliest elections. The polling exercise will cost an unprecedented 500 billion rupees ($7 billion), according to the New Delhi-based Centre for Media Studies.</p><p>It is a bunch of panicky candidates throwing money around to voters but also to vendors selling all kinds of stuff useful in a political campaign.</p><h4 id="rallying-the-troops-with-chicken-curry">Rallying the troops with chicken curry</h4><p>Indian politicians love a good rally: They crisscross the country, corralling locals into large tents where they're bombarded with campaign rhetoric. To attract a crow, some politicians may need to offer a box of food filled with chicken curry that can be too expensive for average citizens.</p><h4 id="dummy-candidates">Dummy candidates</h4><p>India's Election Commission has long warned about dummy candidates: nominating someone with the same name to confuse voters and split the vote. But even fielding dummy candidates can cost as much as 120 million rupees.</p><h4 id="brand-building-make-india-great-again-swag">Brand building: Make India Great Again Swag</h4><p>As much as 26 million rupees will be spent on advertising in the upcoming elections. In February alone, more than 40 million rupees was spent on political advertising on just one site--Facebook. Them there are T-shirts with the slogan "Namo Again" (Narendra Modi Again) peddled by his camp.</p><h3 id="a-hit-tv-series-in-china-skewers-cranky-old-parents">A hit TV series in China skewers cranky old parents</h3><p>It is no mean feat to be one of top-ten trending hashtags on Weibo, China's equivalent of Twitter, for 20 consecutive days and counting. "All is Well", a show on provincial television which premiered on March 1st, has done just that.</p><p>The show tells the story of a fictional Chinese family torn by internal conflict. The female protagonist, Su Mingyu, is barely on speaking terms with her widowed father and one of her two brothers. The father is a nagging crank who expects his two adult sons to bankroll his lavish tastes. This leads to constant bickering between the brothers, neither of whom want to be called unfilial.</p><p>Many Chinese can relate to the Su family's troubles. The daughter holds a grudge against her father, and especially against her late mother, for having mistreated her while pampering her brothers.</p><p>But the biggest reaction has been to the drama's critique of filial piety. Even today, the Confucian principle of unswerving loyalty to one's parents remains hallowed. Many people say the best measure of adherence to this virtue is whether a son takes good care of his parents in old age.</p><p>In the series, however, the widowed father does not attract much sympathy. He throws tantrums and insists that his eldest son buy him a three-bedroom apartment (the som grudgingly does so). Commentators on social media have taken to calling the father a juying ("giant baby")--a characteristic common among parents in real life, they say. The Su children do their duty, but the audience is supposed to applaud the resentment they express.</p><h3 id="the-worlds-most-expensive-cities">The world's most expensive cities</h3><p>For the first time in its 30-year history, the Worldwide Cost of Living Survey from the Economist Intelligence Unit gives the title of the world's most expensive city to three places. Singapore marks its sixth straight year at the top of the rankings, and is joined there by Hong Kong and Paris. The survey, which compares prices across 160 products and services, finds that living costs in all there cities are 7% higher than in New York, the benchmark city. Two Swiss cities, Zurich and Geneva, and Osaka in Japan also have higher prices than New York; Copenhagen and Seoul cost tha same as the Big Apple.</p><p>Strong economic growth in the United States in 2018 led to a sharp appreciation of the dollar and a rise up the rankings for 14 of the 16 American cities for which prices are collected. The highest climbers were San Francisco (up 12 places to 25th) and Houston (up to 30th from 41st); New York itself moved up six spots to seventh and Los Angeles rose by four places to tenth. These movements represent a sharp increase in the relative cost of living compared with five years ago, when New York and Los Angeles tied in 39th position.</p><p>Elsewhere, inflation, devaluations and economic turmoil caused the cost of living in Argentina, Brazil, Turkey and Venezuela to plunge. Turkey's currency crisis and surging consumer-price inflation saw Istanbul drop 48 places to joint 120th. But Caracas in Venezuela claims the title of the least expensive city in the world, thanks to hyperinflation and the plummeting bolivar.</p><h3 id="the-unstoppable-love-of-the-snowplow-parent">The unstoppable love of the snowplow parent</h3><p>Taken to its criminal extreme, that means bribing SAT proctors and paying off college coaches to get children into elite colleges--and then going to great lengths to make sure they never face the humiliation of knowing how they got there.</p><p>Learning to solve problems, take risks and overcome frustration are crucial life skills, many child development experts say, and if parents don't let their children encounter failure, the children don't acquire them.</p><h3 id="americans-are-going-bankrupt-from-getting-sick">Americans are going bankrupt from getting sick</h3><p>Still, some patients do wind up with medical debt, which discourages them from seeking medical care, because they fear they will incur even more debt if they go to the doctor again. In the end, they get sicker, and risk plunging even further into debt.</p><p>When everything fails, and the person is at imminent risk of having wages garnished because they've been sued for medical debt, it might be time to file for bankruptcy, says Sandoval-Moshenberg. The people who do become the tip of a very big debt iceberg.</p><h3 id="starbucks-china-challenger-luckin-to-raise-up-to-800-million-in-u.s.-ipo-sources">Starbucks' China challenger Luckin to raise up to $800 million in U.S. IPO: sources</h3><p>Luckin Coffee Inc, the Chinese challenger to Starbucks Corp, filed on Monday for a U.S. initial public offering (IPO) through which, sources said, it is looking to raise up to $800 million.</p><p>Luckin has waged its cash-burning caffeine war with generous subsidies, speedy delivery and viral promotions on social media, which in turn has also pushed Starbucks to form a tie-up with local tech giant Alibaba to deliver coffee to customers.</p><p>The big question for the brand long terms is if, when it rolls back discounts, enough customers stick around.</p><h3 id="k-pop-stardom-lures-japanese-youth-to-korea-despite-diplomatic-chill">K-Pop stardom lures Japanese youth to Korea despite diplomatic chill</h3><p>She is one of an estimated one million other K-pop star wannabes, from South Korea and abroad, hoping to get a shot at super competitive auditions by majot talent agencies that will take on just a select few as "trainees".</p><p>The influx of Japanese talent that is reshaping the K-pop industry comes at a time of increasingly bitter political acrimony between the two countries that has damaged diplomatic ties.</p><p>That the tension has done little to dent the K-pop craze among Japanese youth, and the willingness by Korean agencies to take on Japanese talent, speak to the strength of the ties between their people.</p><p>Some Japanese transplants have already made it big. The three Japanese members of the girl band Twice helped make the group the second most popular act in Japan, after BTS.</p><p>Agencies officials are reluctant to discuss their success in Japan and the infusion of Japanese talent, wary of fuelling a politically charged backlash, industry sources said.</p><h3 id="what-male-ceos-are-wearing">What male CEOs are wearing</h3><p>It now seems like years ago that Silicon Valley's hoodie culture made male executives' wardrobes more casual across most industries. From financial companies to insurers to automakers, traditional dress codes morphed into "dress appropriately."</p><p>Now executive men in more traditional offices are arguing out how to dress down but still look professional, and those at tech firms are growing up and wondering how to dress up but still look cool.</p><p>"It should look effortless. People shouldn't be able to put their finger on why you look as good as you do," said Nick Hart, founder of Spencer Hart.</p><p>Here are some items high-powered men are wearing when dressing for work and weekends:</p><p>The sneakers: Sneakers have gone totally upscale, with eye-popping price tags to match. Some executives will throw down more than $1,000 for a pair.</p><p>The jeans Of course, great jeans with a blazer and smart loafers, if not sneakers, is a modern classic look.</p><p>Some handsome staples include the JBrand Kane fit jeans, the AG Graduate fit, and the Frame L'Homme, said Jacci Jaye of Wall Street Stylist.</p><p>The glasses If you have to wear glasses, there's no reason they can't add a touch of style. Microsoft CEO Satya Nadella is often seen in a striking pair.</p><p>For colorful frames that are not exorbitantly priced, you might try Eyebobs. The company has noted that top execs like Revlon chairman Ron Perelman have been spotted in them.</p><h3 id="trum-administration-owes-americans-answers-about-restrictions-on-huawei">Trum administration owes Americans answers about restrictions on Huawei</h3><p>Chinese telecommunications giant Huawei's global influence has been steadily growing in recent years, and the U.S. government's concern has been growing along with it.</p><p>Fearing that the spread of Chinese-made cellphones, routers and other equipment may render Western countries, including those that share the most sensitive intelligence with the United States, vulnerable to Beijing, Washington has tried to hold Huawei accountable for alleged intellectual-property theft -- while discouraging key allies such as Britain, Australia and New Zealand from relying on Huawei technology.</p><h3 id="rihanna-breaking-new-ground-joins-with-lvmh-for-fashion-brand">Rihanna, breaking new ground, joins with LVMH for fashion brand</h3><p>On Friday, LVMH Moet Hennessy Louis Vuitton, the world's largest luxury group, officially confirmed not only that the fashion line created by Rihanna was becoming part of its glided stable but also that the first products from the new company would be unveiled -- in a few weeks.</p><p>The Group resources are notably large -- LVMH reported first-quarter revenue in April of 12.5 billion euros ($14.1 billion), an increase of 16% -- and the emphasis on "multicultural," in a time when many luxury brands are suffering from charges of cultural insensitivity and discrimination, is significant.</p><p>Fenty, however, has made inclusivity of all kinds -- size, race, gendet identity -- part of its identity from the beginning.</p><h3 id="how-creepy-is-your-smart-speaker">How creepy is your smart speaker?</h3><p>As Alexa herself contends, smart speakers are not sending every utterance into the the tech giants' digital vaults. Despite their name, the devices are simple-minded. They listen out for wake words, and then send what follows to the cloud as an audio clip; when an answer arrives, in the form of another audio clip, they play it back.</p><p>But Amazon notes that users can delete these clips at any time. There's always the mute button if you are worried about accidentally triggering your speaker and sending a clip into the cloud during a sensitive conversation. Users, the firm insists, are in control.</p><p>Not everyone is convinced by such assurance, however. Are their makers using them to snoop on people and then exploiting that information to target online ads or offer them particular products?</p><p>If eavesdropping is your problem, eschewing smart speakers does not resolve it. Smartphones, which people blithely carry around with them, are even worse. Spy agencies are said to be able to activate the microphone in such devices.</p><h3 id="a-majority-of-americans-want-abortion-to-be-legal-in-the-first-two-trimesters">A majority of Americans want abortion to be legal in the first two trimesters</h3><p>In the abortion argument, both sides long ago drove each other to extremes. The pro-life, fundamentalist view behind the Alabama bill is that a fertilised egg is no different from a person, and thus should enjoy the same legal rights. Accept that, and what right does a woman have to take a morning-after pill, or to end a pregnancy after a rape? The pro-choice extreme is that any restriction on abortion is an unacceptable attempt by government to control women's bodies. With debate</p><h3 id="the-joy-of-absence">The joy of absence</h3><p>Turning an office into a prison, with inmates allowed home for the evenings, does nothing for the creativity that is increasingly demanded of office workers as routine tasks are automated. To be productive you need absence of mind, not being present in the flesh.</p><h3 id="nearing-the-endgame-is-hollywoods-lust-for-sequels-destroying-cinema">Nearing the endgame: is Hollywood's lust for sequels destroying cinema?</h3><p>The industry's eagerness to recycle is not hard to explain. Revenue is a big factor. Of the all-time top 10 grossing films, six are now sequels. More important than the scale of these earnings is their reliability. Nobody knows whether an untried property will succeed, but the success of a sequel is virtually guaranteed.</p><p>Just why sequels are so successful is no mystery either. They meet intense audience demand. Nothing whets filmgoers' appetites like what the industry calls "preawareness". Informed anticipation is part of the fun, and provide a ready topic od conversation.</p><p>Inevitably, the sequels juggernaut is throttling fresh ideas, the lifeblood of any creativity. It may be offering the movies a short-term fix by imperilling their long-term health.</p><p>Our world is in flux, and yet we are not encouraged to engage with change. Instead, we are offered the chance to retreat into thought bunkers with those of like mind, and cut ourselves off from unsettling ideas.</p><p>It is understandable that in such circumstances we should look to the big screen for the comfort of repetition. Children find it consoling to be told the same bedtime story every night. Maybe, however, it is time for filmgoers to grow up.</p><h3 id="why-grown-ups-keep-talking-like-little-kids">Why Grown-Ups Keep Talking Like Little Kids?</h3><p>More and more. adults are sprinkling their speech with the language of children. The adoption of some of these linguistic tics by adults has given rise to a register we might call kidspeak.</p><p>Clearly, kidspeak affords its users certain rhetorical advantages -- the way it playfully softens blows is part of why younger people on social media now often couch what they say to one another in toddler-esque. But what made bright teenagers and 20-somethings start imitating 5-year-olds in the first place? And why are many older Americans following suit?</p><p>Young people today are afraid in ways that generations before them were not. They're facing new, compounding economic hardships -- many Millennials and older members of Gen Z depend on their parents to help cover exorbitant rents or student-loan payments. A pair of 2016 studies led by April Smith, a psychology professor in Miami University, in Ohio, showed that over the past few decades, young people have become newly fearful of reaching adulthood, agreeing more and more with statements such as "I wishh that I could return to the security of childhood" and disagreeing with ones such as "I feel happy that I am not a child anymore."</p><p>A generation understandably spooked by "adulting" may well embrace the linguistic comfort food of childlike language. And once established. the habit can easily make the jump to those of us more advanced in years. After all, a kid lurks inside every one of us, and few people are immune to the sheer infectiousness of creativity.</p><h3 id="what-if-an-asteroid-was-about-to-hit-earth-scientists-ponder-question">What if an asteroid was about to hit Earth? Scientists ponder question</h3><p>Here's a hypothetical: a telescope detects an asteroid between 100 and 300 meters in diameter racing through our solar system at 14 kilometers per second, 57 million kilometers from Earth.</p><p>This week's exercise seeks to stimulate global response to a catastrophic meteorite. The first step is aiming telescopes at the threat to precisely calculate its speed and trajectory, following rough initial estimate.</p><p>Then it boils down to two choices: try to deflect the object, or evacuate. If it is less than 165 feet in diameter, the international consensus is to evacuate the threatened region. What about bigger objects? The plan is to launch a device toward the divert its trajectory -- like a cosmic bumper car.</p><h3 id="british-politics-after-theresa-may">British politics after Theresa May</h3><p>Theresa May strode out of the door of 10 Downing Street in a crimson business suit and bowed to the inevitable. Having failed three times to get her Brexit deal through a hopelessly divided parliament, and confronted with a Tory insurrection over her proposed fourth attempt, she announced on May 24th tha she would step down as leader of the Conservative Party and Britain's prime minister.</p><p>The immediate trigger for Mrs May's departure was the failure of her revised withdrawal bill. But the bigger trigger was the general election that she called back in June 2017. Her instincts were right: increasing the Tories' majority would have allowed her to increase her bargaining power with her party and ensure that she was not hostage to pro- or anti- Brexit factions. But the election was a debacle.</p><p>British politics will now be consumed by the race to succeed Mrs May. The next leader will have to grapple with the same forces that have destroyed Mrs May: a country as well as a party that is torn over Brexit; a political system that is being pulled apart by the tension between representative and direct democracy; and a conflict with the European Union that, despite repeated statements to the contrary by Brexiteers, has most of the cards in its hand. Mrs May will not be the last prime minister to be brought to tears by Brexit.</p><h3 id="techs-latest-stars-have-it-all----except-a-path-to-high-profits">Tech's latest stars have it all -- except a path to high profits</h3><p>Investors often describe world of business in terms of animals. Right now, mere ponies are being represented as unicorns: privately held tech firms worth over $1bn that are supposedly strong and world-beating -- miraculous almost.</p><p>As we report this week, a dozen unicorns that have listed, or are likely to, posted combined losses of $14bn last year. Their cumulative losses are $47bn. Their services, from ride-hailing to office rental, are often deeply discounted in order to supercharge revenue growth. The justification for this is the Silicon Valley doctrine of "blitzscaling" in order to conquer "winner-takes-all" markets -- or in plain English, conducting a high-speed land grab in the hope of finding gold.</p><p>The big worry is that the unicorns' losses reflect not temporary growing pains but markets which are contested and customers who are promiscuous. Despite subsides, ride-sharing customers are not locked in to one firm. No wonder Lyft's shares have fallen by over 20% below their IPO price. Anyone can lease an office and rent out desks, not just WeWork.</p><p>External forces will make blitzscaling harder, too. Compared with the earlier generations of firms, today a reaction is in full swing, including over digital taxes and data and content laws.</p><p>All this is good for consumers. Investors, meanwhile, need to hold their nerve. It is tempting to extrapolate the triumph of Google and Alibaba to an entire new group of firms.</p><p>The blitzscale philosophy of buying customers at any price is speaking. After the unicorns, a new and more convincing species of startup will have to be engineered.</p><h3 id="theyre-brilliant-liverpool-fans-line-streets-to-honour-idols">'They're brilliant': Liverpool fans line streets to honour idols</h3><p>Under leaden English skies, Liverpool became a carnival of red on Sunday as the football-mad city hailed its heroes for bringing home the Champions League trophy for the first time in 14 years.</p><p>It followed an epic season in which Liverpool narrowly missed out on a first league title in three decades, causing agony -- now somewhat relieved -- for fans impatient to end the long wait.</p><p>Celebrations ran late into the night in Liverpool city centre, as ecstatic supporters danced drunkenly in the streets singing a repertoire of the club's anthems.</p><p>The morning after, bleary-eyed fans began streaming back onto the streets ahead of the parade, which worked its way slowly towards the city centre.</p><p>A jubilant atmosphere rippled through the city, as all manner of Liverpool flags flew from buildings and cars, drivers sounded horns and fans carried everything from banners to balloon replicas of the trophy.</p><h3 id="want-a-visa-for-the-u.s.-youll-have-to-hand-over-your-social-media-handles">Want a visa for the U.S.? You'll have to hand over your social media handles</h3><p>For now the form only includes the most popular social media platforms, but soon applicants will be able to list all the sites they use. "As we've seen around the world in recent years, social media can be a major forum for terrorist sentiment and activity. This will be a vital tool to screen out terrorists, public safety threats, and other dangerous individuals from gaining immigration benefits and setting foot on U.S. soil," the official added.</p><p>When the new rule was proposed last year, the ACLU called it "ineffective and deeply problematic". The move "will infringe on the rights of immigrants and U.S. citizens by chilling freedom of speech and association, particularly because people will now have to wonder if what they say online will be misconstrued or misunderstood by a government official," the ACLU said.</p><h3 id="western-firms-increasingly-admire----and-want----chinese-technology">Western firms increasingly admire -- and want -- Chinese technology</h3><p>American officials have been treating technology bosses to classified briefings on the dangers of doing business in China. Those who operate there see things differently.</p><p>For the first time last year, in annual poll by the EU Chamber of Commerce in China, a majority of foreign companies (61%) said that domestic firms were as innovative as European ones, or more so. This year four-fifths of them saw opportunities in Chinese sparkiness.</p><p>Today, acquiring a Chinese startup can help some foreign firms gain an edge. Takeover targets have their own research teams, patents, clients and, sometimes, lavish state subsidies.</p><p>All told, American technology companies have invested $1bn in Chinese ones since the start of last year, according to Dealogic, a data provider. Without trade tensions and the technological cold war, deals would multiply. That they now might not will delight America's spooks. Its companies, less so.</p><h3 id="female-spies-and-their-secrets">Female spies and their secrets</h3><p>Are women useful as spies? If so, in what capacity? Maxwell Knight, an officier in MI5, Britain's domestic-counterintelligence agency, sat pondering these questions. Outside his office, World War II had begun, and Europe's baptism by blitzkrieg was under way. In England the intelligence community was still an all-male domain. But a lady spy could come in handy, as Knight was about to opine.</p><p>Intelligence officers had long presumed that women's special assets for spying were limited to strategically deployed female abilities: batting eyelashes, soliciting pillow talk, and of course maintaining files and typing reports. Overseeing operations? Not so much.</p><p>World War II, a "total war" that required all able male bodies for global fighting, offered new opportunities.</p><p>As Sarah Rose writes in D-Day Girls, women were considered good couriers -- a high-risk role -- because they could rely on ingratiation and seeming naivete as tools in tight spots. The war also provided openings for women to show that they could execute operations, masking strategic life-and-death decisions.</p><p>Virginia Hall was, as the British journalist Sonia Purnell writes, "the most successful Allied female secret agent." SHe directed guerrilla forces to support the D-Day landings. Disguised as a milkmaid, she sold cheese and eavesdropped on the German Seventh Army, which, Purnell writes, helped "pave the wat for the Allied recapture of Paris."</p><p>However, after the war, the contribution of these women was overlooked and then forgotten. In 1953, the head of the CIA, Allen Dulles, convened a "Petticoat Panel" to look into attitudes toward women at the agency. Compared with men, they were seen as more emotional, less objective, and insufficiently aggressive.</p><p>That was then. Now the CIA is directed by a woman, Gina Haspel, who has promoted veteran women to head top directorates. These leaders have antecedents, whether or not they know it.</p><h3 id="why-monica-from-friends-is-this-summers-unlikely-style-icon">Why Monica from Friends is this summer's unlikely style icon?</h3><p>If you tuned into Channel 4 every Friday night when Friends ran the first time around, from 1994, your favorite Friends character was Joey (the adorable one), Rachel (the sexy one), Chandler (the funny one) or Phoebe (the eccentric one). But in the second coming of Friends, since all 236 episodes arrived on Netflix in January of last year, Monica has unexpectedly found a breakout role as a style icon.</p><p>The funny thing is that the first time around, nobody was watching Friends for wardrobe tips. Audiences tuned into Friends not for the wardrobe or the lifestyle, but because it was really funny. It's just as funny this time around. But what adds an frisson to watching in 2019 is the fact that the 20-year revival cycle that drives fashion has the effect of making the outfits look delightfully retro and, somehow, absolutely right for now.</p><p>What does it say about 2019 that Monica has leapfrogged Rachel to become the breakout style icon of the Friends revival? Rachel, the girl next door, always supercute in a waitress apron, tucking those blowdried strands behind one ear, is perhaps little too stay-in-your-lane girly for current tastes. Monica had a seam of tomboy running through her character and her wardrobe. But when she dresses up, she could pull off a spaghetti strap dress or a black lace cami with the aplomb of 90s supermodel.</p><h3 id="a-new-word-for-condescending-geezer-reveals-a-lot-about-hierarchy-in-south-korea">A new word for "condescending geezer" reveals a lot about hierarchy in South Korea</h3><p>Kkondae is a modern word of uncertain origin -- perhaps an adaption of the English word "condescend". It means an older pearson, usually a man, who expects unquestioning obedience from people who are junior. South Koreans apply the word to everyone from narcissistic bosses to overbearing uncles and corrupt politicians.</p><p>South Korea is notorious for its suffocating workplace hierarchies based on age, sex and length of service. Many South Koreans are outraged when younger colleagues or relatives fail to use the correct honorific to address them.</p><p>However, the popularity of kkondae as an insult is a sign of change. Open rebellion against hierarchical strictures is still rare and frowned upon. But young people are beginning to question authority.</p><p>South Korea's evolving political culture has been making an impact, too. Since democracy began to take hold in the country in the late 1980s, belief in universal rights, including people's right to be treated equally, has become stronger. This encouraged people to question their deference to others.</p><p>An ancient culture of authority is unlikely to disappear overnight. Young people are becoming more willing to challenge hierarchy, but what will happen as they get older? Some of today's young upstarts will not achieve the success they dream of. They may find themselves yearning for the kind of deference their elders once automatically enjoyed. Today's kknondae critics may grow up to be kknondae themselves. But whether tomorrow's young people will let them get away with that is doubtful.</p><h3 id="a-horrible-way-to-die-how-chernobyl-recreated-a-nuclear-meltdown">'A horrible way to die': how Chernobyl recreated a nuclear meltdown</h3><p>The job of Daniel Parker, head of makeup and prosthetic, had no such light touches either. His task was to recreate the effect of radiation burns on human skin. Ask Parker what radiation does to the body and he is distressingly blunt. "You melt," he says. "The only way you can really describe it is putting salt on a slug. Tissue is breaking down. Skin just slips off. It'll just go. One day you move your arm and the skin will just fall off."</p><p>Surprisingly, Parker didn't look to photos of Hiroshima or Nagasaki victims for examples of radiation damage. He went instead to medical textbooks, and this allowed him to pioneer a technique for Chernobyl where the "layered" the skin: painting the actors' bodies with wounds, then putting a semi-translucent layer on top, giving the impression that sores are forcing themselves to the surface as the body degrades from within. The effect is dreadful to see. Yet, Parker was strict in saying these men must not be relegated to Hollywood "zombies", and he explains that the director make sure sympathy stayed within these characters: even as they lie rigid on the bed, gurgling and fading, they still speak, and a wife may still hold her husband's rotting fingers.</p><p>"It's the worst way to die," says Parker. "Beyond anything you can imagine. The most horrible way to die. I think it's the worst, in line with medieval torture." What makes it particularly atrocious is that the victims were denied pain relief. In the latter stages oof radiation sickness you cannot inject morphine, he explains. "The walls of the veins are breaking down."</p><h3 id="too-many-people-want-to-travel">Too Many People Want to Travel</h3><p>This phenomenon is knows as overtourism. A confluence of macroeconomic factors and changing business trends have led more tourists. That has led to environmental degradation, dangerous conditions, and the immiseration and pricing-out of locals in many places. And it has cities around the world asking one questions: Is there anything to be done about being too popular?</p><p>There's too much of a good thing in some of these spots, and mayors in some of these spots, and mayors and city councils are doing their part to take it away. A mumber of places have implemented or expanded or proposed tourist taxes. Governments are also rolling out regulations. Those kinds of measures stand to become more important in the coming years, as the global middle class gets bigger, social media more ubiquitous, and travel cheaper.</p><p>These phenomena inevitably mean more complaints from locals, and more damage and lines and selfies and bad behavior. But they also mean more cross-cultural exposure, more investment, more global connection, more democratization of travel, and perhaps more awe and wonder. Even overtourism has its upsides.</p><h3 id="alabama-lawmakers-approve-chemical-castration-for-sex-offenders">Alabama lawmakers approve 'chemical castration' for sex offenders</h3><p>A bill in Alabama awaiting the governor's signature would require people convicted of certain sex offenses to undergo "chemical castration".</p><p>Reducing someone's sex drive does not necessarily prevent reoffending, he said, because libido is not the only motivating factor behind sexual abuse.</p><p>"I think sexual assault has always been more than just about sex," Marshall said. "It's about power, it's about exercising control over someone else and a variety of other things."</p><h3 id="this-means-everything-to-the-city-toronto-parties-as-raptors-clinch-nba-title">'This means everything to the city': Toronto parties as Raptors clinch NBA title</h3><p>After nearly 25 years of ambition marred by disappointment and frustration, the Toronto Raptors have brought the city -- and country --its first-ever NBA championship. As Game of 6 of the NBA finals ended and the team hoisted the coveted Larry O'Brien trophy, tens of thousands of Toronto residents poured into the streets, converting busy thoroughfares and city squares into temples of celebration.</p><p>Fans in the city's downtown core scaled anything they could -- including buses, buildings and traffic lights -- as car horns blared and the night was lit up by fireworks. Police blocked off major roads to allow fans to celebrate util the early hours of Friday.</p><p>Much of what fuelled the energy was a belied that Toronto had been unjustly overlooked by analysts and the broader basketball community. Even though the Raptors methodically dispatched elite teams one-by-one, the cognoscenti of the NBA had Golden State, the reigning champions, as the favorites.</p><h3 id="defense-in-china-students-beheading-in-illinois-says-he-did-it">Defense in China student's beheading in Illinois says he did it</h3><p>Brendt Christensen, 29, could face the death penalty if he is convicted in the June 2017 abduction and murder of Yingying Zhang, a 26-year-old student at the university of Illinois at Urbana-Champaign.</p><p>Christensen's defence lawyer acknowledged in his opening statement that his client killed Zhang but disputed the prosecution's version of events. "You need to know who Brendt was and what he was gion through -- a downward spiral in his life," George Taseff told the jury as he sought to protect his client from the death penalty.</p><p>Christensen took Zhang to his apartment where she fought for her life as he hit her over the head with a baseball bat, raped her and stabbed her in the neck before cutting off her head, Miller said.</p><p>"Her blood ran down the wall," Miller said, as Zhang's relatives listened to an interpreter through headsets. "Thousands of miles away from her parents, alone with a stranger, she breaths her last breath."</p><h3 id="japans-anime-industry-in-crisis-even-as-its-popularity-soars">Japan's anime industry in crisis even as its popularity soars</h3><p>Three of the 10 feature films in the running for top prize at the world's most important animation festival in Annecy in France are from Japan.</p><p>With talk of a talent shortage, its greatest star, the legendary Studio Ghibli founder Hayao Miyazaki, has come out of retirement at 78 to make "How Do You Live?" -- with speculation that he could take on another feature if his health holds.</p><p>Yoshiaki Nishimura, a former Miyazaki stalwart who produced the Oscar-nominated "The Tale of The Princess Kaguya", told AFR that his peers also complain of low pay, a paucity of emerging young talent and burn-out in overworked animation teams who often put in 12- to 18-hour days.</p><p>Amel Lacombe, whose company Eurozoom is a key French animation distributor, said the industry's travails are due to its rapid growth, and now "we are in a period of adjustment". She believes that the Japanese authorities are waking up to anime's importance and global reach "as a export force".</p><h3 id="an-ageless-question-when-is-someone-old">An ageless question: When is someone 'old'?</h3><p>Scherbov explained that young and old are relative notions, and their common reference point is life expectancy. It makes sense that "old" would vary between nations, especially between more-and less-developed countries, with differences in education, mortality rates, access to health care and life expectancy.</p><p>But who is "old" also varies -- widely -- between individuals. The point, says Scherbov, is that personal age is dependent on our "characteristic" -- cognitive abilities, disability, health history and even education levels. Those with more education tend not to smoke, exercise more frequently, have better diets and have regular checkups -- and, therefore, live longer, meaning their old-age threshold comes later, says Scherbov.</p><h3 id="why-spoilers-are-ruining-storytelling">Why spoilers are ruining storytelling</h3><p>Today, writers and film-makers are obsessed with spoilers. Anthony and Joseph Russo, the directors of Marvel films, have issued statements requesting that fans not ruin them for others. Audiences are zealous about the matter, too. One fan was assaulted outside a screening of "Avengers: Endgame" for "loudly revealing" the movie's outcome.</p><p>Film-makers realised that fans were invested in the surprise and pandered to them. The Russo brothers begging viewers not to ruin the Avengers films probably helped them to secure the biggest opening weekend in film history. See it now, that seemed to say. or have someone else spoil it for you.</p><p>However, this attitude stifles proper discussion of stories by critics as much as viewers, and it inflicts damage on storytelling as a craft. By promoting one technique, the twist. and one effect, surprise, stories get bent out of shape. They try too hard to counter expectation and resist predictability.</p><p>Stories that promote surprise over character end up as mere soap opera, a series of sensational shocks. That corrodes credibility, while some reveals do not so much blow minds as waste time. More significant than all of this, though, is the fact that surprise is overrated.</p><h3 id="why-woody-isnt-wooden">Why Woody isn't wooden?</h3><p>This summer. one of the most versatile performances working today returns to the screen. Over the past 34 years, he has proved himself a consummate comedian, a fearless action hero and a compelling leading man.</p><p>He's Sheriff Woody, the floppy, knock-kneed, pull-string doll with the voice of Tom Hanks.</p><p>Since "Toy Story" began, Woody, the diminutive protagonist, has delivered what are, by any measure, delightful and engaging performances. Behind them are the combined efforts of scores of animators -- around 100 on "Toy Story 4" alone.</p><p>A live-action performance occurs, often intuitively, in the moment, while animation can look more like the work of technicians than thespians. Laboring in dimly lit rooms on the Pixar campus in Emeryville, California, animators spend a lot of time staring at computer monitors. With Woody as something like their high-tech onscreen marionette, they painstakingly contrive one pose, one movement and one facial expression at a time -- frame by frame, scene by scene -- until he begins to resemble a living, breathing character.</p><h3 id="homophobic-and-transphobic-hate-crimes-surge-in-england-and-wales">Homophobic and transphobic hate crimes surge in England and Wales</h3><p>Homophobic and transphobic hate crimes, including stalking, harassment and violent assault, have more than doubled in England and Wales over five years, a Guardian analysis has shown.</p><p>The findings come after two women were attacked on a bus in London for refusing to kiss in front of a group of men. The incident sparked widespread condemnation.</p><p>The rate of LBGT hate crime per capita rose by 144% between 2013-14 and 2017-18. In the most recent year of data, police recorded 11,600 crimes, more than doubling from 4,600 during this period.</p><p>Campaigners said the rise could partly be down to better reporting but added that hatred was growing on British streets because of the rise of rightwing populism.</p><p>Taz Edwards-White, an alliance manager at Metro, an equalities and diversity organisation, said the hate crime figures were likely to be 'the tip of the iceberg'.</p><p>'What we see in our services is lots of people experience day-to-day verbal attacks or violence and aggressive language and homophobic attitudes … We do believe the political climate has had an impact: people feel unsafe. What is happening in central government and all the scapegoating has an effect. We saw a spike [in racist attacks] after Brexit and there has been a steady increase since then.'</p><h3 id="a-starving-polar-bear-wandered-into-a-russian-city-scavenging-for-food">A starving polar bear wandered into a Russian city, scavenging for food</h3><p>Photos showed the female animal trudging across a busy road with muddy paws, digging through rubble and trash and lying down from apparent exhaustion on the ground.</p><p>Experts believe the animal, initially thought to have wandered hundreds of miles from its natural habitat in the Arctic, instead may have been taken as a cub and raised in captivity by pelt traders, according to the Siberian Times.</p><p>The experts explained that it is typically males, not females, that migrate long distances, and said this polar bear was too clean to have made such a trek, the newspaper reported. They told the newspaper it's possible that poachers released the animal to avoid being caught and punished.</p><p>Polar bear poaching has been banned in Russia for more than 60 years. Still, it has been estimated that as many as 200 of them are killed each year in the Far East region.</p><p>As the Washington Post's Isaac Stanley-Becker reported, polar bears "are classified as a vulnerable species because of the 'ongoing and potential loss of their sea ice habitat resulting from climate change,' according to the World Wildlife Fund."</p><h3 id="the-queen-of-eating-shellfish-online">The queen of eating shellfish online</h3><p>Hundreds of thousands of people tune in each week to watch Bethany Gaskin binge-eat shellfish on YouTube.</p><p>Gaskin, 44, has capitalized on the popularity if a food-video genre known as mukbang, which involves scarfing down, on camera, more grub than should rightly be consumed in a single sitting.</p><p>On her two YouTube channels, Gaskin chats up her audience while eating king crab legs, mussels, lobster tails, hard-boiled eggs and roasted red potatoes.</p><p>Mukbang seems to have begun as an internet trend more than a decade ago in South Korea. The name is a mash-up of the Korean words for let's eat ('muk-ja) and broadcasting ('bang-song'). Korean live-streamers often schedule their mukbang videos to align with dinnertime hours, so their viewers eating alone at home feel like they're sharing a meal with a friend.</p><p>Viewers cite other benefits too. Watching the videos can serve as an appetite-curbing exercise.</p><p>Though the chatter around mukbang videos trends to focus on their benefits, there are also some who see reason for concern. Theresa Kinsella, 42, a dietitian, said in a phone interview that these videos 'glorify overeating,' promote disordered eating and ignore the possible risks associated with overconsumption.</p><p>'The short-term health risk are physical discomfort, gastrointestinal distress, lethargy and fatigue,' Kinsella said. As for the long-term effects, she cited weight gain, heart diseases and diabetes.</p><h3 id="trump-takes-20-steps-into-north-korea-making-history-as-first-sitting-us-leader-to-enter-the-nation">Trump takes 20 steps into North Korea, making history as first sitting US leader to enter the nation</h3><p>The encounter at the heavily fortified Korean Demilitarized Zone -- their third in person -- came a day after Trump raised the prospect of a border handshake in a tweet and declared he'd have "no problem" stepping into North Korea.</p><h3 id="chinas-grand-sci-fi-is-going-global">China's grand sci-fi is going global</h3><p>As China's economy has grown over the past 30 years, its sci-fi writers' vision has expanded, too. Their stories tend to focus on Earth itself -- eschewing galaxies far far away and long ago -- while being conceived on a stupendous scale. One recurring wide-angle shot in " The Wandering Earth", for example, shows the planet gliding through space on a pincushion of blue flame, its atmosphere trailing off into a vacuum.</p><p>The writers of Chinese science fiction anticipated their genre's rising profile. In 2010 Fei Dao, another author, described its devotees as a "lonely hidden army". Chinese sci-fi, he said, might "unexpectedly rush out and change heaven and Earth". That has not quite happened yet. But in the future, anything is possible.</p><h3 id="shanghai-citizens-out-of-sorts-over-new-trash-separation-rules">Shanghai citizens out of sorts over new trash separation rules</h3><p>Household trash has occupied minds of Shanghai residents this week: specifically, are the contents of their bins "wet", "dry", "hazardous" or "recycle"?</p><p>Shanghai aims to eventually burn or recycle all waste. By next year, dry waste incineration and wet waste treatment rates are expected to reach 27,800 tonnes a day, around 80% of the city's total garbage.</p><p>China is building hundreds of "waste to energy" plants that use garbage to generate power. Its also establishing a "waste-free city" scheme and constructing high-tech "comprehensive utilization bases" across the country.</p><p>"We need to step up the propaganda, and we need to step up the construction of infrastructure and guarantee that the separation of trash meets out requirements," she said.</p><h3 id="dogs-eyes-have-changed-since-humans-befriended-them">Dogs' eyes have changed since humans befriended them</h3><p>Dogs, more so than almost any other domesticated species, are desperate for human eye contact. It's hard for most people to resist a petulant flash of puppy-dog eyes -- and according to a new study, that pull on the heartstrings might be exactly why dogs can give us those looks at all.</p><p>Dogs split off from their wolf relatives -- specifically, gray wolves -- as many as 33,000 years ago. Studying the two animals is a bit like cracking open a four-legged time capsule. A paper published in the Proceedings of the National Academy of Sciences found that dogs' faces are structured for complex expression in a way that wolves' aren't.</p><p>For the study, a team looked at two muscles that work together to widen and open dog's eyes, droopier, and objectively cuter. But in the four gray wolves the researchers looked at, neither muscles was present.</p><p>Research has also shown that when dogs work these muscles, humans respond more positively. This isn't simply a fortuitous love story, in which the eyes of two species just so happen to meet across a crowded planet. Like all the best partnerships, this one is more likely the result of years of evolution and growth.</p><p>For a species to change quickly, a pretty powerful force must be acting on it. And that's where human come in. We connect profoundly with animals capable of exaggerating the size and width of their eyes, which makes them look like our own human babies and "hijacks" our nurturing instincts.</p><p>"These muscles evolved during domestication", says Brian Hare, the editor of the paper, "but almost certainly due to an advantage they gave dogs during interactions with humans that we humans have been all but unaware of."</p><h3 id="whaling-ships-set-sail-as-japan-resumes-commercial-hunts">Whaling ships set sail as Japan resumes commercial hunts</h3><p>Japan began its first commercial whale hunts in more than three decades on Monday, brushing aside outrage over its resumption of a practice that conservationists say is cruel and outdated.</p><p>Whaling has long proved a rare diplomatic flashpoint for Tokyo, which says the practice is a Japaneses tradition that should not be subject to international interference.</p><p>With its withdraw from the IWC, Tokyo will carry out whale hunting off Japan, but will end the most controversial hunts in the Antarctic.</p><p>Some believe that Japan's return to commercial whale hunting will effectively sound the death knell for the industry.</p><p>"Japan is quitting high-seas whaling … that is a huge step towards the end of killing whales for their meat and other products," said Patrick Ramage, director of marine conservation at the International Fund for Animal Welfare. He said commercial whaling in Japanese waters was unlikely to have much of a future given dwindling subsides and the shrinking market for whale meat.</p><h3 id="with-escape-of-xxs-son-mexico-suffers-a-stunning-defeat">With escape of xx's son, Mexico suffers a stunning defeat</h3><p>The latest family member to escape apprehension -- xx's son, yy - managed his own feat of government humiliation this week, when cartel henchmen forced a patrol of armed forces to release him after he had been captured.</p><p>The stunning surrender -- with Mexican forces badly outmatched, taken hostage by outlaws and forced to let loose a prominent suspect in their custody -- began with a seige on the city of xxx on Thursday. Videos of fierce gunbattles in the street, armed men blocking roads, residents fleeing to safety and clouds of black smoke rising from burning vehicles swamped social media.</p><p>Reports swirled that after the capture of the younger xxx, his cartel mounted a fierce assault to win his freedom and prevent his extradition to the U.S.</p><p>The cartel's victory offered a frightening glimpse into the power wielded by organized crime in Mexico, distilling in a single, eight-hour stretch the extent to which the nation is held captive by criminal networks -- without a plan to combat the scourge of violence that has brought the country to its deadliest point in decades.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A collection of some latest news.
    
    </summary>
    
      <category term="English" scheme="http://yoursite.com/categories/English/"/>
    
    
      <category term="English" scheme="http://yoursite.com/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>Notes of Mathematics</title>
    <link href="http://yoursite.com/posts/notes/2019-06-19-notes-math.html"/>
    <id>http://yoursite.com/posts/notes/2019-06-19-notes-math.html</id>
    <published>2019-06-19T13:40:09.000Z</published>
    <updated>2021-01-12T19:34:13.746Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="泛函">泛函</h2><center><img src="/assets/img/Rules/lines.png" width=500"></center><h3 id="不动点定理">不动点定理</h3><ol type="1"><li><p>不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。</p><p><a href="https://zhuanlan.zhihu.com/p/33885648">链接</a></p></li></ol><h3 id="紧性的利用">紧性的利用</h3><ul><li>证明存在性</li><li>在无限维空间中“模仿”有限维的欧式空间</li></ul><p>紧集是为了模仿描述欧式空间中的有界闭集合么？ 紧=相对紧+闭</p><h3 id="流形manifolds">流形(Manifolds)</h3><ol type="1"><li>概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。</li></ol><h3 id="支撑集support">支撑集(Support)</h3><ol type="1"><li>概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合</li></ol><h2 id="数学分析">数学分析</h2><h3 id="lipschitz连续">Lipschitz连续</h3><ol type="1"><li>若存在一个常数K，使得定义域内的任意两点x1,x2满足： $ |f(x_1)-f(x_2)|=K|x_1-x_2|$ 则称函数为Lipschitz连续函数。此性质限定了f的导函数的绝对值不超过K，规定了函数的最大局部变动幅度。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;泛函&quot;&gt;泛函&lt;/h2&gt;
&lt;center&gt;
&lt;img src=&quot;/assets/img/Rules/lines.png&quot; width=500&quot;&gt;
&lt;/center&gt;
&lt;h3 id=&quot;不动点定理&quot;&gt;不动点定理&lt;/h3&gt;

      
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>About</title>
    <link href="http://yoursite.com/posts/uncategorized/2019-06-13-about.html"/>
    <id>http://yoursite.com/posts/uncategorized/2019-06-13-about.html</id>
    <published>2019-06-13T17:02:18.000Z</published>
    <updated>2020-01-23T22:44:45.908Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to Mia Feng's Blog!</p><p>I'm doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in Montreal, Canada. I'm interested in data analysis, reinforce learning, transfer learning and democratising machine learning and AI.</p><p>I got the bachelor degree from Wuhan University, China, and then I did my master at the National University of Defense Technology, China. During my studies, I've processed spatial-temporal data, and financial data. I also worked as an intership of fintech in Meituan-Dianping. The main areas I have learned including machine learning, geographical information system, software engineering, data assimilation, and finance. Currently I am learning something about computational neuroscience. I want to learn more about transfer learning and visualization and explanation of neural networks.</p><p>Have a look at my <a href="https://github.com/skaudrey/cv/blob/master/cv.pdf">resume</a> for more information.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to Mia Feng&#39;s Blog!&lt;/p&gt;
&lt;p&gt;I&#39;m doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in M
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>rl-intro</title>
    <link href="http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html</id>
    <published>2019-05-26T17:02:18.000Z</published>
    <updated>2021-01-09T22:31:46.251Z</updated>
    
    <content type="html"><![CDATA[This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver. <a id="more"></a> ## Why RL? ### The difference between supervised learning and RL * task directed from fixed data sets vs goal directed learning from interaction * characteristics: * RL * trial-and-error search * delayed reward * ML * generalization methods: regularization, data augmentation * real-time loss * methods: * RL * exploit: get reward * exploration: make better action selection in the future. * ML * discriminative and generative: parameterized and semi-parameterized * supervised and unsupervised: depends on whether have labeled data<center><img src="/assets/img/RLIntro/RLML.png" width=400"></center><h3 id="the-capacity-of-rl">The capacity of RL</h3><ul><li>sequential decision maker facing unknown or known environment</li><li>works for non i.i.d. data</li></ul><center><img src="/assets/img/RLIntro/RLCapacity.png" width=300"></center><h2 id="whats-rl">What's RL?</h2><h3 id="the-agent-environment-interaction">The agent-environment interaction</h3><center><img src="/assets/img/RLIntro/RLEA.png" width=300"></center><ul><li>At each step t the agent:<ul><li>Executes action at</li><li>Transform to state St</li><li>Receives scalar reward rt</li></ul></li><li>The environment:<ul><li>Receives action at</li><li>Transform to state St</li><li>Emits scalar reward rt+1</li></ul></li><li>t increments at env. step</li></ul><h3 id="the-elements-of-rl">The elements of RL</h3><ul><li><p>Policy: agent's behaviour function, mostly is a PDF mapping state to action</p><ul><li><p>Deterministic policy</p><p><span>$$a = \pi\left(S\right)$$</span><!-- Has MathJax --></p></li><li><p>Stochastic policy</p><p><span>$$\pi\left(a|S\right)=\mathbb{P}\left(A_t=a|S_t=s\right)$$</span><!-- Has MathJax --></p></li></ul></li><li><p>Value function: how good is each state and/or action, the scalar value is also named reward.</p><p><span>$$v_{\pi}\left(s\right)=\mathbb{E}\left(R_{t+1}+\gamma R_{t+1}+{\gamma}^2 R_{t+2}+\cdots|S_t=s\right)$$</span><!-- Has MathJax --></p><p>Mostly, to make algorithm converge, a final state will be rewarded 0, and other non-final states are rewarded as a minus value.</p></li><li><p>Model: agent's representation of the environment, they can be modeled by TKinter, gym etc.</p><ul><li><p>e.g.: models in assimilation, maze</p><p><span>$$\mathcal{P}_{ss&apos;}^{a}=\mathbb{P}\left(S_{t+1}=s&apos;|S_t=s,A_t=a\right)\\\mathcal{R}_{s}^{a}=\mathbb{E}\left(R_{t+1}=s&apos;|S_t=s,A_t=a\right)$$</span><!-- Has MathJax --></p></li><li><p>unknown environment can be stimulated by sampling ### Classification of RL #### What you want</p></li><li><p>Value Based: No Policy (Implicit), Value Function</p></li><li><p>Policy Based: Policy, No Value Function</p></li><li><p>Actor Critic: Policy, Value Function #### What you knew</p></li><li><p>Model-free: Policy and/or Value Function, No Model</p></li><li><p>Model-based: Policy and/or Value Function, Model</p></li></ul></li></ul><h2 id="how-to-rl">How to RL?</h2><h3 id="markov-process----to-simplify">Markov process -- to simplify</h3><h4 id="markov-process">Markov Process</h4><span>$$\mathbb{P}\left(S_{t+1}\right)=\mathbb{P}\left(S_{t+1}|S_1,\cdots,S_t\right)\\\mathcal{P}_{ss&apos;}=\mathbb{P}\left(S_{t+1}=s&apos;|S_t=s\right)$$</span><!-- Has MathJax --><h4 id="markov-rewarded-process">Markov Rewarded Process</h4><span>$\langle S,\mathcal{P},\mathcal{R},\gamma\rangle$</span><!-- Has MathJax --><p>solve the reward from state at time t to the final state, which can be also solved by adding immediate reward and discounted value of successor state.</p><span>$v\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)=\mathbb{E}\left(R_{t+1}+\gamma v\left(S_{t+1}|S_t=s\right)\right)$</span><!-- Has MathJax --><h4 id="markov-decision-process">Markov Decision Process</h4><span>$\langle S,\mathcal{A},\mathcal{P},\mathcal{R},\gamma\rangle$</span><!-- Has MathJax --><p>Sequential decision making. * state-value function <span>$v_{\pi}\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)$</span><!-- Has MathJax --></p><ul><li>action-value function <span>$q_{\pi}\left(s,a\right)=\mathbb{E}_{\pi}\left(G_t|S_t=s,A_t=a\right)$</span><!-- Has MathJax --></li></ul><h3 id="valuepolicy">Value？Policy？</h3><h4 id="policy-iteration">Policy Iteration</h4><center><img src="/assets/img/RLIntro/policyitr.png" width=300"></center><h4 id="value-iteration">Value Iteration</h4><center><img src="/assets/img/RLIntro/valueitr.png" width=300"></center>]]></content>
    
    <summary type="html">
    
      This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>gnn</title>
    <link href="http://yoursite.com/posts/notes/2019-05-24-gnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-24-gnn.html</id>
    <published>2019-05-24T22:17:30.000Z</published>
    <updated>2019-09-23T09:27:01.032Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list the generation and improvement of graph neural networks. # Why Use * Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,</p><h1 id="basic-lines">Basic lines</h1><p>Contents in this block mainly comes from paper <a href="https://arxiv.org/pdf/1812.08434.pdf">Graph Neural Networks: A Review of Methods and Applications</a> and</p><a id="more"></a><h2 id="history">History</h2><h3 id="the-proposal-of-gnns">The proposal of GNNs</h3><p>learn a target node’s representation by propagating neighbor information via recurrent neural architectures in an iterative manner until a stable fixed point is reached Computation expensive * <a href="https://www.researchgate.net/publication/4202380_A_new_model_for_earning_in_raph_domains">A new model for learning in graph domains</a> * Big Question: processing the graph without losing topological information * reason<br />Traditional preprocessing methods for graphs dropped topological information, and thus leads to poor performance and generalization. * background RNN can only handle graph-level problems; Traditional methods dropped topological information.</p><ul><li>the approximation capability of GNN, <a href="https://www.researchgate.net/publication/23763868_Computational_Capabilities_of_Graph_Neural_Networks">Computational Capabilities of Graph Neural Networks</a> under mild generic conditions, most of the practically useful functions on graphs can be approximated in probability by GNNs up to any prescribed degree of accuracy.</li><li><a href="https://ieeexplore.ieee.org/document/4773279">Neural network for graphs: A contextual constructive approach</a></li><li><a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a> ### Go to GNNs #### Spectral-based Graph difficult to parallel or scale to large graphs,cause they need to load the whole graph into the memory. relies on eigen-decomposition of the Laplacian matrix.</li><li>Spectral GNN, <a href="https://arxiv.org/pdf/1312.6203v3.pdf">Spectral networks and locally connected networks on graphs</a></li><li>ChebNet, <a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a>，codes <a href="https://github.com/mdeff/cnn%20graph">here</a>.</li><li>1st ChebNet, <a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a>, code <a href="https://github.com/tkipf/gcn">here</a>. localized in space, but the computation requirement grow exponentially, to reduce it, sampling methods are proposed. See <a href="https://arxiv.org/pdf/1801.10247.pdf">FASTGCN</a>, <a href="https://arxiv.org/pdf/1710.10568.pdf">reduce variance</a> and <a href="https://arxiv.org/pdf/1809.05343v1.pdf">adaptive sampling</a> for details.</li><li>AGCN, calculate a pairwise distance of nodes to construct a residual graph, see <a href="https://arxiv.org/pdf/1801.03226.pdf">Adaptive Graph Convolutional Neural Networks</a> for details.</li></ul><h4 id="spatial-based-graph-convolution">Spatial-based Graph Convolution</h4><p>has gained more attention * <a href="https://arxiv.org/pdf/1706.02216.pdf">Inductive representation learning on large graphs</a> * <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model cnns</a> * <a href="https://arxiv.org/pdf/1605.05273.pdf">Learning convolutional neural networks for graphs</a> * <a href="https://arxiv.org/abs/1808.03965">Large-scale learnable graph convolutional networks</a> [1],[4] used sampling strategy the common way is to stack multiple graph convolution layer together. ##### Recurrent-based Spatial GCNs update a node's representation recursively until a stable fixed point is reached * GNNs, <a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a> * GGNNs, used GRU, <a href="https://www.aclweb.org/anthology/D14-1179">Learning phrase representations using rnn encoder-decoder for statistical machine translation</a>, codes <a href="https://github.com/yujiali/ggnn">here</a>. * Stochastic Steady-state Embedding (SSE), updates the node latent representations stochastically in an asynchronous fashion, <a href="http://proceedings.mlr.press/v80/dai18a/dai18a.pdf">Learning steady-states of iterative algorithms over graphs</a>, codes <a href="https://github.com/Hanjun-Dai/steady%20state%20embedding">here</a>.</p><h5 id="composition-based-spatial-gcns">Composition-based Spatial GCNs</h5><ul><li>Message Passing Neural Networks (MPNNs), <a href="https://arxiv.org/pdf/1704.01212.pdf">Neural Message Passing for Quantum Chemistry</a></li><li>GraphSage, <a href="https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf">Inductive representation learning on large graphs</a>, codes <a href="https://github.com/williamleif/GraphSAGE">here</a>. ###### Miscellaneous Variants of Spatial GCNs</li><li>Diffusion Convolution Neural Networks (DCNN), the hidden node representation is get by independently convolving inputs with power series or transition probability matrix, <a href="https://arxiv.org/pdf/1511.02136.pdf">Diffusion-convolutional neural networks</a></li><li>Build GCN into a standard grid to do CNN, <a href="http://proceedings.mlr.press/v48/niepert16.pdf">Learning convolutional neural networks for graphs</a>, but it ignored the node information.</li><li>Large-scale Graph Convolution Networks (LGCN), <a href="https://arxiv.org/pdf/1808.03965.pdf">Large-scale learnable graph convolutional networks</a>, still using standard grid, but it also collects nodes' information and draw subgraph for mini-batch training. Codes <a href="https://github.com/divelab/lgcn/">here</a>.</li><li>Mixture Model Network (MoNet), <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model CNNs</a>, introduce pseudo-coordinates and weight functions to let the weight of a node’s neighbor be determined by the relative position (pseudo-coordinates) between the node and its neighbor.</li><li><a href="https://arxiv.org/abs/1802.00910">Geniepath: Graph neural networks with adaptive receptive paths</a>, everages gating mechanisms to control the depth and breadth of a node's neighborhood.</li><li><a href="https://persagen.com/files/misc/zhuang2018dual.pdf">Dual graph convolutional networks for graph-based semi-supervised classification</a>, one for global representation the other for local representation.</li><li><a href="https://arxiv.org/pdf/1811.10435.pdf">On filter size in graph convolutional networks</a>, introduce a hyperparameter to influence the receptive field size of a node.</li></ul><h4 id="pooling-module">Pooling module</h4><ul><li><a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a></li><li>pooling by rearranging vertices into meaningful order, <a href="https://arxiv.org/pdf/1506.05163.pdf">Deep convolutional networks on graph-structured data</a></li><li><a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf">An end-to-end deep learning architecture for graph classification</a></li><li>DIFFPOOL pools nodes hierarchically by learning a cluster assignment matrix in each layer to get a cluster embedding, which can be combined with any standard GCN module, <a href="https://arxiv.org/pdf/1806.08804.pdf">Hierarchical graph representation learning with differentiable pooling</a></li></ul><h3 id="graph-attention-networks">Graph attention networks</h3><p>For sequence-based tasks, in total, assigning attention weights to different neighbors when aggregating feature information, ensembling multiple models according to attention weights, and using attention weights to guide random walks. * <a href="https://arxiv.org/pdf/1710.10903.pdf">Graph attention networks</a>, (GAT), multi-head weights. Codes <a href="https://github.com/PetarV-/GAT">here</a>. * <a href="https://arxiv.org/pdf/1803.07294.pdf">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</a>， also use multi-head, but it use a self attention mechanism to compute a different head for each head. * <a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">Graph classification using structural attention</a>, Graph Attention Model (GAM), adaptively visiting a sequence of important nodes. * <a href="https://arxiv.org/pdf/1710.09599.pdf">Watch your step: Learning node embeddings via graph attention</a>, factorize the co-occurrence matrix with differentiable attention weights. ### Graph Autoencoders had to handle the problem caused by the sparsity of adjacency matrix. * <a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf">Deep neural networks for learning graph representations</a>, uses the stacked denoising auto-encoders to reconstruct PPMI matrix. Codes <a href="https://github.com/ShelsonCao/DNGR">here</a> * <a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">Structural deep network embedding</a>, preserve nodes first-order proximity (drive representations of adjacent nodes close to each other) and second-order proximity (a node's neighbourhood information) jointly. Codes <a href="https://github.com/suanrong/SDNE">here</a> * <a href="https://arxiv.org/pdf/1611.07308.pdf">Variational graph auto-encoders</a>, Graph Auto-Encoder (GAE), combined with GCN firstly. Codes <a href="https://github.com/limaosen0/Variational-Graph-Auto-Encoders">here</a> * <a href="https://shiruipan.github.io/pdf/CIKM-17-Wang.pdf">Mgae: Marginalized graph autoencoder for graph clustering</a>, reconstruct node's hidden state. * <a href="https://www.ijcai.org/proceedings/2018/0362.pdf">Adversarially regularized graph autoencoder for graph embedding</a>, using GANs to regularize the graph auto-encoders, recover adjacency matrix. Codes <a href="https://github.com/Ruiqi-Hu/ARGA">here</a> * <a href="https://www.kdd.org/kdd2018/accepted-papers/view/learning-deep-network-representations-with-adversarially-regularized-autoen">Learning deep network representations with adversarially regularized autoencoders</a>, recover node sequences rather than adjacency matrix. * <a href="http://pengcui.thumedialab.com/papers/NE-RegularEquivalence.pdf">Deep recursive network embedding with regular equivalence</a>, codes <a href="https://github.com/tadpole/DRNE">here</a> ### Graph Generative Networks Not scalable to large graphs. * <a href="https://arxiv.org/pdf/1802.08773.pdf">Graphrnn: A deep generative model for graphs</a>, graph-level RNN + node-level RNN, use breadth-first-search (BFS) to sequence the nodes and Bernoulli assumption for edge generation. Codes <a href="https://github.com/snap-stanford/GraphRNN">here</a>. * <a href="https://arxiv.org/pdf/1803.03324.pdf">Learning deep generative models of graphs</a>, utilize spatial-based GCNs to obtain a hidden representation of an existing graph. * <a href="https://arxiv.org/pdf/1805.11973.pdf">Molgan: An implicit generative model for small molecular graphs</a>, RL+GAN+GCN * <a href="https://arxiv.org/pdf/1803.00816.pdf">Net-gan: Generating graphs via random walks</a>, combines LSTM with Wasserstein GAN to generate graphs from a random-walk-based approach. As for random walk, see <a href="http://leogrady.net/wp-content/uploads/2017/01/grady2004multilabel.pdf">here</a>. * <a href="https://arxiv.org/pdf/1809.02630.pdf">Constrained generation of semantically valid graphs via regularizing variational autoencoders</a> ### Graph Spatial-Temporal Networks</p><ul><li><a href="https://arxiv.org/pdf/1612.07659.pdf">Structured sequence modeling with graph convolutional recurrent networks</a></li><li><a href="https://arxiv.org/pdf/1707.01926.pdf">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</a>, can work forwardly or backwardly. Codes <a href="https://github.com/liyaguang/DCRNN">here</a>.</li><li><a href="https://arxiv.org/pdf/1709.04875.pdf">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</a>, codes <a href="https://github.com/VeritasYin/STGCN_IJCAI-18">here</a>.</li><li><a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial temporal graph convolutional networks for skeleton-based action recognition</a>, extend the temporal flow as graph edges, and then assign each a label to each edge. Codes <a href="https://github.com/yysijie/st-gcn">here</a>.</li><li><a href="https://arxiv.org/pdf/1511.05298.pdf">Structural-rnn: Deep learning on spatio-temporal graphs</a>, aims at predicting nodes' labels at each time, has nodeRNN and edgeRNN, and split nodes and edges into semantic groups. Codes <a href="https://github.com/asheshjain399/RNNexp">here</a>.</li></ul><h2 id="main-methodologies----graph-embedding">Main Methodologies -- Graph Embedding</h2><h3 id="matrix-factorization">Matrix Factorization</h3><ul><li><a href="https://www.ijcai.org/proceedings/2018/0493.pdf">Discrete network embedding</a></li><li><a href="https://shiruipan.github.io/pdf/ICDM-18-Yang.pdf">Binarized attributed network embedding</a> ### Random Walks</li><li><a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf">Deepwalk: Online learning of social representations</a></li></ul><h2 id="problems">Problems</h2><ul><li>Does going deeper always work in GNNs?</li><li>How to select representative receptive field for a node?</li><li>How to work on large graphs?</li><li>How to handle dynamic and heterogeneous graph structures?</li></ul><h1 id="papers">Papers</h1><h2 id="introduction">Introduction</h2><ul><li>M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, "Geometric deep learning: going beyond euclidean data,"IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 18–42, 2017 ## others ### <a href="https://arxiv.org/pdf/1905.02296v1.pdf">Are Graph Neural Networks Miscalibrated?</a></li></ul><h3 id="estimating-node-importance-in-knowledge-graphs-using-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.08865.pdf">Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks</a></h3><p>GENI, a GNN-based method designed to deal with distinctive challenges involved with predicting node importance in KGs.</p><h3 id="understanding-attention-in-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.02850.pdf">Understanding attention in graph neural networks</a></h3><p>We aim to better understand attention over nodes in graph neural networks and identify factors influencing its effectiveness. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 40% in some of our classification tasks</p><h3 id="are-graph-neural-networks-miscalibrated"><a href="https://arxiv.org/pdf/1905.02296.pdf">Are Graph Neural Networks Miscalibrated?</a></h3><p>Graph Neural Networks (GNNs) have proven to be successful in many classification tasks, outperforming previous state-of-the-art methods in terms of accuracy</p><h3 id="graph-convolutional-networks-with-eigenpooling"><a href="https://arxiv.org/pdf/1904.13107.pdf">Graph Convolutional Networks with EigenPooling</a></h3><p>To apply graph neural networks for the graph classification task, approaches to generate the  from node representations are demanded. Experimental results of the graph classification task on <span class="math inline">\(6\)</span> commonly used benchmarks demonstrate the effectiveness of the proposed framework.</p><h3 id="pan-path-integral-based-convolution-for-deep-graph-neural-networks"><a href="https://arxiv.org/pdf/1904.10996.pdf">PAN: Path Integral Based Convolution for Deep Graph Neural Networks</a></h3><p>Experimental results show that the path integral based graph neural networks have great learnability and fast convergence rate, and achieve state-of-the-art performance on benchmark tasks.</p><h3 id="attacking-graph-based-classification-via-manipulating-the-graph-structure"><a href="https://arxiv.org/pdf/1903.00553.pdf">Attacking Graph-based Classification via Manipulating the Graph Structure</a></h3><p>We evaluate our attacks and compare them with a recent attack designed for graph neural networks. Results show that our attacks 1) can effectively evade graph-based classification methods; 2) do not require access to the true parameters, true training dataset, and/or complete graph; and 3) outperform the existing attack for evading collective classification methods and some graph neural network methods</p><h3 id="deep-learning-in-bioinformatics-introduction-application-and-perspective-in-big-data-era"><a href="https://arxiv.org/abs/1903.00342">Deep learning in bioinformatics: introduction, application, and perspective in big data era</a></h3><p>After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures</p><h3 id="constant-time-graph-neural-networks"><a href="https://arxiv.org/pdf/1901.07868.pdf">Constant Time Graph Neural Networks</a></h3><p>Recent advancements in graph neural networks (GNN) have led to state-of-the-art performance in various applications including chemo-informatics, question answering systems, and recommendation systems, to name a few</p><h3 id="a-comprehensive-survey-on-graph-neural-networks"><a href="https://arxiv.org/pdf/1901.00596.pdf">A Comprehensive Survey on Graph Neural Networks</a></h3><p>We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories</p><h3 id="graph-transformation-policy-network-for-chemical-reaction-prediction"><a href="https://openreview.net/pdf?id=r1f78iAcFm">Graph Transformation Policy Network for Chemical Reaction Prediction</a></h3><p>To this end, we propose Graph Transformation Policy Network (GTPN) -- a novel generic method that combines the strengths of graph neural networks and reinforcement learning to learn the reactions directly from data with minimal chemical knowledge. Evaluation results show that GTPN improves the top-1 accuracy over the current state-of-the-art method by about 3% on the large USPTO dataset</p><h3 id="contextualized-non-local-neural-networks-for-sequence-learning"><a href="https://arxiv.org/pdf/1811.08600.pdf">Contextualized Non-local Neural Networks for Sequence Learning</a></h3><p>Recently, a large number of neural mechanisms and models have been proposed for sequence learning, of which self-attention, as exemplified by the Transformer model, and graph neural networks (GNNs) have attracted much attention. Specifically, we propose contextualized non-local neural networks (CN<span class="math inline">\(^{\textbf{3}}\)</span>), which can both dynamically construct a task-specific structure of a sentence and leverage rich local dependencies within a particular neighborhood.</p><h3 id="automated-theorem-proving-in-intuitionistic-propositional-logic-by-deep-reinforcement-learning"><a href="https://arxiv.org/pdf/1811.00796.pdf">Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning</a></h3><p>Using the large volume of augmented data, we train highly accurate graph neural networks that approximate the value function for the set of the syntactic structures of formulas. Within the specified time limit, our prover solved 84% of the theorems in a benchmark library, while <span class="math inline">\(\texttt{tauto}\)</span> was able to solve only 52%.</p><h3 id="pileup-mitigation-at-the-large-hadron-collider-with-graph-neural-networks"><a href="https://arxiv.org/pdf/1810.07988.pdf">Pileup mitigation at the Large Hadron Collider with Graph Neural Networks</a></h3><p>We present a classifier based on Graph Neural Networks, trained to retain particles coming from high-transverse-momentum collisions, while rejecting those coming from pileup collisions. This model is designed as a refinement of the PUPPI algorithm, employed in many LHC data analyses since 2015</p><h3 id="weisfeiler-and-leman-go-neural-higher-order-graph-neural-networks"><a href="https://arxiv.org/pdf/1810.02244.pdf">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</a></h3><p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. The following work investigates GNNs from a theoretical point of view and relates them to the <span class="math inline">\(1\)</span>-dimensional Weisfeiler-Leman graph isomorphism heuristic (<span class="math inline">\(1\)</span>-WL). We show that GNNs have the same expressiveness as the <span class="math inline">\(1\)</span>-WL in terms of distinguishing non-isomorphic (sub-)graphs</p><h3 id="multitask-learning-on-graph-neural-networks---learning-multiple-graph-centrality-measures-with-a-unified-network"><a href="https://arxiv.org/pdf/1809.07695.pdf">Multitask Learning on Graph Neural Networks - Learning Multiple Graph Centrality Measures with a Unified Network</a></h3><p>Graph neural networks (GNN), consisting of trained neural modules which can be arranged in different topologies at run time, are sound alternatives to tackle relational problems which lend themselves to graph representations. The proposed model achieves <span class="math inline">\(89\%\)</span> accuracy on a test dataset of random instances with up to 128 vertices and is shown to generalise to larger problem sizes</p><h3 id="meta-gnn-on-few-shot-node-classification-in-graph-meta-learning"><a href="https://arxiv.org/pdf/1905.09718.pdf">Meta-GNN: On Few-shot Node Classification in Graph Meta-learning</a></h3><p>However, there are very few works applying meta-learning to non-Euclidean domains, and the recently proposed graph neural networks (GNNs) models do not perform effectively on graph few-shot learning problems. Additionally, Meta-GNN is a general model that can be straightforwardly incorporated into any existing state-of-the-art GNN</p><h3 id="mr-gnn-multi-resolution-and-dual-graph-neural-network-for-predicting-structured-entity-interactions"><a href="https://arxiv.org/pdf/1905.09558.pdf">MR-GNN: Multi-Resolution and Dual Graph Neural Network for Predicting Structured Entity Interactions</a></h3><p>In recent years, graph neural networks have become attractive. Experiments conducted on real-world datasets show that MR-GNN improves the prediction of state-of-the-art methods.</p><h3 id="revisiting-graph-neural-networks-all-we-have-is-low-pass-filters"><a href="https://arxiv.org/pdf/1905.09550.pdf">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</a></h3><p>In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property</p><h3 id="multi-hop-reading-comprehension-across-multiple-documents-by-reasoning-over-heterogeneous-graphs"><a href="https://arxiv.org/pdf/1905.07374.pdf">Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs</a></h3><p>We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based model (single model) achieves state-of-the-art result.</p><h3 id="ipc-a-benchmark-data-set-for-learning-with-graph-structured-data"><a href="https://arxiv.org/pdf/1905.06393.pdf">IPC: A Benchmark Data Set for Learning with Graph-Structured Data</a></h3><p>The data set, named IPC, consists of two self-contained versions, grounded and lifted, both including graphs of large and skewedly distributed sizes, posing substantial challenges for the computation of graph models such as graph kernels and graph neural networks</p><h1 id="datasets">Datasets</h1><ul><li><a href="https://www.aminer.cn/citation">Citation Networks</a>: <a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/cora_raw.zip">Cora</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/citeseer.zip">Citeseer</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/pubmed.zip">Pubmed</a>,<a href="https://www.aminer.cn/citation">DBLP</a></li><li><a href="http://networkrepository.com/soc_BlogCatalog.php">Social Networks</a>: <a href="http://socialcomputing.asu.edu/datasets/BlogCatalog">BlogCatalog</a>,<a href="https://github.com/linanqiu/reddit-dataset">Reddit</a>,<a href="http://www.trustlet.org/downloaded_epinions.html">Epinions</a></li><li>Chemical/Biological Graphs: <a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI1.zip">NCI-1</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI109.zip">NCI-9</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/MUTAG.zip">MUTAG</a>, D&amp;D,<a href="https://github.com/bigdata-ustc/QM9nano4USTC">QM9</a>,<a href="https://tripod.nih.gov/tox21/challenge/data.jsp">Tox21</a>,<a href="http://snap.stanford.edu/graphsage/ppi.zip">PPI</a>.</li><li>Unstructured Graphs: convert <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, <a href="http://www.mattmahoney.net/dc/textdata.html">Wikipedia</a> or News Groups into graphs.</li><li>Others: <a href="https://pan.baidu.com/s/14Yy9isAIZYdU__OYEQGa_g#list/path=%2F">METR-LA</a>, <a href="https://grouplens.org/datasets/movielens/1m/">Movies-Lens1M</a>, NELL.</li></ul><p>Thanks for the links given <a href="https://www.jianshu.com/p/67137451b67f">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list the generation and improvement of graph neural networks. # Why Use * Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,&lt;/p&gt;
&lt;h1 id=&quot;basic-lines&quot;&gt;Basic lines&lt;/h1&gt;
&lt;p&gt;Contents in this block mainly comes from paper &lt;a href=&quot;https://arxiv.org/pdf/1812.08434.pdf&quot;&gt;Graph Neural Networks: A Review of Methods and Applications&lt;/a&gt; and&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>UNet proposed by Olaf Ronneberger etc.</title>
    <link href="http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html</id>
    <published>2019-05-23T21:18:21.000Z</published>
    <updated>2021-01-12T19:33:13.151Z</updated>
    
    <content type="html"><![CDATA[<p>The notes of paper <a href="https://arxiv.org/abs/1505.04597v1">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>.</p><a id="more"></a><h2 id="big-question-classification-in-pixel-level-and-thus-image-segmentation">Big Question: classification in pixel level and thus image segmentation</h2><h3 id="reason">reason</h3><ul><li>single label for a image is not enough to support segmentation. ### background</li><li>why focus on it<ul><li>biomedical images: like cells segmentation. Add: also appropriate for other entity segmentation</li></ul></li><li>how have been done:<ul><li>the development of deeper CNNs</li><li>using CNNs segmenting electron microscopy images</li></ul></li><li>what have been missed:<ul><li>computation efficiency and redundancy: slow cause every patch require a running of network; patch overlapping</li><li>difficult trade-off for localization and the usage of context.</li></ul></li></ul><h2 id="methods">Methods</h2><h3 id="for-what">For what?</h3><p>Cells segmentation.</p><h3 id="framework-of-methods">Framework of Methods</h3><p>Convolution2D + deconvolution (upsampling 2D). The output of one downsampling layer is contracted as part of the input of the corresponding symmetric upsampling layer.</p><h3 id="novelty">Novelty</h3><ul><li>data augmentation randomly elastic deformations: shift, rotation, gray value, random elastic deformations are the most important</li><li>replace pooling by upsampling.</li><li>No fully connection layers.</li><li>weighted the loss of touching objects (cells).</li></ul><h2 id="details">Details</h2><h3 id="weighted-map-to-segment-overlapped-cells">weighted map to segment overlapped cells</h3><p>According to the paper, they pre-compute the weight map for each ground truth segmentation to compensate the different frequency of pixels.</p><h3 id="data-augmentation">data augmentation</h3><p>Smooth deformations using random displacements vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation.</p><h2 id="abstract">Abstract</h2><p>The main idea in abstract are contracted NNs and data augmentation so that the new NNs can get reasonable results by fewer images.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The notes of paper &lt;a href=&quot;https://arxiv.org/abs/1505.04597v1&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>paper--Faster RCNN</title>
    <link href="http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster%20rcnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster rcnn.html</id>
    <published>2019-05-19T15:17:12.000Z</published>
    <updated>2021-01-12T19:31:54.427Z</updated>
    
    <content type="html"><![CDATA[<p>Some understanding about the details in Faster RCNN, based on the codes in tensorflow.</p><a id="more"></a><h2 id="big-question">Big Question</h2><h3 id="reason">reason</h3><h3 id="background">background</h3><h2 id="region-proposal-network">Region Proposal Network</h2><h3 id="some-numbers">Some numbers</h3><ul><li>The number of anchor boxes for one anchor target <span>$k = scale \times ratios$</span><!-- Has MathJax -->,</li><li>The number of anchor boxes for one feature layer (which has <span>$W \times H$</span><!-- Has MathJax --> grids), will get <span>$W \times H \times k$</span><!-- Has MathJax --> anchor boxes. Every grid in the feature map (the output of a popular CNN without FC layers) will have <span class="math inline">\(k\)</span> anchor boxes.</li><li>Not like the ROI method, the size of features are fixed, but anchor boxes are rescaled by <span>$k$</span><!-- Has MathJax --> regressors.</li></ul><h2 id="experiments">Experiments</h2><h3 id="prove">prove</h3><ul><li>The top-ranked RPN proposals are accurate.</li><li>NMS does not harm the detection mAP and may reduce false alarms.</li></ul><h2 id="construct">Construct</h2><h3 id="add-loss">Add loss</h3><h3 id="problems-using-it-processing-typhoon-data">Problems using it processing typhoon data</h3><ul><li>Does NMS lead to loss of typhoon? Not really, the texture of typhoon is obvious in image.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Some understanding about the details in Faster RCNN, based on the codes in tensorflow.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Positioning Data of FY4 AGRI.</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html</id>
    <published>2019-04-26T22:53:05.000Z</published>
    <updated>2019-04-26T15:31:57.701Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.</p><p>There are two ways for positioning, one is querying the lookup table given by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">NSMC</a>, the other is calculating by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">formulas</a>.</p><p>However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days. <a id="more"></a></p><h2 id="querying-the-lookup-table.">Querying the lookup table.</h2><p>There are two errors in the files.</p><ul><li>The first 8 bytes denote latitude, and the next 8 bytes are reserved for longitude.</li><li>The data are stored as little-endian data.<center><img src="/assets/img/FY4-AGRI/lookup.png" width="400"></center></li></ul><h2 id="calculating-by-formulas">Calculating by formulas</h2><p>The formulas are OK, but the constant variable <span class="math inline">\(\lambda_D\)</span> should be measured in rad before being used.</p><center><img src="/assets/img/FY4-AGRI/formulas.png" width="400"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.&lt;/p&gt;
&lt;p&gt;There are two ways for positioning, one is querying the lookup table given by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;NSMC&lt;/a&gt;, the other is calculating by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;formulas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="Sensor" scheme="http://yoursite.com/tags/Sensor/"/>
    
  </entry>
  
  <entry>
    <title>A Tex Template of Cornell Notes</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2019-04-26T15:34:08.234Z</updated>
    
    <content type="html"><![CDATA[<p>I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these. <a id="more"></a></p><h2 id="preliminaries">Preliminaries</h2><p>Before starting, you need install * CTex</p><h2 id="install-missed-packages-of-ctex">Install missed packages of CTex</h2><h3 id="download-the-required-packages">Download the required packages</h3><p>Search <a href="https://www.ctan.org/pkg">here</a>.</p><h3 id="unzip-and-compile-manually-if-needed.">Unzip and compile manually if needed.</h3><p>Unzip the downloaded file and jump to the directory after unzip. If need compile, run <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  pdflatex ***.ins</span><br></pre></td></tr></table></figure> ### Install Copy the compiled file folder to the CTex path: ~/CTex/CTex/tex/latex</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ texhash --admin</span><br></pre></td></tr></table></figure>The notes will be generated like this:<center><img src="/assets/img/CornellNotes.png" width="400"></center><h1 id="acknowledgement">Acknowledgement</h1><p>Thank <a href="https://blog.csdn.net/Myriad_Dreamin/article/details/83384110">this blog</a>.</p><h1 id="resources">Resources</h1><p>Tex file can be found <a href="https://github.com/skaudrey/skaudrey.github.io/tree/master/assets/notes/Cornell">here</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>Understand and debug the codes of GCN proposed by Thomas N. Kipf</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2021-01-12T19:34:09.102Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1609.02907.pdf">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>.</p><a id="more"></a><h2 id="big-question-semi-supervised-classification-of-graph-data">Big Question: semi-supervised classification of graph data</h2><ul><li>reason<ul><li>computation effective: semi-supervision</li><li>the complex of graphs, the information of nodes and edges are not structural information.</li></ul></li><li>background<ul><li>the improvement of GCNs: spectral GCNs</li></ul></li></ul><h2 id="key-points">Key points</h2><h3 id="the-approximation-of-spectral-graph-convolution">The approximation of spectral graph convolution</h3><p>The lines in paper had confused me at first before I ran the codes.</p><p>The difference of graph convolution and valina convolution is the input, as the input is a graph rather than data in same dimension, the key point is how to convert data represented by node and graph to a tensor in fixed dimension.</p><p>To solve it, Thomas maps the graph into a spectral space and also, to be computational efficient, approximate the infinite coefficients by second-order Chebyshev polynomial formulas.</p><p>After those approximation, it is input into the whole network with features.</p><h3 id="build-model">Build model</h3><p>Actually, except the complicated preprocess to represent graph G into a sparse tensor, the other step are not that complex, just the similar as what a convolution layer do. <span>$Z = f\left(\mathbf{X},A\right)=softmax\left(\hat{A}ReLU\left({\hat{A}XW^{\left(0\right)}}\right)W^{\left(1\right)}\right)$</span><!-- Has MathJax --></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
  </entry>
  
</feed>
