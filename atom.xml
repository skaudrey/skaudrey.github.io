<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mia&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/2d5354ebc5a8c2413323ef55a6c6d252</icon>
  <subtitle>Je marche lentement, mais je ne recule jamais.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2022-05-17T16:20:38.237Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Mia Feng</name>
    <email>skaudreymia@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Important  Papers of SSL.</title>
    <link href="http://yoursite.com/posts/notes/2022-04-24-notes-paper-ssl-milestones.html"/>
    <id>http://yoursite.com/posts/notes/2022-04-24-notes-paper-ssl-milestones.html</id>
    <published>2022-04-24T16:02:00.000Z</published>
    <updated>2022-05-17T16:20:38.237Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>Paper 1: Vincent et al (2008) <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume11%2Fvincent10a%2Fvincent10a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0SspBuh4yVzZEMYUyio3wn"><em>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</em></a>. JMLR 2008</p></li><li><p>Analysis of SSL methods</p><ul><li>Paper 2: Kolesnikov, Zhai and Beyer (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FKolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Kgfk_3VH635pQwvKeEBE0"><em>Revisiting Self-Supervised Visual Representation Learning</em></a>. CVPR 2019</li><li>Paper 3: Zhai et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.04867&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2ITTaWDSpqKWi3PBBkP5xL"><em>A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark</em></a> (A GLUE-like benchmark for images) ArXiv 2019</li><li>Paper 4: Asano et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1904.13132&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2Fb1pUVDy26_-64K2-T_oR"><em>A critical analysis of self-supervision, or what we can learn from a single image</em></a> ICLR 2020</li></ul></li><li><p>Contrastive methods</p><ul><li><p>Paper 5: van den Oord et al. (2018) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1807.03748&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw33PilvMsYAOmPqiPojZcUY"><em>Representation Learning with Contrastive Predictive Coding</em></a> (CPC), ArXiv 2018</p></li><li><p>Paper 6: Hjelm et al. (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1808.06670&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw31h2Nm4lF3JBWFDXbG1miu"><em>Learning deep representations by mutual information estimation and maximization</em></a> (DIM) ICLR 2019</p></li><li><p>Paper 7: Tian et al. (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.05849&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0s1M_14J8haUyPCJRRILoQ"><em>Contrastive Multiview Coding</em></a> (CMC) ArXiv 2019</p></li><li><p>Paper 8: Hénaff et al. (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.09272&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1GXq6QkuyYGzUQ2EqiWXnL"><em>Data-Efficient Image Recognition with Contrastive Predictive Coding</em></a> (CPC v2: Improved CPC evaluated on limited labelled data) ArXiv 2019</p></li><li><p>Paper 9: He et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FHe_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3wwgkDHILZxygHEqFpue3W"><em>Momentum Contrast for Unsupervised Visual Representation Learning</em></a> (MoCo, see also <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.04297&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0tEX1f7G3quzSTWL6rfgzw"><em>MoCo v2</em></a>). CVPR 2020</p></li><li><p>Paper 10: Chen T et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2002.05709.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3nN2zgMpeyBgpXG--pO5Kx"><em>A Simple Framework for Contrastive Learning of Visual Representations</em></a> (SimCLR). ICML 2020</p></li><li><p>Paper 11: Chen T et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.10029&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw263B6UxGrQeiE53_baVKp-"><em>Big Self-Supervised Models are Strong Semi-Supervised Learners</em></a> (SimCLRv2) ArXiv 2020</p></li><li><p>Paper 12: Caron et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.09882&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0IYamHH9j-b56C3eUiwNXP"><em>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</em></a> (SwAV) ArXiv 2020</p></li><li><p>Paper 13: Xiao et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.05659&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3gwV78jCI0fs0R9wrj-y7S"><em>What Should Not Be Contrastive in Contrastive Learning</em></a> ArXiv 2020</p></li><li><p>Paper 14: Misra and van der Maaten (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FMisra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3sER0wiTJlhLbi88owlXXE"><em>Self-Supervised Learning of Pretext-Invariant Representations</em></a><em>.</em> CVPR 2020</p></li></ul></li><li><p>Generative methods</p><ul><li>Paper 15: Dumoulin et al (2017) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1606.00704&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2xx-blDKLroqk6XrXTa7Tp">Adversarially Learned Inference</a> (ALI) ICLR 2017</li><li>Paper 16: Donahue, Krähenbühl and Darrell <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1605.09782&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Ea7cOor2n6pmoJ44576vf">Adversarial Feature Learning</a> (BiGAN, concurrent and similar to ALI) ICLR 2017</li><li>Paper 17: Donahue and Simonyan (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1907.02544&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3bxhaVdSwniyMV3EFLcUrl">Large Scale Adversarial Representation Learning</a> (Big BiGAN) ArXiv 2019</li><li>Paper 18: Chen et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fimage-gpt%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2hhRvO-SL2S08MmJ1VjBfw">Generative Pretraining from Pixels</a> (iGPT) ICML 2020</li></ul></li><li><p>BYoL: boostrap your own latents</p><ul><li>Paper 19: Tarvainen and Valpola (2017) <a href="https://www.google.com/url?q=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2-LuidEMZJI7fUiE-oWqoa"><em>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</em></a>. NeurIPS 2017</li><li>Paper 20: Grill et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.07733&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2igxNmTekOxWbWzL9y5IyY"><em>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</em></a> (BYoL). ArXiv 2020</li><li>Paper 21: Abe Fetterman, Josh Albrecht, (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Funtitled-ai.github.io%2Funderstanding-self-supervised-contrastive-learning.html&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0qgrINDjCLX30tzD20JOti"><em>Understanding self-supervised and contrastive learning with "Bootstrap Your Own Latent" (BYOL)</em></a> Blog post</li><li>Paper 22: Schwarzer and Anand et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19"><em>Data-Efficient Reinforcement Learning with Momentum Predictive Representations</em></a><a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19">.</a> ArXiv 2020</li></ul></li><li><p>self-distillation methods</p><ul><li>Paper 23: Furlanello et al (2017) <a href="http://www.google.com/url?q=http%3A%2F%2Fmetalearning.ml%2F2017%2Fpapers%2Fmetalearn17_furlanello.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3xt23sIkAffPt8Ym8vqZDS"><em>Born Again Neural Networks</em></a>. NeurIPS 2017</li><li>Paper 24: Yang et al. (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.05551&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2HyME0KdJDI4vO-jOuk3Zp"><em>Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students</em></a><a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.05551&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2HyME0KdJDI4vO-jOuk3Zp">.</a> AAAI 2019</li><li>Paper 25: Ahn et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FAhn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3XI1Xtrprs4vWTy6AcLiYH"><em>Variational information distillation for knowledge transfer</em></a>. CVPR 2019</li><li>Paper 26: Zhang et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_ICCV_2019%2Fpapers%2FZhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3lmWaXmFh5PictqZWwKIG5"><em>Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation</em></a> ICCV 2019</li><li>Paper 27: Müller et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F8717-when-does-label-smoothing-help.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2aZPLQTJt3rY082Bdqc3K9"><em>When Does Label Smoothing Help?</em></a> NeurIPS 2019</li><li>Paper 28: Yuan et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FYuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3zJVfBf6c8hlwGdl32vMNP"><em>Revisiting Knowledge Distillation via Label Smoothing Regularization</em></a>. CVPR 2020</li><li>Paper 29: Zhang and Sabuncu (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2006.05065v1.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0MHYHwHxUixNKEr1znpFVs"><em>Self-Distillation as Instance-Specific Label Smoothing</em></a> ArXiv 2020</li><li>Paper 30: Mobahi et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2002.05715&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0SktJ4r4OuAXv0PwzzVEnu"><em>Self-Distillation Amplifies Regularization in Hilbert Space</em></a>. ArXiv 2020</li></ul></li><li><p>self-training / pseudo-labeling methods</p><ul><li>Paper 31: Xie et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1911.04252&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1DhfgYIRMPv6XislXO6ry_"><em>Self-training with Noisy Student improves ImageNet classification</em></a>. CVPR 2020</li><li>Paper 32: Sohn and Berthelot et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2001.07685&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3LFMCJQrLt8NabRsym2vQW"><em>FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</em></a>. ArXiv 2020</li><li>Paper 33: Chen et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2006.10032.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2yldjcUPMdKaLjP5mcG1Xp">Self-training Avoids Using Spurious Features Under Domain Shift</a>. ArXiv 2020</li></ul></li><li><p>Iterated learning/emergence of compositional structure</p><ul><li>Paper 34: Ren et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DHkePNpVKPB&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2kXj4HM38k2b1gLv7cLC9G">Compositional languages emerge in a neural iterated learning model</a>. ICLR 2020</li><li>Paper 35: Guo, S. et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.05291&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw26-y5c29PiVpbAZYCrsj-o">The emergence of compositional languages for numeric concepts through iterated learning in neural agents</a>. ArXiv 2020</li><li>Paper 36: Cogswell et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F1904.09067.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0I5RdaelW5fHhwWfXWf9V2">Emergence of Compositional Language with Deep Generational Transmission</a> ArXiv 2020</li><li>Paper 37: Kharitonov and Baroni (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2004.03420%23%3A~%3Atext%3DEmergent%20Language%20Generalization%20and%20Acquisition%20Speed%20are%20not%20tied%20to%20Compositionality%2C-Eugene%20Kharitonov%2C%20Marco%26text%3DStudies%20of%20discrete%20languages%20emerging%2Cfor%20evidence%20of%20compositional%20structure.&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw14HOo95bLPmafk6gFvaAOT">Emergent Language Generalization and Acquisition Speed are not tied to Compositionality</a> ArXiv 2020</li></ul></li><li><p>NLP</p><ul><li>Paper 38: Peters et al (2018) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1802.05365&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw10eV0YxRuEz3vADuc94kFn"><em>Deep contextualized word representations</em></a> (ELMO), NAACL 2018</li><li>Paper 39: Devlin et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN19-1423%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1oPB9GeT2i05ilyj5OAlBE"><em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</em></a> (BERT) NAACL 2019</li><li>Paper 40: Brown et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.14165&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0k8WZSIpb3qv17TQpoA0HA"><em>Language Models are Few-Shot Learners</em></a> (GPT-3, see also GPT-<a href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Flanguage-unsupervised%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0AevPs9tr7a9kx--G4mr7M"><em>1</em></a>and <a href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fbetter-language-models%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1A50m11hjqmW-6njiC3YUp"><em>2</em></a>for more context) ArXiv 2020</li><li>Paper 41: Clark et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.10555&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1FzuTywUomI9d2oAiBVFxm"><em>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</em></a> ICLR 2020</li><li>Paper 42: He and Gu et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DSJgdnAVKDH&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2BOCV5w8r7GPa0z64Xb1sC"><em>REVISITING SELF-TRAINING FOR NEURAL SEQUENCE GENERATION</em></a> (Unsupervised NMT) ICLR 2020</li></ul></li><li><p>video/multi-modal data</p><ul><li>Paper 43: Wang and Gupta (2015) <a href="https://www.google.com/url?q=https%3A%2F%2Fxiaolonw.github.io%2Funsupervise.html&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3MTp7nv2L5RX59nyRPqMfM">Unsupervised Learning of Visual Representations using Videos</a> ICCV 2015</li><li>Paper 44: Misra, Zitnick and Hebert (2016) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1603.08561&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3oZeSlFwM0jkzrL2Mq_Hwf"><em>Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</em></a> ECCV 2016</li><li>Paper 45: Lu et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1908.02265&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw39ViPN0TptsZxtRypcY1Ya"><em>ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</em></a><a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1908.02265&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw39ViPN0TptsZxtRypcY1Ya">,</a> NeurIPS 2019</li><li>Paper 46: Hjelm and Bachman (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.13278&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0fIuuJYkqfT84nt0_QyFfZ"><em>Representation Learning with Video Deep InfoMax</em></a><a href="https://www.google.com/url?q=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN19-1423%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1oPB9GeT2i05ilyj5OAlBE"><em>.</em></a> (VDIM) Arxiv 2020</li></ul></li><li><p>the role of noise in representation learning</p><ul><li>Paper 47: Bachman, Alsharif and Precup (2014) <a href="http://www.google.com/url?q=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5487-learning-with-pseudo-ensembles&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2hqOPAo5QSUwexnM7P62j4"><em>Learning with Pseudo-Ensembles</em></a> NeurIPS 2014</li><li>Paper 48: Bojanowski and Joulin (2017 ) <a href="http://www.google.com/url?q=http%3A%2F%2Fproceedings.mlr.press%2Fv70%2Fbojanowski17a%2Fbojanowski17a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3_A9d13BdOXeC_QmofU3sV"><em>Unsupervised Learning by Predicting Noise.</em></a> ICML 2017</li></ul></li><li><p>SSL for RL, control and planning</p><ul><li>Paper 49: Pathak et al. (2017) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1705.05363&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1IrpWV_kOjuIQTMmDFS3wd"><em>Curiosity-driven Exploration by Self-supervised Prediction</em></a> (see also a <a href="https://www.google.com/url?q=https%3A%2F%2Fpathak22.github.io%2Flarge-scale-curiosity%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0ZeG_gMDAsY1Z_VMac-vsK"><em>large-scale follow-up</em></a>) ICML 2017</li><li>Paper 50: Aytar et al. (2018) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.11592&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw35lA1NGt9l0sZszcrO-f6F"><em>Playing hard exploration games by watching YouTube</em></a> (TDC) NeurIPS 2018</li><li>Paper 51: Anand et al. (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.08226&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1te1a2v-cmeICbUGL2_oav"><em>Unsupervised State Representation Learning in Atari</em></a> (ST-DIM) NeurIPS 2019</li><li>Paper 52: Sekar and Rybkin et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.05960&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1zh2h0rWm82Aq8AZKySxPl"><em>Planning to Explore via Self-Supervised World Models.</em></a> ICML 2020</li><li>Paper 53: Schwarzer and Anand et al. (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19"><em>Data-Efficient Reinforcement Learning with Momentum Predictive Representations</em></a><a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19">.</a> ArXiv 2020</li></ul></li><li><p>SSL theory</p><ul><li>Paper 54: Arora et al (2019) <a href="http://www.google.com/url?q=http%3A%2F%2Fproceedings.mlr.press%2Fv97%2Fsaunshi19a%2Fsaunshi19a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1Jv6bPfprDaifia4UCyFhE"><em>A Theoretical Analysis of Contrastive Unsupervised Representation Learning</em></a>. ICML 2019</li><li>Paper 55: Lee et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.01064&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2cgSGuqGOXlczR6HPQRBwq"><em>Predicting What You Already Know Helps: Provable Self-Supervised Learning</em></a> ArXiv 2020</li><li>Paper 56: Tschannen, et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1907.13625&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Y3TCY3KlkCbEIXQ61liXo">On mutual information maximization for representation learning</a>. ArXiv 2019.</li></ul></li><li><p>Unsupervised domain adaption</p><ul><li>Paper 57: Shu et al (2018) <a href="https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DH1q-TM-AW&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1DzTvyS29_50se_tZob9IX"><em>A DIRT-T APPROACH TO UNSUPERVISED DOMAIN ADAPTATION</em></a>. ICLR 2018</li><li>Paper 58: Wilson and Cook (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1812.02849&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2n226swg-lx1H29furztaL"><em>A Survey of Unsupervised Deep Domain Adaptatio</em></a><em>n</em>. ACM Transactions on Intelligent Systems and Technology 2020<strong>.</strong></li><li>Paper 59: Mao et al. (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.04215&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3tp1FCFSItdeJjpQUUvBJX"><em>Virtual Mixup Training for Unsupervised Domain Adaptation</em></a>. CVPR 2019</li><li>Paper 60: Vu et al. (2018) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1811.12833&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1ZjGV5QwOyB0ZPMqgiSGXh"><em>ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</em></a> CVPR 2019</li></ul></li><li><p>Scaling</p><ul><li>Paper 61: Kaplan et al (2020) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2001.08361&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0pY71xR2C_948Bp-wDv-aA"><em>Scaling Laws for Neural Language Models</em></a>. ArXiv 2020</li></ul></li></ul><a id="more"></a><h2 id="paper-1-stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion">Paper 1: <a href="https://www.google.com/url?q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume11%2Fvincent10a%2Fvincent10a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0SspBuh4yVzZEMYUyio3wn"><em>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</em></a></h2><p>Codes:</p><h3 id="previous">Previous</h3><ul><li>What works much better is to initially use a local unsupervised criterion to (pre)train each layer in turn, with the goal of learning to produce a useful higher-level representation from the lower-level representation output by the previous layer.</li><li>Initializing a deep network by stacking autoencoders yields almost as good a classification performance as when stacking RBMs. But why is it almost as good?</li><li>looking for unsupervised learning principles likely to lead to the learning of feature detectors that detect important structure in the input patterns.</li></ul><h4 id="traditional-classifiers-training-with-noisy-inputs">Traditional classifiers training with noisy inputs</h4><ul><li>Training with noise is equivalent to applying generalized Tikhonov regularization (Bishop, 1995), which means L2 weight decay penalty but on linear regression with additive noise. For non-linear case, the regularization is more complex. Authors of this paper even show that different result when using DAEmon and when using regular autoencoders with a L2 weight decay.</li></ul><h4 id="pseudo-likelihood-and-dependency-networks">Pseudo-Likelihood and Dependency Networks</h4><ul><li>Pseudo-likelihood , dependency network paradigms etc.</li></ul><h3 id="ideas">Ideas</h3><p>stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. Find that denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. our results clearly establish the value of using a denoising criterion as an unsupervised objective to guide the learning of useful higher level representations.</p><h4 id="reasoning-what-makes-good-representations">Reasoning: what makes good representations</h4><ul><li>A good representation is those that retain a significant amount of information about the input, can be expressed in information-theoretic terms as maximizing the mutual information</li><li>Mutual information can be decomposed into an entropy and a conditional entropy term in two different ways<ul><li>ICA: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104529719.png" alt="image-20220426104529719" /></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104552474.png" title="fig:" alt="image-20220426104552474" /><ul><li>consider a parameterized distribution $p(X|Y;') $ that parameterized by <span class="math inline">\(\mathrm{\theta}&#39;\)</span>, then <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105305843.png" alt="image-20220426105305843" /> leads to maximizing a lower bound on<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105332503.png" alt="image-20220426105332503" />and thus on the mutual information.</li><li>ICA is when <span class="math inline">\(Y=f_\theta(X)\)</span>.</li><li>As <span class="math inline">\(q(X)\)</span> is unknown, but with samples, the empirical average over the training samples can be used instead as an unbiased estimate (i.e., replacing <span class="math inline">\(\mathbb{E}_{q(X)}\)</span> by $_{q^0(X)} $: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105801573.png" alt="image-20220426105801573" /></li></ul></li></ul></li><li>The equation above corresponds to the reconstruction error criterion used to train autoencoders<ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111611299.png" alt="image-20220426111611299" />: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111628453.png" alt="image-20220426111628453" />To choose <span class="math inline">\(p(\mathrm{x|z}),L(\mathrm{x|z})\)</span><ul><li>For real-valued $ $ squared error: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111826403.png" alt="image-20220426111826403" style="zoom:80%;" /> on <span class="math inline">\(\sigma^2\)</span>, the variance of <span class="math inline">\(X\)</span>. Since gaussian, not to use a squashing nonlinearity in the decoder.</li><li>Binary $ $: finally become cross-entropy loss, which can be used when $ $ is not strictly binary but rather $^d $</li></ul></li><li><strong>training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between input X and learnt representation Y</strong></li></ul></li><li><strong>Merely Retaining Information is Not Enough</strong><ul><li>non-zero reconstruction error to separate useful information from noise: traditional AE uses bottleneck to produce an under-complete representation where <span class="math inline">\(d&#39;&lt;d\)</span>, and result in a lossy compressed representation of <span class="math inline">\(X\)</span>. When using affine encoder and decoder without any nonlinearity an a squared error loss, the AE performs PCA actually. But not in cross-entropy loss.</li><li>using over-complete (i.e., higher dimensional than the input) but sparse representations is popular now, which is a special case of imposing on <span class="math inline">\(Y\)</span> different constraints than that of a lower dimensionality. A sparse over-complete representations can be viewed as an alternative “compressed” representation.</li></ul></li></ul><h3 id="how">How?</h3><ul><li><p>DAE: denoising autoencoder</p><ul><li><p>Procedures</p><ul><li>first denoising: corrupt the initial input $ $ into <span class="math inline">\(\tilde{\mathrm{x}}\)</span> by a stochastic mapping. This will lead to force the learning of a mapping that extracts features useful for denoising.</li><li>Then map <span class="math inline">\(\tilde{\mathrm{x}}\)</span> to <span class="math inline">\(\mathrm{y}\)</span>. And reonstruct <span class="math inline">\(\mathrm{z}\)</span> to be close to the $ $ but a function of <span class="math inline">\(\mathrm{y}\)</span>.</li></ul></li><li><p>Geometric Interpretation</p><ul><li>Thus stochastic operator <span class="math inline">\(p(X|\tilde{X})\)</span> learns a map that tends to go from lower probability points <span class="math inline">\(\tilde{X}\)</span> to nearby high probability points <span class="math inline">\(X\)</span>, on or near the manifold.</li><li>Successful denoising implies that the operator maps even far away points to a small region close to the manifold</li><li>Think of <span class="math inline">\(Y=f(X)\)</span> as a representation of <span class="math inline">\(X\)</span> which is well suited to capture the main variations in the data, that is, those along the manifold.</li></ul></li><li><p>Types of corruption considered</p><ul><li>additive: natural</li><li>masking noise: natural for input domains which are interpretable as binary or near binary such as black an white images or the representation produced at the hidden layer after a signoid squashing function</li><li>salt and pepper noise: same as masking noise, only earse a changing subsets of the inputs components while leaving others untouched</li></ul></li><li><p>Emphasize corrupted dimensions</p><ul><li><p>only put an emphasis on the corrupted dimensions. Like use <span class="math inline">\(\alpha\)</span> for the Reconstruction error on corrupted components and <span class="math inline">\(\beta\)</span> for untouched components</p></li><li><p>Sqaured loss</p><figure><img src="C:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220427142848736.png" alt="image-20220427142848736" /><figcaption aria-hidden="true">image-20220427142848736</figcaption></figure></li><li><p>Cross-entropy loss</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220427142828464.png" alt="image-20220427142828464" /><figcaption aria-hidden="true">image-20220427142828464</figcaption></figure></li><li><p>If <span class="math inline">\(\alpha=1, \beta=0\)</span>, then this is full emphasis that only consider the error on the prediction of corrupted elements</p></li></ul></li><li><p>Stacking DAE for deep architecture</p></li></ul></li></ul><h3 id="experiments">Experiments</h3><ul><li><p>Single DAE</p><ul><li>feature detectors from natural image patches<ul><li>The under-complete autoencoder appears to learn rather uninteresting local blob detectors. Filters obtained in the overcomplete case have no recognizable structure, looking entirely random</li><li>training with sufficiently large noise yields a qualitatively very different outcome than training with a weight decay regularization and this proves that the two are not equivalent for a non-linear autoencoder.</li><li>Salt-and-pepper noise yielded Gabor-like edge detectors, whereas masking noise yielded a mixture of edge detectors and grating filters. They all yield some potentially useful edge detectors.</li></ul></li><li>feature detectors from handwrite digits<ul><li>With increased noise levels, a much larger proportion of interesting (visibly non random and with a clear structure) feature detectors are learnt. These include local oriented stroke detectors and detectors of digit parts such as loops.</li><li>But denoising a more corrupted input requires detecting bigger, less local structures.</li></ul></li></ul></li><li><p>Stacked DAE (SDAE): compare with SAE and DBN.</p><ul><li>Classification problem and experimental methodology: As increasing the noise level, denoising training forces the filters to differentiate more, and capture more distinctive features. Higher noise levels tend to induce less local filters, as expected.</li><li>Compared with other strategies: denoising pretraining with a non-zero noise level is a better strategy than pretraining with regular autoencoders</li><li>Influence of Number of Layers, Hidden Units per Layer, and Noise Level<ul><li>Depth: denoising pretraining being better than autoencoder pretraining being better than no pretraining. The advantage appears to increase with the number of layers and with the number of hidden units</li><li>noise levels: SDAE appears to perform better than SAE (0 noise) for a rather wide range of noise levels, regardless of the number of hidden layers.</li></ul></li></ul></li><li><p>Denoising pretraining v.s. training with noisy input</p><ul><li>Note: SDAE uses a denoising criterion to learn good initial feature extractors at each layer that will be used as initialization for a noiseless supervised training; which is different from training with noisy inputs that amounts to training with a virtually expanded data set.</li><li>Denoising pretraining with SDAE, for a large range of noise levels, yields significantly improved performance, whereas training with noisy inputs sometimes degrades the performance, and sometimes improves it slightly but is clearly less beneficial than SDAE.</li></ul></li><li><p>Variations on the DAE, alternate corruption types and emphasizing</p><ul><li>An emphasized SDAE with salt-and-pepper noise appears to be the winning SDAE variant.</li><li>A judicious choice of noise type and added emphasis may often buy us a better performance.</li></ul></li><li><p>Are Features Learnt in an Unsupervised Fashion by SDAE Useful for SVMs?</p><ul><li>SVM performance can benefit significantly from using the higher level representation learnt by SDAE</li><li>linear SVMs can benefit from having the original input processed non-linearly</li></ul></li><li><p>Generating Samples from Stacked Denoising Autoencoder Networks</p><ul><li>Top-Down Generation of a Visible Sample Given a Top-Layer Representation: it is thus possible to generate samples at one layer from the representation of the layer above in the exact same way as in a DBN.</li><li>Bottom-up (infer representation of the top layer based on representation on the bottom layer) is similar as approximate inference of a factorial Bernoulli top-layer distribution given the low level input. The top-layer representation is to be understood as the parameters (the mean) of a factorial Bernoulli distribution for the actual binary units.</li><li>SDAE and DBN are able to resynthesize a variety of similarly good quality digits, whereas the SAE trained model regenerates patterns with much visible degradation in quality, this also prove that evidence of the qualitative difference resulting from optimizing a denoising criterion instead of mere reconstruction criterion.</li><li>Contrary to SAE, the regenerated patterns from SDAE or DBN look like they could be samples from the same unknown input distribution that yielded the training set.</li></ul></li></ul><h2 id="paper-2-revisiting-self-supervised-visual-representation-learning">Paper 2: <a href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FKolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Kgfk_3VH635pQwvKeEBE0"><em>Revisiting Self-Supervised Visual Representation Learning</em></a></h2><p>https://github.com/google/revisiting-self-supervised</p><h3 id="previous-1">Previous</h3><ul><li>Compared with pretext task, the choice of CNN has not received equal attention</li><li>patch-based self-supervised visual representation learning methods: predicting the relative location of image patches; “jigsaw puzzle” created from the full image etc.</li><li>image-level classification tasks: randomly rotate an image by one of four possible angles and let the model predict that rotation; use clustering of the images</li><li>tasks with dense spatial outputs: image inpainting, image colorization, its improved variant split-brain and motion segmentation prediction.</li><li>equivariance relation to match the sum of multiple tiled representations to a single scaled representation; predict future patches in via autoregressive predictive coding.</li><li>many works have tried to combine multiple pretext tasks in one way or another</li></ul><h3 id="what">What?</h3><ul><li>Standard architecture design recipes do not necessarily translate from the fully-supervised to the self-supervised setting. Architecture choices which negligibly affect performance in the fully labeled setting, may significantly affect performance in the self-supervised setting.</li><li>The quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</li><li>Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations.</li><li>The <strong>evaluation</strong> procedure, where a linear model is trained on a fixed visual representation using <strong>stochastic gradient descent, is sensitive to the learning rate schedule</strong> and may take many epochs to converge.</li></ul><h3 id="how-1">How</h3><ul><li>revisit a prominent subset of the previously proposed pretext tasks and perform a large-scale empirical study using various architectures as base models.</li><li>The CNN candidates<ul><li>ResNet:</li><li>RevNet: stronger invertibility guarantees while being structurally similar to ResNets. Set it to have the same depth and number of channels as the original Resnet50 model.</li><li>VGG: no skip, has BN</li></ul></li><li>The pretext tasks candidates<ul><li>Rotation: 0，90，180，270</li><li>Exemplar: heavy random data augmentation such as translation, scaling, rotation, and contrast and color shifts</li><li>Jigsaw: recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches. They extract representations by averaging the representations of nine uniformly sampled, colorful, and normalized patches of an image.</li><li>Relative patch location: 8 possible relative spatial relations between two patches need to be predicted. use the same patch prepossessing as in the Jigsaw model and also extract final image representations by averaging representations of 9 cropped patches.</li></ul></li><li>Dataset candidates<ul><li>ImageNet</li><li>Places205</li></ul></li><li>Evaluation protocol<ul><li>measures representation quality as the accuracy of a linear regression model trained and evaluated on the ImageNet dataset</li><li>the pre-logits of the trained self-supervised networks as representation.</li></ul></li></ul><h3 id="experiments-1">Experiments</h3><ul><li><p>similar models often result in visual representations that have significantly different performance. Importantly, neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220429135409169.png" alt="image-20220429135409169" /><figcaption aria-hidden="true">image-20220429135409169</figcaption></figure></li><li><p>increasing the number of channels in CNN models improves performance of self-supervised models.</p></li><li><p>ranking of models evaluated on Places205 is consistent with that of models evaluated on ImageNet, indicating that our findings generalize to new datasets.</p></li><li><p>self-supervised learning architecture choice matters as much as choice of a pretext task</p></li><li><p>MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged. We thus conclude that the linear model is adequate for evaluation purposes.</p></li><li><p>Better performance on the pretext task does not always translate to better representations. For residual architectures, the pre-logits are always best.</p></li><li><p>Skip-connections prevent degradation of representation quality towards the end of CNNs. We hypothesize that this is a result of ResNet’s residual units being invertible under some conditions. RevNet, boosts performance by more than 5 % on the Rotation task, albeit it does not result in improvements across other tasks.</p></li><li><p><strong>Model width and representation size strongly influence the representation quality</strong>: disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer. self-supervised learning techniques are likely to benefit from using CNNs with increased number of channels across wide range of scenarios.</p></li><li><p>SGD optimization hyperparameters play an important role and need to be reported. very long training (≈ 500 epochs) results in higher accuracy. They decay lr at 480 epochs.</p></li><li></li></ul><h2 id="paper-3-zhai-et-al-2019-a-large-scale-study-of-representation-learning-with-the-visual-task-adaptation-benchmark-a-glue-like-benchmark-for-images">Paper 3: Zhai et al (2019) <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.04867&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2ITTaWDSpqKWi3PBBkP5xL"><em>A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark</em></a> (A GLUE-like benchmark for images)</h2><p>https://github.com/google-research/task_adaptation</p><h3 id="previous-2">Previous</h3><ul><li>the absence of a unified evaluation for general visual representations hinders progress. Each sub-domain has its own evaluation protocol, and the lack of a common benchmark impedes progress.</li><li>Popular protocols are often too constrained (linear classification), limited in diversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to representation quality (ELBO, reconstruction error)</li></ul><h3 id="what-1">What？</h3><ul><li>present the Visual Task Adaptation Benchmark (VTAB), which defines good representations as those that adapt to diverse, unseen tasks with few examples. (i) minimal constraints to encourage creativity, (ii) a focus on practical considerations, and (iii) make it challenging.</li><li>Conduct a large-scale study of many popular publicly-available representation learning algorithms on VTAB.</li><li>They found:<ul><li>Supervised ImageNet pretraining yields excellent representations for natural image classification tasks</li><li>Self-supervised is less effective than supervised learning overall, but surprisingly, can improve structured understanding</li><li>Combining supervision and self-supervision is effective, and to a large extent, self-supervision can replace, or compliment labels.</li><li>Discriminative representations appear more effective than those trained as part of a generative model, with the exception of adversarially trained encoders.</li><li>GANs perform relatively better on data similar to their pre-training source (here, ImageNet), but worse on other tasks.</li><li>Evaluation using a linear classifier leads to poorer transfer and different conclusions.</li></ul></li></ul><h3 id="how-2">How？</h3><ul><li>To design, must ensure that the algorithms are not pre-exposed to specific evaluation samples.</li><li>VTAB benchmark<ul><li>practical benchmark<ul><li>Define task distribution as Tasks that a human can solve, from visual input alone.”</li><li>For each evaluation sample a new task.</li><li>mitigating meta-overfitting : treat the evaluation tasks unseen, and thus algorithms use pretraining must not pre-train on any of the evaluation tasks.</li><li>Unified implementation: algorithms must have no prior knowledge of the downstream tasks, and hyperparameter searches need to work well across the benchmark.</li><li>All tasks are classification in this paper, such as the detection task is mapped to the classification of the <span class="math inline">\((x,y,z)\)</span> coordinates. The diverse set of visual features are learnt from object identification, scene classification, pathology detection, counting, localization and 3D geometry.</li></ul></li><li>Tasks<ul><li>NATURAL: classical vision problems, group includes Caltech101, CIFAR100, DTD, Flowers102, Pets, Sun397, and SVHN</li><li>SPECIALIZED: images are captured through specialist equipment, and human can recognize the structures. One is remote sensing, the other is medical</li><li>STRUCTURED: assesses comprehension of the structure of a scene, for example, object counting, or 3D depth prediction.</li></ul></li><li>pretraining on pretext tasks and then fine tune</li></ul></li><li>The methods are divided into five groups: Generative, training from-scratch, all methods using 10% labels (Semi Supervised), and all methods using 100% labels (Supervised),</li></ul><h3 id="experiments-2">Experiments</h3><ul><li>Upstream, control the data and architecture (pretrained on ImageNet). They find bigger architectures perform better on VTAB, and use resnet or resnet-similar nets for all models as the encoder.</li><li>Downstream, run VTAB in two modes: the light weight mode sweepstakes 2 initial learning rates and 2 learning rate schedules but fix other parameters while the heavy weights perform a large random search over learning rate, schedule, optimizers, batch size, train preprocessing functions, evaluation preprocessing and weight decay. The main study is done in lightweight.</li><li>Evaluate with top-1 accuracy. To aggregate scores across tasks, take the mean accuracy.</li><li>Lightweight<ul><li>Generative models perform worst, GANs fit more strongly to ImageNet’s domain (natural images), than self-supervised alternatives.</li><li>All self-supervised representations outperform from-scratch training. Methods applied to the entire image outperform patch-based method, and these tasks require sensitivity to local textures. self supervised is worst than supervised on natural tasks, similar on specialized tasks and slightly better than structured tasks.</li><li>Supervised models perform the best. additional self-supervision even improves on top of 100% labelled ImageNet, particularly on STRUCTURED tasks</li></ul></li><li>Heavyweight<ul><li>across all task groups, pre-trained representations are better than a tuned from-scratch model.</li><li>a combination supervision and self-supervision (SUP-EXEMPLAR-100%) getting the best performance.</li></ul></li><li>Frozen feature extractors<ul><li>linear evaluation significantly lowers performance, even when downstream data is limited to 1000 examples. Linear transfer would not by used in practice unless infrastructural constraints required it. These self-supervised methods extract useful representations, just without linear separability. Linear evaluation results are sensitive to additional factors that we do not vary, such as ResNet version or pre-training regularization parameters.</li></ul></li><li>Vision benchmarks<ul><li>The methods ranked according to VTAB are more likely to transfer to new tasks, than those ranked according to the Visual Decathlon</li><li>VTAB is more flexible than Facebook AI SSL challenge, such as the diversity of domain, the evaluation form.</li><li>Meta-dataset: designed for few-shot learning rather than 1000 examples which may entail different solutions.</li></ul></li><li>Some details<ul><li><strong>while training from scratch , inception crop, horizontal flip preprocessing</strong>, 1e-3 in weight decay, 0.1~1 lr in SGD mostly give good results.</li><li>While doing <strong>SSL by ImageNet and then finetuning</strong>, <strong>Inception crop, non-horizontal flip</strong> give good results.</li></ul></li></ul><h3 id="discussion">Discussion</h3><ul><li>how effective are supervised ImageNet representations? ImageNet labels are indeed effective for natural tasks.</li><li>how do representations trained via generative and discriminative models compare? The generative losses seem less promising as means towards learning how to represent data. BigBiGAN is notable.</li><li>To what extent can self-supervision replace labels?<ul><li>self-supervision can almost (but not quite) replace 90% of ImageNet labels; the gap between pre-training on 10% labels with self-supervison, and 100% labels, is small</li><li>self-supervision adds value on top of ImageNet labels on the same data.</li><li>simply adding more data on the SPECIALIZED and STRUCTURED tasks is better than the pre-training strategies we evaluated</li></ul></li><li>Varying other factors to improve VTAB is valuable future research. The only approach that is out-of-bounds is to condition the algorithm explicitly on the VTAB tasks</li></ul><h2 id="paper-4-a-critical-analysis-of-self-supervision-or-what-we-can-learn-from-a-single-image">Paper 4: <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1904.13132&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2Fb1pUVDy26_-64K2-T_oR"><em>A critical analysis of self-supervision, or what we can learn from a single image</em></a></h2><p>https://github.com/yukimasano/linear-probes</p><h3 id="previous-3">Previous</h3><ul><li>For a given model complexity, pre-training by using an off-the-shelf annotated image datasets such as ImageNet remains much more efficient.</li><li>Methods often modify information in the images and require the network to recover them. However, features are learned on modified images which potentially harms the generalization to unmodified ones.</li><li>Learning from a single sample<ul><li>Object tracking: max margin correlation filters learn robust tracking templates from a single sample of the patch.</li><li>learn and interpolate multi-scale textures with a GAN framework</li><li>semi-parametric exemplar SVM model</li><li>we do not use a large collection of negative images to train our model. Instead we restrict ourselves to a single or a few images with a systematic augmentation strategy.</li></ul></li><li>Classical learned and hand-crafted low-level feature extractors: insufficient to clarify the power and limitation of self-supervision in deep networks.</li></ul><h3 id="what-2">What</h3><ul><li>Aim to investigate the effectiveness of current self-supervised approaches by characterizing how much information they can extract from a given dataset of images. Then try to answer <strong>whether a large dataset is beneficial to unsupervised learning, especially for learning early convolutional features</strong></li><li>Three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used.</li><li>For deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training.</li><li>Conclusion<ul><li>the weights of the early layers of deep networks contain limited information about the statistics of natural images</li><li>such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that</li><li>the low-level statistics can be captured via synthetic transformations instead of using a large image dataset. (training these layers with self-supervision and a single image already achieves as much as two thirds of the performance that can be achieved by using a million different images.)</li></ul></li></ul><h3 id="how-3">How</h3><ul><li>Data: use DAA augmentation to replace some source images.<ul><li>Augmentations: involving cropping, scaling, rotation, contrast changes, and adding noise. Augmentation can be seen as imposing a prior on how we expect the manifold of natural images to look like<ul><li>limit the size of cropped patch: the smallest size of the crops is limited to be at least βWH and at most the whole image. Additionally, changes to the aspect ratio are limited by γ. In practice we use β = 10e−3 and γ = 3/4 .</li><li>before cropping, rotate in <span class="math inline">\((-35,35)\)</span> degrees. And also flip images in 50% possibility</li><li>linear transformation in RGB space, color jitter with additive brightness, contrast and saturation</li></ul></li><li>Real samples: give some drawn samples which are not real captured but has plenty of textures and in small size, and a real picture in large size but with large areas no objects.</li></ul></li><li>Representation learning methods<ul><li>BiGAN with leaky ReLU nonlinearities in discriminators</li><li>Rotation: do it on horizontal flips and non-scaled random crops to 224 × 224</li><li>clustering:</li></ul></li></ul><h3 id="experiments-3">Experiments</h3><ul><li>ImageNet and CIFAR-10/100 using linear probes</li><li>Base encoder: AlexNet. They insert the probes right after the ReLU layer in each block</li><li>Learning lasts for 36 epochs and the learning rate schedule starts from 0.01 and is divided by five at epochs 5, 15 and 25</li><li>extracting 10 crops for each validation image (four at the corners and one at the center along with their horizontal flips) and averaging the prediction scores before the accuracy is computed.</li><li>Effect of augmentations: <strong>Random rescaling</strong> adds at least ten points at every depth (see Table 1 (f,h,i)) and is the most important single augmentation. <strong>Color jittering and rotation</strong> slightly improve the performance of all probes by 1- 2% points</li><li>Benchmark evaluation<ul><li>Mono is enough: Mono means train with one source image and its augmented images.</li><li>Image contents:<ul><li>RotNet cannot extract photographic bias from a single image. the method can extract rotation from low level image features such as patches which is at first counter intuitive=&gt; <strong>lighting and shadows even in small patches</strong> can indeed give important cues on the up direction which can be learned even from a single (real) image.</li><li><strong>the augmentations can even compensate for large untextured areas and the exact choice of image is not critical.</strong> A trivial image without any image gradient (e.g. picture of a white wall) would not provide enough signal for any method.</li></ul></li><li>More than one image<ul><li>for conv1 and conv2, a single image is enough</li><li>In deeper layers, DeepCluster seems to require large amounts of source images to yield the reported results as the deka- and kilo- variants start improving over the single image case</li></ul></li><li>Generalization<ul><li>GAN trained on the smaller Image B outperforms all other methods including the fully-supervised trained one for the first convolutional layer</li><li>our method allows learning very generalizable early features that are not domain dependent.</li></ul></li></ul></li><li>the neural network is only extracting patterns and not semantic information because we do not find any neurons particularly specialized to certain objects even in higher levels as for example dog faces or similar which can be fund in supervised networks</li></ul><h2 id="paper-5-representation-learning-with-contrastive-predictive-coding-cpc-arxiv-2018">Paper 5: <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1807.03748&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw33PilvMsYAOmPqiPojZcUY"><em>Representation Learning with Contrastive Predictive Coding</em></a> (CPC), ArXiv 2018</h2><h3 id="previous-4">Previous</h3><ul><li>It is not always clear what the ideal representation is and if it is possible that one can learn such a representation without additional supervision or specialization to a particular data modality.</li><li>One of the most common strategies for unsupervised learning has been to predict future, missing or contextual information<ul><li>Recently in unsupervised learning some learn word representations by predicting neighboring words.</li><li>For images, predicting color from grey-scale or the relative position of image patches</li></ul></li><li>predicting high-dimensional data<ul><li>unimodal losses such as mean-squared error and cross-entropy are not very useful</li><li>powerful conditional generative models which need to reconstruct every detail in the data are usually required.</li></ul></li></ul><h3 id="what-3">What</h3><ul><li>Main contributions<ul><li>Compress features into a latent embedding space in which conditional predictions are easier to model.</li><li>predict the future in latent space by autoregressive models</li><li>Use NCE loss</li></ul></li><li>Intuition<ul><li>learn the representations that encode the underlying shared information between parts of the signal</li><li>meanwhile discard low-level information and noise that is more local.</li></ul></li></ul><h3 id="how-4">How</h3><ul><li>use a NCE which induces the latent space to capture information that is maximally useful to predict future samples</li><li>Model a density ratio which preserves the mutual information between <span class="math inline">\(x_{t+k},c_t\)</span> as <span class="math inline">\(f_k(x_{t+k},c_t)\propto \frac{p(x_{t+k}|c_t)}{p(x_{t+k})}\)</span>, they choose <span class="math inline">\(f_k(x_{t+k},c_t)=\exp(z_{t+k}^TW_kc_t)\)</span>, the log-bilinear model.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504131635374.png" title="fig:" alt="image-20220504131635374" /></li></ul><h3 id="experiments-4">Experiments</h3><p>For every domain we train CPC models and probe what the representations contain with either a linear classification task or qualitative evaluations.</p><ul><li>Audio<ul><li>use a 100-hour subset of the publicly available LibriSpeech dataset</li><li>use a GRU for the autoregressive part of the model</li><li>We found that not all the information encoded is linearly accessible.</li><li>CPCs capture both speaker identity and speech contents</li><li></li></ul></li><li>Vision<ul><li>Use ImageNet and ResNet v2 101, no BN. use the outputs from the third residual block, and spatially mean-pool to get a single 1024-d vector per 64x64 patch. This results in a 7x7x1024 tensor. Next, we use a PixelCNN-style autoregressive model to make predictions about the latent activations in following rows top-to-bottom.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504133549850.png" title="fig:" alt="image-20220504133549850" /></li><li>CPCs improve upon state-of-the-art by 9% absolute in top-1 accuracy, and 4% absolute in top-5 accuracy.</li></ul></li><li>Natural language<ul><li>a linear mapping is constructed between word2vec and the word embeddings learned by the model. A L2 regularization weight was chosen via cross-validation (therefore nested cross-validation for the first 4 datasets)</li><li>found that more advanced sentence encoders did not significantly improve the results, which may be due to the simplicity of the transfer tasks, and the fact that bag-of-words models usually perform well on many NLP tasks.</li><li>The performance of our method is very similar to the skip-thought vector model, with the advantage that it does not require a powerful LSTM as word-level decoder, therefore much faster to train</li></ul></li><li>Reinforcement learning<ul><li>take the standard batched A2C agent as base model and add CPC as an auxiliary loss</li><li>The unroll length for the A2C is 100 steps and we predict up to 30 steps in the future to derive the contrastive loss</li><li>4 out of the 5 games performance of the agent improves significantly with the contrastive loss after training on 1 billion frames.</li></ul></li></ul><h2 id="paper-6-learning-deep-representations-by-mutual-information-estimation-and-maximization-dim-deep-infomax-iclr-2019">Paper 6: <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1808.06670&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw31h2Nm4lF3JBWFDXbG1miu"><em>Learning deep representations by mutual information estimation and maximization</em></a> (DIM Deep InfoMax ) ICLR 2019</h2><p>https://github.com/rdevon/DIM</p><h3 id="previous-5">Previous</h3><ul><li>in typical settings, models with reconstruction-type objectives provide some guarantees on the amount of information encoded in their intermediate representations.</li><li>MI estimation<ul><li>MINE: strongly consistent, can be used to learn better implicit bidirectional generative models</li><li>DIM: follow MINE, but they find the generator is unnecessary. And no necessary for the exact KL-divergence, alternatively the JSD is more stable and provides better results.</li></ul></li><li>CPC and DIM<ul><li>CPC: make predictions about specific local features in the “future” of each summary feature. This equates to ordered autoregression over the local features, and requires training separate estimators for each temporal offset at which one would like to predict the future.</li><li>DIM uses a single summary feature that is a function of all local features, and this “global” feature predicts all local features simultaneously in a single step using a single estimator.</li></ul></li></ul><h3 id="what-4">What</h3><ul><li>structure matters: maximizing the average MI between the representation and local regions of the input can improve performance while maximizing MI between the complete input and the encoder output not always do this.</li><li>JSD helps MI estimation. JSD and DVD all maximize the expected log-ratio of the joint over the product of marginals.</li></ul><h3 id="how-5">How</h3><h4 id="mutual-information-estimation-and-maximization">Mutual information estimation and maximization</h4><ul><li><p>Basic MI maximization framework</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506132933037.png" alt="image-20220506132933037" /><figcaption aria-hidden="true">image-20220506132933037</figcaption></figure></li><li><p>share layers between encoder and mutual information</p></li><li><p>The different losses of DIM with different estimator</p><ul><li>With non-KL divergences such as JSD: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125714185.png" alt="image-20220506125714185" /></li><li>With NCE, <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125757738.png" alt="image-20220506125757738" /></li><li>For DIM, a key difference between the DV, JSD, and infoNCE formulations is whether an expectation over <span class="math inline">\(\mathrm{\mathbb{P/ \tilde{P}}}\)</span> appears inside or outside of a <span class="math inline">\(\log\)</span>. DIM sets the noise distribution to the product of marginals over <span class="math inline">\(X/Y\)</span> , and the data distribution to the true joint.</li></ul></li><li><p>infoNCE often outperforms JSD on downstream tasks, though this effect diminishes with more challenging data and also requires more negative samples compare with the JSD version.</p></li><li><p>DIM with the JSD loss is insensitive to the number of negative samples, and in fact outperforms infoNCE as the number of negative samples becomes smaller.</p></li></ul><h4 id="local-mutual-information-maximization">Local mutual information maximization</h4><ul><li>To obtain a representation more suitable for classification, one can maximize the average MI between the high-level representation and local patches of the image.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220505143215455.png" title="fig:" alt="image-20220505143215455" /><ul><li>summarize this local feature map into a global feature</li><li>then the MI estimator on global/local pairs, maximizing the average estimated MI: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506133343254.png" alt="image-20220506133343254" /></li></ul></li></ul><h4 id="matching-representation-to-a-prior-distribution">Matching representation to a prior distribution</h4><ul><li><p>A good representation can be compact, independent, disentangled or independently controllable.</p></li><li><p>DIM imposes statistical constraints onto learned representations by implicitly training the encoder so that the push-forward distribution, <span class="math inline">\(\mathrm{\mathbb{U}}_{\psi,\mathrm{\mathbb{P}}}\)</span>, matches a prior, <span class="math inline">\(\mathrm{\mathbb{V}}\)</span>.</p></li><li><p>This is done by training a discriminator <span class="math inline">\(D_\phi: \mathcal{Y}\rightarrow \mathrm{\mathbb{R}}\)</span> to estimate the divergence, <span class="math inline">\(\mathcal{D}(\mathrm{\mathbb{V}}|\mathrm{\mathbb{U}}_{\psi,\mathrm{\mathbb{P}}})\)</span>. Then training the encoder to minimize</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506134524556.png" alt="image-20220506134524556" /><figcaption aria-hidden="true">image-20220506134524556</figcaption></figure></li><li><p>trains the encoder to match the noise implicitly rather than using a priori noise samples as targets</p></li></ul><h4 id="complete-loss">Complete loss</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506140132378.png" alt="image-20220506140132378" /><figcaption aria-hidden="true">image-20220506140132378</figcaption></figure><h3 id="experiments-5">Experiments</h3><ul><li>Datasets: CIFAR10+CIFAR100, Tiny ImageNet, STL-10, CelebA (a face image dataset )</li><li>Compared methods: VAE, <span class="math inline">\(\beta\)</span>-VAE, adversarial AE, BiGAN, NAT, and CPC.</li></ul><h4 id="evaluate-the-quality-of-a-representation">Evaluate the quality of a representation</h4><ul><li>Linear separability has no help in showing the representation has high MI with the class labels when the representation is not disentangled.</li><li>To measure:<ul><li>They use MINE to more directly measure the MI between the input the the output of the encoder.</li><li>NDM (neural dependency measure): Then measure the independence of the representation using a discriminator. train a discriminator to estimate the KL-divergence between the original representations (joint distribution of the factors) and the shuffled representations. The higher the KL-divergence, the more dependent the factors. NDM is sensible and empirically consistent.</li></ul></li><li>The classification<ul><li>Linear classification</li><li>Non-linear classification (with a single hidden layer NN)</li><li>Semi-supervised learning: finetuning the encoder by adding a small NN.</li><li>MS-SSIM: decoder trained on the L2 Reconstruction loss.</li><li>MINE: maximized the DV estimator of the KL-divergence</li><li>NDM: using a second discriminator to measure the KL between <span class="math inline">\(E_\psi(x)\)</span> and a batch-wise shuffled version of <span class="math inline">\(E_\psi(x)\)</span>.</li></ul></li></ul><h4 id="representation-learning-comparison-across-models">Representation learning comparison across models</h4><ul><li>Test DIM(G) the global only, DIM (L) the local only and ablation study.</li><li>Classification:<ul><li>DIM(L) outperforms all models. The representations are as good as or better than the raw pixels given the model constraints in this setting</li><li>infoNCE tends to perform best, but differences between infoNCE and JSD diminish with larger datasets</li><li>Overall DIM only slightly outperforms CPC in this setting, which suggests that the <strong>strictly ordered autoregression of CPC may be unnecessary for some tasks.</strong></li></ul></li><li>Extended comparison: For MI, DIM combining local and global DIM objectives had very high scores .</li><li>Adding coordinate information and occlusions<ul><li>can be interpreted as context prediction and generalizations of inpainting respectively.</li><li>For occlusion: occluded part of input for global representations, but no occlusion for local representations. Maximizing MI between occluded global features and unoccluded local features aggressively encourages the global features to encode information which is shared across the entire image.</li></ul></li></ul><h3 id="appendix">Appendix</h3><ul><li>KL (traditional definition of mutual information) and the JSD have an approximately monotonic relationship. Overall, the distributions with the highest mutual information also have the highest JSD.</li><li>We found both infoNCE and the DV-based estimators were sensitive to negative sampling strategies, while the JSD-based estimator was insensitive.</li><li>DIM with a local-only objective, DIM(L), learns a representation with a much more interpretable structure across the image.</li><li>In general, good classification performance is highly dependent on the local term, <span class="math inline">\(\beta\)</span>, while good reconstruction is highly dependent on the global term, <span class="math inline">\(\alpha\)</span>. the local objective is crucial, the global objective plays a stronger role here than with other datasets.</li></ul><h2 id="paper-7-contrastive-multiview-coding-cmc-arxiv-2019">Paper 7: <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.05849&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0s1M_14J8haUyPCJRRILoQ"><em>Contrastive Multiview Coding</em></a> (CMC) ArXiv 2019</h2><p>http://github.com/HobbitLong/CMC/</p><h3 id="previous-6">Previous</h3><ul><li>some bits are in fact better than others.</li><li>In these models, an input <span class="math inline">\(X\)</span> to the model is transformed into an output <span class="math inline">\(\hat{X}\)</span>, which is supposed to be close to another signal <span class="math inline">\(Y\)</span> (usually in Euclidean space), which itself is related to <span class="math inline">\(X\)</span> in some meaningful way. and provides us with nearly infinite amounts of training data.</li><li>The objective functions are usually reconstruction-based loss or contrastive losses.</li><li>CPC learns from the past and the future view simultaneously, while Deep InfoMax takes the past as the input and the future as the output. They both use instance discrimination learns to match two sub-crops of the same image.</li><li>They extend the objective to the case of more than two views and explore a different set of view definitions, architectures and application settings.</li></ul><h3 id="what-5">What</h3><ul><li>Idea: a powerful representation is one that models view-invariant factor. We learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact.</li><li>study the setting where the different views are different image channels, such as luminance, chrominance, depth, and optical flow. The fundamental supervisory signal we exploit is the co-occurrence, in natural data, of multiple views of the same scene.</li><li>Goal: learn information shared between multiple sensory channels but that are otherwise compact (i.e. discard channel-specific nuisance factors). we learn a feature embedding such that views of the same scene map to nearby points (measured with Euclidean distance in representation space) while views of different scenes map to far apart points.</li><li>Use CPC as the backbone but remove the recurrent network part.</li><li>We find that the quality of the representation improves as a function of the number of views used for training.</li><li>demonstrate that the contrastive objective is superior to cross-view prediction.</li></ul><h3 id="how-6">How</h3><ul><li><p>Suppose a dataset of <span class="math inline">\(V_1, V_2\)</span> that consists of a collection of samples <span class="math inline">\(\{v_1 ^i, v_2 ^i\}_{i=1}^N\)</span>, consider $x={v_1 ^i, v_2 ^i} $ as the positive and the <span class="math inline">\(y=\{v_1 ^i, v_2 ^j\}\)</span> as the negatives, aka the positive are the same picture in different views while the negative are the dissimilar images in different views.</p></li><li><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143138612.png" alt="image-20220507143138612" />, simply fix one view and enumerate positives and negatives from the other view, then the objective is written as</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143236840.png" alt="image-20220507143236840" /><figcaption aria-hidden="true">image-20220507143236840</figcaption></figure></li><li><p>But directly minimizing the above function is infeasible since <span class="math inline">\(k\)</span> is pretty large. To approximate,</p><ul><li>Implementing the critic: implement <span class="math inline">\(h_\theta(\cdot)\)</span> as a neural network. For each view, build a NN as the encoder, then compute the features cosine similarity as score and adjust its dynamic range by a hyper-parameter <span class="math inline">\(\tau\)</span>. The two view loss is then <span class="math inline">\(\mathcal{L} (V_1，V_2) = \mathcal{L}_{constrast}^{V_1，V_2}+\mathcal{L}_{constrast}^{V_2，V_1}\)</span>.</li><li>Connecting to mutual information: minimizing the objective L maximizes the lower bound on the mutual information. But recent works show that the bound can be very weak.</li></ul></li><li><p>Multi-views</p><ul><li>the full graph formulation is that it can handle missing information (e.g. missing views) in a natural manner. <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507144936733.png" alt="image-20220507144936733" /></li><li>The core view is <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507145012695.png" alt="image-20220507145012695" /></li></ul></li><li><p>To estimate, they use memory bank to store latent features for each training sample.</p></li></ul><h3 id="experiments-6">Experiments</h3><ul><li>Benchmarks: ImageNet and STL-10.</li></ul><h4 id="benchmarking-cmc-on-imagenet">Benchmarking CMC on ImageNet</h4><ul><li>Convert the RGB images to the Lab image color space and split each image into L and ab channels.</li><li>L and ab from the same image are treated as the positive pair, and ab channels from other randomly selected images are treated as a negative pair (for a given L)</li><li>set the temperature τ as 0.07 and use a momentum 0.5 for memory update.</li><li>learning from luminance and chrominance views in two colorspaces, {L, ab} and {Y, DbDr}.</li><li>{Y, DbDr} provides <span class="math inline">\(0.7\%\)</span> improvement, strengthening data augmentation with RandAugment yields better or comparable results to other SOTA methods.</li></ul><h4 id="cmc-on-videos">CMC on videos</h4><ul><li>given an image it that is a frame centered at time <span class="math inline">\(t\)</span>, the ventral stream associates it with a neighbouring frame <span class="math inline">\(i_{t+k}\)</span>, while the dorsal stream connects it to optical flow <span class="math inline">\(f_t\)</span> centered at <span class="math inline">\(t\)</span>.</li><li>extract <span class="math inline">\(i_t\)</span>, <span class="math inline">\(i_{t+k}\)</span> and <span class="math inline">\(f_t\)</span> from two modalities as three views of a video.<ul><li>Take <span class="math inline">\((i_t,i_{t+k})\)</span> as the positive, and negative pairs for <span class="math inline">\(i_t\)</span> is chosen as a random frame from another randomly chosen video;</li><li>Take <span class="math inline">\((i_t,f_t)\)</span> as the positive, then negative pairs for <span class="math inline">\(i_t\)</span> are those flow corresponding to a random frame in another randomly chosen video.</li></ul></li><li>Pretrain the encoder on UCF101 and use two CaffeNets for extracting features from images and optical flows.</li><li>Increasing the number of views of the data from 2 to 3 (using both streams instead of one) provides a boost for UCF-101</li></ul><h4 id="extending-cmc-to-more-views">Extending CMC to more views</h4><ul><li>Consider views: luminance (L channel), chrominance (ab channel), depth, surface normal, and semantic labels.</li><li>the sub-patch based contrastive objective to increase the number of negative pairs</li><li>Does representation quality improve as number of views increases?<ul><li>UNet style architecture</li><li>The 2-4 view cases contrast L with ab, and then sequentially add depth and surface normals.</li><li>measured by mean IoU over all classes and pixel accuracy.</li><li><strong>performance steadily improves as new views are added</strong></li></ul></li><li>Is CMC improving all views?<ul><li>train these encoders following the full graph paradigm, where each view is contrasted with all other views.</li><li>evaluate the representation of each view v by predicting the semantic labels from only the representation of v, where v is L, ab, depth or surface normals.</li><li>the full-graph representation provides a good representation learnt for all views.</li></ul></li></ul><h4 id="predictive-learning-vs.-contrastive-learning">Predictive Learning vs. Contrastive Learning</h4><ul><li>consider three view pairs on the NYU-Depth dataset: (1) L and depth, (2) L and surface normals, and (3) L and segmentation map. For each of them, we train two identical encoders for L, one using contrastive learning and the other with predictive learning.</li><li>evaluate the representation quality by training a linear classifier on top of these encoders on the STL-10 dataset</li><li>For predictive learning, pixel-wise reconstruction losses usually impose an independence assumption on the modeling. While contrastive learning does not assume conditional independence across dimensions of <span class="math inline">\(v_2\)</span>. Also the use of random jittering and cropping between views allows the contrastive learning approach to benefit from spatial co-occurrence (contrasting in space) in addition to contrasting across views.</li></ul><h4 id="how-does-mutual-information-affect-representation-quality">How does mutual information affect representation quality?</h4><ul><li><p><strong>cross-view representation learning is effective because it results in a kind of information minimization</strong>, discarding nuisance factors that are not shared between the views.</p></li><li><p>a good collection of views is one that shares some information but not too much</p></li><li><p>To test, build two domains: learning representations on images with different colorspaces forming the two views; and learning representations on pairs of patches extracted from an image, separated by varying spatial distance. (use high resolution images to avoid overlapping and cropped patches around boundary.)</p></li><li><p>using colorspaces with minimal mutual information give the best downstream accuracy</p></li><li><p>For patches with different offset with each other, views with too little or too much MI perform worse.</p></li><li><p>the relationship between mutual information and representation quality is meaningful but not direct.</p></li><li><p>patch-based contrastive loss is computed within each mini-batch and does not require a memory bank, but usually yields suboptimal results compared to NCE-based contrastive loss, according to our experiments</p></li><li><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220508144638659.png" alt="image-20220508144638659" /><figcaption aria-hidden="true">image-20220508144638659</figcaption></figure></li><li><p>combining CMC with the MoCo mechanism or JigSaw branch in PIRL can consistently improve the performance, verifying that they are compatible.</p></li></ul><h2 id="paper-8-data-efficient-image-recognition-with-contrastive-predictive-coding-cpc-v2-improved-cpc-evaluated-on-limited-labelled-data">Paper 8: <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.09272&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1GXq6QkuyYGzUQ2EqiWXnL"><em>Data-Efficient Image Recognition with Contrastive Predictive Coding</em></a> (CPC v2: Improved CPC evaluated on limited labelled data)</h2><h3 id="previous-7">Previous</h3><ul><li><p>CPC only requires in its definition that observations be ordered along e.g. temporal or spatial dimensions. It learns representations by training neural networks to predict the representations of future observations from those of past ones.</p><ul><li><p>Loss</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509105849555.png" alt="image-20220509105849555" /><figcaption aria-hidden="true">image-20220509105849555</figcaption></figure></li><li><p>The loss is inspired by NCE, called as InfoNCE.</p></li><li><p>The negative samples <span class="math inline">\(\{z_l\}\)</span> are taken from other locations in the image and other images in the mini-batch.</p></li></ul></li><li><p>AMDIM is most similar to CPC in that it makes predictions across space, but differs in that it also predicts representations across layers in the model.</p></li><li><p>For improving data efficiency , one way is label-propogation.</p><ul><li>a classifier is trained on a subset of labeled data</li><li>then used to label parts of the unlabeled dataset</li></ul></li><li><p>Representation learning and label propagation have been shown to be complementary and can be combined to great effect (Zhai et al., 2019).</p></li><li><p>This work focus on representation learning.</p></li></ul><h3 id="what-6">What</h3><ul><li>hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable</li><li><strong>removing low-level cues which might lead to degenerate solutions.</strong></li></ul><h3 id="how-7">How</h3><ul><li>Pretrain the encoder by CPC on local patches, and during test, apply the encoder on the entire image.</li><li>Evaluation<ul><li>Linear classification: mean pooling followed by a single linear layer as the classifier. Use cross-entropy loss.</li><li>Efficient classification: fix / fine tune the pretrained encoder, and then train the classifier (ResNet-33). Use a smaller learning rate and early-stopping for fine tuning incase the encoder deviates too much from the solution by the CPC objective.</li><li>transfer learning: transfer the pretrained encoder to faster-RCNN to do classification. This is a multi-task.</li><li>supervised training: directly train classifier by the input data, use cross-entropy loss.</li></ul></li><li></li></ul><h3 id="experiments-7">Experiments</h3><ul><li>Compare <span class="math inline">\(1\%\)</span> supervised DNN based on CPC encoder with Semi-supervised methods and then supervised ResNets.</li></ul><h4 id="from-cpc-v1-to-cpc-v2">From CPC v1 to CPC v2</h4><ul><li>Four axes for model capacity<ul><li>increasing depth and width: <span class="math inline">\(+5\%\)</span>. $+2% $ Top-1 accuracy with larger patches.</li><li>improves training efficiency by importing layer normalization: can reclaim much of batch normalization’s training efficiency by using layer normalization (<span class="math inline">\(+2\%\)</span> accuracy)</li><li>making predictions in all four direction: Additional predictions tasks incrementally increased accuracy (adding bottom-up predictions: +2% accuracy; using all four spatial directions: +2.5% accuracy).</li><li>perform patch-based augmentation: ‘color dropping’ (+3% accuracy); adding a fixed, generic augmentation scheme using the primitives from (shearing, rotation etc.), as well as random elastic deformations and color transforms +4.5% accuracy in total.</li></ul></li></ul><h4 id="efficient-image-classification">Efficient image classification</h4><ul><li>fine-tune the entire stack hψ ◦ fθ for the supervised objective, for a small number of epochs (chosen by cross-validation)</li><li>with only 1% of the labels, our classifier surpasses the supervised baseline given 5% of the labels</li><li>the family of ResNet-50, -101, and -200 architectures are designed for supervised learning, and their capacity is calibrated for the amount of training signal present in ImageNet labels; larger architectures only run a greater risk of overfitting.</li><li>fine-tuned representations yield only marginal gains over fixed ones</li><li>we find that CPC provides gains in data efficiency that were previously unseen from representation learning methods, and rival the performance of the more elaborate label-propagation algorithms.</li></ul><h4 id="transfer-learning-image-detection-on-pascal-voc-2007">Transfer learning: image detection on PASCAL VOC 2007</h4><ul><li>unsupervised pre-training surpasses supervised pretraining for transfer learning</li></ul><h3 id="conclusion">Conclusion</h3><ul><li>images are far from the only domain where unsupervised representation learning is important.</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509121402548.png" alt="image-20220509121402548" /><figcaption aria-hidden="true">image-20220509121402548</figcaption></figure><h2 id="paper-9-momentum-contrast-for-unsupervised-visual-representation-learning-moco-see-also-moco-v2">Paper 9: <a href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FHe_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3wwgkDHILZxygHEqFpue3W"><em>Momentum Contrast for Unsupervised Visual Representation Learning</em></a> (MoCo, see also <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.04297&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0tEX1f7G3quzSTWL6rfgzw"><em>MoCo v2</em></a>)</h2><p>Have read MoCo before, here only list MoCo v2.</p><h3 id="previous-8">Previous</h3><ul><li>Momentum Contrast (MoCo) shows that unsupervised pre-training can surpass its ImageNet-supervised counterpart in multiple detection and segmentation tasks.</li><li>the negative keys are maintained in a queue, and only the queries and positive keys are encoded in each training batch.</li><li>MoCo decouples the batch size from the number of negatives.</li><li>SimCLR further reduces the gap in linear classifier performance between unsupervised and supervised pre-training representations.</li><li>In an end-to-end mechanism, the negative keys are from the same batch and updated endto-end by back-propagation. SimCLR is based on this mechanism and requires a large batch to provide a large set of negatives.</li></ul><h3 id="what-7">What</h3><ul><li>verify the effectiveness of two of SimCLR’s design improvements by implementing them in the MoCo framework</li><li>using an MLP projection head and more data augmentation—we establish stronger baselines that outperform SimCLR and do not require large training batches.</li></ul><h3 id="how-8">How</h3><ul><li>Evaluation<ul><li>ImageNet linear classification: features are frozen and a supervised linear classifier is trained</li><li>Transferring to VOC object detection: a Faster R-CNN detector (C4-backbone) is fine-tuned end-to-end on the VOC 07+12 <em>trainval</em> set and evaluated on the VOC 07 <em>test</em> set using the COCO suite of metrics</li></ul></li><li>Amending<ul><li>replace the fc head in MoCo with a 2-layer MLP head (hidden layer 2048-d, with ReLU). This MLP only influences the unsupervised training stage; the linear classification or transferring stage does not use this MLP head.</li><li>including the blur augmentation because stronger color distortion in <em>A simple framework for contrastive learning of visual representations</em> has diminishing gains in our higher baselines.</li></ul></li></ul><h3 id="experiments-8">Experiments</h3><ul><li>pre-training with the MLP head improves from 60.6% to 62.9%. in contrast to the big leap on ImageNet, the detection gains are smaller</li><li><strong>linear classification accuracy is not monotonically related to transfer performance in detection</strong></li><li><strong>large batches are not necessary for good accuracy</strong>, and state-of-the-art results can be made more accessible.</li></ul><h2 id="paper-10-a-simple-framework-for-contrastive-learning-of-visual-representations-simclr.-icml-2020">Paper 10: <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2002.05709.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3nN2zgMpeyBgpXG--pO5Kx"><em>A Simple Framework for Contrastive Learning of Visual Representations</em></a> (SimCLR). ICML 2020</h2><p>https://github.com/google-research/simclr</p><h3 id="previous-9">Previous</h3><ul><li>Many such approaches have relied on heuristics to design pretext tasks, which could limit the generality of the learned representations.</li><li>Types of augmentations<ul><li>spatial/geometric transformation of data: cropping and resizing (with horizontal flipping), rotation and cutout</li><li>appearance transformation, such as color distortion (including color dropping, brightness, contrast, saturation, hue), Gaussian blur, and Sobel filtering.</li></ul></li><li>it is not clear if the success of contrastive approaches is determined by the mutual information, or by the specific form of the contrastive loss</li></ul><h3 id="what-8">What</h3><ul><li><strong>composition of multiple data augmentations</strong> plays a critical role in defining effective predictive tasks</li><li>introducing <strong>a learnable nonlinear transformation between the representation and the contrastive loss</strong> substantially improves the quality of the learned representations</li><li>contrastive learning benefits from <strong>larger batch sizes</strong> and more training steps compared to supervised learning</li><li>Representation learning with contrastive cross entropy loss benefits from <strong>normalized embeddings and an appropriately adjusted temperature parameter</strong>.</li></ul><h3 id="how-9">How</h3><ul><li><p>SimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510105556229.png" alt="image-20220510105556229" /><figcaption aria-hidden="true">image-20220510105556229</figcaption></figure><p>Where the <span class="math inline">\(x\)</span> is the input data from one image, <span class="math inline">\(t\sim\mathcal{T}\)</span> is the augmentation operator, and <span class="math inline">\(f(\cdot)\)</span> is the encoder operator, <span class="math inline">\(g(\cdot)\)</span> is the projection head. In downstream task, the <span class="math inline">\(h\)</span> is used as the extracted features for classification.</p><ul><li>define the contrastive prediction task on pairs of augmented examples derived from the minibatch. Negative samples are not sampled explicitly.</li><li>Use dot product as the similarity measurement. The final loss is termed as NT-Xent (the normalized temperature-scaled cross entropy loss).</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510110242447.png" alt="image-20220510110242447" /><figcaption aria-hidden="true">image-20220510110242447</figcaption></figure><ul><li>vary the training batch size N from 256 to 8192. A batch size of 8192 gives us 16382 negative examples per positive pair from both augmentation views. Since large batch size with SGD will induce to unstable, use LARS optimizer instead.</li><li>as positive pairs are computed in the same device, use global BN by aggregating BN mean and variance over all devices during the training.</li><li>shuffling data examples across devices, replacing BN with layer norm</li></ul></li><li><p>Evaluation protocol</p><ul><li>Linear classifier, and compare with SOTA on Semi-supervised and transfer learning.</li><li>default settings<ul><li>augmentation: random crop and resize (with random flip), color distortions, and Gaussian blur</li><li>use ResNet-50 as the base encoder network</li><li>a 2-layer MLP projection head to project the representation to a 128-dimensional latent space</li><li>train at batch size 4096 for 100 epochs</li><li>linear warmup for the first 10 epochs, and decay the learning rate with the cosine decay schedule without restarts</li></ul></li></ul></li></ul><h3 id="experiments-9">Experiments</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510104751609.png" alt="image-20220510104751609" /><figcaption aria-hidden="true">image-20220510104751609</figcaption></figure><h4 id="data-augmentation">Data augmentation</h4><ul><li>Found no single transformation suffices to learn good representations</li><li>One composition of augmentations stands out: random cropping and random color distortion<ul><li>Though color histograms alone suffice to distinguish images, most patches from an image share a similar color distribution.</li><li><strong>Stronger color augmentation</strong> substantially <strong>improves the linear evaluation</strong> of the learned unsupervised models. But for supervised methods, the stronger color augmentation even hurt the performance.==&gt; unsupervised contrastive learning benefits from stronger (color) data augmentation than supervised learning</li></ul></li></ul><h4 id="architectures-for-encoder-and-head">Architectures for encoder and head</h4><ul><li>Unsupervised contrastive learning benefits (more) from bigger models<ul><li>the gap between supervised models and linear classifiers trained on unsupervised models shrinks as the model size increases,<ul><li>suggesting that unsupervised learning benefits more from bigger models than its supervised counterpart.</li><li>training logger does not improve supervised methods.</li></ul></li></ul></li><li>A nonlinear projection head improves the representation quality of the layer before it</li><li>a nonlinear projection is better than a linear projection (+3%), and much better than no projection (&gt;10%)</li><li>hidden layer before the projection head is a better representation than the layer after<ul><li>They explain this is due to loss of information induced by the contrastive loss</li><li><span class="math inline">\(g\)</span> (projection head) can remove information that may be useful for the downstream task, such as the color or orientation of objects</li></ul></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510114836113.png" title="fig:" alt="image-20220510114836113" /></li></ul><h4 id="loss-functions-and-batch-size">Loss functions and batch size</h4><ul><li>Normalized cross entropy loss with adjustable temperature works better than alternatives<ul><li>NT-Xent loss against such as logistic loss and margin loss.<ul><li><span class="math inline">\(\ell_2\)</span> normalization (i.e. cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives</li><li>unlike cross-entropy, other objective functions do not weigh the negatives by their relative hardness</li></ul></li><li>without normalization and proper temperature scaling, performance is significantly worse</li></ul></li><li>Contrastive learning benefits (more) from larger batch sizes and longer training<ul><li>when the number of training epochs is small (e.g. 100 epochs), larger batch sizes have a significant advantage over the smaller ones</li><li>With more training steps/epochs, the gaps between different batch sizes decrease or disappear, provided the batches are randomly resampled</li><li>Training longer also provides more negative examples, improving the results.</li></ul></li></ul><h4 id="learning-rate-and-projection-matrix">Learning rate and projection matrix</h4><ul><li>square root learning rate scaling improves the performance for models trained with small batch sizes and in smaller number of epoch</li><li>The linear projection matrix for computing <span class="math inline">\(z\)</span> is approximately low rank, indicating by the plot of eigenvalues.</li></ul><h4 id="the-way-to-train-downstream-classifier">The way to train downstream classifier</h4><p>attaching the linear classifier on top of the base encoder (with a stop_gradient on the input to linear classifier to prevent the label information from influencing the encoder) and train them simultaneously during the pretraining achieves <strong>similar performance</strong></p><h4 id="the-best-temperature-parameter">The best temperature parameter</h4><ul><li>the optimal temperature in {0.1, 0.5, 1.0} is 0.5 and seems consistent regardless of the batch sizes.</li></ul><h4 id="compared-with-sota">Compared with SOTA</h4><ul><li>fine-tuning our pretrained ResNet-50 (2×, 4×) on full ImageNet are also significantly better then training from scratch (up to 2%)</li><li>our self-supervised model significantly outperforms the supervised baseline on 5 datasets, whereas the supervised baseline is superior on only 2</li></ul><h2 id="paper-11-big-self-supervised-models-are-strong-semi-supervised-learners-simclrv2-arxiv-2020">Paper 11: <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.10029&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw263B6UxGrQeiE53_baVKp-"><em>Big Self-Supervised Models are Strong Semi-Supervised Learners</em></a> (SimCLRv2) ArXiv 2020</h2><h3 id="previous-10">Previous</h3><ul><li>semi-supervised learning involves unsupervised or self-supervised pretraining. It leverages unlabeled data in a task-agnostic way during pretraining, as the supervised labels are only used during fine-tuning.</li><li>An alternative approach, directly leverages unlabeled data during supervised learning, as a form of regularization. uses unlabeled data in a task-specific way to encourage class label prediction consistency on unlabeled data among different models or under different data augmentations.</li><li></li></ul><h3 id="what-9">What</h3><ul><li>Found the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network</li><li>The proposed semi-supervised learning algorithm in three steps:<ul><li>unsupervised pretraining of a big ResNet model using SimCLRv2,</li><li>supervised fine-tuning on a few labeled examples,</li><li>and distillation with unlabeled examples for refining and transferring the task-specific knowledge.</li></ul></li><li>make use of unlabeled data for a second time to encourage the student network to mimic the teacher network’s label predictions</li></ul><h3 id="how-10">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220511122629288.png" alt="image-20220511122628256" /><figcaption aria-hidden="true">image-20220511122628256</figcaption></figure><ul><li>train the student network by the labels predicted by the teacher network, also can add some real labels, while the student and teacher network are both trained by unlabeled data.</li><li>SimCLR v2<ul><li>With bigger backbone (encoder): The largest model we train is a 152-layer ResNet [25] with 3× wider channels and selective kernels (SK). From ResNet-50 to ResNet-152 (3×+SK), we obtain a 29% relative improvement in top-1 accuracy when fine-tuned on 1% of labeled examples</li><li>Increase the capacity of the projection head by making it deeper. Use a 3-layer projection head and fine tuning from the 1st layer of projection head. It results in as much as 14% relative improvement in top-1 accuracy when fine-tuned on 1% of labeled examples.</li><li>Incorporate the memory mechanism from MoCo. It yields an improvement of ∼1% for linear evaluation as well as when fine-tuning on 1% of labeled examples when the SimCLR is trained in larger batch size.</li></ul></li><li>Fine-tuning<ul><li>Instead of throwing away the projection head directly, they fine-tune the model from a middle layer of the projection head, instead of the input layer of the projection head as in SimCLR.</li></ul></li><li>Self-training<ul><li>leverage the unlabeled data directly for the target task</li><li>minimize the following distillation loss where no real labels are used: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220511130512584.png" alt="image-20220511130512584" style="zoom:50%;" /></li><li>during distillation the teacher network is fixed.</li><li>If do it in semi-supervised: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220511130823288.png" alt="image-20220511130823288" /></li><li>The student model can be the same as the teacher or be smaller.</li></ul></li></ul><h3 id="experiments-10">Experiments</h3><ul><li>Benchmarks: ImageNet ILSVRC-2012. only a randomly sub-sampled 1% (12811) or 10% (128116) of images are associated with labels.</li><li>with a batch size of 4096 and global batch normalization, for total of 800 epochs</li><li>The memory buffer is set to 64K, use the same augmentations as in SimCLR v1, namely random crop, color distortion, and Gaussian blur.</li><li>Two student networks, one is the same as the teacher, the other is smaller than the teacher to test the self-distillation and big-to-small distillation. <em>Only random crop and horizontal flips of training images are applied during fine-tuning and distillation</em>.</li></ul><h4 id="bigger-models-are-more-label-efficient">Bigger models are more label-efficient</h4><ul><li>train ResNet by varying width and depth as well as whether or not to use selective kernels (SK). If use SK, they use the ResNet-D version.</li><li>Use SK will cause larger model size.</li><li>increasing width and depth, as well as using SK, all improve the performance. But the benefits of width will plateau.</li><li>bigger models are more label-efficient for both supervised and semi-supervised learning, but gains appear to be larger for semi-supervised learning.</li></ul><h4 id="biggerdeeper-projection-heads-improve-representation-learning">Bigger/deeper projection heads improve representation learning</h4><ul><li>using a deeper projection head during pretraining is better when fine-tuning from the optimal layer of projection head, and this optimal layer is typically the first layer of projection head rather than the input (0th layer).</li><li>when using bigger ResNets, the improvements from having a deeper projection head are smaller.</li><li>it is possible that increasing the depth of the projection head has limited effect when the projection head is already relatively wide.</li><li>Correlation is higher when fine-tuning from <strong>the optimal middle layer of the projection head</strong> than when fine-tuning from the projection head input, which indicates the accuracy of fine-tuned models is more related with the optimal middle layer of the projection head.</li></ul><h4 id="distillation-using-unlabeled-data-improves-semi-supervised-learning">Distillation using unlabeled data improves semi-supervised learning</h4><ul><li><p>Two loss: distillation loss and an ordinary supervised cross-entropy loss on the labels.</p></li><li><p>Using the distillation loss alone works almost as well as balancing distillation and label losses when the labeled fraction is small (1%, 10%).</p></li><li><p>To get the best performance for smaller ResNets, the big model is self-distilled before distilling it to smaller models</p></li><li><p>when the student model has a smaller architecture than the teacher model, it improves the model efficiency by transferring task-specific knowledge to a student model; even when the student model has the same architecture as the teacher model (excluding the projection head after ResNet encoder), self-distillation can still meaningfully improve the semi-supervised learning performance.</p></li><li><p>task-agnostically learned general representations can be distilled into a more specialized and compact network using unlabeled examples.</p></li><li><p>The benefits are larger when (1) regularization techniques (such as augmentation, label smoothing) are used, or (2) the model is pretrained using unlabeled examples</p></li><li><p>a better fine-tuned model (measured by its top-1 accuracy), regardless their projection head settings, is a better teacher for transferring task specific knowledge to the student using unlabeled data.</p></li></ul><h2 id="paper-12-unsupervised-learning-of-visual-features-by-contrasting-cluster-assignments-swav-arxiv-2020">Paper 12: <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.09882&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0IYamHH9j-b56C3eUiwNXP"><em>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</em></a> (SwAV) ArXiv 2020</h2><p>https://github.com/facebookresearch/swav</p><h3 id="previous-11">Previous</h3><ul><li>contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally expensive.</li><li>In contrastive methods, the loss function and augmentations both contribute to the performance.<ul><li>most implementations approximate the loss by reducing the number of comparisons to random subsets of images during training</li><li>An alternative to approximate the loss is to approximate the task—that is to relax the instance discrimination problem. E.g., clustering-based methods discriminate between groups of images with similar features instead of individual images. But it does not scale well with the dataset as it requires a pass over the entire dataset to form image “codes” (i.e., cluster assignments) that are used as targets during training.</li><li>Amendment: <em>They avoid comparing every pair of images by mapping the image features to a set of trainable prototype vectors.</em></li></ul></li><li>Typical clustering-based methods are offline in the sense that they alternate between a cluster assignment step where image features of the entire dataset are clustered, and a training step where the cluster assignments, i.e., “codes” are predicted for different image views.</li></ul><h3 id="what-10">What</h3><ul><li>propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Their method can be interpreted as a way of contrasting between multiple image views by comparing their cluster assignments instead of their features.</li><li>simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image</li><li><strong>predict the code of a view from the representation of another view</strong>, and called it as swapped assignments between multiple views of the same image (SwAV)</li><li>memory efficient since it does not require a large memory bank or a special momentum network.</li><li>Propose multi-crop as a new augmentation, uses a mix of views with different resolutions in place of two full-resolution views. It consists in simply sampling multiple random crops with two different sizes: a standard size and a smaller one.</li><li>mapping small parts of a scene to more global views significantly boosts the performance.</li><li>Use ResNet as the backbone and ImageNet as the benchmark.</li></ul><h3 id="how-11">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512112020319.png" alt="image-20220512112020319" /><figcaption aria-hidden="true">image-20220512112020319</figcaption></figure><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512140025628.png" alt="image-20220512140025628" /><figcaption aria-hidden="true">image-20220512140025628</figcaption></figure><ul><li><p>Prototype vectors <span class="math inline">\(C\)</span> are learned along with the ConvNet parameters by backpropragation, and they are mapped with the predicted <span class="math inline">\(z\)</span> to search the codes <span class="math inline">\(Q\)</span>.</p><ul><li>For two image features <span class="math inline">\(z_t,z_s\)</span>, predict the codes <span class="math inline">\(q_t,q_s\)</span>, the the loss is <span class="math inline">\(L(z_t,z_s)=\ell(z_t,q_s)+\ell(z_s,q_t)\)</span>. <span class="math inline">\(\ell(z,q)\)</span> measures the fit between features <span class="math inline">\(z\)</span> and a code <span class="math inline">\(q\)</span>. <span class="math inline">\(\ell(z_t,q_s)=-\sum_k q_s^{(k)}\log p_t^{(k)}\)</span>, where <span class="math inline">\(p_t^{(k)}=\frac{\exp{(\frac{1}{\tau}}z_t^Tc_k)}{\sum_{k&#39;}\exp{(\frac{1}{\tau}z_t^Tc_{k&#39;}})}\)</span>.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512122436330.png" title="fig:" alt="image-20220512122436330" /></li></ul></li><li><p>Online codes computing: to get the soft predicted codes <span class="math inline">\(Q^*\)</span></p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512135955654.png" alt="image-20220512135955654" /><figcaption aria-hidden="true">image-20220512135955654</figcaption></figure><ul><li>optimize <span class="math inline">\(Q\)</span> to maximized the similarity between the features and the prototypes, i.e., <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512123010624.png" alt="image-20220512123010624" style="zoom: 33%;" /> <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512123038832.png" alt="image-20220512123038832" style="zoom: 33%;" /></li><li>the entropy <span class="math inline">\(H(Q)\)</span> controls the diversity of the codes, and a strong entropy regularization (i.e. using a high ε) generally leads to a trivial solution where all samples collapse into an unique representation and are all assigned uniformely to all prototypes. They keep <span class="math inline">\(\varepsilon\)</span> small.</li><li>Enforce that on average each prototype is selected at least <span class="math inline">\(\frac{B}{K}\)</span> times in the batch.<ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512123621658.png" title="fig:" alt="image-20220512123621658" /></li></ul></li><li>In online version, the continuous codes work better than discrete codes (rounding the continuous codes)<ul><li>An explanation is that the rounding needed to obtain discrete codes is a more aggressive optimization step than gradient updates. While it makes the model converge rapidly, it leads to a worse solution.</li></ul></li><li>The soft codes <span class="math inline">\(Q^*\)</span> is <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512124151867.png" alt="image-20220512124151867" style="zoom:50%;" />, where <span class="math inline">\(u,v\)</span> are renormalization vectors in <span class="math inline">\(\mathbb{R}^K, \mathbb{R}^B\)</span> respectively (prototypes and feature space).</li><li>In practice, we observe that using only 3 iterations is fast and sufficient to obtain good performance for solving <span class="math inline">\(Q^*\)</span>.</li><li>When working with small batches, we use features from the previous batches to augment the size of <span class="math inline">\(Z\)</span> to equally partition the batch into the <span class="math inline">\(K\)</span> prototypes. Then, we only use the codes of the batch features in our training loss</li></ul></li><li><p>In practice, the size of keep batches</p><ul><li>we store around 3K features, i.e., in the same range as the number of code vectors.</li><li>we only keep features from the last 15 batches with a batch size of 256, while contrastive methods typically need to store the last 65K instances obtained from the last 250 batches</li></ul></li><li><p>Multi-crop</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512140312637.png" alt="image-20220512140312637" /><figcaption aria-hidden="true">image-20220512140312637</figcaption></figure><ul><li>use two standard resolution crops and sample <span class="math inline">\(V\)</span> additional low resolution crops that cover only small parts of the image. The low resolution is for computation efficiency.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512130430962.png" title="fig:" alt="image-20220512130430962" /></li><li>We compute codes <span class="math inline">\(q\)</span> using only the full resolution crops. using only partial information (small crops cover only small area of images) degrades the assignment quality</li><li>Why <span class="math inline">\(i=\{1,2\}\)</span> here? Because for each image, apply <span class="math inline">\(s,t\)</span> transformations? And swap just swap the predicted codes for the same source images (<span class="math inline">\(x_s,x_t\)</span>).</li></ul></li></ul><h3 id="experiments-11">Experiments</h3><ul><li><p>main results</p><ul><li><p>SwAV is not specifically designed for semi-supervised learning</p><p>Outperform other Self-supervised methods if using frozen features.</p></li><li><p>The performance of our model increases with the width of the model, and follows a similar trend to the one obtained with supervised learning</p></li><li><p>one epoch of MoCov2 or SimCLR is faster in wall clock time than one of SwAV, but these methods need more epochs for good downstream performance.</p></li></ul></li><li><p>Clustering-based SSL</p><ul><li>In DeepCluster-v2, instead of learning a classification layer predicting the cluster assignments, we perform explicit comparison between features and centroids.</li><li>SwAV and DeepCluster-v2 outperform SimCLR by 2% without multi-crop and by 3.5% with multi-crop.</li><li>DeepCluster-v2 is not online which makes it impractical for extremely large datasets. DeepCluster-v2 can be interpreted as a special case of our proposed swapping mechanism: swapping is done across epochs rather than within a batch.</li></ul></li><li><p>Multi-crop to different methods: <em>multi-crop seems to benefit more clustering-based methods than contrastive methods</em>. We note that multi-crop does not improve the supervised model</p></li><li><p>Longer training: While SwAV benefits from longer training, it already achieves strong performance after 100 epochs.</p></li><li><p>Unsupervised pretraining on a large uncurated dataset</p><ul><li>evaluate SwAV on random, uncurated images that have different properties from ImageNet which allows us to test if our online clustering scheme and multi-crop augmentation work out of the box: pretrain SwAV on an uncurated dataset of 1 billion random public non-EU images from Instagram. Then the trained model is used to evaluate ImageNet task.</li><li>SwAV maintains a similar gain of 6% over SimCLR as when pretrained on ImageNet==&gt; our improvements do not depend on the data distribution</li><li>Capacity<ul><li>SwAV outperforms training from scratch by a significant margin showing that it can take advantage of the increased model capacity.</li></ul></li></ul></li><li><p>Other results</p><ul><li>start using a queue composed of the feature representations from previous batches <strong>after 15 epochs of training</strong>.</li><li>the prototypes in SwAV are not strongly encouraged to be categorical and <strong>random fixed prototypes work almost as well</strong>==&gt; they help contrasting different image views without relying on pairwise comparison with many negatives samples.</li></ul></li></ul><h2 id="paper-13-what-should-not-be-contrastive-in-contrastive-learning-arxiv-2020">Paper 13: <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.05659&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3gwV78jCI0fs0R9wrj-y7S"><em>What Should Not Be Contrastive in Contrastive Learning</em></a> ArXiv 2020</h2><h3 id="previous-12">Previous</h3><ul><li>The methods of SSL contrastive methods assume a particular set of representational invariances.</li><li>The property of contrastive sampling like in MoCo negatively affects the learnt representations:<ul><li>Generalizability and transferability are harmed if they are applied to the tasks where the discarded information is essential, e.g., color plays an important role in fine-grained classification of birds;</li><li>Adding an extra augmentation is complicated as the new operator may be helpful to certain classes while harmful to others, e.g., a rotated flower could be very similar to the original one, whereas it does not hold for a rotated car;</li><li>The hyper-parameters which control the strength of augmentations need to be carefully tuned for each augmentation to strike a delicate balance between leaving a short-cut open and completely invalidate one source of information.</li></ul></li></ul><h3 id="what-11">What</h3><ul><li>introduce a contrastive learning framework which does not require prior knowledge of specific, task-dependent invariances. The model learns to capture varying and invariant factors for visual representations by constructing separate embedding spaces, each of which is invariant to all but one augmentation.</li><li>We use a multi-head network with a shared backbone which captures information across each augmentation and alone outperforms all baselines on downstream tasks.</li><li>does not require hand-selection of data augmentation strategies, and achieves better performance against state-of-the-art MoCo baseline</li><li>Propose LOOC (leave-one-out contrastive learning).</li></ul><h3 id="how-12">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220513193229770.png" alt="image-20220513193229770" /><figcaption aria-hidden="true">image-20220513193229770</figcaption></figure><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220513193545502.png" title="fig:" alt="image-20220513193545502" /></li><li><span class="math inline">\(\mathcal{Z}_i\)</span> is dependent on the <span class="math inline">\(i^{th}\)</span> type of augmentation but invariant to other types of augmentations.<ul><li>In <span class="math inline">\(\mathcal{Z}_0\)</span>, all features <span class="math inline">\(v\)</span> should be mapped to a single point</li><li>In <span class="math inline">\(\mathcal{Z}_i\)</span>, only <span class="math inline">\(v^q,v^{k_i}\)</span> should be mapped to a single point while <span class="math inline">\(v^{k_j} \forall j\neq i\)</span> should be mapped to <span class="math inline">\(n-1\)</span> separate points.</li></ul></li><li>Use MoCo as the framework, and ResNet50 as backbone. Use augmentations: color jittering (including random gray scale), random rotation (90，180，or 270) and texture randomization.</li><li>Use random-resized croppping, horizontal flipping and Gaussian blur as augmentations without designated embedding space.</li><li>In LooC++, use the concatenation of the output of Conv5 and the last layer of <span class="math inline">\(h\)</span> from each head to build a new input for the projection head.</li><li>After training, we adopt linear classification protocol by training a supervised linear classifier on frozen features of feature space <span class="math inline">\(\mathcal{V}\)</span> for LooC, or concatenated feature spaces <span class="math inline">\(\mathcal{Z}\)</span> for LooC++.</li></ul><h3 id="experiments-12">Experiments</h3><ul><li>Test on In-100 validation set, the iNaturalist (iNat-1k) dataset, VGG flowers, ObjectNet and ImageNet-C.</li><li>Use two-layer MLP head with a 2048-d hidden later and ReLU for each individual embedding space.</li><li>Inductive augmentation: Adding rotation augmentation into baseline MoCo significantly reduces its capacity to classify rotation angles while downgrades its performance on IN-100.<ul><li>In contrast, our method better leverages the information gain of the new augmentation.</li><li>We can include all augmentations with our LooC multi-self-supervised method and obtain improved performance across all condition without any downstream labels or a prior knowledged invariance</li></ul></li><li>Fine-grained recognition results<ul><li>MoCo has marginally superior performance on IN-100</li><li>But the original MoCo trails our LooC counterpart on all other datasets by a noticeable margin</li><li>our method can better preserve color information.</li><li>Rotation augmentation also boosts the performance on iNat-1k and Flowers-102, while yields smaller improvements on CUB-200, which supports the intuition that some categories benefit from rotation-invariant representations while some do not</li><li>The performance is further boosted by using LooC with both augmentations ==&gt; the effectiveness in simultaneously learning the information w.r.t. multiple augmentations</li><li>the benefits of explicit feature fusion without hand-crafting what should or should not be contrastive in the training objective</li></ul></li><li>Robustness learning results<ul><li>The fully supervised network is most sensitive to perturbations</li><li>The rotation augmentation is beneficial for ON-13, but significantly downgrades the robustness to data corruptions in IN-C-100.</li><li><strong>texture randomization increases the robustness on IN-C-100 across all corruption types</strong></li><li>Combining rotation and texture augmentation yields improvements on both datasets</li></ul></li><li>Multiple heads<ul><li>Using multiple heads boosts the performance of baseline MoCo</li><li>our method achieves better or comparable results compared with its baseline counterparts.</li></ul></li><li>Ablation of augmentation<ul><li>Textures are (overly) strong cues for ImageNet classification (Geirhos et al., 2018), thus the linear classifier is prone to use texture-dependent features, loosing the gains of texture invariance</li><li>By checking the histograms of correct predictions (activations x weights of classifier). The classifier on IN-100 heavily relies on texture-dependent information, whereas it is much more balanced on iNat-1k.</li><li>when human or animal faces dominant an image ((a), bottom-left), LooC++ sharply prefers rotation-dependent features, which also holds for face recognition of humans.</li></ul></li></ul><h2 id="paper-14-self-supervised-learning-of-pretext-invariant-representations.-cvpr-2020">Paper 14: <a href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FMisra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3sER0wiTJlhLbi88owlXXE"><em>Self-Supervised Learning of Pretext-Invariant Representations</em></a><em>.</em> CVPR 2020</h2><h3 id="previous-13">Previous</h3><ul><li>Pre-defined semantic annotations scale poorly to the long tail of visual concepts, while SSL tries to address this problem.</li><li>Representations ought to be invariant under image transformations to be useful for image recognition, because the transformations do not alter visual semantics.</li><li></li></ul><h3 id="what-12">What</h3><ul><li>focuses on image-based pretext tasks</li><li>Hint: semantic representations ought to be invariant under such transformations.==&gt; we develop Pretext-Invariant Representation Learning (PIRL, pronounced as “pearl”) that learns invariant representations based on pretext tasks</li><li>PIRL outperforms supervised pre-training in learning image representations for object detection.</li><li>adapt the “Jigsaw” pretext task to work with PIRL</li><li>PIRL even outperforms supervised pre-training in learning image representations suitable for object detection</li><li>PIRL can be viewed as extending the set of data augmentations to include prior pretext tasks and provides a new way to combine pretext tasks with contrastive learning.</li></ul><h3 id="how-13">How</h3><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516113317398.png" alt="image-20220516113310607" style="zoom: 80%;" /> <img src="C:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220516115120389.png" alt="image-20220516115120389" style="zoom: 80%;" /></p><ul><li><p>adopt the existing Jigsaw pretext task in a way that encourages the image representations to be invariant to the image patch perturbation</p><ul><li><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516114526058.png" alt="image-20220516114526058" /><figcaption aria-hidden="true">image-20220516114526058</figcaption></figure></li><li><p>Use NCE to control the similarity between the features of <span class="math inline">\(I\)</span> and the augmented patches of <span class="math inline">\(I_t\)</span>.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516121917957.png" alt="image-20220516121917957" /><figcaption aria-hidden="true">image-20220516121917957</figcaption></figure></li><li><p>apply different “heads” to the features before computing the score</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516121932767.png" alt="image-20220516121932767" /><figcaption aria-hidden="true">image-20220516121932767</figcaption></figure></li></ul></li><li><p>Use memory bank</p><ul><li>The representation <span class="math inline">\(m_I\)</span> in memory bank is an exponential moving average of feature representations <span class="math inline">\(f(v_I)\)</span> that were computed in prior epochs.</li></ul></li><li><p>Final loss</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516122506492.png" title="fig:" alt="image-20220516122506492" /></li><li>The first term is the equation above, and the second term encourages the representation <span class="math inline">\(f(v_I)\)</span> to be similar to its memory representation so to control the parameters updating; and encourages the representations <span class="math inline">\(f(v_I),f(v_{I&#39;})\)</span> to be dissimilar.</li></ul></li><li><p>Backbone: ResNet50</p></li><li><p>Representations</p><ul><li><span class="math inline">\(f(v_I)\)</span> from <span class="math inline">\(I\)</span> is by extracting res5 features, average pooling and a linear projection to obtain a 128-dimensional representation.</li><li></li></ul></li></ul><h3 id="experiments-13">Experiments</h3><ul><li>Compare between PIRL (<span class="math inline">\(\lambda =0.5\)</span>): NPID (<span class="math inline">\(\lambda =0\)</span>) and NPID++ (with more negative samples, (<span class="math inline">\(\lambda =0\)</span>) by setting <span class="math inline">\(\lambda\)</span> in final loss.</li></ul><h4 id="tasks">Tasks</h4><ul><li>Object detection<ul><li>benchmark: Pascal VOC</li><li>encoder: faster RCNN C4 object detection model implement in detectron 2 with a ResNet-50.</li><li>The success of PIRL underscore the importance of learning invariant (rather than covariant) image representations.</li><li>a self-supervised learner can outperform supervised pre-training for object detection.</li><li>PIRL learns image representations that are amenable to sample-efficient supervised learning.</li></ul></li><li>Image classification with linear models<ul><li>PIRL outperforms all prior self-supervised learners on ImageNet in terms of the trade-off between model accuracy and size. Indeed, PIRL even outperforms most self-supervised learners that use much larger models</li><li>PIRL sets a new state-of-the-art for self-supervised representations in this learning setting on the VOC07, Places205, and iNaturalist datasets, while NPID++ performs well but is consistently outperformed by PIRL.</li></ul></li><li>Semi-supervised: finetuning the models on just 1% (∼13,000) labeled images leads to a top-5 accuracy of 57%.</li><li>If pretrained on another unlabeled dataset and then test on ImageNet, PIRL even outperforms Jigsaw and DeeperCluster models that were trained on 100× more data from the same distribution.</li><li>Other discussion<ul><li>Does PIRL learn invariant representations? for PIRL, an image representation and the representation of a transformed version of that image are generally similar, measured by the histogram of distance of normalized features <span class="math inline">\(f(v_I), g(v_{I_t})\)</span>.</li><li>Which layer produces the best representations?<ul><li>the quality of Jigsaw representations improves from the conv1 to the res4 layer but that their quality sharply decreases in the res5 layer==&gt; surmise this happens because the res5 representations in the last layer of the network covary with the image transformation t and are not encouraged to contain semantic information.</li><li>, the best image representations are extracted from the res5 layer of PIRL-trained networks</li></ul></li><li>learning invariance to Jigsaw is important for better representations</li><li>Loss functions<ul><li>At λ= 1, the network does not compare untransformed images at training time and updates to the memory bank <span class="math inline">\(m_I\)</span> are not dampened</li><li>The performance of PIRL is sensitive to the setting of λ, and the best performance is obtained by setting λ= 0.5</li></ul></li><li>Effect of the number of image transforms<ul><li>PIRL outperforms Jigsaw for all cardinalities of T . PIRL particularly benefits from being able to use very large numbers of image transformations (i.e., large |T |) during training</li></ul></li><li>Effect of the number of negative samples: increasing the number of negatives tends to have a positive influence on the quality of the image representations constructed by PIRL.</li></ul></li></ul><h2 id="paper-15-adversarially-learned-inference-ali-iclr-2017">Paper 15 <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1606.00704&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2xx-blDKLroqk6XrXTa7Tp">Adversarially Learned Inference</a> (ALI) ICLR 2017</h2><h3 id="previous-14">Previous</h3><ul><li>three classes of algorithms for learning deep directed generative models<ul><li>VAE</li><li></li></ul></li></ul><h3 id="what-13">What</h3><ul><li>Introduce ALI (adversarially learned inference) jointly learns a generation network and an inference network using an adversarial process</li><li></li></ul><h3 id="how-14">How</h3><h3 id="experiments-14">Experiments</h3>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;p&gt;Paper 1: Vincent et al (2008) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume11%2Fvincent10a%2Fvincent10a.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0SspBuh4yVzZEMYUyio3wn&quot;&gt;&lt;em&gt;Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion&lt;/em&gt;&lt;/a&gt;. JMLR 2008&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analysis of SSL methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 2: Kolesnikov, Zhai and Beyer (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FKolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3Kgfk_3VH635pQwvKeEBE0&quot;&gt;&lt;em&gt;Revisiting Self-Supervised Visual Representation Learning&lt;/em&gt;&lt;/a&gt;. CVPR 2019&lt;/li&gt;
&lt;li&gt;Paper 3: Zhai et al (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.04867&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2ITTaWDSpqKWi3PBBkP5xL&quot;&gt;&lt;em&gt;A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark&lt;/em&gt;&lt;/a&gt; (A GLUE-like benchmark for images) ArXiv 2019&lt;/li&gt;
&lt;li&gt;Paper 4: Asano et al (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1904.13132&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2Fb1pUVDy26_-64K2-T_oR&quot;&gt;&lt;em&gt;A critical analysis of self-supervision, or what we can learn from a single image&lt;/em&gt;&lt;/a&gt; ICLR 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Contrastive methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Paper 5: van den Oord et al. (2018) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1807.03748&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw33PilvMsYAOmPqiPojZcUY&quot;&gt;&lt;em&gt;Representation Learning with Contrastive Predictive Coding&lt;/em&gt;&lt;/a&gt; (CPC), ArXiv 2018&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paper 6: Hjelm et al. (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1808.06670&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw31h2Nm4lF3JBWFDXbG1miu&quot;&gt;&lt;em&gt;Learning deep representations by mutual information estimation and maximization&lt;/em&gt;&lt;/a&gt; (DIM) ICLR 2019&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paper 7: Tian et al. (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.05849&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0s1M_14J8haUyPCJRRILoQ&quot;&gt;&lt;em&gt;Contrastive Multiview Coding&lt;/em&gt;&lt;/a&gt; (CMC) ArXiv 2019&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paper 8: Hénaff et al. (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.09272&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1GXq6QkuyYGzUQ2EqiWXnL&quot;&gt;&lt;em&gt;Data-Efficient Image Recognition with Contrastive Predictive Coding&lt;/em&gt;&lt;/a&gt; (CPC v2: Improved CPC evaluated on limited labelled data) ArXiv 2019&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paper 9: He et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FHe_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3wwgkDHILZxygHEqFpue3W&quot;&gt;&lt;em&gt;Momentum Contrast for Unsupervised Visual Representation Learning&lt;/em&gt;&lt;/a&gt; (MoCo, see also &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.04297&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0tEX1f7G3quzSTWL6rfgzw&quot;&gt;&lt;em&gt;MoCo v2&lt;/em&gt;&lt;/a&gt;). CVPR 2020&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paper 10: Chen T et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2002.05709.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3nN2zgMpeyBgpXG--pO5Kx&quot;&gt;&lt;em&gt;A Simple Framework for Contrastive Learning of Visual Representations&lt;/em&gt;&lt;/a&gt; (SimCLR). ICML 2020&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paper 11: Chen T et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.10029&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw263B6UxGrQeiE53_baVKp-&quot;&gt;&lt;em&gt;Big Self-Supervised Models are Strong Semi-Supervised Learners&lt;/em&gt;&lt;/a&gt; (SimCLRv2) ArXiv 2020&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paper 12: Caron et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.09882&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0IYamHH9j-b56C3eUiwNXP&quot;&gt;&lt;em&gt;Unsupervised Learning of Visual Features by Contrasting Cluster Assignments&lt;/em&gt;&lt;/a&gt; (SwAV) ArXiv 2020&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paper 13: Xiao et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.05659&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3gwV78jCI0fs0R9wrj-y7S&quot;&gt;&lt;em&gt;What Should Not Be Contrastive in Contrastive Learning&lt;/em&gt;&lt;/a&gt; ArXiv 2020&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paper 14: Misra and van der Maaten (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FMisra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3sER0wiTJlhLbi88owlXXE&quot;&gt;&lt;em&gt;Self-Supervised Learning of Pretext-Invariant Representations&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt; CVPR 2020&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Generative methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 15: Dumoulin et al (2017) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1606.00704&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2xx-blDKLroqk6XrXTa7Tp&quot;&gt;Adversarially Learned Inference&lt;/a&gt; (ALI) ICLR 2017&lt;/li&gt;
&lt;li&gt;Paper 16: Donahue, Krähenbühl and Darrell &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1605.09782&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3Ea7cOor2n6pmoJ44576vf&quot;&gt;Adversarial Feature Learning&lt;/a&gt; (BiGAN, concurrent and similar to ALI) ICLR 2017&lt;/li&gt;
&lt;li&gt;Paper 17: Donahue and Simonyan (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1907.02544&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3bxhaVdSwniyMV3EFLcUrl&quot;&gt;Large Scale Adversarial Representation Learning&lt;/a&gt; (Big BiGAN) ArXiv 2019&lt;/li&gt;
&lt;li&gt;Paper 18: Chen et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fimage-gpt%2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2hhRvO-SL2S08MmJ1VjBfw&quot;&gt;Generative Pretraining from Pixels&lt;/a&gt; (iGPT) ICML 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;BYoL: boostrap your own latents&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 19: Tarvainen and Valpola (2017) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2-LuidEMZJI7fUiE-oWqoa&quot;&gt;&lt;em&gt;Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results&lt;/em&gt;&lt;/a&gt;. NeurIPS 2017&lt;/li&gt;
&lt;li&gt;Paper 20: Grill et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.07733&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2igxNmTekOxWbWzL9y5IyY&quot;&gt;&lt;em&gt;Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning&lt;/em&gt;&lt;/a&gt; (BYoL). ArXiv 2020&lt;/li&gt;
&lt;li&gt;Paper 21: Abe Fetterman, Josh Albrecht, (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Funtitled-ai.github.io%2Funderstanding-self-supervised-contrastive-learning.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0qgrINDjCLX30tzD20JOti&quot;&gt;&lt;em&gt;Understanding self-supervised and contrastive learning with &quot;Bootstrap Your Own Latent&quot; (BYOL)&lt;/em&gt;&lt;/a&gt; Blog post&lt;/li&gt;
&lt;li&gt;Paper 22: Schwarzer and Anand et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19&quot;&gt;&lt;em&gt;Data-Efficient Reinforcement Learning with Momentum Predictive Representations&lt;/em&gt;&lt;/a&gt;&lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19&quot;&gt;.&lt;/a&gt; ArXiv 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;self-distillation methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 23: Furlanello et al (2017) &lt;a href=&quot;http://www.google.com/url?q=http%3A%2F%2Fmetalearning.ml%2F2017%2Fpapers%2Fmetalearn17_furlanello.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3xt23sIkAffPt8Ym8vqZDS&quot;&gt;&lt;em&gt;Born Again Neural Networks&lt;/em&gt;&lt;/a&gt;. NeurIPS 2017&lt;/li&gt;
&lt;li&gt;Paper 24: Yang et al. (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.05551&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2HyME0KdJDI4vO-jOuk3Zp&quot;&gt;&lt;em&gt;Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students&lt;/em&gt;&lt;/a&gt;&lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.05551&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2HyME0KdJDI4vO-jOuk3Zp&quot;&gt;.&lt;/a&gt; AAAI 2019&lt;/li&gt;
&lt;li&gt;Paper 25: Ahn et al (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FAhn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3XI1Xtrprs4vWTy6AcLiYH&quot;&gt;&lt;em&gt;Variational information distillation for knowledge transfer&lt;/em&gt;&lt;/a&gt;. CVPR 2019&lt;/li&gt;
&lt;li&gt;Paper 26: Zhang et al (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_ICCV_2019%2Fpapers%2FZhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3lmWaXmFh5PictqZWwKIG5&quot;&gt;&lt;em&gt;Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation&lt;/em&gt;&lt;/a&gt; ICCV 2019&lt;/li&gt;
&lt;li&gt;Paper 27: Müller et al (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F8717-when-does-label-smoothing-help.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2aZPLQTJt3rY082Bdqc3K9&quot;&gt;&lt;em&gt;When Does Label Smoothing Help?&lt;/em&gt;&lt;/a&gt; NeurIPS 2019&lt;/li&gt;
&lt;li&gt;Paper 28: Yuan et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FYuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3zJVfBf6c8hlwGdl32vMNP&quot;&gt;&lt;em&gt;Revisiting Knowledge Distillation via Label Smoothing Regularization&lt;/em&gt;&lt;/a&gt;. CVPR 2020&lt;/li&gt;
&lt;li&gt;Paper 29: Zhang and Sabuncu (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2006.05065v1.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0MHYHwHxUixNKEr1znpFVs&quot;&gt;&lt;em&gt;Self-Distillation as Instance-Specific Label Smoothing&lt;/em&gt;&lt;/a&gt; ArXiv 2020&lt;/li&gt;
&lt;li&gt;Paper 30: Mobahi et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2002.05715&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0SktJ4r4OuAXv0PwzzVEnu&quot;&gt;&lt;em&gt;Self-Distillation Amplifies Regularization in Hilbert Space&lt;/em&gt;&lt;/a&gt;. ArXiv 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;self-training / pseudo-labeling methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 31: Xie et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1911.04252&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1DhfgYIRMPv6XislXO6ry_&quot;&gt;&lt;em&gt;Self-training with Noisy Student improves ImageNet classification&lt;/em&gt;&lt;/a&gt;. CVPR 2020&lt;/li&gt;
&lt;li&gt;Paper 32: Sohn and Berthelot et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2001.07685&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3LFMCJQrLt8NabRsym2vQW&quot;&gt;&lt;em&gt;FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence&lt;/em&gt;&lt;/a&gt;. ArXiv 2020&lt;/li&gt;
&lt;li&gt;Paper 33: Chen et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2006.10032.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2yldjcUPMdKaLjP5mcG1Xp&quot;&gt;Self-training Avoids Using Spurious Features Under Domain Shift&lt;/a&gt;. ArXiv 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Iterated learning/emergence of compositional structure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 34: Ren et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DHkePNpVKPB&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2kXj4HM38k2b1gLv7cLC9G&quot;&gt;Compositional languages emerge in a neural iterated learning model&lt;/a&gt;. ICLR 2020&lt;/li&gt;
&lt;li&gt;Paper 35: Guo, S. et al (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.05291&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw26-y5c29PiVpbAZYCrsj-o&quot;&gt;The emergence of compositional languages for numeric concepts through iterated learning in neural agents&lt;/a&gt;. ArXiv 2020&lt;/li&gt;
&lt;li&gt;Paper 36: Cogswell et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F1904.09067.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0I5RdaelW5fHhwWfXWf9V2&quot;&gt;Emergence of Compositional Language with Deep Generational Transmission&lt;/a&gt; ArXiv 2020&lt;/li&gt;
&lt;li&gt;Paper 37: Kharitonov and Baroni (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2004.03420%23%3A~%3Atext%3DEmergent%20Language%20Generalization%20and%20Acquisition%20Speed%20are%20not%20tied%20to%20Compositionality%2C-Eugene%20Kharitonov%2C%20Marco%26text%3DStudies%20of%20discrete%20languages%20emerging%2Cfor%20evidence%20of%20compositional%20structure.&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw14HOo95bLPmafk6gFvaAOT&quot;&gt;Emergent Language Generalization and Acquisition Speed are not tied to Compositionality&lt;/a&gt; ArXiv 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NLP&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 38: Peters et al (2018) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1802.05365&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw10eV0YxRuEz3vADuc94kFn&quot;&gt;&lt;em&gt;Deep contextualized word representations&lt;/em&gt;&lt;/a&gt; (ELMO), NAACL 2018&lt;/li&gt;
&lt;li&gt;Paper 39: Devlin et al (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN19-1423%2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1oPB9GeT2i05ilyj5OAlBE&quot;&gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.&lt;/em&gt;&lt;/a&gt; (BERT) NAACL 2019&lt;/li&gt;
&lt;li&gt;Paper 40: Brown et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.14165&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0k8WZSIpb3qv17TQpoA0HA&quot;&gt;&lt;em&gt;Language Models are Few-Shot Learners&lt;/em&gt;&lt;/a&gt; (GPT-3, see also GPT-&lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Flanguage-unsupervised%2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0AevPs9tr7a9kx--G4mr7M&quot;&gt;&lt;em&gt;1&lt;/em&gt;&lt;/a&gt;and &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fbetter-language-models%2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1A50m11hjqmW-6njiC3YUp&quot;&gt;&lt;em&gt;2&lt;/em&gt;&lt;/a&gt;for more context) ArXiv 2020&lt;/li&gt;
&lt;li&gt;Paper 41: Clark et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.10555&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1FzuTywUomI9d2oAiBVFxm&quot;&gt;&lt;em&gt;ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators&lt;/em&gt;&lt;/a&gt; ICLR 2020&lt;/li&gt;
&lt;li&gt;Paper 42: He and Gu et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DSJgdnAVKDH&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2BOCV5w8r7GPa0z64Xb1sC&quot;&gt;&lt;em&gt;REVISITING SELF-TRAINING FOR NEURAL SEQUENCE GENERATION&lt;/em&gt;&lt;/a&gt; (Unsupervised NMT) ICLR 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;video/multi-modal data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 43: Wang and Gupta (2015) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fxiaolonw.github.io%2Funsupervise.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3MTp7nv2L5RX59nyRPqMfM&quot;&gt;Unsupervised Learning of Visual Representations using Videos&lt;/a&gt; ICCV 2015&lt;/li&gt;
&lt;li&gt;Paper 44: Misra, Zitnick and Hebert (2016) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1603.08561&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3oZeSlFwM0jkzrL2Mq_Hwf&quot;&gt;&lt;em&gt;Shuffle and Learn: Unsupervised Learning using Temporal Order Verification&lt;/em&gt;&lt;/a&gt; ECCV 2016&lt;/li&gt;
&lt;li&gt;Paper 45: Lu et al (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1908.02265&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw39ViPN0TptsZxtRypcY1Ya&quot;&gt;&lt;em&gt;ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks&lt;/em&gt;&lt;/a&gt;&lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1908.02265&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw39ViPN0TptsZxtRypcY1Ya&quot;&gt;,&lt;/a&gt; NeurIPS 2019&lt;/li&gt;
&lt;li&gt;Paper 46: Hjelm and Bachman (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.13278&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0fIuuJYkqfT84nt0_QyFfZ&quot;&gt;&lt;em&gt;Representation Learning with Video Deep InfoMax&lt;/em&gt;&lt;/a&gt;&lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN19-1423%2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1oPB9GeT2i05ilyj5OAlBE&quot;&gt;&lt;em&gt;.&lt;/em&gt;&lt;/a&gt; (VDIM) Arxiv 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the role of noise in representation learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 47: Bachman, Alsharif and Precup (2014) &lt;a href=&quot;http://www.google.com/url?q=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5487-learning-with-pseudo-ensembles&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2hqOPAo5QSUwexnM7P62j4&quot;&gt;&lt;em&gt;Learning with Pseudo-Ensembles&lt;/em&gt;&lt;/a&gt; NeurIPS 2014&lt;/li&gt;
&lt;li&gt;Paper 48: Bojanowski and Joulin (2017 ) &lt;a href=&quot;http://www.google.com/url?q=http%3A%2F%2Fproceedings.mlr.press%2Fv70%2Fbojanowski17a%2Fbojanowski17a.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3_A9d13BdOXeC_QmofU3sV&quot;&gt;&lt;em&gt;Unsupervised Learning by Predicting Noise.&lt;/em&gt;&lt;/a&gt; ICML 2017&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SSL for RL, control and planning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 49: Pathak et al. (2017) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1705.05363&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1IrpWV_kOjuIQTMmDFS3wd&quot;&gt;&lt;em&gt;Curiosity-driven Exploration by Self-supervised Prediction&lt;/em&gt;&lt;/a&gt; (see also a &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fpathak22.github.io%2Flarge-scale-curiosity%2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0ZeG_gMDAsY1Z_VMac-vsK&quot;&gt;&lt;em&gt;large-scale follow-up&lt;/em&gt;&lt;/a&gt;) ICML 2017&lt;/li&gt;
&lt;li&gt;Paper 50: Aytar et al. (2018) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.11592&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw35lA1NGt9l0sZszcrO-f6F&quot;&gt;&lt;em&gt;Playing hard exploration games by watching YouTube&lt;/em&gt;&lt;/a&gt; (TDC) NeurIPS 2018&lt;/li&gt;
&lt;li&gt;Paper 51: Anand et al. (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.08226&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1te1a2v-cmeICbUGL2_oav&quot;&gt;&lt;em&gt;Unsupervised State Representation Learning in Atari&lt;/em&gt;&lt;/a&gt; (ST-DIM) NeurIPS 2019&lt;/li&gt;
&lt;li&gt;Paper 52: Sekar and Rybkin et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.05960&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1zh2h0rWm82Aq8AZKySxPl&quot;&gt;&lt;em&gt;Planning to Explore via Self-Supervised World Models.&lt;/em&gt;&lt;/a&gt; ICML 2020&lt;/li&gt;
&lt;li&gt;Paper 53: Schwarzer and Anand et al. (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19&quot;&gt;&lt;em&gt;Data-Efficient Reinforcement Learning with Momentum Predictive Representations&lt;/em&gt;&lt;/a&gt;&lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19&quot;&gt;.&lt;/a&gt; ArXiv 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SSL theory&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 54: Arora et al (2019) &lt;a href=&quot;http://www.google.com/url?q=http%3A%2F%2Fproceedings.mlr.press%2Fv97%2Fsaunshi19a%2Fsaunshi19a.pdf&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1Jv6bPfprDaifia4UCyFhE&quot;&gt;&lt;em&gt;A Theoretical Analysis of Contrastive Unsupervised Representation Learning&lt;/em&gt;&lt;/a&gt;. ICML 2019&lt;/li&gt;
&lt;li&gt;Paper 55: Lee et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.01064&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2cgSGuqGOXlczR6HPQRBwq&quot;&gt;&lt;em&gt;Predicting What You Already Know Helps: Provable Self-Supervised Learning&lt;/em&gt;&lt;/a&gt; ArXiv 2020&lt;/li&gt;
&lt;li&gt;Paper 56: Tschannen, et al (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1907.13625&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3Y3TCY3KlkCbEIXQ61liXo&quot;&gt;On mutual information maximization for representation learning&lt;/a&gt;. ArXiv 2019.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Unsupervised domain adaption&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 57: Shu et al (2018) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DH1q-TM-AW&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1DzTvyS29_50se_tZob9IX&quot;&gt;&lt;em&gt;A DIRT-T APPROACH TO UNSUPERVISED DOMAIN ADAPTATION&lt;/em&gt;&lt;/a&gt;. ICLR 2018&lt;/li&gt;
&lt;li&gt;Paper 58: Wilson and Cook (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1812.02849&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw2n226swg-lx1H29furztaL&quot;&gt;&lt;em&gt;A Survey of Unsupervised Deep Domain Adaptatio&lt;/em&gt;&lt;/a&gt;&lt;em&gt;n&lt;/em&gt;. ACM Transactions on Intelligent Systems and Technology 2020&lt;strong&gt;.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Paper 59: Mao et al. (2019) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.04215&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw3tp1FCFSItdeJjpQUUvBJX&quot;&gt;&lt;em&gt;Virtual Mixup Training for Unsupervised Domain Adaptation&lt;/em&gt;&lt;/a&gt;. CVPR 2019&lt;/li&gt;
&lt;li&gt;Paper 60: Vu et al. (2018) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1811.12833&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1ZjGV5QwOyB0ZPMqgiSGXh&quot;&gt;&lt;em&gt;ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation&lt;/em&gt;&lt;/a&gt; CVPR 2019&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Scaling&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper 61: Kaplan et al (2020) &lt;a href=&quot;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2001.08361&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw0pY71xR2C_948Bp-wDv-aA&quot;&gt;&lt;em&gt;Scaling Laws for Neural Language Models&lt;/em&gt;&lt;/a&gt;. ArXiv 2020&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/posts/uncategorized/2021-11-20-tmp-cat.html"/>
    <id>http://yoursite.com/posts/uncategorized/2021-11-20-tmp-cat.html</id>
    <published>2021-11-20T16:29:11.369Z</published>
    <updated>2021-11-20T16:54:17.066Z</updated>
    
    <content type="html"><![CDATA[<h1 id="白娇娇观察日记">白娇娇观察日记</h1><h2 id="section">2021-11.20</h2><p>周三室友下午去上课，关了房间的门导致猫无法进入后，终于摸到了一次猫。然，自此至今早起床，再未摸到猫。</p><p>和室友委婉表达希望猫待在客厅培养感情后，终于在她的帮助下将猫抓了出来。让我惊叹的是：她趴在室友门前不动了，甚至用叫声来证明她不是一个哑巴。坐在客厅的我五味杂陈，刚喂完罐头就转头委屈，我心中的委屈也被一声声勾了起来。和我一起待在客厅很委屈么？</p><p>和朋友谈及此事，忆起三日前她长待在客厅时我和她之间还算相处愉快。难道这就是她终于碰到了自己的命中注定？</p><p>同样的事情若切换到人身上呢？若你的伴侣在遇到你之前和你亲近有佳，但某一天突然迷上了另一人，将另一人的居所视为庇护，此时的你当如何自处？潇洒转身不再留恋？然骨子里你抗拒不了她的撒娇；强行将她带回身边渴望与她亲近？然她的不悦你不可能视而不见。一个以前与你熟识但是失忆的人，所有的感情尽数遗留在己方，对方却跳出留你一人耽之。此种情况，纵然再酸楚，也只能放任发展，毕竟不可控。然当对方并非失忆，只是遇到了她认为更喜欢的，两相比较之下将你的亲近弃于脑后，似乎也不需要再过多投入。脑海中又有一个声音，是否因为你投入的时间于精力过于微薄，才导致她可以轻松放下？又或者是本身这个人天性凉薄，寄希望于暖化只能竹篮打水。可什么时候你才能意识到你们之间的相处处于究竟何种情况？该继续投入还是就此作罢？投入，怕的是某天虽然她对你依赖，但只要有一个拥有她更喜欢的味道的人出现，她就会悄然转身。不投入，倒也从此可以斩断希冀。放弃之后不再留恋？大抵是有的。毕竟之前的求而不得，后来在一次次酸楚后就深埋于心。如今回忆着当时的时时分分，竟已忘记为何难过，为何心伤。人大抵没什么多的优点，但遗忘能力总是值得一提的。遗忘的好，有利于治愈；遗忘的冷漠，虽自己轻松，但舆论上会被贴为凉薄之类。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;白娇娇观察日记&quot;&gt;白娇娇观察日记&lt;/h1&gt;
&lt;h2 id=&quot;section&quot;&gt;2021-11.20&lt;/h2&gt;
&lt;p&gt;周三室友下午去上课，关了房间的门导致猫无法进入后，终于摸到了一次猫。然，自此至今早起床，再未摸到猫。&lt;/p&gt;
&lt;p&gt;和室友委婉表达希望猫待在客
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Papers of CVPR2021, SSL and graph.</title>
    <link href="http://yoursite.com/posts/notes/2021-08-27-notes-paper-cvpr2021-ssl-graph.html"/>
    <id>http://yoursite.com/posts/notes/2021-08-27-notes-paper-cvpr2021-ssl-graph.html</id>
    <published>2021-08-27T15:16:00.000Z</published>
    <updated>2021-11-30T04:27:48.378Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Paper 1: <a href="https://arxiv.org/pdf/2104.00323.pdf">Jigsaw Clustering for Unsupervised Visual Representation Learning</a></li><li>Paper 2: <a href="https://arxiv.org/pdf/2104.00240.pdf">Self-supervised Motion Learning from Static Images</a></li><li>Paper 3: <a href="https://arxiv.org/pdf/2104.00862.pdf">Self-supervised Video Representation Learning by Context and Motion Decoupling</a></li><li>Paper 4: <a href="https://arxiv.org/pdf/2104.11487.pdf">Skip-convolutions for Efficient Video Processing</a></li><li>Paper 5: <a href="https://arxiv.org/pdf/2104.09496.pdf">Temporal Query Networks for Fine-grained Video Understanding</a></li><li>Paper 6: <a href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></li><li>Paper 7:<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Bi-GCN_Binary_Graph_Convolutional_Network_CVPR_2021_paper.pdf">Bi-GCN: Binary Graph Convolutional Network</a></li><li>Paper 8: <a href="https://arxiv.org/pdf/2105.09711.pdf">An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</a></li><li>Paper 9: <a href="https://arxiv.org/abs/2008.03087">Cascade Graph Neural Networks for RGB-D Salient Object Detection</a></li><li>Paper 10: <a href="https://arxiv.org/abs/2103.01302">Coarse-Fine Networks for Temporal Activity Detection in Videos</a></li><li>Paper 11: <a href="https://arxiv.org/abs/2104.03851">CoCoNets: Continuous Contrastive 3D Scene Representations</a></li><li>Paper 12: <a href="https://arxiv.org/abs/2104.04015">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</a></li><li>Paper 13: <a href="https://arxiv.org/abs/2108.03662">Discriminative Latent Semantic Graph for Video Captioning</a></li><li>Paper 14: <a href="https://arxiv.org/abs/2108.02183?context=cs">Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</a></li><li>Paper 15: <a href="https://arxiv.org/abs/2011.10566">Exploring simple siamese representation learning</a></li><li>Paper 16: <a href="https://arxiv.org/abs/2107.05475">GiT: Graph Interactive Transformer for Vehicle Re-identification</a></li><li>Paper 17: <a href="https://arxiv.org/abs/2103.01730">Graph-Time Convolutional Neural Networks</a></li><li>Paper 18: <a href="https://arxiv.org/abs/1910.02370">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</a></li><li>Paper 19: <a href="https://arxiv.org/pdf/2107.09787.pdf">Group Contrastive Self-Supervised Learning on Graphs</a></li><li>Paper 20: <a href="https://link.springer.com/article/10.1007/s10618-021-00750-y">Homophily outlier detection in non-IID categorical data</a></li><li>Paper 21: <a href="https://arxiv.org/abs/2108.02113">Hyperparameter-free and Explainable Whole Graph Embedding</a></li><li>Paper 22: <a href="https://arxiv.org/abs/1908.01000">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</a></li><li>Paper 23: <a href="https://arxiv.org/abs/2010.12609">Iterative graph self-distillation</a></li><li>Paper 24: <a href="https://arxiv.org/abs/2103.17260">Learning by Aligning Videos in Time</a></li><li>Paper 25: <a href="https://arxiv.org/abs/2103.13125">Learning graph representation by aggregating subgraphs via mutual information maximization</a></li><li>Paper 26: <a href="https://arxiv.org/abs/1802.09612">Mile: A multi-level framework for scalable graph embedding</a></li><li>Paper 27: <a href="https://arxiv.org/pdf/2108.03400.pdf">Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</a></li><li>Paper 28: <a href="https://arxiv.org/abs/2107.02639">Multi-Level Graph Contrastive Learning</a></li><li>Paper 29: <a href="https://arxiv.org/abs/2104.09856">Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</a></li><li>Paper 30: <a href="https://arxiv.org/abs/2008.04575">PiNet: Attention Pooling for Graph Classification</a></li><li>Paper 31: <a href="https://arxiv.org/abs/2106.04113">Self-supervised Graph-level Representation Learning with Local and Global Structure</a></li><li>Paper 32: <a href="https://arxiv.org/abs/2105.09111">Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</a></li><li>Paper 33: <a href="https://arxiv.org/abs/2107.01903">SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</a></li><li>Paper 34: <a href="https://arxiv.org/abs/2006.14613">Space-time correspondence as a contrastive random walk</a></li><li>Paper 35: <a href="https://arxiv.org/abs/2103.06122">Spatially consistent representation learning</a></li><li>Paper 36: <a href="https://arxiv.org/abs/2008.03800">Spatiotemporal contrastive video representation learning</a></li><li>Paper 37: <a href="https://arxiv.org/abs/2104.11452">SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</a></li><li>Paper 38: <a href="https://arxiv.org/abs/2105.13033">SSAN: Separable Self-Attention Network for Video Representation Learning</a></li><li>Paper 39: <a href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411953">tdgraphembed: Temporal dynamic graph-level embedding</a></li><li>Paper 40: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf">Videomoco: Contrastive video representation learning with temporally adversarial examples</a></li><li>Paper 41: <a href="https://arxiv.org/abs/2107.01181">Visual Relationship Forecasting in Videos</a></li><li>Paper 42: <a href="https://openreview.net/pdf?id=AAes_3W-2z">Wasserstein embedding for graph learning</a></li><li>Paper 43: <a href="https://arxiv.org/abs/2011.07491">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning</a></li><li>Paper 44: <a href="https://arxiv.org/abs/2101.08482">Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning</a></li><li>Paper 45: <a href="https://arxiv.org/abs/2103.16516">Recognizing Actions in Videos from Unseen Viewpoints</a></li><li>Paper 46: <a href="https://arxiv.org/abs/2104.13537">Shot Contrastive Self-Supervised Learning for Scene Boundary Detection</a></li></ul><a id="more"></a><h2 id="paper-1-jigsaw-clustering-for-unsupervised-visual-representation-learning">Paper 1: <a href="https://arxiv.org/pdf/2104.00323.pdf">Jigsaw Clustering for Unsupervised Visual Representation Learning</a></h2><p>Codes: https://github.com/dvlab-research/JigsawClustering</p><h3 id="previous-pretext-task">Previous pretext task</h3><ul><li>Intra-image tasks: including colorization and jigsaw puzzle, design a transform of one image and train the network to learn the transform.<ul><li>Since only the training batch itself is forwarded each time, they name these methods as single -batch methods.</li><li>Can be achieved using only one image's information, limiting the learning ability of feature extractors.</li></ul></li><li>Inter-image tasks<ul><li>require the network to discriminate among different images.</li><li>Try to reduce the distance between representations of positive pairs and enlarge the distance between representations of negative samples.</li><li>since each training batch and its augmented version are forwarded simultaneously, methods are named as dual-batches methods.</li></ul></li><li>The way to design an efficient single-batch based method with similar performance to dual-batches methods is still an open problem.</li></ul><h3 id="ideas">Ideas</h3><p>They propose a framework for efficient training of unsupervised models using Jigsaw clustering, which combines advantages of solving jigsaw puzzles and contrastive learning, and makes use of both intra- and inter-image information to guide feature extractor.</p><h4 id="jigsaw-clustering-task">Jigsaw Clustering task</h4><ul><li>Every image in a batch is split into different patches. They are randomly permuted and stitched to form a new batch for training.</li><li><strong>Goal</strong> : recover the disrupted parts back to the original images.</li><li>The patches are permuted in a batch</li><li>The network has to distinguish between different parts of one image and identifies their original positions to recover the original image from multiple montage input images.</li><li><strong><em>Why works?</em></strong><ul><li>Discriminating among different patches in one stitched image forces the model to <em>capture instance-level information inside an image</em>. This level of feature is missing in general in other contrastive learning methods.</li><li>Clustering different patches from multiple input images helps the model <em>learn image-level features across images.</em></li><li>arranging every patch to the correct location requires detailed location information, which was considered in single-batch methods.</li></ul></li></ul><h3 id="how">How?</h3><ul><li><p>Batches</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827135623377.png" alt="image-20210827135623377" style="zoom:80%;" /></p><p>Each batch will have <span class="math inline">\(n\)</span> images, and after splitting <strong>(patches split in images have a level of overlap)</strong>, there will be <span class="math inline">\(n\times m\times m\)</span> patches, the patches are latterly stitched into <span class="math inline">\(n\)</span> images. The cluster branch will cluster the <span class="math inline">\(n\times m\times m\)</span> patches into <span class="math inline">\(n\)</span> classes so as to define which original image that one patch comes from.</p><ul><li>Using montage images as input instead of every single patch is noteworthy, since directly using small patches as input leads to the solution with only global information.</li><li>the input images form only one batch with the same size as the original batch, which costs half of resource during training compared with recent methods.</li><li><strong><em>The choice of <span class="math inline">\(m\)</span> affects the difficulty of the task</em></strong>. They show that <span class="math inline">\(m=2\)</span> is good.</li></ul></li><li><p>Network</p><ul><li>The logits is in size <span class="math inline">\(n\times m \times m\)</span>. (in location branch)</li></ul></li><li><p>Loss function</p><ul><li><p>The target of clustering is pulling together objects from the same class and pushing away patches from different classes.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827151850175.png" alt="image-20210827151850175" /><figcaption aria-hidden="true">image-20210827151850175</figcaption></figure></li><li><p>The loss function of location branch is simply cross-entropy loss.</p></li><li><p>The final objective is the weighted summation of the two losses mentioned above. But in their experiments, when the two loss are simply summed, they get the best result.</p></li></ul></li></ul><h2 id="paper-2-self-supervised-motion-learning-from-static-images">Paper 2: <a href="https://arxiv.org/pdf/2104.00240.pdf">Self-supervised Motion Learning from Static Images</a></h2><h3 id="why">Why</h3><ul><li>To well distinguish actions, correctly locating the prominent motion areas is of crucial importance.</li></ul><h3 id="previous-work">Previous work</h3><ul><li>Motion learning by architectures: two-stream networks and 3D convolutional networks. The two-stream networks extract motions representations explicitly from optical flows, while 3D structures apply convolutions on the temporal dimension or space-time cubics to extract motion cues implicitly.</li><li>Self-supervised image representation learning: patch-based approaches , image-level pretext tasks such as image inpainting, image colorization, motion segment prediction and predicting image rotations.</li><li>Self-supervised video representation learning: extend patch-based context prediction to spatial-temporal scenarios, e.g., spatio-temporal puzzles, video cloze procedure and frame/clip order prediction; learn representations by predicting future frames; generate supervision signals, such as speed up prediction and play back rate prediction.</li></ul><h3 id="how-1">How</h3><h4 id="idea">Idea</h4><ul><li>Learn <strong>M</strong>otion from <strong>S</strong>tatic <strong>I</strong>mages (MoSI), take images as our data source, and generate deterministic motion patterns.</li><li>Given the desired direction and the speed of the motions, MoSI generates pseudo motions from static images. By correctly classifying the direction and speed of the movement in the image sequence, models trained with MoSI is able to well encode motion patterns.</li><li>Furthermore, a static mask is applied to the pseudo motion sequences. This produces inconsistent motions between the masked area and the unmasked one, which guides the network to focus on the inconsistent local motions</li></ul><h4 id="motion-learning-from-static-images">Motion learning from static images</h4><h5 id="pseudo-motions">Pseudo motions</h5><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172717585.png" alt="image-20210827172717585" /><figcaption aria-hidden="true">image-20210827172717585</figcaption></figure><ul><li><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172910767.png" alt="image-20210827172910767" /><figcaption aria-hidden="true">image-20210827172910767</figcaption></figure></li><li><p>Label pool:</p><ul><li>For each label, a non-zero speed only exists on one axis.</li></ul></li><li><p>Pseudo motion generation</p><ul><li><p>To generate the samples with different speeds, the moving distance from the start to the end of the Pseudo sequences are need.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827174808696.png" alt="image-20210827174808696" /><figcaption aria-hidden="true">image-20210827174808696</figcaption></figure></li><li><p>The start location is randomly sampled from a certain area which ensures the end location is located completely within the source image.</p></li><li><p>For label <span class="math inline">\((x, y) = (0, 0)\)</span>, where the sampled image sequence is static on both axis, the start location is selected from the whole image with uniform distribution.</p></li></ul></li><li><p>Classification</p><ul><li>each batch contains all transformed image sequences generated from the same source image</li><li>The model is trained by cross entropy loss.</li></ul></li></ul><h5 id="static-masks">Static masks</h5><ul><li>The static masks creates local motion patterns that are inconsistent with the background.</li><li>introduce static masks as the second core component of the proposed MoSI since the model may possibly focus on just several pixels.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181344187.png" title="fig:" alt="image-20210827181344187" /></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181534056.png" title="fig:" alt="image-20210827181534056" /></li><li>the model is now required not only to recognize motion patterns, but also to spot where the motion is happening</li></ul><h5 id="implementation">Implementation</h5><ul><li>Data preparations: the source images need to be first sampled from the videos in the video datasets.</li><li>Augmentation: randomize the location and the size of the unmasked area. In addition, randomize the selection of the background frames in the MoSI.</li></ul><h2 id="paper-3-self-supervised-video-representation-learning-by-context-and-motion-decoupling">Paper 3: <a href="https://arxiv.org/pdf/2104.00862.pdf">Self-supervised Video Representation Learning by Context and Motion Decoupling</a></h2><h3 id="why-1">Why</h3><ul><li>What to learn ?<ul><li>Context representation can be used to classify certain actions, but also leads to background bias.</li><li>Motion representation</li></ul></li><li>Problem<ul><li>The source of supervision: video in compressed format (such as MPEG-4) roughly decouples the context and motion information in its I-frames and motion vectors.<ul><li>I-frames can represent relatively static and coarse-grained context information, while motion vectors depict dynamic and fine-grained movements</li></ul></li></ul></li></ul><h3 id="previous-work-1">Previous work</h3><ul><li>SS video representation learning<ul><li>video specific pretext tasks: estimating video playback rates, verifying temporal order of clips, predicting video rotations, solving space-time cubic puzzles, and dense predictive coding.</li><li>Contrastive learning<ul><li>clips from the same video are pulled together while clips from different videos are pushed away.</li><li>employ adaptive cluster assignment, where the representation and embedding clusters are simultaneously learned.</li><li>But they may suffer from the context bias problem.</li></ul></li><li>mutual supervision across modalities</li><li>DSM: enhance the learned video representation by decoupling the scene and the motion. It simply changes the construction of positive and negative pairs in contrastive learning.</li></ul></li><li>Action recognition in compressed videos<ul><li>Video compression techniques (e.g., H.264 and MPEG4) usually store only a few key frames completely, and reconstruct other frames using motion vectors and residual errors from the key frames.</li><li>Some methods directly build models on the compressed data.<ul><li>One replace the optical flow stream in two-stream action recognition models with a motion vector stream.</li><li>CoViAR: use all modalities, including I-frames, motion vectors and residuals.</li></ul></li></ul></li><li>Motion prediction<ul><li>deduce the states of an object in a near future.</li><li>Typical models: RNNs, Transformers, and GNNs.</li></ul></li></ul><h3 id="goal">Goal</h3><p>Design a self-supervised video representation learning method that jointly learns motion prediction and context matching.</p><ul><li>The context matching task aims to give the video network a rough grasp of the environment in which actions take place. It casts a NCE loss between global features of video clips and I-frames, where clips and I-frames from the same videos are pulled together, while those from different videos are pushed away.</li><li>The motion prediction task requires the model to predict pointwise motion dynamics in a near future based on visual information of the current clip.<ul><li>They use pointwise contrastive learning to compare predicted and real motion features at every spatial and temporal location <span class="math inline">\((x,y,t)\)</span>, which will lead to more stable pretraining and better transferring performance.</li><li>It works as a strong regularization for video networks, and it can also be regarded as an auxiliary task clearly improves the performance of supervised action recognition.</li></ul></li></ul><h3 id="how-2">How</h3><ul><li>Data: compressed videos, to be exactly, MPEG-2 Part2, where every I-frames is followed by 11 consecutive P-frames.</li><li>Methods: context matching task for coarse-grained and relatively static context representation, and a motion prediction task for learning fine-grained and high-level motion representation.<ul><li>context matching<ul><li>where (video clip, I-frame) pairs from the same videos are pulled together, while pairs from different videos are pushed away</li><li></li></ul></li><li>Motion prediction<ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827225903818.png" title="fig:" alt="image-20210827225903818" /></li><li>Only feature points corresponding to the same video <span class="math inline">\(i\)</span> and at the same spatial and temporal position <span class="math inline">\((x, y, t)\)</span> are regarded as positive pairs, otherwise they are regarded as negative pairs.</li><li>The input and output for Transformer is considered as a 1-D sequence.</li><li>Some findings<ul><li>Predicting future motion information leads to significantly better video retrieval performance compared with estimating current motion information;</li><li>Matching predicted and “groundtruth” motion features using the pointwise InfoNCE loss brings better results than directly estimating motion vector values;</li><li>Different encoder-decoder networks lead to similar results, while using Transformer performs slightly better.</li></ul></li></ul></li></ul></li></ul><h2 id="paper-4-skip-convolutions-for-efficient-video-processing">Paper 4: <a href="https://arxiv.org/pdf/2104.11487.pdf">Skip-convolutions for Efficient Video Processing</a></h2><p>Codes: https://github.com/Qualcomm-AI-research/Skip-Conv</p><h3 id="why-2">Why</h3><ul><li>Leverage the large amount of redundancies in video streams and save computations</li><li>The spiking nets is lack of efficient training algorithms</li><li>Residual frames provide a strong prior on the relevant regions, easing the design of effective gating functions</li></ul><h3 id="previous-work-2">Previous work</h3><ul><li>Efficient video models<ul><li>feature propagation, which computes the expensive backbone features only on key-frames.</li><li>interleave deep and shallow backbones between consecutive frames: methods are mostly suitable for global prediction tasks where a single prediction is made for the whole clip.</li></ul></li><li>Efficient image models: The reduction of parameter redundancies<ul><li>model compression: Skip-Conv leverages temporal redundancies in activations.</li><li>conditional computations in developing efficient models for images.</li></ul></li></ul><h3 id="goal-1">Goal</h3><p>To speed up any convolutional network for inference on video streams. Considering a video as a series of changes across frames and network activations, denotes as residual frames. They reformulate standard convolution to be efficiently computed over such residual frames by limiting the computation only to the regions with significant changes while skipping the others. The important residuals are learned by a gating function.</p><h3 id="how-3">How</h3><ul><li>The contributions<ul><li>a simple reformulation of convolution, which computes features on highly sparse residuals instead of dense video frames</li><li>Two gating functions, Norm gate and Gumbel gate, to effectively decide whether to process or skip each location, where Gumbel gate is trainable.</li></ul></li></ul><h4 id="skip-convolutions">Skip Convolutions</h4><ul><li><p>Convolutions on residual frames</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901091727352.png" title="fig:" alt="image-20210901091727352" /></li><li>for every kernel support filled with zero values in <span class="math inline">\(\mathrm{r}_t\)</span>, the corresponding output will be trivially zero, and the convolution can be skipped by copying values from <span class="math inline">\(\mathrm{z}_{t−1}\)</span> to <span class="math inline">\(\mathrm{z}_{t}\)</span>.</li><li><em>Introduce a gating function for each convolutional layer to predict a binary mask indicating which locations should be processed, and taking only <span class="math inline">\(\mathrm{r}_t\)</span> as input.</em> <span class="math inline">\(\mathrm{r}_t\)</span> as input will provide a strong prior to the gating function.</li></ul></li><li><p>Gating functions</p><ul><li><p>Norm gate: decides to skip a residual if its magnitude (norm) is small enough, not learnable</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901093537848.png" title="fig:" alt="image-20210901093537848" /></li><li>indicate regions that change significantly across frames, but not all changes are equally important for the final prediction.</li></ul></li><li><p>Gumbel gate, trainable with the convolutional kernels.</p><ul><li><p>A higher efficiency can be gained by introducing a higher</p></li><li><p>pixel-wise Bernoulli distributions by applying a sigmoid function. During training, sample binary deisions from the Bernoulli distribution.</p></li><li><p>Employ the Gumbel reparametrization and a straight-through gradient estimator in order to backpropagate through the sampling procedure.</p></li><li><p>The Gating parameters are learned jointly with all model parameters by minimizing <span class="math inline">\(\mathcal{L}_{task}+\beta \mathcal{L}_{gate}\)</span>.</p></li><li><p>The gating loss is defined as the average multiply-accumulate (MAC) count needed to process <span class="math inline">\(T\)</span> consecutive frames as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901130744146.png" alt="image-20210901130744146" style="zoom:80%;" /></p></li><li><p>train the model over a fixed-length of frames and do inference iteratively on an binary number of frames.</p></li><li><p>By simply adding a downsampling and an unsampling function on the predicted gates, the Skip-conv can be extended to generate structured sparsity. This structure will enable more efficient implementation with minimal effect on performance.</p></li></ul></li></ul></li><li><p>Generalization and future work</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901141700865.png" title="fig:" alt="image-20210901141700865" /></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901142059270.png" title="fig:" alt="image-20210901142059270" /></li></ul></li></ul><h2 id="paper-5-temporal-query-networks-for-fine-grained-video-understanding">Paper 5: <a href="https://arxiv.org/pdf/2104.09496.pdf">Temporal Query Networks for Fine-grained Video Understanding</a></h2><p>Codes: http://www.robots.ox.ac.uk/~vgg/research/tqn/</p><h3 id="why-3">Why</h3><ul><li>For finer-grain classification which depends on subtle differences in pose, the specific sequence, duration and number of certain subactions, it requires reasoning about events at varying temporal scales and attention to fine details.</li><li>the constraints imposed by finite GPU memory. To overcome this, one way is to use pretrained features, but this relies on good initializations and ensures a small domain gap. Another solution focuses on extracting key frames from untrimmed videos.</li><li>VQA (visual question and answering)<ul><li>Have queries which attends to relevant features for predicting the answers.</li><li>The problem in this paper is more interested in a common set of queries shared across the whole dataset.</li></ul></li></ul><h3 id="goal-2">Goal</h3><ul><li><p>Fine-grained classification of actions in untrimmed videos.</p></li><li><p>Propose a Transformer-based video network, namely the Temporal Query Network (TQN) for fine-grained action classification, which will take a video and a predefined set of queries as input and output responses for each query, where the response is query dependent.</p></li><li><p>The queries act as "experts" that are able to pick out from the video the temporal segments required for their response.</p><ul><li>Pick out relevant temporal segments and ignore irrelevant segments.</li><li>Since only relevant segments will help in classification, the excessive temporal aggregation may lose the signal in the noise.</li></ul></li><li><p>Introduce a stochastically updated feature bank to solve memory constraints.</p><ul><li>features from densely sampled contiguous temporal segments are cached over the course of training,</li><li>only a random subset of these features is computed online and backpropagated through in each training iteration</li></ul></li></ul><h3 id="how-4">How</h3><p>Train with weak supervision, meaning that at training time the temporal location information for the response is not proposed.</p><h4 id="tqn">TQN</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901152032964.png" alt="image-20210901152032964" style="zoom:50%;" /></p><ul><li>Identifies rapidly occurring discriminative events in untrimmed videos and can be trained given only weak supervision.</li><li>Achieves by learning a set of permutation-invariant query vectors corresponding to predefined queries about events and their attributes, which are transformed into response vectors using Transformer decoder layers attending to visual features extracted from a 3DCNN backbone.</li><li>Given an untrimmed video, first visual features for <em>contiguous non-overlapping clips of 8 frames</em> are extracted using a 3D ConvNet.</li><li>multiple layers of a parallel non-autoregressive Transformer decoder</li><li>The training loss is a multi-task combination of individual classifier losses, which are Softmax cross-entropy, where the labels are the ground-truth attribute for the label query.</li></ul><h4 id="stochastically-updated-feature-bank">Stochastically updated feature bank</h4><ul><li>The memory bank caches the clip-level 3DCNN visual features.</li><li>In each training iteration, a fixed number <span class="math inline">\(n_{online}\)</span> of randomly samples consecutive clips are forwarded through the visual encoder, while the remaining <span class="math inline">\(t-n_{online}\)</span> clip features are retrieved from the memory bank.</li><li>The two sets mentioned above are then combined and input into the TQN decoder for final prediction and backpropagation.</li><li>During inference, all features are computed online without the memory bank.</li><li>Advantages: fixed number of clips to reduce the memory price. Also enables to extend temporal context and promotes diversity in each mini-batch as multiple different videos can be included instead of just a single long video.</li></ul><h4 id="factorizing-categories-into-attribute-queries">Factorizing categories into attribute queries</h4><ul><li>This factorization unpacks the monolithic category labels into their semantic constituents</li><li>The categories are factorized into multiple queries that with several attributes respectively.</li></ul><h4 id="implementation-1">Implementation</h4><ul><li>Use S3D as visual backbone, operating on non-overlapping contiguous video clips of 8 frames.</li><li>The decoder consists of 4 standard post-normalization Transformer decoder layers, each with 4 attention heads.</li><li>The visual encoder is pre-trained on Kinetics-400.</li></ul><h2 id="paper-6-unsupervised-disentanglement-of-linear-encoded-facial-semantics">Paper 6: <a href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></h2><h3 id="why-4">Why</h3><ul><li>Sampling along the linear-encoded representation vector in latent space will change the associated facial semantics accordingly.</li><li>Current frameworks that maps a particular facial semantics to a latent representation vector relies on training offline classifiers with manually labeled datasets. Therefore they require artificially defined semantics and provide the associated labels for all facial images. If training with labeled facial semantics:<ul><li>They demand extra effort on human annotations for each new attributes proposed</li><li>Each semantics is defined artificially</li><li>unable to give any insights on the connections among different semantics</li></ul></li><li>Previous work<ul><li>Synthesizing faces by GAN, which changes the target attribute but keep other information ideally unchanged.<ul><li>The comprehensive design of loss functions.</li><li>the involvement of additional attribute features</li><li>the architecture design</li></ul></li><li>To achieve meaningful representations, one should always introduce either supervision or inductive biases to the disentanglement method<ul><li>Inductive bias: rise from the symmetry of natural objects and the 3D graphical information.</li><li>Reconstruct the face images by carefully remodeling the graphics of camera principal, which makes it possible to decompose the facial images into environmental semantics and other facial semantics.</li><li>It's unable to generate realistic faces and perform pixel-level face editing on it.</li></ul></li></ul></li></ul><h3 id="goal-3">Goal</h3><ul><li>Photo-realistic images synthesizing, minimize the demand for human annotations</li><li>Capture linear-encoded facial semantics.</li></ul><h3 id="how-5">How</h3><p>With a given collection of coarsely aligned faces, a GAN is trained to mimic the overall distribution of the data. Then use the faces that the trained GAN generates as training data and trains a 3D deformable face reconstruction method. A mutual reconstruction strategy stabilizes the training significantly. Then they keep a record of the latent code from StyleGAN and apply linear regression to disentangle the target semantics in the latent space.</p><h4 id="decorrelating-latent-code-in-stylegan">Decorrelating latent code in StyleGAN</h4><ul><li><p>Enhance the disentangled representation by decorrelating latent codes.</p><ul><li><p>In order to maximizes the utilization of all dimensions, they use Pearson correlation coefficient to zero and variance of all dimension.</p></li><li><p>Introduce decorrelation regularization via a loss function</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901225711236.png" alt="image-20210901225711236" /><figcaption aria-hidden="true">image-20210901225711236</figcaption></figure></li><li><p>The mapping network is the only one to update with the new loss.</p></li></ul></li></ul><h4 id="stabilized-training-for-3d-face-reconstruction">Stabilized training for 3D face reconstruction</h4><ul><li><p>Use the decomposed semantics to reconstruct the original input image with the reconstruction loss</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901230258180.png" title="fig:" alt="image-20210901230258180" /></li><li>The 3D face reconstruction algorithm struggles to estimate the pose of profile or near-profile faces.</li><li>The algorithm tries to use extreme values to estimate the texture and shape of each face independently, which deviate far away from the actual texture and shape of the face. To solve this, the mutual reconstruction strategy is proposed to prevent the model from using extreme values to fit individual reconstruction, and the model learns to reconstruct faces with a minimum variance of the shape and texture among all samples.</li></ul></li><li><p>During training, they swap the albedo and depth map between two images with a probability <span class="math inline">\(\epsilon\)</span> to perform the reconstruction with the alternative loss.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902113314940.png" alt="image-20210902113314940" /><figcaption aria-hidden="true">image-20210902113314940</figcaption></figure></li><li><p>simply concatenate the two images channel-wise as input to the confidence network</p></li></ul><h4 id="disentangle-semantics-with-linear-regression">Disentangle semantics with linear regression</h4><ul><li>The Ultimate goal of disentangling semantics is to find a vector in StyleGAN, such that it only takes control of the target semantics.</li><li>Semantic gradient estimation<ul><li>It's observed that with StyleGAN, many semantics can be linear-encoded. Therefore, the gradient is now independent of the input latent code.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902120453628.png" title="fig:" alt="image-20210902120453628" /></li></ul></li><li>Semantic linear regression<ul><li>In real world scenario, the gradient is hard to estimate directly because back-propagation only captures local gradient, making it less robust to noises.</li><li>Propose a linear regression model to capture global linearity for gradient estimation.</li></ul></li></ul><h4 id="image-manipulation-for-data-augmentation">Image manipulation for data augmentation</h4><ul><li>One application is to perform data augmentation.</li><li>By extrapolating along <span class="math inline">\(\mathrm{v}\)</span> beyond its standard deviation, we can get samples with more extreme values for the associated semantics.</li></ul><h4 id="localized-representation-learning">Localized representation learning</h4><ul><li>Find the manipulation vectors <span class="math inline">\(\hat{\mathrm{v}}\)</span> that capture interpretable combinations of pixel value variations.</li><li>Start by defining a Jacobian matrix, which is the concatenation of all canonical pixel-level <span class="math inline">\(\mathrm{v}\)</span>.</li><li>interpolation along <span class="math inline">\(\hat{\mathrm{v}}\)</span> should result in significant but localized (i.e. sparse) change across the image domain.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902133618545.png" title="fig:" alt="image-20210902133618545" /></li></ul><h2 id="paper-7-bi-gcn-binary-graph-convolutional-network">Paper 7: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Bi-GCN_Binary_Graph_Convolutional_Network_CVPR_2021_paper.pdf">Bi-GCN: Binary Graph Convolutional Network</a></h2><p>Codes: https://github.com/bywmm/Bi-GCN</p><h3 id="why-5">Why</h3><ul><li>The current success of GNNs is attributed to an implicit assumption that the input of GNNs contains the entire attributed graph, which will collapse or the accuracy will decrease if the entire graph is too large.<ul><li>One intuitive solution for the problem is sampling: neighbor sampling or graph sampling. The graph sampling will sample subgraphs and can avoid neighbor explosion. But not like neighbor sampling, it cannot guarantee that each node is sampled.<ul><li>Neighbor sampling: GraphSAGE, VRGCN</li><li>Sampling subgraphs: Fast-GCN, ClusterGCN, DropEdge, DropConnection, GraphSAINT for edge sampling.</li></ul></li><li>Another solution is compressing the size of input graph data the the GNN model: such as pruning, shallow networks, designing compact layers and quantizing the parameters.</li></ul></li><li>The challenges of compressed GNN<ul><li>The compression of the loaded data demands more attention</li><li>The original GNN is shallow and therefore the compression will be more difficult to be achieved.</li><li>Require the compressed GNNs to possess sufficient parameters for representations.</li></ul></li></ul><h3 id="goal-4">Goal</h3><ul><li>reduce the redundancies in the node representations while maintain the principle information.</li></ul><h3 id="how-6">How</h3><ul><li>Binarizes both the network parameters and input node features. The original matrices multiplications are revised to binary operations for accelerations. Design a new gradient approximation based back-propagation to train the proposed Bi-GCN.</li></ul><h4 id="gcn">GCN</h4><ul><li><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151200499.png" alt="image-20210902151200499" /><figcaption aria-hidden="true">image-20210902151200499</figcaption></figure><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151226323.png" alt="image-20210902151226323" /><figcaption aria-hidden="true">image-20210902151226323</figcaption></figure></li><li><p>Use task-dependent loss function, e.g. the cross-entropy.</p></li></ul><h4 id="bi-gcn">Bi-GCN</h4><p>Only focus on binarizing the feature extraction step, because the aggregation step possesses no learnable parameters and it only requires a few calculations. To reduce the computational complexities and accelerates the inference process, the XNOR and bit count operations are utilized.</p><h5 id="binarization-of-the-feature-extraction-step">Binarization of the feature extraction step</h5><ul><li><p>Binarization of the parameters</p><p>Each column if the parameter matrix is splitted as a bucket, and <span class="math inline">\(\alpha\)</span> maintain the scalars for each bucket.</p></li><li><p>Binarization of the node features</p><p>Processed by the graph convolutional layers.</p><ul><li>Split the hidden state at layer <span class="math inline">\(l\)</span> into row buckets based on the constraints of the matrix multiplication.</li><li>Let <span class="math inline">\(F^{(l)}\)</span> be the binarized buckets, the binary approximation of <span class="math inline">\(H^{(l)}\)</span> can be obtained via <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902160626221.png" alt="image-20210902160626221" style="zoom:67%;" /></li><li><span class="math inline">\(\beta\)</span> can be considered as the node-weights for the features representations. Each element of <span class="math inline">\(F^{(l)}\)</span> and <span class="math inline">\(B^{(l)}\)</span> is either -1 or 1.</li><li>This Binarization also possesses the ability of activation, therefore the activation operations can be eliminated.</li></ul></li></ul><h5 id="binary-gradient-approximation-based-back-propagation">Binary gradient approximation based back propagation</h5><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902164243961.png" alt="image-20210902164243961" /><figcaption aria-hidden="true">image-20210902164243961</figcaption></figure><h2 id="paper-8-an-attractor-guided-neural-networks-for-skeleton-based-human-motion-prediction">Paper 8: <a href="https://arxiv.org/pdf/2105.09711.pdf">An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</a></h2><h3 id="why-6">Why</h3><ul><li>Most existing methods tend to build the relations among joints, where local interactions between joint pairs are well learned. However, the global coordination of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously.</li><li>Most graphs are designed according to the kinematic structure of the human to extract motion features, but hardly do they learn the relations between spatial separated joint pairs directly.</li><li>Except for speed, other dynamic information like accelerated speed are not counted into previous work, which ignores important motion information.</li><li>Previous work<ul><li>Human motion prediction<ul><li>Many works suffer from discontinuities between the observed poses and the predicted future ones.</li><li>Consider global spatial and temporal features simultaneously, such as transform temporal space to trajectory space to take the global temporal information into account.</li></ul></li><li>Joint relation modeling<ul><li>Focus on skeletal constraints to model correlation between joints.</li><li>adaptive graph: the existed works weak the global coordination of all joints since they are learned from parts.</li></ul></li><li>Dynamic representation of skeleton sequence<ul><li>Many attempts proposed to extract enriching dynamic representation from raw data, but they only extract the dynamics from neighbor frames</li><li>Extract the dynamic features among frames through multiple timescale will extract more motion features.</li></ul></li></ul></li></ul><h3 id="goal-5">Goal</h3><p>To characterize the global motion features and thus can learn both the local and global motion features simultaneously.</p><p>Generate predicted poses through proposed framework AGN and the historical 3D skeleton-based poses.</p><h3 id="how-7">How</h3><ul><li>Pipeline: A BA (balance attractor) is learned by calculating dynamic weighted aggregation of single joint feature. Then the difference between the BA and each joint feature is calculated. Later the resulting new joint features are used to calculate joints similarities to generate final joint relations.</li><li>Framework: Attractor-Guided Neural Network, which first learn an enriching dynamic representation from raw position information adaptively through MTDE (multi-timescale dynamics extractor). Then the AJRE (attractor-based joint relation extractor) is imported , including a LIE (local interaction extractor), a GCE (global coordination extractor) and an adaptive feature fusing module.<ul><li>AJRE: a joint relation modeling = GCE+LIE. The GCE models the global coordination of all joints, while LIE mines the local interactions between joint pairs.</li><li>MTDE: extract enriching dynamic information from raw input data for effective prediction.</li></ul></li></ul><h4 id="mtde">MTDE</h4><ul><li><p>A combination of different time scales motion dynamics</p></li><li><p>Two stream, one path is the raw input poses, the other is the difference between adjacent frames in raw input. The dynamics of each joint separately is also modeled to avoid the interference of other joints.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903184316148.png" alt="image-20210903184316148" /><figcaption aria-hidden="true">image-20210903184316148</figcaption></figure></li><li><p>The MTDE uses three 1DCNN but in different kernel size (5，3，1) to extract the local (joint-level) dynamics.</p></li></ul><h4 id="ajre">AJRE</h4><ul><li>Consists of GCE and LIE to separately model global coordination of all joints and local interactions between joint pairs, and also AFFM which is used to fuse features according to channel-wise attention to improve the flexibility of joint relation modeling.</li><li>GCE and LIE work in parallel, and they are followed by AFFM.</li></ul><h5 id="gce">GCE</h5><p>Global coordination of all joints, so they learn a medium to build new joint relations indirectly.</p><ul><li>BA (balance attractor unit) unit calculates all joints' aggregation to characterize the global motion features. After transpose the input features, then BA unit applies <span class="math inline">\(1\times 1\)</span> conv to get a dynamic weighted feature aggregation of N joints features. And <span class="math inline">\(X_{new}\)</span> is the difference between the output features of BA and the original <span class="math inline">\(X\)</span>.</li><li>The new relations of all joints is built by the Cosine similarity unit, which measure between <span class="math inline">\(X_{new},X\)</span>. The cosine similarity between all row vector pairs to illustrate the correlation between joint pairs. The correlation matrix on each channel is calculated since each channel encodes specific spatiotemporal features and should focus on different correlations compared with other channels.</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232718696.png" alt="image-20210903232718696" /><figcaption aria-hidden="true">image-20210903232718696</figcaption></figure><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232735446.png" alt="image-20210903232735446" /><figcaption aria-hidden="true">image-20210903232735446</figcaption></figure><h5 id="lie">LIE</h5><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232837194.png" alt="image-20210903232837194" /><figcaption aria-hidden="true">image-20210903232837194</figcaption></figure><ul><li>It's used to learn local interactions between joint pairs, including adjacent and distant joints.</li><li>To learn the relations between adjacent joint pairs, a pure <span class="math inline">\(3\times 3\)</span> convolution is adopted. To learn the relations between distant joint pairs, the self-attention is used.</li></ul><h5 id="affm">AFFM</h5><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903234025866.png" title="fig:" alt="image-20210903234025866" /></li><li>Channel attention to fuse features adaptively and reform more reliable representation.</li><li>After the sigmoid in the AFFM unit, the importance ratio of each channel is obtained. Then the channel-wise multiplication between ratio and raw input is done to reform features.</li></ul><h2 id="paper-9-cascade-graph-neural-networks-for-rgb-d-salient-object-detection">Paper 9: <a href="https://arxiv.org/abs/2008.03087">Cascade Graph Neural Networks for RGB-D Salient Object Detection</a></h2><p>Codes: https://github.com/LA30/Cas-Gnn</p><h3 id="why-7">Why</h3><ul><li>How to leverage the two complementary data sources: color and depth information</li><li>Current works either simply distill prior knowledge from the corresponding depth map for handling the RGB-image or blindly fuse color and geometric information to generate the coarse depth-aware representations, hindering the performance of RGB-D saliency detectors</li><li>Identify saliency objects of varying shape and appearance, show robustness towards heavy occlusion, various illumination and background.</li><li>Network cascade is an effective scheme for a variety of high-level vision applications. It will ensemble a set of models to handle challenging tasks in a coarse-to-fine or easy-to-hard manner.</li></ul><h3 id="goal-6">Goal</h3><p>Salient object detection for RGB-D images. To distill and reason the mutual benefits between the color and depth data sources through a set of cascade graphs.</p><p>Predict a saliency Map given an input image and its corresponding depth image.</p><h3 id="how-8">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210904162325031.png" alt="image-20210904162325031" /><figcaption aria-hidden="true">image-20210904162325031</figcaption></figure><ul><li>CGR (cascade graph reasoning ) module to learn dense features, from which the saliency map can be easily inferred. It explicitly reasons about the 2D appearance and 3D geometry information for RGBD SOD.</li><li>Each graph consists of two types of nodes, geometry nodes storing depth features and appearance nodes storing RGB-related features.</li><li>Multiple-level graphs sequentially chained by coarsening the preceding graph into two domain-specific guidance nodes for the following cascade graph.</li></ul><h4 id="cross-modality-reasoning-with-gnns">Cross-modality reasoning with GNNs</h4><ul><li>Build a directed graph, where the edges connect i) the nodes from the same modality but different scales and ii) the nodes of the same scale from different modalities.</li><li>The backbone is VGG-16 plus dilated network technique, which will extract 2D appearance representations and 3D geometry representations. They also propose a graph-based reasoning (GR) module to reason about the cross-modality, high-order relations between them.</li><li>GRU module<ul><li>Input 2D features and 3D features.</li><li>gated recurrent unit for node state updating</li></ul></li></ul><h4 id="cascade-graph-neural-networks">Cascade Graph Neural networks</h4><ul><li><p>To overcome the drawbacks of independent multilevel (graph-based) reasoning, propose cascade GNNs.</p></li><li><p>coarsening the preceding graph into two domain-specific guidance nodes for the following cascade graph to perform the joint reasoning</p></li><li><p>The guidance nodes only deliver the guidance information, and will stay fixed during the message passing process.</p></li><li><p>The guidance node is built by firstly concatenation and then the fusion via <span class="math inline">\(3\times 3\)</span> convolution layer.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905202700652.png" alt="image-20210905202700652" /><figcaption aria-hidden="true">image-20210905202700652</figcaption></figure></li><li><p>Each guidance node propagates the guidance information to other nodes of the same domain in the graph through the attention mechanism.</p></li><li><p>Multi-level feature fusion: The merge function is either element-wise addition or channel-wise concatenation.</p></li></ul><h2 id="paper-10-coarse-fine-networks-for-temporal-activity-detection-in-videos">Paper 10: <a href="https://arxiv.org/abs/2103.01302">Coarse-Fine Networks for Temporal Activity Detection in Videos</a></h2><p>Code: https://github.com/kkahatapitiya/Coarse-Fine-Networks</p><h3 id="why-8">Why</h3><ul><li>One main challenge for video representation learning is capturing long-term motion from a continuous video.</li><li>Use of frame striding or temporal pooling has been a successful strategy to cover a larger time interval without increasing the number of parameters</li><li>Previous work<ul><li>Action localization: temporal action localization task, which tends to annotate every frame with multiple ongoing activities. Use of sequential models such as LSTMs have been popular.</li><li>Dynamic sampling: selective processing of information, like spatially, temporally or spatio-temporally sampling.</li></ul></li><li>Two challenges of the network<ul><li>how to abstract the information at a lower temporal resolution meaningfully, and</li><li>how to utilize the fine-grained context information effectively.</li></ul></li></ul><h3 id="goal-7">Goal</h3><p>learn better video representations for long-term motion, works in multiple temporal resolutions of the input and selects frames dynamically.</p><h3 id="how-9">How</h3><ul><li>Grid Pool, a learned temporal downsampling layer to extract coarse features, which adaptively samples the most informative frame locations with a differentiable process.</li><li>Multi-stage Fusion, a spatio-temporal attention mechanism to fuse a fine-grained context with the coarse features.</li></ul><h4 id="grid-pool">Grid pool</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905223900318.png" alt="image-20210905223900318" /><figcaption aria-hidden="true">image-20210905223900318</figcaption></figure><ul><li>Samples by interpolating on a non-uniform grid with learnable grid locations. The intuition comes from that sampling frames at a higher frame rate where the confidence is high and at a lower frame rate where it is low. The stride between the interpolated frame locations should be small and vice-versa.</li><li>The confidence value is modeled as a function of the input representation.</li><li>To get a set of <span class="math inline">\(\alpha T\)</span> (an integer) grid locations based on confidence values, the CDF is considered.</li><li>When a grid location is non-integer, the corresponding sampled frame is a temporal interpolation between the adjacent frames.</li><li>A grid unpool operation is coupled with the grid locations learned by the Grid pool layer, which simply performs the inverse operation of the Grid pool. In this way, one will resamples with a low frame-rate in the regions where one used a high frame-rate in Grid pool, and vice-versa.</li></ul><h4 id="multi-stage-fusion">Multi-stage fusion</h4><ul><li>Fuse the context from the fine stream and the coarse stream. Aims: filter out what fine-grained information should be passed down to the coarse stream, have a calibration step to align the coarse features and fine features, learn and benefit from multiple abstraction-levels of fine-grained context at each fuse-location in the coarse stream</li><li>filtering fine-grained information: self-attention mask by processing the fine feature through a lightweight head consists of point-wise convolutional layers followed by a sigmoid non-linearity.</li><li>Fine to coarse correspondence: use a set of temporal Gaussian distributions centered at each coarse frame location which abstract a location dependent weighted average of the fine feature.</li><li>Multiple abstraction-levels: allow each fusion connection to look at the features from all abstraction levels by concatenating them channel-wise. The scale and shift features at each fusion location is calculated to finally fuse the features from any abstraction-levels.</li></ul><h4 id="model-details">Model details</h4><ul><li>Backbone: X3D, which follows ResNet structure but designed for efficiency in video models.</li><li>The coarse stream takes in segmented clips of <span class="math inline">\(T=64\)</span> frames to follow the standard X3D architecture after the Grid pool later during training, while the fine stream always process the entire input clip.</li><li>The main difference between the coarse and the fine stream is the Grid pool layer and the corresponding grid unpool operation.</li><li>The grid pool later is placed after the 1st residual block.</li><li>The peak magnitude of each mask is normalized to 1.</li><li>The standard deviation <span class="math inline">\(\sigma\)</span> is set to be <span class="math inline">\(\frac{T&#39;}{8}\)</span>, empirically.</li></ul><h2 id="paper-11-coconets-continuous-contrastive-3d-scene-representations">Paper 11: <a href="https://arxiv.org/abs/2104.03851">CoCoNets: Continuous Contrastive 3D Scene Representations</a></h2><p>https://mihirp1998.github.io/project_pages/coconets/</p><h3 id="why-9">Why</h3><ul><li>Combine 3D voxel grids and implicit functions and learn to predict 3D scene and object 3D occupancy from a single view with unlimited spatial resolution</li></ul><h3 id="goal-8">Goal</h3><p>SSL learning of amodal 3D feature representations from RGB and RGBD posed images and videos, and finally generate the representations to help object detection, object tracks or visual correspondence.</p><p>Trained for contrastive view prediction by rendering 3D feature clouds in queried viewpoints and matching against the 3D feature point cloud predicted frim the query view.</p><p>Finally, the model forms plausible 3D completions of the scene given a single RGB-D image as input.</p><h3 id="how-10">How</h3><ul><li>3d feature grids as a 3D-informed neural bottleneck for contrastive view prediction, and implicit functions for handling the resolution limitations of 3D grids.</li><li>Propose CoCoNets (continuous contrastive 3D networks) that learns to map RGB-D images to infinite-resolution 3D scene representations by contrastively predicting views. Specifically, the model is trained to lift 2.5D images to 3D feature function grids of the scene by optimizing for view-contrastive prediction</li><li>There are two branches in CoCoNet, one is to encode RGB-D images into a 3D feature map, and the other is to encode the RGB-D of the target viewpoint into a 3D feature cloud.</li><li>Two branches, one adopts top-down idea which encodes the input RGB-D image and orienting the feature map to the target viewpoint, and predict the features for the target 3D points in target domain by querying, and later output a feature cloud for the target domain. The other branch is the bottom-up one, which simply encodes the target RGB-D image and predict the features for the target 3D position in target domain, obtaining the feature cloud for target domains</li><li>The positive samples are from the target domain that works in bottom-up branch.</li></ul><h4 id="result">Result</h4><ul><li>The scene representations learnt by CoCoNets can detect objects in 3D across large frame gaps</li><li>Using the learnt 3D point features as initialization boosts the performance of the SOTA Deep Hough Voting detector.</li><li>The learnt 3D feature representations can infer 6DoF alignment between the same object in different viewpoints, and across different objects of the same category.</li><li>optimize a contrastive view prediction objective but uses a 3D girder of implicit functions as its latent bottleneck.</li></ul><h2 id="paper-12-cutpaste-self-supervised-learning-for-anomaly-detection-and-localization">Paper 12: <a href="https://arxiv.org/abs/2104.04015">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</a></h2><p>Codes: https://github.com/Runinho/pytorch-cutpaste</p><h3 id="why-10">Why</h3><ul><li>Difficult to obtain a large amount of anomalous data, and the difference between normal and anomalous patterns are often fine-grained.</li><li>The anomaly score defined as an aggregation of pixel-wise reconstruction error or probability densities lacks to capture a high-level semantic information.</li><li>deep one-class classifier outperforms, but most existing work focus on detecting semantic outliers, which cannot generalize well in detecting fine-grained anomalous patterns as in defet detection.</li><li>Naively applying existed methods such as rotation prediction or contrastive learning, is sub-optimal for detecting local defects.</li><li>Rotation and translation ect. lacks of irregularity.</li></ul><h3 id="goal-9">Goal</h3><ul><li>detects unknown anomalous patterns of an image without anomalous data</li><li>Design a pretext task that can identify local irregularity.</li><li>The pretext task is also amenable to combine with existing methods, such as transfer learning from pretrained models for better performance or patch-based models for more accurate localization</li></ul><h3 id="how-11">How</h3><ul><li>Learn representations by classifying normal data from the CutPaste (data augmentation strategy that cuts an image patch at a random location of a large image). First learn SS deep representations and then build a generative one-class classifier.</li><li>transfer learning on pretrained representations on ImageNet.</li><li>designing a novel proxy classification task between normal training data and the ones augmented by the CutPaste. The CutPaste motivated to produce a spatial irregularity to serve as a coarse approximation of real defects.</li><li>A two-stage framework to build an anomaly detector, where in the first stage they learn deep representations from normal data and then construct an one-class classifier using learned representations.</li></ul><h4 id="ssl-with-cutpaste">SSL with CutPaste</h4><ul><li>CutPaste augmentation as follows:<ul><li>Cut a small rectangular area of variable sizes and aspect ratios from a normal training image.</li><li>Optionally, we rotate or jitter pixel values in the patch.</li><li>Paste a patch back to an image at a random location</li></ul></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906193605112.png" title="fig:" alt="image-20210906193605112" /></li><li>In practice , data augmentation or color jitter, are applied before feeding x into g or CP.</li></ul><h4 id="cutpaste-variants">CutPaste variants</h4><ul><li>CutPaste scar: a long-thin rectangular box filled with an image patch</li><li>Multi-class classification: formulate a finer-grained 3-way classification task among normal, CutPaste and CutPaste-Scar by treating CutPaste variants as two separate classes.</li><li>Similarity between CutPaste and real defects: outliers exposure. CutPaste creates examples preserving more local structures of the normal examples, while is more challenging for the model to learn to find this irregularity.</li><li>CutPaste does look similar to some real defects.</li></ul><h4 id="computing-anomaly-score">Computing anomaly score</h4><ul><li><p>A simple Gaussian density estimator whose log-density is computed as follows</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194329698.png" alt="image-20210906194329698" style="zoom:50%;" /></p></li><li></li></ul><h4 id="localization-with-patch-representation">Localization with patch representation</h4><ul><li><p>CutPaste prediction is readily applicable to learn a patch representation – all we need to do at training is to crop a patch before applying CutPaste augmentation.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194519932.png" alt="image-20210906194519932" style="zoom:50%;" /></p></li></ul><h2 id="paper-13-discriminative-latent-semantic-graph-for-video-captioning">Paper 13: <a href="https://arxiv.org/abs/2108.03662">Discriminative Latent Semantic Graph for Video Captioning</a></h2><p>https://github.com/baiyang4/D-LSG-Video-Caption</p><h3 id="why-11">Why</h3><ul><li>Key challenge of the video captioning task: no explicit mapping between video frames and captions, and the output sentence should be natural</li><li>GNNs show particular advantages in modeling relationships between objects, but they don't jointly consider the frame-based spatial-temporal contexts in the entire video sequence</li><li>discriminative modeling for caption generation suffers from stability issues and requires pre-trained generators.</li><li>Traditional GNNs for video captioning cannot take adequate information into consideration, while the work of this paper (conditional graph) jointly consider objects, contexts and motion information at both region and frame levels.</li></ul><h3 id="goal-10">Goal</h3><ul><li>Video captioning</li><li>Encode-decoder frameworks cannot explicitly explore the object-level interactions and frame-level information from complex spatio-temporal data to generate semantic-rich captions.</li><li>Contributions on three key sub-tasks in video captioning<ul><li>Enhanced object proposal: propose a novel conditional graph that can fuse spatio-temporally information into latent object proposal.</li><li>visual knowledge: latent proposal aggregation to dynamically extract visual words</li><li>sentence validation: a novel discriminative language validator</li></ul></li><li>Propose D-LSG, where the graph model for feature fusion from multiple base models, the latent semantic refers to the higher-level semantic knowledge that can be extracted from the enhanced object proposals. The discriminative module is designed as a plug-in language validator, which uses the Multimodal Low-rank Bi-linear (MLB) pooling as metrics.</li></ul><h3 id="how-12">How</h3><ul><li>a semantic relevance discriminative graph based on Wasserstein gradient penalty.</li><li>Modeled as a sequence to sequence process.</li></ul><h4 id="architecture-design">Architecture Design</h4><ul><li>Multiple feature extraction: use 2D CNNs for appearance features and 3D CNNs for motion features. Then these two features are concatenated and apply LSTM on them.</li><li>Enhanced object proposal: enhanced by their visual contexts of appearance and motion respectively, which result in enhanced appearance proposals and enhanced motion proposal, together these two form the enhanced object proposals.</li><li>Visual knowledge: latent semantic proposals as <span class="math inline">\(K\)</span> dynamic visual words, after introducing the dynamic graph built by LPA to summarize the enhanced appearance and motion features.</li><li>Language decoder: language generation decoder will take the visual knowledge extracted by the LPA to generate captions. it consists of an attention LSTM for weighting dynamic visual words and a language LSTM for caption generation.</li></ul><h4 id="latent-semantic-graph">Latent Semantic graph</h4><ul><li><p>conditional graph operation : model the complex object-level interactions and relationships, and learn informative object-level features that are in context of frame-based background information.</p><ul><li>To build the graph, each region feature is regarded as a node. During message passing, the enhanced appearance proposal and object-level region features are handled with a kernel function to encode relations between them. The kernel is defined by linear functions followed by Tanh activation function.</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908111357002.png" alt="image-20210908111357002" /><figcaption aria-hidden="true">image-20210908111357002</figcaption></figure></li><li><p>LPA</p><ul><li>to further summarize the enhanced object proposals</li><li>propose a latent proposal aggregation method to generate visual words dynamically based on the enhanced features.</li><li>Introduce a set of object visual words, which means potential object candidates in the given video, and then they summarize the enhanced proposals into informative dynamic visual words.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908112654724.png" title="fig:" alt="image-20210908112654724" /></li></ul></li></ul><h4 id="discriminative-language-validation">Discriminative language validation</h4><ul><li>The module is designed to as a language validation process that encourages the generated captions to contain more informative Semantic concepts via reconstructing the visual words or knowledge based on the input sentences under the condition of corresponding true visual words encoded by LSG.</li><li>Use WGAN-GP</li><li>Extract sentence features from given captions by 1DCNN,</li><li>The output of the discriminative model is weighted since sentences have different proportions of object and motion concepts<ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120140074.png" title="fig:" alt="image-20210908120140074" /></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120117673.png" title="fig:" alt="image-20210908120117673" /></li></ul></li></ul><h2 id="paper-14-enhancing-self-supervised-video-representation-learning-via-multi-level-feature-optimization">Paper 14: <a href="https://arxiv.org/abs/2108.02183?context=cs">Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</a></h2><p>https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization</p><h3 id="why-12">Why</h3><ul><li>most recent works have mainly focused on high-level semantics and neglected lower-level representations and their temporal relationship</li><li>The requirement of developing unsupervised video representation learning without resorting to manual labeling</li><li>Drawbacks<ul><li>previous works only explore either instance-wise or semantic-wise distribution, lacking a comprehensive perspective over both sides.</li><li>less effort has been placed on low-level features than high-level representations, while the former is proven critical for knowledge transfer</li><li>Third, directly performing temporal augmentations, e.g., shuffle and reverse, at input level instead of feature level could impair feature learning</li></ul></li><li>High-level features are more representative towards instances or semantics but less feasible towards cross-task transfer, while low-level features are transfer-friendly but lack structural information over samples.</li><li>a line of works expanded contrastive learning pipeline to video domain<ul><li>InfoNCE loss for dense future prediction</li><li>the temporal information in videos is not well leveraged</li><li>require a simple yet effective operation to apply temporal augmentations on extracted multi-level features</li></ul></li><li></li></ul><h3 id="goal-11">Goal</h3><ul><li>proposes a multi-level feature optimization framework to improve the generalization and temporal modeling ability of learned video representations</li><li>avoids forcing the backbone model to adapt to unnatural sequences which corrupts spatiotemporal statistics.</li><li>Jointly consider the instance and semantic-wise similarity distribution to form a reliable SS signal.</li></ul><h3 id="how-13">How</h3><ul><li>high-level features obtained from naive and prototypical contrastive learning are utilized to build distribution graphs</li><li>devise a simple temporal modeling module from multi-level view features to enhance motion pattern learning.</li><li>For low-level representation, apply temporal augmentation on multi-level features to construct contrastive pairs that have different motion patterns with the objective designed to distinguish the augmented samples and original ones. And one retrieval task is proposed to match the features in short and long time spans based on their semantic consistency.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910122543720.png" title="fig:" alt="image-20210910122543720" /></li></ul><h4 id="beyond-instance-discrimination">Beyond instance discrimination</h4><ul><li>The one-hot labels in InfoNCE loss neglect the relationship between different samples. But there exist some negative samples that may share similar characteristics.</li><li>besides instance-wise discrimination, we explicitly develop another branch on the projected high-level feature vectors for inter-sample relationship modeling.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130142797.png" title="fig:" alt="image-20210910130142797" /></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130231035.png" title="fig:" alt="image-20210910130231035" /></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130246505.png" title="fig:" alt="image-20210910130246505" /></li><li>design a queue to store the semantic-wise distributions from previous batches to ensure equal partition into K prototypes, but using only those from the current batch for gradient back-propagation</li><li>Finally, we jointly leverage <span class="math inline">\(\mathcal{L}_{ins}\)</span> and <span class="math inline">\(\mathcal{L}_{sem}\)</span> to form the self-supervisory objective for high-level representations:</li></ul><h4 id="graph-constraint-for-multi-level-features">Graph constraint for multi-level features</h4><ul><li>It is the lower-level features that mainly transfer from the pretrained network to downstream tasks. One can infer instance- and semantic-wise distribution from high-level features.</li><li>Denote the instance-wise similarity distribution as a <strong><em>directed</em></strong> graph <span class="math inline">\(\mathcal{G}_{ins}\)</span>, and semantic-wise distribution as an <strong><em>undirected</em></strong> graph <span class="math inline">\(\mathcal{G}_{sem}\)</span>. Each graph contains N nodes representing N different samples within a batch, and edges indicating the relationship between each sample.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131617208.png" title="fig:" alt="image-20210910131617208" /></li><li><span class="math inline">\(\mathcal{E}_{ins}\)</span> indicates the inferred instance-wise similarity distribution, which respects inter-sample relationship and is more realistic data distribution than one-hot encoding. <span class="math inline">\(\mathcal{E}_{sem}\)</span> to truncate the edges between nodes of different pseudo categories.</li><li>Jointly leverage <span class="math inline">\(\mathcal{G}_{ins}\)</span> and <span class="math inline">\(\mathcal{G}_{sem}\)</span> to form the combined graph <span class="math inline">\(\mathcal{G}\)</span>, whose edge weights serve as the final soft targets: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131811218.png" alt="image-20210910131811218" style="zoom:53%;" /></li><li>The cross-entropy between <span class="math inline">\(\mathcal{E}\)</span> and inferred similarity distribution to optimize lower-level features.</li></ul><h4 id="temporal-modeling">Temporal modeling</h4><ul><li><p>Use the temporal information at diverse time scales to enhance motion pattern modeling since the features at different layers possess different temporal characteristic.</p></li><li><p>A robust temporal model requires two aspects: semantic discrimination between different motion patters; semantic consistency under different temporal views.==&gt; Two learning objectives</p></li><li><p>perform temporal augmentation on multi-level features <span class="math inline">\(\mathrm{f}_r\)</span>, and then leverage a lightweight motion excitation module to extract motion enhanced feature representations</p></li><li><p>Temporal transformations that result in semantically inconsistent motion patterns can be regarded as a negative pair of the original sample</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910132656530.png" alt="image-20210910132656530" /><figcaption aria-hidden="true">image-20210910132656530</figcaption></figure></li><li><p>To boost the consistency, they propose to match feature of a specific timestamp from sequences of different lengths. For one short sequence <span class="math inline">\(v_s\)</span> that is contained in a long sequence <span class="math inline">\(v_l\)</span>, they retrieve the feature at each timestep of <span class="math inline">\(v_s\)</span> in the feature set of <span class="math inline">\(v_l\)</span>. The feature of corresponding timestamp in <span class="math inline">\(v_l\)</span> serves as the positive key, while others serve as negatives.</p></li></ul><h2 id="paper-15-exploring-simple-siamese-representation-learning">Paper 15: <a href="https://arxiv.org/abs/2011.10566">Exploring simple siamese representation learning</a></h2><p>codes: https://github.com/facebookresearch/simsiam</p><h3 id="why-13">Why</h3><ul><li>Siamese networks are natural tools for comparing (including but not limited to “contrasting”) entities</li><li>Recent methods define the inputs as two augmentations of one image, and maximize the similarity subject to different conditions</li><li>An undesired trivial solution to Siamese networks is all outputs “collapsing” to a constant.<ul><li>Methods like Contrastive learning, e.g., SimCLR etc. work to fix this.</li><li>Clustering is another way of avoiding constant output. While these methods do not define negative exemplars, this cluster centers can play as negative prototypes.</li><li>BYOL relies only on positive pairs but it does not collapse in case a momentum encoder is used. The momentum encoder is important for BYOL to avoid collapsing, and it reports failure results if removing the momentum encoder</li></ul></li><li>the weight-sharing Siamese networks can model invariance w.r.t. more complicated transformations</li></ul><h3 id="goal-12">Goal</h3><ul><li>report that simple Siamese networks can work surprisingly well with none of the above strategies (contrastive learning, clustering or BYOL) for preventing collapsing</li><li>our method ( SimSiam) can be thought of as “<em>BYOL without the momentum encoder”</em>. Directly shares the weights between the two branches, so it can also be thought of as “SimCLR without negative pairs”, and “SwAV without online clustering”.</li><li>SimSiam is related to each method by removing one of its core components.</li><li>The importance of stop-gradient suggests that <em>there should be a different underlying optimization problem that is being solved</em>.</li></ul><h3 id="how-14">How</h3><ul><li><p>The proposed architecture takes as input two randomly augmented views <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> from an image <span class="math inline">\(x\)</span>. The two views are processed by an encoder network <span class="math inline">\(f\)</span> consisting of a backbone and a project MLP head. A predict MLP head is denoted as <span class="math inline">\(h\)</span>.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160437825.png" alt="image-20210910160437825" /><figcaption aria-hidden="true">image-20210910160437825</figcaption></figure></li><li><p>The symmetrized loss is denoted as <span class="math inline">\(\mathcal{L}=\frac{1}{2}\mathcal{D}(p_1,p_2)+\frac{1}{2}\mathcal{D}(p_2,z_1)\)</span>. Its minimum possible value is −1.</p></li><li><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160719249.png" alt="image-20210910160719249" /><figcaption aria-hidden="true">image-20210910160719249</figcaption></figure></li><li><p>the encoder on <span class="math inline">\(x_2\)</span> receives no gradient from <span class="math inline">\(z_2\)</span>, but gradients from <span class="math inline">\(p_2\)</span>.</p></li><li><p>Use SGD as optimizer, with a base <span class="math inline">\(lr=0.05\)</span>, the learning rate is <span class="math inline">\(lr\times \mathrm{BatchSize}/256\)</span>.</p></li><li><p>Use ResNet50 as the default backbone.</p></li><li><p>Unsupervised pretraining on the 1000-class ImageNet training set without using labels.</p></li></ul><h2 id="paper-16-git-graph-interactive-transformer-for-vehicle-re-identification">Paper 16: <a href="https://arxiv.org/abs/2107.05475">GiT: Graph Interactive Transformer for Vehicle Re-identification</a></h2><h3 id="why-14">Why</h3><ul><li>Vehicle re-identification aiming to retrieve a target vehicle from non-overlapping cameras. But there are challenges<ul><li>vehicle images of different identifications usually have similar global appearances and subtle differences in local regions</li></ul></li><li>The technologies of vehicle re-identification<ul><li>Early methods: pure CNNs, fail to catch local information</li><li>Based on CNNs, cooperate part divisions (uniform spatial division suffer from partition misalignment, part detection requires a high cost of extra manual part annotations) to learn global features and local features.</li><li>CNNs cooperate GNNs to learn global and local features: the CNN's downsampling and convolution operations reduce the resolution of feature maps, the CNN and GNN branches are supervised with two independent loss functions and lack interaction.</li><li>This paper: couple global and local features via transformer and local correction graph modules.</li></ul></li><li>The advantages of transformer<ul><li>The transformer can use multi-head attention module to capture global context information to establish long-distance dependence on global features of vehicles.</li><li>The multi-head attention module of transformer does not require convolution and down-sampling operations, which retain more detailed vehicle information.</li></ul></li></ul><h3 id="goal-13">Goal</h3><p>Propose a graph interactive transformer (GiT) for vehicle-reidentification. Each GiT block employs a novel local correlation graph (LCG) module to extract discriminative local features within patches.</p><p>LCG Modules and transformer layers are in a coupled status.</p><h3 id="how-15">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913111201445.png" alt="image-20210913111201445" /><figcaption aria-hidden="true">image-20210913111201445</figcaption></figure><ul><li>The transformer later and LCG module interact each other by skip connection when these two components work in sequence.</li></ul><h4 id="lcg-module">LCG module</h4><ul><li><p>To aggregate and learn discriminative local features within every patch.</p></li><li><p>flatten <span class="math inline">\(n\)</span> local features <span class="math inline">\(d\)</span> dimensions and map to <span class="math inline">\(d&#39;\)</span> dimensions with a trainable linear projection in every patch</p></li><li><p>The spatial graph's edges are constructed as <span class="math inline">\(E_{v_{i,j}}=\frac{\exp (F_{cos}(v_i,v_j))}{\sum\limits_{k=1}^{n}\exp {(F_{cos}(v_i,v_k))}}\)</span>, where <span class="math inline">\(i,j\in[1，2,...,n]\)</span>. The score of the cosine distance is denoted as <span class="math inline">\(F_{cos}=\frac{v_i,v_j}{\|v_i\|\|v_j\|}\)</span>.</p></li><li><p>To aggregate and update nodes, the aggregation node <span class="math inline">\(U\)</span> of <span class="math inline">\(i\)</span>-th graph is updated according as follows</p><ul><li><p><span class="math inline">\(U=(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}X_i)\cdot W\)</span>,</p></li><li><p>Then <span class="math inline">\(U\)</span> is processed non-linearly as</p><p><span class="math inline">\(O=GELU(LN(U))\)</span>, where GELU represents the gaussian error linear units and LN denotes the layer normalization.</p></li></ul></li></ul><h4 id="transformer-layer">Transformer layer</h4><ul><li>Model the global features between the different patches.</li><li>Patches are the input for multi-head attention layer</li><li>Later, the output from the attention layer is normalized and then processed by MLP.</li></ul><h4 id="graph-interactive-transformer">Graph interactive Transformer</h4><ul><li>Each GiT block consists of a LCG module and a Transformer layer.</li></ul><h4 id="loss-function-design">Loss function design</h4><ul><li><p>The proposed GiT's total loss function is</p><p><span class="math inline">\(L_{total}=\alpha L_{CE}+\beta L_{Triplet}\)</span>, where <span class="math inline">\(L_{CE}\)</span> denote cross-entropy loss, and <span class="math inline">\(L_T\)</span> denotes triplet loss.</p></li><li><p>The <span class="math inline">\(L_{CE}\)</span> formulates the cross-entropy of each patch's label</p></li><li><p>The triplet loss is</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913112252883.png" alt="image-20210913112252883" /><figcaption aria-hidden="true">image-20210913112252883</figcaption></figure></li></ul><h2 id="paper-17-graph-time-convolutional-neural-networks">Paper 17: <a href="https://arxiv.org/abs/2103.01730">Graph-Time Convolutional Neural Networks</a></h2><p>Codes: https://github.com/gtcnnpaper/DSLW-Code</p><h3 id="why-15">Why</h3><ul><li>The key for learning on multivariate temporal data is to embed spatiotemporal relations into into its inner-working mechanism.</li><li>Spatiotemporal graph-base models<ul><li>Hybrid: combine learning algorithms developed separately for the graph domain and the temporal domain<ul><li>Such as a temporal RNN, CNN</li><li>their spatial and temporal blocks are modular and can be implemented efficiently</li><li>unclear how to best interleave these blocks for learning from spatiotemporal relationships</li></ul></li><li>Fused<ul><li>force the graph structure into conventional spatiotemporal solutions and provide a single strategy to jointly capture the spatiotemporal relationships.</li><li>substitute the parameter matrices in these models with graph convolutional filters</li><li>fused models capture naturally these relationships as they have graph-time dependent inner-working mechanisms.</li></ul></li></ul></li></ul><h3 id="goal-14">Goal</h3><ul><li>Represent spatiotemporal relations through product graphs and develop a first principle graph-time convolutional neural network (GTCNN).</li><li>For multivariate temporal data such as sensor or social networks</li></ul><h3 id="how-16">How</h3><ul><li>Each layer consists of a graph-time convolutional module, a graphtime pooling module, and a nonlinearity.</li><li>The product graph itself is parametric to learn the spatiotemporal coupling</li><li>The zero-pad pooling preserves the spatial graph while reducing the number of active noes and parameters</li></ul><h4 id="signals-over-product-graphs">Signals over product graphs</h4><ul><li><p>Two graphs:</p><ul><li>Spatial graph <span class="math inline">\(\mathcal{G}\)</span>, consider the original sensor network, each node has a time sequence</li><li>Temporal graph <span class="math inline">\(\mathcal{G}_T\)</span>: for each node, there is a line graph that take each timestep as a node.</li></ul></li><li><p>Given graphs <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}_T\)</span> , we can capture the spatiotemporal relations in <span class="math inline">\(\mathrm{X}\)</span> through the product graph <span class="math inline">\(\mathcal{G}_\diamond = \mathcal{G}_T\times \mathcal{G}=(\mathcal{V}_\diamond,\mathcal{E}_\diamond)\)</span> , where the vertex set <span class="math inline">\(\mathcal{V}_\diamond = \mathcal{V}_T \times \mathcal{V}\)</span> is the Kronecker product between <span class="math inline">\(\mathcal{V}_T\)</span> and <span class="math inline">\(\mathcal{V}\)</span>.</p></li><li><p>Product graphs</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913131247274.png" alt="image-20210913131247274" /><figcaption aria-hidden="true">image-20210913131247274</figcaption></figure><ul><li>The Kronecher product preserves the relations between different nodes along temporal dimension. The 1st node at the 1st timestep have influences for the 2nd node at the 2nd timestep. And the influence is bidirectional.</li><li>The cartesian product preserves the original spatial infromation and the temporal evolution on each node's temporal dimension.</li></ul></li><li><p>Goal: to learn spatiotemporal representations in a form akin to temporal or graph CNNs.</p></li></ul><h4 id="graph-time-cnns">Graph-time CNNs</h4><ul><li>A compositional architecture of <span class="math inline">\(L\)</span> layers each having a graph-time convolutional module, a graph-time pooling module and a nonlinearity.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913133851127.png" title="fig:" alt="image-20210913133851127" /></li><li>at convolutions allow for effective parameter sharing, inductive learning, and efficient implementation, while zero-pad pooling and pointwise nonlinearities make the architecture independent from graph-reduction techniques or other modules.</li><li>Graph-time convolutional filtering<ul><li>The graph-time convolutional filter aggregates at the space-time location <span class="math inline">\((i, t)\)</span> information from space-time neighbors that are up to <span class="math inline">\(K\)</span> hops away over the product graph <span class="math inline">\(\mathcal{G}_\diamond\)</span>.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913135843608.png" title="fig:" alt="image-20210913135843608" /></li><li>Implement it recursively, and expand all polynomials of order <span class="math inline">\(k\)</span>. Then the computational cost is liner in the product graph dimensions.</li></ul></li><li>Graph-time pooling: The pooling approach has three steps: i) summarization; ii) slicing; iii) downsampling<ul><li>Summarization: up to <span class="math inline">\(\alpha_l\)</span> hops away for each node . Use mean or max function.<ul><li>Summarization is an implicit low-pass operation and the type of product graph has an impact on its severit</li></ul></li><li>Slicing: reduces the dimensionality across the temporal dimension.</li><li>Downsampling: reduces the number of active nodes across the spatial dimension from <span class="math inline">\(N_{l-1}\)</span> to <span class="math inline">\(N_l\)</span> without modifying the underlying spatial graph.</li></ul></li></ul><h2 id="paper-18-graphzoom-a-multi-level-spectral-approach-for-accurate-and-scalable-graph-embedding">Paper 18: <a href="https://arxiv.org/abs/1910.02370">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</a></h2><p>Codes: https://github.com/cornell-zhang/GraphZoom</p><h3 id="why-16">Why</h3><ul><li>Challenges<ul><li>existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy</li><li>few of them <em>scale to large graphs</em> due to their high computational complexity and memory usage</li></ul></li><li>Graph embedding techniques<ul><li>Random-walk-based embedding algorithms<ul><li>embed a graph based on its topology without incorporating node attribute information==&gt; limits the embedding power</li><li>GCN with the basic notion that node embeddings should be smoothed over the entire graph and so can leverage both topology and node attribute information.==&gt; But may suffer from high-frenquency noise in the inital node features.</li></ul></li></ul></li><li>Only one of the solution for increasing the accuracy or improving the scalability of graph embedding methods is well-handeled. Not all of them</li><li>Previous work<ul><li>Multi-level graph embedding: GraphZoom is motivated by theoretical results in spectral graph embedding.</li><li>Graph filtering: GCN model implicitly exploits graph filter to remove high-frequency noise from the node feature matrix. In GraphZoom we adopt graph filter to properly smooth the intermediate embedding results during the iterative refinement step.</li></ul></li></ul><h3 id="goal-15">Goal</h3><ul><li>Propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms.</li></ul><h3 id="how-17">How</h3><ul><li>GraphZoom consists of four major kernels: (1) graph fusion, (2) spectral graph coarsening, (3) graph embedding, and (4) embedding refinement.<ul><li>The graph fusion kernel first converts the node feature matrix into a feature graph and then fuses it with the original topology graph.</li><li>Spectral graph coarsening produces a series of successively coarsened graphs by merging nodes based on their spectral similarities.</li><li>During the graph embedding step, any of the existing unsupervised graph embedding techniques can be applied to obtain node embeddings for the graph at the coarsest level.</li><li>Embedding refinement is then employed to refine the embeddings back to the original graph by applying a proper graph filter to ensure embeddings are smoothed over the graph.</li></ul></li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914123913163.png" alt="image-20210914123913163" /><figcaption aria-hidden="true">image-20210914123913163</figcaption></figure><h4 id="graph-fusion">Graph Fusion</h4><ul><li>To construct a weighted graph that has the same number of nodes as the original graph but potentially different set of edges (weights) that encapsulate the original graph topology as well as node attribute information</li><li>Firstly generate a KNN graph based on the <span class="math inline">\(\ell^2-norm\)</span> distance between the attribute vectors of each node pair so to convert initial attribute matrix <span class="math inline">\(X\)</span> into a weighted node attribute graph.</li><li>To implement in a linear cost, they start with coarsening the original graph <span class="math inline">\(\mathcal{G}\)</span> to obtain a substantially reduced graph that has much fewer nodes with an <span class="math inline">\(\mathcal{O}(|\mathcal{E}|)\)</span> , similar as to spectral graph clustering to group nodes into clusters of high conductance. After the attribute graph is formed, we assign a weight to each edge based on the cosine similarity between the attribute vectors of the two incident nodes. Finally, we can construct the fused graph by combining the topology graph and the attribute graph using a weighted sum: <span class="math inline">\(\mathrm{A}_{fusion}=\mathrm{A}_{topo}+\beta \mathrm{A}_{feat}\)</span>.</li></ul><h4 id="spectral-coarsening">Spectral coarsening</h4><ul><li>For graph coarsening via global spectral embedding: calculating eigenvectors of the original gragh Laplacian is very costly. To eliminate the cost, they propose a method for graph coarsening via local spectral embedding.<ul><li>The analogies between the traditional signal processing (Fourier analysis) and graph signal processing.<ul><li>The signals at different time points in classical Fourier analysis correspond to the signals at different nodes in an undirected graph;</li><li>The more slowly oscillating functions in time domain correspond to the graph Laplacian eigenvectors associated with lower eigenvalues or the more slowly varying (smoother) components across the graph.</li></ul></li><li>apply the <em>simple smoothing (low-pass graph filtering) function to <span class="math inline">\(k\)</span> random vectors to obtain smoothed vectors for <span class="math inline">\(k\)</span>-dimensional graph embedding</em>, which can be achieved in linear time.</li></ul></li><li>We adopt low-pass graph filters to quickly filter out the high-frequency components of the random graph signal or the eigenvectors corresponding to high eigenvalues of the graph Laplacian, and then get the smoothed vectors in <span class="math inline">\(T\)</span> (initial random vectors).</li><li>The aggregation scheme: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914141059850.png" alt="image-20210914141059850" /></li><li>Once the aggregation scheme is defined, the coarsening in multilevel is formed as a set of graph mapping matrices.</li></ul><h4 id="graph-embedding">Graph embedding</h4><p>Just obtain embedding according to the previously defined formulas.</p><h4 id="embedding-refinement">Embedding refinement</h4><ul><li>To get the embedding vectors for the original graph eventually. It's kind of like remapping the graph from the coarsest level to the finest level.</li><li>The refinement process is motivated by Tikhonov regularization to smooth the node embedding over the graph by minimizing <span class="math inline">\(\min\limits_{E_i}\{\|E_i-\hat{E}_i\|^2_2+tr(E_i^\top L_iE_i)\}\)</span>, where <span class="math inline">\(L_i\)</span> and <span class="math inline">\(E_i\)</span> are the normalized Laplacian matrix and mapped embedding matrix of the graph at the <span class="math inline">\(i\)</span>-th coarsening level, respectively. The solving the equation, the refined embedding matrix of edges are solved.</li><li>To solve the equation efficiently , they work in spectral domain, and approximate the graph filter by it first-order Taylor expansion.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914142505161.png" title="fig:" alt="image-20210914142505161" /></li></ul><h2 id="paper-19-group-contrastive-self-supervised-learning-on-graphs">Paper 19: <a href="https://arxiv.org/pdf/2107.09787.pdf">Group Contrastive Self-Supervised Learning on Graphs</a></h2><h3 id="why-17">Why</h3><ul><li>Capability : Contrasting graphs in multiple subspaces enables graph encoders to capture more abundant characteristics.</li><li>CL methods train models on pretext tasks that encode the agreement between two views of representations. These two views can be global-local pairs or differently transformed graph data. The learning goal is to make these two-view representations similar if they are from the same graph and dissimilar if they are from different graphs.</li><li>The idea of using groups has been shown to be effective in the image domain.</li></ul><h3 id="goal-16">Goal</h3><ul><li>Study SSL on graphs by contrastive learning.</li><li>Propose a group contrastive learning framework. Embed the given graph into multiple subspaces, of which each representation is prompted to encode specific characteristics of graphs.</li><li>Further develop an attention-based representor function to compute representations. Develop principled objectives that enable us to capture the relations among both intra-space and inter-space representations in groups.</li></ul><h3 id="how-18">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914160553376.png" alt="image-20210914160553376" /><figcaption aria-hidden="true">image-20210914160553376</figcaption></figure><ul><li>refer to a group as a set of representations of different graph views within the same subspace</li><li>propose to maximize the MI between two views of representations in the same group while minimizing the MI between the representations of one view across different groups.</li><li>a graph-level encoder usually consists of a node encoder which computes the node embedding matrix and a readout function summarize the mdoe embeddigs into the desired graph-level embedding.</li></ul><h4 id="the-proposed-group-contrastive-learning-framework">The proposed group contrastive learning framework</h4><ul><li>To build two views and their corresponding multiple graph-level representations. The used GNN encoder can be the same, or one can just duplicate the calculated representation for <span class="math inline">\(p\)</span> times. Then for view <span class="math inline">\(u,r\)</span>, they can be combined into <span class="math inline">\(p\)</span> pairs which are cross-view pairs and two groups that each group from one specific view.</li><li>So for two views: the main view <span class="math inline">\(u\)</span>, and auxiliary view <span class="math inline">\(r\)</span>, there are in total two encoders which are parameterized by <span class="math inline">\(\theta, \phi\)</span> respectively.</li></ul><h4 id="intra-space-objective-function">Intra-space objective function</h4><ul><li>For the intra-space objective, they seek to maximize the mutual information between representations of two views within each group. The optimization is done based on paramters <span class="math inline">\(\theta,\phi\)</span>.</li><li>To implement, maximizes the MI's lower-bound. Precisely, they adopt the Jensen-Shannon estimator of MI, and the disciriminator is the dot product between two representations.</li></ul><h4 id="inter-space-objective-function">Inter-space objective function</h4><ul><li>Constraint the pairwise relation across different groups of the same view to enforce the diversity of inter-space. They employ an inter-space optimization objective based on mutual information minimization.</li><li>To minimize the MI, we introduce an upper bound of MI as an efficient estimation, based on the contrastive log-ratio upper bound: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914165125847.png" alt="image-20210914165125847" style="zoom:33%;" />. To solve the key challenge of modeling <span class="math inline">\(P(y|x)\)</span>, there are two approaches based on whether the same dimensions of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> correspond to each other across different representations. Suppose the distribution <span class="math inline">\(y\)</span> conditional on <span class="math inline">\(x\)</span> is subject to a Gaussian distribution.<ul><li>Non-parameterized estimation<ul><li>Assume <span class="math inline">\(\mathbb{E}[y|x]=x\)</span>, and the variance <span class="math inline">\(\Sigma\)</span> is a diagonal matrix with the same values on its diagonal. Then after deduction, the goal minimizing CLUB is equivalent to minimize <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914170432752.png" alt="image-20210914170432752" style="zoom: 25%;" /></li><li>This goal enlarges the agreement between <span class="math inline">\(\mathrm{u}^{(k)},\mathrm{u}^{(l)}\)</span>, under the joint distribution so to keep the diversity among one group.</li></ul></li><li>Parameterized estimation<ul><li>When there is no correspondence between dimensions of <span class="math inline">\(x,y\)</span>.</li><li>Via a parameterized variational distribution. Concretely, they use two independent MLP to generate the mean and variance respectively.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914171121061.png" alt="image-20210914171121061" style="zoom: 25%;" /></li></ul></li></ul></li></ul><h4 id="the-overall-objective-function">The overall objective function</h4><ul><li>Combine the intra-space and the inter-space objectives together.</li><li>Either optimize based on non-parameterized way or parameterized way.</li></ul><h4 id="groupcl-graphcl-with-group-contrast">GroupCL: GraphCL with Group Contrast</h4><ul><li>the generation of multiple representations shares the same node encoder and envolves a parameterized representor function that computes multiple graph representations from node embeddings of a given graph.</li><li>For view <span class="math inline">\(u\)</span>, GroupCL performs a random data augmentation on the input graph to generate the view data, and then use encoder to encode each nodes of the view data into node embeddings <span class="math inline">\(\mathrm{U}\)</span>. Given <span class="math inline">\(\mathrm{U}\)</span>, use attention to capture the information from different node combinations where each graph representation depends greatly on the heavily attended nodes with respect to different queries, and thus propose the representor function.</li><li>the multiple representations are prompted to focus on different and informative combinations of nodes and thereupon encode different subgraph patterns.</li></ul><h4 id="graphig-infograph-with-group-contrast">GraphIG: InfoGraph with Group Contrast</h4><ul><li>In InfoGraph, view <span class="math inline">\(u,r\)</span> are the graph-level embedding an the node-level embedding, respectively.</li><li>In view <span class="math inline">\(u\)</span>, the encoder GNN and representor function <span class="math inline">\(\mathcal{R}_Q\)</span> is similar as GroupCL.</li><li>In view <span class="math inline">\(r\)</span>, they generate the node embeddings through encder GNN and duplicate them for <span class="math inline">\(p\)</span> times to obtain <span class="math inline">\(p\)</span> representations.</li></ul><h2 id="paper-20-homophily-outlier-detection-in-non-iid-categorical-data">Paper 20: <a href="https://arxiv.org/pdf/2103.11516.pdf">Homophily outlier detection in non-IID categorical data</a></h2><p><font color="red"> Will be completed later.</font></p><h3 id="why-18">Why</h3><h3 id="goal-17">Goal</h3><ul><li>identify outliers in <em>categorical data</em> by capturing non-IID outlier factors</li></ul><h3 id="how-19">How</h3><ul><li>first defines and incorporates distribution-sensitive outlier factors and their interdependence into a value-value graph-based representation. Then model an outlierness propagation process in the value graph to learn the outlierness of feature values.</li><li></li></ul><h2 id="paper-21-hyperparameter-free-and-explainable-whole-graph-embedding">Paper 21: <a href="https://arxiv.org/abs/2108.02113">Hyperparameter-free and Explainable Whole Graph Embedding</a></h2><p>Codes: https://github.com/HW-HaoWang/DHC-E</p><h3 id="why-19">Why</h3><ul><li>most node embedding or whole graph embedding methods suffer from the problem of having more sophisticated methodology, hyperparameter optimization, and low explainability.</li><li>Previous work<ul><li>Before 2000, traditional methods like PCA, LDS are linear, which cannot handle the nonlinear relationships within datasets properly.</li><li>Around 2000, like IsoMap and LLE (Laplacian eigenmaps) are manifold.</li><li>From practical view, like word2vec is popular.</li></ul></li><li>Whole graph embedding methods: previously they are mathematically intractable, sophisticated, and challenging to interpret, and suffer hyperparameter selection problems in comparing different graphs.<ul><li>FGSD (family of graph spectral distances): calculate the Moore-Penrose spectrum of the normalized Laplacian and use the histogram of the spectral features as a whole graph representation.</li><li>Graph2vec: unsupervised, derive fixed-length task-agnostic embedding of graphs</li><li>NetLSD (Network Laplacian Spectral Descriptor) : calculate the heat kernel trace of the normalized Laplacian matrix over a vector of time scales.</li><li>IGE (Invariant graph embedding): compute a mixture of spectral and noes embedding based features and pool node feature embedding to create graph descriptor.</li><li>GL2vec: complement the edge label information or the structural information .</li></ul></li><li></li></ul><h3 id="goal-18">Goal</h3><ul><li>This paper only considers unweighted and undirected graphs with no self-loops or multiple edges.</li><li>Propose a hyperparameter free, extensible, and explainable whole graph embedding method, combining the DHC (Degree, H-index and Coreness) theorem and Shannon Entropy (E), abbreviated as DHC-E.</li><li>The new whole graph embedding scheme can obtain a trade-off between simplicity and quality under supervised classification learning tasks, using molecular, social, and brain networks. In addition, the proposed approach has a good performance in lower dimensional graph visualization</li></ul><h3 id="how-20">How</h3><ul><li>DHC-E can be regarded as an adaptive and self-converging system that depicts the information of complex systems from the fine-grained to coarse-scale.</li><li>Assume that different networks have different convergence steps and each iteration of H-index sequences encodes information that can distinguish the network’s properties.</li><li>metrics for vital node identification<ul><li>Degree</li><li>H-index: The h-index of a node in the graph is defined as the maximum value <span class="math inline">\(h\)</span> such that there exist at least <span class="math inline">\(h\)</span> neighbors who have a degree no less than <span class="math inline">\(h\)</span>. The H-index of every node can be updated based on the previous H-indices of neighbors following the same procedure.</li><li>Coreness: take the location of a node in the graph to measure its influence based on the <span class="math inline">\(k\)</span>-core decomposition process. A larger coreness of a noes indicate that it is located more centrally in the graph.</li></ul></li></ul><h4 id="dhc-entropy">DHC-entropy</h4><ul><li><p>DHC theorem proves that each nodal H-index sequence eventually converges to the nodal coreness.</p></li><li><p>The zero-order H-index of every node is initialized to its degree, as shown in the initial state. The following updated states can be calculated by the DHC theorem</p></li><li><p>Combining Shannon entropy and DHC theorem:</p><ul><li>Problems: : (1) How to extract and integrate the information generated from the DHC updating process (i.e., the H-index sequences of all nodes) to construct whole graph embedding; (2) How to unify and align the whole graph embedding with different dimensions.</li><li>In the <span class="math inline">\(m\)</span>-th iteration, for the H-indices, calculate the probability distribution of each node's H-index, and then take the probability distribution to calculate Shannon-entropy, so to quantify the uncertainty of the graph in this state.</li><li>To align, for one graph set with different subgraphs to have same dimension of embeddings so as to perform whole graph embedding aggregation, the key is to keep the number of H-indices of each subgraphs to be the same. They propose to expand those with smaller H-indices to be expanded with their last element.</li></ul></li><li><p>The DHC-E algorithm</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210915222052643.png" alt="image-20210915222052643" /><figcaption aria-hidden="true">image-20210915222052643</figcaption></figure></li></ul><h2 id="paper-22-infograph-unsupervised-and-semi-supervised-graph-level-representation-learning-via-mutual-information-maximization">Paper 22: <a href="https://arxiv.org/abs/1908.01000">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</a></h2><p>Codes: https://github.com/fanyun-sun/InfoGraph</p><h3 id="why-20">Why</h3><ul><li>Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs</li><li>one of the most difficult obstacles for supervised learning on graphs is that it is often very costly or even impossible to collect annotated labels.</li><li>the handcrafted features of graph kernels lead to high dimensional, sparse or non-smooth representations and thus result in poor generalization performance, especially on large datasets.</li><li>Deep Graph InfoMax (DGI) aims to train a node encoder that maximizes mutual information between node representations and the pooled global graph representation.</li><li>Mean Teacher adds a loss term which encourages the distance between the original network’s output and the teacher’s output to be small. The teacher’s predictions are made using an exponential moving average of parameters from previous training steps.</li><li>Explicitly extracting the graph can be more straightforward and optimal for graph-oriented tasks</li></ul><h3 id="goal-19">Goal</h3><ul><li>Graph-level representations learning</li><li>maximize the mutual information between the graph-level representation and the representations of substructures of different scales</li><li>Propose InfoGraph and InfoGraph* for unsupervised and semi-supervised separately. The InfoGraph* employs a student-teacher framework similar to Mean-Teacher method. It deploys two separate encoders but instead of explicitly encouraging the output of the student model to be similar to the teacher model’s output, they enable the student model to learn from the teacher model by maximizing mutual information between intermediate representations learned by two models.</li></ul><h3 id="how-21">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210917161308152.png" alt="image-20210917161308152" /><figcaption aria-hidden="true">image-20210917161308152</figcaption></figure><h4 id="infograph">InfoGraph</h4><ul><li>The encoder is GNN, finally the representations from different GNN layers are concatenated and readout (they use sum over mean ) as the global representations.</li><li>The MI is between global and local pairs, and is estimated by Jensen-Shannon MI estimator.</li><li>In practice, the negative samples are generated using all possible combinations of global and local patch representations across all graph instances in a batch.</li><li>Although similar to DGI, they show their difference in<ol type="1"><li>They extract global representations rather than node-level representations</li><li>For graph convolution encoders, they use GIN rather than GCN since GIN provides a better inductive bias for graph level representations.</li></ol></li></ul><h4 id="infograph-1">InfoGraph*</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210917161807256.png" alt="image-20210917161807256" /><figcaption aria-hidden="true">image-20210917161807256</figcaption></figure><ul><li><p>Simply combining the two loss functions using the same encoder may lead to “negative transfer" (transfer knowledge from a less related source and thus may hurt the target performance).</p></li><li><p>Therefore, they propose a simple way to alleviate this problem: deploy two encoder models: the encoder on the labeled data and the encoder on the unlabeled data. For transferring, they define a loss term that encourages the representations learned by the two encoders to have high mutual information, at all levels of representations.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210918084309019.png" alt="image-20210918084309019" /><figcaption aria-hidden="true">image-20210918084309019</figcaption></figure></li><li><p>The formulation can be seen as a special instance of the student-teacher framework.</p></li><li><p>To reduce computation cost, instead of enforcing the mutual-information maximization over all the layers of the encoders, at each training update, they enforce mutual-information maximization on a randomly chosen layer of the encoder.</p></li></ul><h2 id="paper-23-iterative-graph-self-distillation">Paper 23: <a href="https://arxiv.org/abs/2010.12609">Iterative graph self-distillation</a></h2><h3 id="why-21">Why</h3><ul><li>The limitation of existing GNN architecture is that they often require a huge amount of labeled data to be competitive but annotating graphs.==&gt; unsupervised learning such as graph kernels and matrix factorization.</li><li>SSL<ul><li>pretext task: needs meticulous designs of hand-crafted tasks</li><li>contrastive learning via InfoMax principle:<ul><li>context-instance contrastive approaches usually need to sample subgraphs as local views to contrast with global graphs. And they usually require an additional discriminator for scoring local-global pairs and negative samples, which is computationally prohibitive. The performance is also very sensitive to the choice of encoders and MI estimators</li><li>context-instance contrastive approaches cannot be handily extended to the semi-supervised setting since local subgraphs lack labels that can be utilized for training.</li></ul></li></ul></li><li>In order to alleviate the dependency on negative samples mining and still be able to learn discriminative graph representations, they propose to use self-distillation as a strong regularization to guide the graph representation learning.</li><li>GCC (graph contrastive coding ): leverage instance discrimination as the pretext task for structural information pre-training.</li><li>GraphCL: learn generalizable, transferrable, and robust representations of graph data in an unsupervised manner. Focuses on the impact of various combination of graph augmentation on multiple datasets and studies unsupervised</li><li>Self-distillation is a special case when two architectures are identical, which can iteratively modify regularization and reduce over-fitting if perform suitable rounds.</li><li>Semi-supervised learning<ul><li>multi-task learning: regularizing the learning process with unlabeled data</li><li>consistency training between two separate networks: student-teacher framework, : introducing a slow-moving average teacher network to measures consistency against a student one, thus providing a consistency-based training paradigm where two networks can be mutually improved</li></ul></li><li>Data augmentation: limited since defining views of graphs is a non-trivial task.<ul><li>Feature-space augmentation</li><li>structure-space augmentation</li></ul></li></ul><h3 id="goal-20">Goal</h3><ul><li>Learn graph-level representations unsupervised by iteratively performing the teacher-student distillation with graph augmentations.</li><li>Propose IGSD (iterative graph self-distillation), which constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself.</li><li>Extend IGSD to semi-supervised by jointly regularizing the network with both supervised and unsupervised contrastive loss.</li><li>Seeking an approach that learns the entire graph representation by contrasting the whole graph directly alleviates the need for MI estimation, discriminator and subgraph sampling.</li></ul><h3 id="how-22">How</h3><ul><li>Intuition: predict the teacher network representation of the graph pairs under different augmented views.</li><li>Define a similarity metric for consistency-based training. The parameters of the teacher network are iteratively updated as an exponential moving average of the student network parameters.</li><li>To extend to semi-supervised learning, they develop a self-training algorithm based on the supervised contrastive loss for fine-tuning.</li><li>To perform augmentation, they have two choices<ul><li>Transform a graph with transition matrix via graph diffusion and sparsification into a new graph with adjacency matrix as an augmented view in their framework</li><li>Randomly remove edges of graphs to attain corrupted graphs as augmented views.</li></ul></li></ul><h4 id="iterative-graph-self-distillation-framework">Iterative graph self-distillation framework</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919095719228.png" alt="image-20210919095719228" /><figcaption aria-hidden="true">image-20210919095719228</figcaption></figure><ul><li>Introduce a teacher-student architecture comprises two networks in similar structure composed by encoder <span class="math inline">\(f_\theta\)</span>, projector <span class="math inline">\(g_\theta\)</span> and predictor <span class="math inline">\(h_\theta\)</span>. Denote the components of the teacher network and the student network as <span class="math inline">\(f_{\theta&#39;},g_{\theta&#39;}\)</span> and <span class="math inline">\(f_{\theta},g_{\theta},h_\theta\)</span> respectively.</li><li>The positive pairs: the original and augmented view of the same graph, they are input into two networks respectively.</li><li>The teacher's parameters are updated as an exponential moving average of the student parameters <span class="math inline">\(\theta\)</span> after weights of the student network have been updated using gradient descent.</li><li>The moving average network to produce prediction targets, enforcing the consistency of teacher and student for training the student network.</li></ul><h4 id="self-supervised-learning-with-igsd">Self-supervised learning with IGSD</h4><ul><li>Employ the Self-supervised InfoNCE objective.</li><li>Obtain the graph representation by interpolating the latent representations with Mixup function.</li></ul><h4 id="semi-supervised-learning-with-igsd">Semi-supervised learning with IGSD</h4><ul><li>The instance-wise supervision limited to standard supervised learning may lead to biased negative sampling problems.</li><li>enforce consistency constraints between latents from different views, which acts as a regularizer for learning directly from labels</li><li>train the model using a small amount of labeled data and then fine-tune it by iterating between assigning pseudo-labels to unlabeled examples and training models using the augmented dataset.</li></ul><h2 id="paper-24-learning-by-aligning-videos-in-time">Paper 24: <a href="https://arxiv.org/abs/2103.17260">Learning by Aligning Videos in Time</a></h2><h3 id="why-22">Why</h3><ul><li>To learn perfect alignment of two videos, a learning algorithm must be able to disentangle phases of the activity in time while simultaneously associating visually similar frames in the two different videos.</li><li>Using temporal alignment for learning video representations<ul><li>some use cycle-consistency losses to perform local alignment between individual frames.</li><li>some have explored global alignment for video classification and segmentation.</li></ul></li><li>Supervised methods require fine-grained annotations which can be prohibitively expensive.</li><li>DTW is a global alignment metric, taking into account entire sequences while aligning, but not differential. The soft-DTW is differential.</li><li>Video-based SSL representation learning<ul><li>predict future frames or forecast their encoding features</li><li>leverage temporal information such as temporal order and temporal coherence.</li></ul></li><li>TCC (temporal cycle consistency ): learns Self-supervised representations by finding frame correspondences across videos.<ul><li>But TCC aligns each frame separately, not like this paper which aligns the video as a whole.</li></ul></li></ul><h3 id="goal-21">Goal</h3><ul><li>SSL for learning video representations using temporal video alignment as a pretext task.</li></ul><h3 id="how-23">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919142229296.png" alt="image-20210919142229296" /><figcaption aria-hidden="true">image-20210919142229296</figcaption></figure><ul><li>Soft-DTW for the minimum cost for temporally aligning videos in the embedding space. And to avoid trivial solutions, they propose a temporal regularization term which encourages different frames to be mapped to different points in the embedding space</li><li>Denote the embedding function as <span class="math inline">\(f_\theta\)</span>. They methods takes as input two videos <span class="math inline">\(X,Y\)</span> with <span class="math inline">\(n,m\)</span> frames respectively.</li></ul><h4 id="temporal-alignment-loss">Temporal alignment loss</h4><ul><li>For two videos <span class="math inline">\(X,Y\)</span>, after obtaining their embedding videos, one can compute the distance matrix <span class="math inline">\(D\)</span>. DTW calculates the alignment loss by finding the minimum cost path in <span class="math inline">\(D\)</span>: <span class="math inline">\(dtw(X,Y)=\min_{A\in A_{n,m}}\langle A,D \rangle\)</span>, where <span class="math inline">\(A_{n,m}\subset \{0，1\}^{n\times m}\)</span> is the set of all possible alignment matrices, which correspond to paths from the top-left corner of <span class="math inline">\(D\)</span> to the bottom-right Corner of <span class="math inline">\(D\)</span> using only <span class="math inline">\(\downarrow,\rightarrow, \searrow\)</span> moves.<ul><li><span class="math inline">\(A(i,j)=1\)</span> if <span class="math inline">\(x_i\)</span> in <span class="math inline">\(X\)</span> is aligned with <span class="math inline">\(y_i\)</span> in <span class="math inline">\(Y\)</span>.</li><li>This can be computed by dynamic programming.</li></ul></li><li>Because the <span class="math inline">\(\min\)</span> operator, the DYW is not differentiable, to change it, soft-DTW is proposed<ul><li>Replace the <span class="math inline">\(\min\)</span> by the smoothed <span class="math inline">\(\min^\gamma\)</span> one, defined as <span class="math inline">\(\min^{\gamma}\{a_1,a_2,\cdots,a_n\}=-\gamma\log\sum\limits_{i=1}^{n}e^{-\frac{-a_i}{\gamma}}\)</span>.</li><li>It returns the alignment cost between <span class="math inline">\(X,Y\)</span> by finding the soft-minimum cost path in <span class="math inline">\(D\)</span>.</li><li>The smoothed <span class="math inline">\(\min^{\gamma}\)</span> operator converges to the discrete <span class="math inline">\(\min\)</span> when <span class="math inline">\(\gamma\)</span> approaches 0.</li><li><span class="math inline">\(\min^\gamma\)</span> help the optimization by enabling smooth gradients and providing better optimization landscapes.</li></ul></li></ul><h4 id="temporal-regularization">Temporal regularization</h4><ul><li><p>To avoid trivial solutions (<span class="math inline">\(X,Y\)</span> are mapped to a small cluster in the embedding space), they add a temporal regularization, which is separately applied on <span class="math inline">\(f_\theta(X),f_\theta(Y)\)</span>.</p></li><li><p>Adapt inverse difference moment (IDM) as their regularization, which is written as: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919153758111.png" alt="image-20210919153758111" style="zoom:30%;" /></p><ul><li>maximizes this equation encourages temporally close frames in <span class="math inline">\(X\)</span> to be mapped to nearby points in the embedding space.</li></ul></li><li><p>But the equation above treats temporally close and far away frames in similar ways</p><ul><li><p>They propose separate terms for temporally close and far away frames.</p></li><li><p>Introduce a contrastive version and named as contrastive-IDM.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919154140301.png" alt="image-20210919154140301" /><figcaption aria-hidden="true">image-20210919154140301</figcaption></figure></li><li><p>It encourages temporally close frames (positive pairs) to be nearby in the embedding space, while penalizing temporally far away frames (negative pairs) when the distance between them is smaller than margin <span class="math inline">\(\lambda\)</span> in the embedding space.</p></li><li><p>Leveraging temporal information by adding weights to different pairs based on their temporal gaps leads to performance gain.</p></li></ul></li></ul><h4 id="final-loss">Final loss</h4><ul><li>Combine soft-DTW alignments loss and contrastive-IDM: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919154511695.png" alt="image-20210919154511695" style="zoom:33%;" /></li><li>It encourages embedding videos to have minimum alignment costs while encouraging discrepancies among embedding frames.</li></ul><h4 id="encoder-network">Encoder network</h4><ul><li>Use resnet-50 as backbone, stack <span class="math inline">\(k\)</span> context frame features along the temporal dimension for each frame.</li><li>Then the combined features are passed through two 3DCNN layers for aggregating temporal information.</li></ul><h2 id="paper-25-learning-graph-representation-by-aggregating-subgraphs-via-mutual-information-maximization">Paper 25: <a href="https://arxiv.org/abs/2103.13125">Learning graph representation by aggregating subgraphs via mutual information maximization</a></h2><h3 id="why-23">Why</h3><ul><li>Labeling graphs procedurally using strong prior knowledge is costly.</li><li>Previous work<ul><li>Graph kernel methods<ul><li>Decompose the graph into several subgraphs and then measure similarities between them</li><li>much work on deciding the most suitable sub-structures by hand-craft similarity measures between sub-structures.</li></ul></li><li>Semi-supervised learning<ul><li>pseudo label methods: regard the prediction of unlabeled data as a pseudo label of unlabeled data, then train the network with all data together, and use a low weight of the loss of the unlabeled data part</li><li>Laddar network: combined supervised learning with unsupervised learning in DNNs to apply unlabeled data information to supervised learning reasonably.</li></ul></li><li>learning by mutual information<ul><li>express the amount of information shared between two random variables</li><li>But to alleviate computational cost, many approximation methods for mutual information are proposed.</li></ul></li></ul></li></ul><h3 id="goal-22">Goal</h3><ul><li>SSL to enhance graph-level representations with a set of subgraphs</li><li>Separate the aggregation into three aspects and they design three information aggregators: <strong>attribute-conv, layer-conv, subgraph-conv</strong>. The attribute-conv aggregate the original information from graphs, the layer-conv aggregate the multi-hop node representations from different GNN layers. The subgraph-conv aggregate the information from the generated subgraphs.</li><li></li></ul><h3 id="how-24">How</h3><ul><li>To constraint, maximizes the mutual information between the reconstructed graph representations rather than between graph and node representations as in previous works.</li><li>a head-tail contrastive construction to provide abundant negative samples.</li><li>In subgraph-agg stage, they introduce an auto-regressive method which is a universal SSL framework for the graph generation.</li></ul><h4 id="node-agg-stage">Node-agg stage</h4><ul><li>Perform the MLP and AGG+MLP on node attributes and edge attributes firstly.</li><li>The kernel size is <span class="math inline">\((2,1)\)</span>, which can squeeze each perspective of embeddings to one channel.</li></ul><h4 id="layer-agg-stage">Layer-agg stage</h4><ul><li>After perform convolution on the initial node representations for <span class="math inline">\(L\)</span> layers, to aggregate the node representations of different scales so that local and global information can be combined organically, they propose to use a <span class="math inline">\((L,1)\)</span> convolution kernel, named as layer-conv.</li><li>Then the whole graph representations are obtained by a readout function.</li></ul><h4 id="subgraph-agg-stage">Subgraph-agg stage</h4><ul><li>Propose subgraph-agg, like an ensemble learning, since the nodes and graph representations do not express the same level of information</li><li>Maximizing the mutual information between nodes and graph representation is not good enough to achieve the purpose of graph representations to express more information</li><li>Steps<ul><li>Build an autoregressive model to generate subgraphs from the original graph first</li><li>Assemble these subgraphs into a reconstructed graph <span class="math inline">\(\mathrm{G}^{rec}\)</span>. Use layer-conv, by maximizing the mutual information between two graphs representations in the same level, the graph representation is learned.<ul><li>This mutual contrastive leads to lower variance than node-graph constrain.</li></ul></li></ul></li><li>Auto-regressive subgraph generation<ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920134338763.png" title="fig:" alt="image-20210920134338763" /></li></ul></li><li>Graph reconstruction<ul><li>The readout function is used to aggregate each subgraph's representation.</li><li>Since there are <span class="math inline">\(S\)</span> subgraphs, the kernel size of subgraph-conv is <span class="math inline">\((S,1)\)</span>, and the resulting representation is taken as the representation of the reconstructed graph <span class="math inline">\(\mathrm{G^{rec}}\)</span></li></ul></li></ul><h4 id="implementation-2">Implementation</h4><ul><li>By softmax, the output matrix <span class="math inline">\(P\)</span> divides the original graph into two subgraphs, since <span class="math inline">\(p_{ij}\)</span> denotes the probability that the node <span class="math inline">\(i\)</span> is in the subgraph <span class="math inline">\(j\)</span>.</li><li>To implement the auto-regressive paradigm of subgraph generation, there are two approaches<ul><li>Tree-split generation<ul><li>recursively utilize the basic operator to the newly generated subgraph. Just lie splitting a binary tree.</li><li>At each non-leaf node, they execute the softmax to partition graphs.</li><li>After <span class="math inline">\(T\)</span> rounds split, they will get <span class="math inline">\(2^T\)</span> subraphs.</li></ul></li><li>multi-head generation<ul><li>Import <span class="math inline">\(S\)</span> learnable matrices to execute the softmax and get <span class="math inline">\(S\)</span> partition matrices.</li><li>In this way, <span class="math inline">\(S\)</span> subgraphs are selected in parallel but they break the rule of auto-regressive generation.</li><li>By assuming all subgraphs are conditionally independent concerning the original graph, one can take this method as an auto-regressive way.</li></ul></li></ul></li><li>Loss function<ul><li>The mutual information estimator is Jensen-Shannon divergence</li><li>The positive samples are the output of subgraph-conv.</li><li>Ways to get easy negative samples<ul><li>using different graphs in the batch</li><li>or use the corruption function to get negative samples from the original graph.</li></ul></li><li>For a graph, its hard negative pair sample can be built by shuffling the node embeddings.</li></ul></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920154704931.png" title="fig:" alt="image-20210920154704931" /></li></ul><h2 id="paper-26-mile-a-multi-level-framework-for-scalable-graph-embedding">Paper 26: <a href="https://arxiv.org/abs/1802.09612">Mile: A multi-level framework for scalable graph embedding</a></h2><p>Codes: https://github.com/jiongqian/MILE</p><h3 id="why-24">Why</h3><ul><li>None of the existing efforts examines how to scale up graph embedding in a generic way.</li><li>HARP is familiar with this work: but it focuses on improving the quality of embeddings by using the learned embeddings from the previous level as the initialized embeddings for the next level. But the way it is extended to other graph embedding is not obvious.</li><li>the quality of such embedding methods be strengthened by incorporating the holistic view of the graph.</li></ul><h3 id="goal-23">Goal</h3><ul><li>Obtain graph embedding efficiently and effectively (save computation cost), introduce MILE (multi-level embedding framework), allow contemporary graph embedding methods to scale to large graphs.</li><li>Given a graph <span class="math inline">\(\mathcal{G} = (V, E)\)</span> and a graph embedding method <span class="math inline">\(f(\cdot)\)</span>, we aim to realize a strengthened graph embedding method <span class="math inline">\(\hat{f}(\cdot)\)</span> so that it is more scalable than <span class="math inline">\(f(\cdot)\)</span> while generating embeddings of comparable or even better quality.</li></ul><h3 id="how-25">How</h3><ul><li>repeatedly coarsen the graph into smaller ones by employing a hybrid matching strategy</li><li>compute the embeddings on the coarsest graph using an existing embedding technique</li><li>propose a novel refinement model based on learning a graph convolution network to refine the embedding from the coarsest graph to the original graph.</li></ul><h4 id="graph-coarsening">Graph coarsening</h4><ul><li>the set of nodes forming a super-node is called a matching</li><li>Structural Equivalence Matching (SEM): Given two vertices <span class="math inline">\(u,v\)</span> in an unweighted graph <span class="math inline">\(\mathcal{G}\)</span>, we call they are structurally equivalent if they are incident on the same set of neighborhoods. Since if two vertices are structurally equivalent, then their node embeddings will be similar.</li><li>Normalized Heavy Edge Matching (NHEM): select an unmatched node, say <span class="math inline">\(u\)</span>, in the graph and find a large weighted edge <span class="math inline">\((u, v)\)</span> incident on node <span class="math inline">\(u\)</span> such that node <span class="math inline">\(v\)</span> is also unmatched. Then collapse nodes <span class="math inline">\(u,v\)</span> into one super-node and mark them as matched. The edge weights are normalized. The normalized method penalizes the weights of edges connected with high-degree nodes.</li><li>Define matching matrix to store the matching information from graph <span class="math inline">\(\mathcal{G}_i\)</span> to <span class="math inline">\(\mathcal{G}_{i+1}\)</span> as a binary matrix, the <span class="math inline">\(r\)</span>-the row and <span class="math inline">\(c\)</span>-the column of <span class="math inline">\(M_{i,i+1}\)</span> is set to 1 if node <span class="math inline">\(r\)</span> in <span class="math inline">\(\mathcal{G}_i\)</span> collapse to super-node <span class="math inline">\(c\)</span> in <span class="math inline">\(\mathcal{G}_{i+1}\)</span>, and is set to 0 if otherwise.</li><li>vertices with a small number of neighbors have a limited choice of finding a match and should be given a higher priority for matching. Otherwise, once their neighbors are matched by others, these vertices cannot be matched.</li><li>The embeddings learned by base embedding method on the coarsened graph can act as an effective initialization for the graph-topology aware refinement model.</li><li>Choice of coarsening level: depends on the application domain and the graph properties</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920200555554.png" alt="image-20210920200555554" /><figcaption aria-hidden="true">image-20210920200555554</figcaption></figure><h4 id="base-embedding-on-coarsened-graph">Base embedding on coarsened graph</h4><ul><li>They can use any graph embedding algorithm for base embedding.</li></ul><h4 id="refinement-of-embeddings">Refinement of embeddings</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920200744561.png" alt="image-20210920200744561" /><figcaption aria-hidden="true">image-20210920200744561</figcaption></figure><ul><li><p>Given the series of coarsened graph and their corresponding matching matrix, and the node embeddings on <span class="math inline">\(\mathcal{G}_m\)</span>, they seek to develop an approach to derive the node embeddings of <span class="math inline">\(\mathcal{G}_0\)</span> from <span class="math inline">\(\mathcal{G}_m\)</span>.</p></li><li><p>Subtask: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920200927480.png" alt="image-20210920200927480" /></p><ul><li>Propose to use GCN. Since the simple project will induce <span class="math inline">\(\mathcal{E}_i^p\)</span> from <span class="math inline">\(\mathcal{E}_{i+1}\)</span>, but one limitation is that nodes will share the same embeddings if they are matched and collapsed into a super-node during the coarsening phase.</li><li>The GCN uses the projected embedding <span class="math inline">\(\mathcal{E}_i^p\)</span> and the adjacency matrix <span class="math inline">\(A_i\)</span> from the input graph. Define the embedding refinement model as a <span class="math inline">\(l\)</span>-layer graph convolution model: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920201450434.png" alt="image-20210920201450434" style="zoom:25%;" /></li><li>The intuition behind this refinement model is to integrate the structural information of the current graph <span class="math inline">\(\mathcal{G}_i\)</span> into the projected embedding <span class="math inline">\(\mathcal{E}_i^p\)</span> by repeatedly performing the spectral graph convolution.</li></ul></li><li><p>Choice of number of GCN layers: <span class="math inline">\(l\)</span> GCN layers correspond to aggregating structural information from all the <span class="math inline">\(l\)</span>-hop neighbours for each node.</p><ul><li>But large <span class="math inline">\(l\)</span> will make the node embeddings homogeneous and less distinguishable across the graph due to the small-world property of real-world graphs.</li><li>In practice, they use 2.</li></ul></li><li><p>Train refinement model</p><ul><li><p>learn <span class="math inline">\(\Theta^{(k)}\)</span> on the coarsest graph and reuse them across all the levels for refinement</p></li><li><p>The loss function is defined as the MSE, where the ground truth is a base embedding on <span class="math inline">\(\mathcal{G}_i\)</span>.</p><ul><li>But two drawbacks: The above loss function requires one more level of coarsening to construct <span class="math inline">\(\mathcal{G}_{m+1}\)</span> and an extra base embedding on <span class="math inline">\(\mathcal{G}_{m+1}\)</span>. Also, the embedding space of graph <span class="math inline">\(\mathcal{G}_{m}\)</span> and <span class="math inline">\(\mathcal{G}_{m+1}\)</span> can be totally different since the two embeddings are learned independently.</li><li>One possible solution for these drawbacks may be force the embeddings to be aligned between the two graphs.</li></ul></li><li><p>Finally, this paper propose to construct a dummy coarsened graph by simply copying <span class="math inline">\(\mathcal{G}_m\)</span>, i.e. <span class="math inline">\(M_{m,m+1}=I, \mathcal{G}_{m+1}=\mathcal{G}_m\)</span>.</p><ul><li>reduce one iteration of graph coarsening</li><li>avoid performing base embeddings on <span class="math inline">\(\mathcal{G}_{m+1}\)</span>.</li></ul><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920202809962.png" alt="image-20210920202809962" style="zoom:33%;" /></p><ul><li>Adopt gradient-descent with BP to learn parameters.</li></ul></li></ul></li><li><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920202919023.png" alt="image-20210920202919023" /><figcaption aria-hidden="true">image-20210920202919023</figcaption></figure></li><li><p>The shared <span class="math inline">\(\Theta^{(k)}\)</span> values do much better than alternative <span class="math inline">\(\Theta^{(k)}\)</span>.</p></li></ul><h2 id="paper-27-missing-data-estimation-in-temporal-multilayer-position-aware-graph-neural-network-tmp-gnn">Paper 27: <a href="https://arxiv.org/pdf/2108.03400.pdf">Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</a></h2><p><font color="blue">Not clear for me.</font></p><h3 id="why-25">Why</h3><ul><li>The goal of node embedding methods: identify a vector representation that captures node location within a broader topological structure of the graph. But most focus on static graphs.</li><li>To deal with dynamic graphs<ul><li>One way is to use a GNN based node embedding for each individual time layer and aggregate the results accordingly.==&gt; ignore inter-layer correlation.</li><li>P-GNN: learn position-aware node embedding that utilize local network structure and the global network position of a given node with respect to randomly selected nodes called anchor-sets that enables us to distinguish among isomorphic nodes.</li></ul></li></ul><h3 id="goal-24">Goal</h3><ul><li>Propose TMP-GNN (Temporal multi-layered positio-aware GNN), a node embedding method for dynamic graphs. (change over time: edges, nodes attributes etc.)</li><li>The method in this paper is an extension of static position-aware GNN (P-GNN).</li></ul><h3 id="how-26">How</h3><ul><li>exploit a supra-adjacency matrix to encode a temporal graph with its intra-layer and inter-layer coupling in a single graph.</li><li>use hidden states learned from bi-directional GRU (bi-GRU) to learn the long-term temporal dependencies in both forward and backward directions to estimate missing values.</li><li>conditional centrality derived from eigenvector based centrality to distinguish nodes of higher influence</li></ul><h4 id="notations-and-preliminaries">Notations and preliminaries</h4><ul><li>The existence of inter-layer edge is restricted between separate instances of same nodes from one layer to another. The inter0later edge weight between two layers is identical for all nodes in those layers.</li><li>To model the coupling between two layers, this paper couples the adjacent time layers by neglecting the directionality of time, considering the short term dependencies between time layers.</li><li>Supracentrality matrix<ul><li>A temporal graph is represented through a sequence of adjacency matrices that each of which refers to one layer of a dynamic network at a specific point of time.</li><li>The supracentrality matrix <span class="math inline">\(\mathbb{C}(\omega)\)</span> is built by linking up centrality matrices across time layers through a weighted inter-layer parameter <span class="math inline">\(\omega\)</span> (used to adjust the extent of coupling strength among pair of time layers.)<ul><li>When <span class="math inline">\(\omega\rightarrow\infin\)</span>, the centrality measures change slowly over time, <span class="math inline">\(CC_{v}(\omega)\)</span> reaches to stationary that is equal to average <span class="math inline">\(CC_{v}(\omega)\)</span> over all time layers.</li><li>On the other hand, small <span class="math inline">\(\omega\)</span> makes <span class="math inline">\(CC_{v}(\omega)\)</span> fluctuate fromega one time layer to the other.</li></ul></li><li>Each item of <span class="math inline">\(\mathbb{C}(\omega)\)</span> shows joint centrality, the importance of every node-layer pair <span class="math inline">\((v,t)\)</span>. Additionally marginal and conditional centralities are defined to represent the relative importance of a node compared to other nodes at time layer <span class="math inline">\(t\)</span>.</li><li><span class="math inline">\(\mathbb{C}(\omega)\)</span> consists of <span class="math inline">\(\hat{\mathbb{C}}=diag [\mathrm{C^{(1)},\cdots,C^{(T)}}]\)</span> and <span class="math inline">\(\omega\hat{\mathbb{A}}\)</span>, where <span class="math inline">\(\hat{\mathbb{C}}\)</span> represents a set of <span class="math inline">\(T\)</span> weighted centrality matrix of individual layers, and the latter encoder the uniform and diagonal coupling with strength parameter <span class="math inline">\(\omega\)</span> between the time layers. Only consider the coupling among same nodes between consecutive pair of layers.</li></ul></li><li>To solve, the dominant eigenvector <span class="math inline">\(\mathbb{V}(\omega)\)</span> is corresponds to largest eigenvalues <span class="math inline">\(\lambda_{max}\)</span>, <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921124235527.png" alt="image-20210921124235527" style="zoom:25%;" />. The elements in <span class="math inline">\(\mathbb{V}(\omega)\)</span> are interpreted as scores that measure the importance of node-layer pairs <span class="math inline">\((v,t)\)</span>.</li><li>Conditional centrality of node <span class="math inline">\(v\)</span>, denoted as <span class="math inline">\(CC_{v}(\omega)\)</span>, shows the importance of the node relative to other nodes at layer <span class="math inline">\(t\)</span>.</li></ul><h4 id="tmp-gnn">TMP-GNN</h4><ul><li>Position-aware GNN rather than original one, which aggregates the positional and feature information of each node with randomly selected number of nodes called anchor-sets.</li><li>The goal is to find the best position-aware embedding <span class="math inline">\(\mathrm{z}_v^t\)</span> with minimum distortion for a given node <span class="math inline">\(v\)</span> at time layer <span class="math inline">\(t\)</span>.</li><li>Modifications to P-GNN:<ul><li>Generalization of P-GNN to time varying graphs: Adopt the input of P-GNN as supracentrality matrix, the embedding <span class="math inline">\(\mathrm{z}_v^t\)</span> will then be aggregated from an RNN based representation to estimate missing data.</li><li>modification of <span class="math inline">\(\mathcal{M}_j\)</span>: Using attention while calculating a message from anchor-set containing node <span class="math inline">\(u\)</span> with respect to a given node <span class="math inline">\(v\)</span>. Attention is used to learn the relative weights between the feature vector of <span class="math inline">\(v\)</span> and its neighbor <span class="math inline">\(u\)</span> from the anchor-set.<ul><li>The <span class="math inline">\(\mathcal{M}_j\)</span> for anchor-set, denoted as <span class="math inline">\(M_v[j]\)</span>, is obtained by the shortest path between a pair of nodes and mean of <span class="math inline">\(\mathcal{M}_j\)</span>.</li></ul></li><li>Modification of <span class="math inline">\(AGG_{\mathcal{R}}\)</span>: <span class="math inline">\(\mathcal{R}\)</span> denotes the anchor-set. Corresponding informative anchor-sets contain at least 1-hop neighbours of node <span class="math inline">\(v\)</span>. Of those neighbor nodes, the ones with higher <span class="math inline">\(CC_v(\omega)\)</span> deserve to have higher <span class="math inline">\(r_j\)</span> for aggregation.<ul><li>Large anchor-sets have higher probability of hitting <span class="math inline">\(v\)</span>, but are less informative of positional information of the node, as <span class="math inline">\(v\)</span> hits at least one of many nodes in the anchor-sets.</li><li>Small anchor-sets have fewer chance of hitting <span class="math inline">\(v\)</span>, but provide positional information with high certainty.</li></ul></li></ul></li><li>Edge embedding is estimated by averaging the embedding of the ending nodes.</li></ul><h4 id="bi-directional-rnn">Bi-directional RNN</h4><ul><li>Take <span class="math inline">\(Z_e,M_e,\Delta_e\)</span> as input, where <span class="math inline">\(Z_e\)</span> is randomly masked by removing missing points and set it to zero. The <span class="math inline">\(\Delta_e\)</span> illustrates the time difference between the current and the last layer which the measurement is recorded. It's defined to handle the different sampling rate associated with data heterogeneity from different sources.</li><li>Goal: Find the best estimate <span class="math inline">\(\mathrm{\hat{x}_{e_d}^t}\)</span> with minimum RMSE for a particular missing point.</li><li>Versions<ul><li>The E-TMP-GNN I aims at extracting additional features out of the embedding yields from TMP-GNN, and use it to further enrich the edge feature sets.</li><li>TMP-GNN II aims at reducing the number of feature streams by implementing a softmax layer.</li></ul></li></ul><h2 id="paper-28-multi-level-graph-contrastive-learning">Paper 28: <a href="https://arxiv.org/abs/2107.02639">Multi-Level Graph Contrastive Learning</a></h2><h3 id="why-26">Why</h3><ul><li><p>Most graph representation learning methods using GNN focus on supervised fashion and heavily depend on label information.</p></li><li><p>Unsupervised methods</p><ul><li>many reconstruction-based unsupervised algorithms have been proposed, either reconstruct the features of graph nodes or the topology structure of the graph.</li></ul></li><li><p>SSL methods</p><ul><li><p>contrastive methods, propose many data augmentation methods</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921154102468.png" alt="image-20210921154102468" /><figcaption aria-hidden="true">image-20210921154102468</figcaption></figure><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921151131234.png" alt="image-20210921151131234" /><figcaption aria-hidden="true">image-20210921151131234</figcaption></figure><ul><li>shuffles node features to augment the training examples</li><li>multi-view, maximizes MI between node representations of one view and graph representation of another view</li><li>subgraphs: MI loss defined on central nodes and their sampled subgraphs representation.</li><li>GCA: adaptive augmentation, extract the important connective structures of the original graph to the augmentation data based on node centrality measures.</li><li>GraphCL: multiple augmentation, including edge perturbation, attribute masking, and maximizes the MI between the node and global semantic representations to learn node representations.</li></ul></li></ul></li><li><p>GNN does not perform well on disassortative graphs. Constructing a KNN graph by feature similarity or dropping the links between nodes indifferent classes are feasible methods.</p></li><li><p>Contrastive learning consists of three main components: data augmentation schemes, the learner model and loss function</p><ul><li>data augmentation: design according to the priors and properties of graph-structured data:<ul><li>vertex and edge missing do not alter the graph semantics;</li><li>the missing partial attributes of each node does not alter the robustness of the graph semantics;</li><li>the subgraph structure of the graph can hint full semantics;</li><li>correlated views of the same graph possess semantic consistency.</li></ul></li><li>Loss function<ul><li>Probability loss: Deepwalk, node2vec. Maximizes the probability of nearest vertices of the given vertex to learn graph representation. Nodes occurring in the same sequence are viewed as the positive samples, and they should have similar representation.</li><li>Adversarial loss: generative-contrastive</li><li>Triplet loss: an important role in deep metric learning. Given an anchor <span class="math inline">\(x\)</span>, a positive <span class="math inline">\(x_p\)</span> of the same class as the anchor, a negative <span class="math inline">\(x_f\)</span> of a different class, the triple loss targets at achieving that the distance of <span class="math inline">\(x,x_f\)</span> greater than the distance of <span class="math inline">\(x,x_p\)</span> plus margin <span class="math inline">\(m\)</span>. <span class="math inline">\(\max(d(x,x_p)-d(x,x_f)+m)\)</span>.</li><li>Contrastive loss: maximizing the similarity of the corresponding nodes between two views, or maximizes the MI between node and graph representation.</li></ul></li></ul></li></ul><h3 id="goal-25">Goal</h3><ul><li>SSL for graph node representation learning</li><li>propose a Multi-Level Graph Contrastive Learning (MLGCL) framework for learning robust representation of graph data by contrasting space views of graphs. Introduce a contrastive view-topological and feature space views. Adopt KNN graph (generated by encoding features preserves high-order proximity).</li><li>Feature space+topology space. The correlated graphs have similar and intrinsic characteristics.</li><li>Contrastive learning aims to learn an encoder model <span class="math inline">\(f(\cdot)\)</span> which can learn a robust representation for each node <span class="math inline">\(v_i\)</span> that is insensitive to the perturbation caused by data augmentations <span class="math inline">\(\tau\)</span>.</li></ul><h3 id="how-27">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921153951384.png" alt="image-20210921153951384" /><figcaption aria-hidden="true">image-20210921153951384</figcaption></figure><ul><li>First sample a pair of graph augmentation functions <span class="math inline">\(\tau_1,\tau_2\)</span> from augmentation pool <span class="math inline">\(\tau\)</span>, which is applied to the input graph to generate the augmented graph of two views. Then use a pair of the shared GNN-based encoder to extract node representation, and further employ the pooling layer to extract the graph representation. Then use the shared MLP layer to project the nodes representations from both views into the space where the node-level contrastive loss is computed. <span class="math inline">\(\tau_2\)</span> includes an encoder and KNN function.</li></ul><h4 id="data-augmentation">Data augmentation</h4><ul><li>exert perturbation on the input graph to generate two correlated graphs of the same graph. Extract augmentation graph structure from space view.</li><li>Given the graph structure of topology space <span class="math inline">\(G(A,X)\)</span> , extraction is done<ul><li>First employ GNN encoder to extract the encoding features <span class="math inline">\(Z\)</span> of topology graph</li><li>Then apply kNN to <span class="math inline">\(Z\)</span> to construct KNN graph with community structure <span class="math inline">\(G_f(A_f,X)\)</span>, where <span class="math inline">\(A_f\)</span> is the adjacency matrix of KNN graph.<ul><li>First calculate the similarity matrix <span class="math inline">\(S\)</span> based on <span class="math inline">\(N\)</span> encoding features <span class="math inline">\(Z\)</span>. The choice of similarity function can be Mahalanobis distance, cosine similar or gaussian kernel. They use cosine similarity in their experiments.</li><li>Then choose top <span class="math inline">\(k\)</span> similar node pairs for each node to set edges and finally get the adjacency matrix of KNN graph <span class="math inline">\(A_f\)</span>.</li></ul></li></ul></li></ul><h4 id="gnn-encoder">GNN Encoder</h4><ul><li>Learn node representation <span class="math inline">\(z_1,z_2\)</span> for two augmented graphs. In inference phase, only use GNN encoder to learn node representation for downstream tasks.</li><li>Given topology graph <span class="math inline">\(G(A,X)\)</span>, KNN graph <span class="math inline">\(G_f(A_f,X)\)</span>, employ a two-layer GCN as the encoder.</li><li>For node representation <span class="math inline">\(Z_a,Z_b\)</span> of each view, use a graph pooling layer to derive their graph representation.</li></ul><h4 id="mlp-and-graph-pooling">MLP and graph pooling</h4><ul><li>MLP: maps the representation to the space in which the contrastive loss is computed. Since two views, there are two MLP.</li><li>Graph pooling: readout layer, used to learn graph representation.</li></ul><h4 id="multi-level-loss-function">Multi-level loss function</h4><ul><li>Used to preserve the low-level "local" and high-level "global" agreement, simultaneously.</li><li>Consists of two parts: the contrast of low-level node representation between two views, and the contrast of high-level graph representation between two views.</li><li>Positive samples <span class="math inline">\((z_i^a,z_i^b)\)</span>, K-1 pair of negative samples <span class="math inline">\((z_i^a,z_j^a),(z_i^a,z_j^b)\)</span>.</li></ul><h2 id="paper-29-permutation-invariant-variational-autoencoder-for-graph-level-representation-learning">Paper 29: <a href="https://arxiv.org/abs/2104.09856">Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</a></h2><h3 id="why-27">Why</h3><ul><li>unsupervised learning on graphs mainly focus on node-level representation learning, which aims at embedding the local graph structure into latent node representations.<ul><li>Usually implemented by an autoencoder framework</li></ul></li><li>A graph-level representation should not depend on the order of the nodes in the input representation of a graph.</li></ul><h3 id="goal-26">Goal</h3><ul><li>Graph-level unsupervised learning</li><li>Propose a permutation-invariant variational autoencoder for graph structured data. The method indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching</li></ul><h3 id="how-28">How</h3><ul><li>Address the order ambiguity issue by training alongside the encoder and decoder model an additional permuter model that assigns to each input graph a permutation matrix that aligns the input graph node ordering with the output graph ordering.</li></ul><h4 id="problem-definition">Problem definition</h4><ul><li>Consider a dataset of graphs <span class="math inline">\(\mathrm{G}={\mathcal{G}^{(i)}}_{i=0}^N\)</span>, the goal is to represent in a low-dimensional continuous space. Assume the data is generated by a process <span class="math inline">\(p_\theta(\mathcal{G}|\mathrm{z})\)</span>, then approximate the intractable posterior by <span class="math inline">\(q_\phi(\mathcal{G}|\mathrm{z})\)</span> and minimize the lower bound on the marginal likelihood of graph <span class="math inline">\(G^{(i)}\)</span>: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922115927078.png" alt="image-20210922115927078" style="zoom: 67%;" /><ul><li>The KL-divergence divergence term regularizes the encoded latent codes of graphs <span class="math inline">\(\mathcal{G}^{(i)}\)</span></li><li>The second term enforce high similarity of decoded graphs to their encoded counterparts</li><li>Parameters <span class="math inline">\(q_\phi,p_\theta\)</span> can be estimated by NNs that encode and decode node features <span class="math inline">\(\mathrm{X}_\pi^{(i)}\)</span> and adjacency matrices of graphs <span class="math inline">\(\mathcal{G}^{(i)}\)</span>.</li></ul></li><li>Considering the permutation invariant, <span class="math inline">\(\mathcal{G}_\pi,\hat{\mathcal{G}}_{\pi&#39;}\)</span> have to be brought in the same node ordering.</li><li>Previous work either fail to track <span class="math inline">\(\mathrm{P}_{\pi&#39;\rightarrow \pi}\)</span>, or is computational costly (by maximizing the similarity which involves up to <span class="math inline">\(O(n^4)\)</span> re-ordering operations at each training step).</li></ul><h4 id="permutation-invariant-variational-graph-autoencoder">Permutation-Invariant variational graph autoencoder</h4><ul><li><p>Propose to silver the reordering problem implicitly by inferring the permutation matrix <span class="math inline">\(\mathrm{P}_{\pi&#39;\rightarrow\pi}\)</span> from the input graph <span class="math inline">\(\mathcal{G}_\pi\)</span> by a model <span class="math inline">\(g_\psi(\mathrm{P}|\mathcal{G}_\pi)\)</span>.</p></li><li><p>Train this model jointly with the encoder model <span class="math inline">\(q_\phi(\mathrm{z}|\mathcal{G})\)</span> and decoder model <span class="math inline">\(p_\theta(\mathcal{G}|\mathrm{z})\)</span>.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922121759262.png" alt="image-20210922121759262" /><figcaption aria-hidden="true">image-20210922121759262</figcaption></figure></li><li><p>The permuter model has to learn how the ordering of nodes in the graph generated by the encoding model will differ from a specific node ordering present in the input graph. It predicts for each node <span class="math inline">\(i\)</span> of the input graph a score <span class="math inline">\(s_i\)</span> corresponding to its probability to have a lower node index in the decoded graph.</p></li><li><p>Next they derive the elements of the permutation matrix <span class="math inline">\(\mathrm{P}\)</span> by sorting the scored nodes: <span class="math inline">\(p_{ij}=\begin{cases}1,&amp; ifj=\arg sort(s)_i\\0,&amp;else\end{cases}\)</span>. The argsort is then replaced by the continuous relaxation: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922123207545.png" alt="image-20210922123207545" style="zoom:60%;" />， where the softmax operator is applied row-wise, <span class="math inline">\(d(x,y)\)</span> is the <span class="math inline">\(L_1\)</span>-norm and <span class="math inline">\(\tau\)</span> a temperature-parameter.</p></li><li><p>The permuter model <span class="math inline">\(g_\psi\)</span> is trained with stochastic gradient descent.</p></li><li><p>In order to push the relaxed permutation matrix towards a real permutation matrix (only one 1 in every row and column), add a row- and column-wise entropy term as additional penalty term to equation (5)</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922123705808.png" alt="image-20210922123705808" /><figcaption aria-hidden="true">image-20210922123705808</figcaption></figure></li><li><p>By enforcing <span class="math inline">\(C(\mathrm{P})=0\)</span>, the real permutation matrix is fulfilled.</p></li></ul><h4 id="details-of-the-model-architecture">Details of the model architecture</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922134834006.png" alt="image-20210922134834006" /><figcaption aria-hidden="true">image-20210922134834006</figcaption></figure><ul><li>Graph representation by directional messages: MPNN<ul><li>Key idea: aggregation of neighborhood information by passing and receiving messages of each node to and from neighbouring noes in a graph. In <span class="math inline">\(k\)</span> hop.</li><li>The item <span class="math inline">\(m_{ij}\)</span> of message matrix <span class="math inline">\(\mathrm{M}\)</span> is <span class="math inline">\(m_{ij}=\sigma([x_i\|x_j\|e_{ij}]\mathrm{W+b})\)</span>.</li><li>Nodes in this view are represented by self-message <span class="math inline">\(\mathrm{diag(M)}\)</span>.</li></ul></li><li>Self-attention on directed messages<ul><li>Embedded in MPNN, take the message matrix to compute key,value and query.</li><li>Let messages <span class="math inline">\(m_{ij}\)</span> only attend on incoming messages <span class="math inline">\(m_{ki}\)</span> to reduce the complexity.</li></ul></li><li>Encoder<ul><li>A dummy node <span class="math inline">\(v_0\)</span> acts as an embedding node, the cumulated messages are updated in <span class="math inline">\(m_{0,0}\)</span>.</li><li>Then use the reparameterization trick and sample the latent representation <span class="math inline">\(\mathrm{z}\)</span> of a graph by sampling from a multivariate normal distribution.</li></ul></li><li>Permuter<ul><li>to predict how to re-order the nodes in the output graph to match the ordering of nodes in the input graph.</li><li>After extract node embeddings represented by self messages on the main diagonal of the encoded message matrix, retrieve the permutation matrix <span class="math inline">\(\hat{\mathrm{P}}\)</span> by scoring these messages with a function which is parameterized by a linear layer and the soft-sort operator.</li></ul></li><li>Decoder<ul><li>Add position embeddings to the initial node embeddings, same as what's defined in Bert. And then the permutation matrix is performed on these position embeddings. After obtaining the permutation matrix, extracted messages are permuted by the permutation matrix and then use self-attention in decoder to extract node and edge features.</li><li>Pad all graphs in a batch with empty nodes to match the number of nodes of the largest graph, attention on empty nodes is masked out at all time.</li></ul></li></ul><h2 id="paper-30-pinet-attention-pooling-for-graph-classification">Paper 30: <a href="https://arxiv.org/abs/2008.04575">PiNet: Attention Pooling for Graph Classification</a></h2><p>Codes: http://github.com/meltzerpete/pinet</p><h3 id="why-28">Why</h3><ul><li>The essential to the success of CNNs is the process of pooling<ul><li>Pooling is invariance to different orderings of the input vectors.</li><li>GCNs achieve invariance by pooling neighbors' feature vectors with symmetric operators such as feature-weighted mean, max, and self-attention weighted means.</li></ul></li><li>Previous work on permutation invariant<ul><li>Propose a permutation invariant function <span class="math inline">\(f(\mathbb{X})\)</span> on the set <span class="math inline">\(\mathbb{X}\)</span> may be learned indirectly through decomposition in the form <span class="math inline">\(f(\mathbb{X})=\rho(\sum\limits_{x\in\mathrm{X}}\phi(x))\)</span>. The model is named as Janossy pooling, where <span class="math inline">\(\rho\)</span> is a normalization function, and the summation occurs over the set of all possible permutations of the input set.</li><li>The use of canonical orderings to tackle permutations in graph representation learning has been demonstrated to be effective in Patchy-SAN.</li><li>DGCNN uses a sorting method to introduce permutation invariance.</li><li>Variants may also be expressed as an instance of the WL graph isomorphism algorithm.</li></ul></li><li>A simple solution is to use a symmetric operator to combine vertex vectors to form a single graph vector.</li></ul><h3 id="goal-27">Goal</h3><ul><li>graph classification</li><li>A generalized differential attention-based pooling mechanism for utilizing graph convolution operations for graph-level classification.</li><li>The proposed pooling mechanism is differentiable, by which the vertex-level invariance to permutation achieved for vertex level tasks may be extended to the graph level.</li></ul><h3 id="how-29">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922205836991.png" alt="image-20210922205836991" /><figcaption aria-hidden="true">image-20210922205836991</figcaption></figure><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922210144596.png" title="fig:" alt="image-20210922210144596" /></li><li>The inner softmax constrains the attention coefficients to sum to 1 and prevents them from all falling to 0. The outer softmax may be replaced for multi-label classification tasks (i.e. sigmoid).</li></ul><h2 id="paper-31-self-supervised-graph-level-representation-learning-with-local-and-global-structure">Paper 31: <a href="https://arxiv.org/abs/2106.04113">Self-supervised Graph-level Representation Learning with Local and Global Structure</a></h2><p>Codes: https://github.com/DeepGraphLearning/GraphLoG</p><h3 id="why-29">Why</h3><ul><li>Existing methods mainly focus on preserving the local similarity structure between different graph instances but fail to discover the global semantic structure of the entire dataset.</li><li>a desirable graph representation should be able to preserve the local-instance structure. It should also reflect the global-semantic structure of the data.</li><li>Require a model that is sufficient to model both the local and global structure of a set of graphs</li></ul><h3 id="goal-28">Goal</h3><ul><li>Study the unsupervised/self supervised graph-level representation</li><li>propose a unified framework called Local-instance and Global-semantic Learning (GraphLoG) for self-supervised whole-graph representation learning</li><li>Introduce hierarchical prototypes to capture the global semantic clusters.</li><li>EM for training</li></ul><h3 id="how-30">How</h3><ul><li>To preserve local similarity between various graph instances, align the embeddings of correlated graphs/subgraphs by discriminating the correlated graph/subgraph pairs from the negative pairs.</li><li>Import hierarchical prototypes to depict the latent distribution of a graph dataset in a hierarchical way. Propose to maximizes the data likelihood with respect to both the GNN parameters and hierarchical prototypes via an online EM algorithm.<ul><li>E-step, infer the embeddings of the mini-batch of graphs sampled from the data distribution with a GNN, and sample the latent variable of each graph from the posterior distribution defined by current model.</li><li>M-step, maximizes the expectation of complete-data likelihood with respect to the current model by optimizing with a mini-batch-induced objective function.</li></ul></li></ul><h4 id="problem-definition-1">Problem definition</h4><ul><li>Expect the graph embeddings <span class="math inline">\(\mathrm{H}=\{h_{\mathcal{G}_1},\cdots,h_{\mathcal{G}_M}\}\)</span> follow both the local-instance and global-semantic structure.<ul><li>Local-instance structure: For a pair of similar graphs/subgraphs, <span class="math inline">\(\mathcal{G,G&#39;}\)</span>, their embeddings are expected to be nearby in the latent space.</li><li>Global-semantic structure: After mapping to the latent space, the embeddings of a set of graphs are expected to form some global structures reflecting the clustering patterns of the original data.</li></ul></li><li>Since <span class="math inline">\(h_v\)</span> summarizes the information of a subgraph centered around node <span class="math inline">\(v\)</span>, they refer <span class="math inline">\(h_v\)</span> as subgraph embedding. Then the entire graph's embedding is derived as <span class="math inline">\(h_\mathcal{G}=f_{\mathcal{R}}(\{h_v|v\in\mathcal{V}\})\)</span>, where <span class="math inline">\(f_{\mathcal{R}}\)</span> is a permutation-invariant readout function, like mean pooling.</li><li>EM<ul><li>In E-step, using the observed <span class="math inline">\(\mathrm{X}\)</span> and pre-estimated <span class="math inline">\(\theta_{t-1}\)</span> to estimate the posterior <span class="math inline">\(p(\mathrm{Z|X,}\theta_{t-1})\)</span></li><li>In M-step, use the posterior to calculate the expectation of <span class="math inline">\(\log p(\mathrm{X,Z|}\theta)\)</span>, and <span class="math inline">\(\theta_t\)</span> is the solution of maximizing this expectation.</li></ul></li></ul><h4 id="graphlog">GraphLoG</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923134844909.png" alt="image-20210923134844909" /><figcaption aria-hidden="true">image-20210923134844909</figcaption></figure><ul><li><p>Learning local-instance structure of graph representation</p><ul><li>The problem is formulated as maximizing the similarity of correlated graph/subgraph pairs while minimizing that of negative pair.</li><li>Given a graph sampled from the data distribution, obtain its correlated counterpart through randomly masking a part of node/edge attributes in the graph.<ul><li>Given a molecular graph, we randomly mask the attributes of 30% nodes (i.e. atoms) in it to obtain its correlated counterpart. Specifically, we add an extra dimension to the feature of atom type and atom chirality to indicate masked attribute, and the input features of all masked atoms are set to these extra dimension.</li></ul></li><li>The similarity function is cosine similarity.</li><li>The negative pairs: for a correlated graph pair <span class="math inline">\(\mathcal{G,G&#39;}\)</span> or correlated subgraph pair <span class="math inline">\(\mathcal{G_v,G&#39;_v}\)</span>, substitute <span class="math inline">\(\mathcal{G(G}_v)\)</span> randomly with another graph from the dataset (a subgraph centered around another node in the same graph) to construct negative pairs.</li><li>The local-instance structure of graph representations is then solved by minimizing <span class="math inline">\(\mathcal{L}_{graph}+\mathcal{L}_{sub}\)</span>.</li></ul></li><li><p>Leaning global-semantic structure of graph representation</p><ul><li><p>Hierarchical prototypes to represent the feature clusters in the latent space in a hierarchical way. Each layer has several prototypes.</p></li><li><p>encourage the graphs to be compactly embedded around corresponding prototypes and, at the same time, refine hierarchical prototypes to better represent the data. Formalize the problem as optimizing a latent variable model.</p></li><li><p>To solve parameters: hierarchical prototypes <span class="math inline">\(\mathrm{C}\)</span>, the online EM is applied.</p><ul><li><p>The online EM is base on i.i.d. assumption, where both the complete-data likelihood and the posterior probability of latent variables can be factorized over each observed-latent variable pair</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923135511275.png" alt="image-20210923135511275" /><figcaption aria-hidden="true">image-20210923135511275</figcaption></figure></li><li><p>The initialization of model parameters</p><p>Pretrained GNN by minimizing <span class="math inline">\(\mathcal{L}_{local}\)</span> and employ the derived GNN model as initialization, then use it to extract the embeddings of all graphs in the data set. After, k-means clustering is applied upon these graph embeddings to initialize the bottom layer prototypes with the output cluster centers.</p><p>The prototypes of upper layers are initialized by iteratively applying kmeans to the prototypes of the layer below.</p><p>Clusters with less than two samples are dropped.</p></li><li><p>E-step</p><ul><li>randomly sample a mini-batch of graphs. Each latent variable is a chain of prototypes from top layer that best represent graph <span class="math inline">\(\mathcal{G}_n\)</span> in the latent space.</li><li>To train, use stochastic EM algorithm, draw a sample for Monte Carlo estimation.</li><li>Formally, we first sample a prototype from top layer according to a categorical distribution over all the top layer prototypes. For the sampling at layer <span class="math inline">\(l\)</span> (<span class="math inline">\(l &gt; 2\)</span>), we draw a prototype from that layer based on a categorical distribution over the child nodes of prototype sampled from the layer above.</li></ul></li><li><p>M-step: Maximizes the expected log-likelihood on mini-batch to obtain <span class="math inline">\(\theta,\mathrm{C}\)</span>.</p></li></ul></li><li><p>This algorithm can indeed maximizes the marginal likelihood function <span class="math inline">\(p(\mathrm{G}|\theta,\mathrm{C})\)</span>.</p></li></ul></li><li><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923144941732.png" alt="image-20210923144941732" /><figcaption aria-hidden="true">image-20210923144941732</figcaption></figure></li></ul><h2 id="paper-32-self-supervised-heterogeneous-graph-neural-network-with-co-contrastive-learning">Paper 32: <a href="https://arxiv.org/abs/2105.09111">Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</a></h2><p>Codes: https://github.com/liun-online/HeCo</p><h3 id="why-30">Why</h3><ul><li>HGNNs can effectively combine the mechanism of message passing with complex heterogeneity.</li><li>Most methods on HGNN are semi-supervised, but the requirement of some node labels are known is not always fullfilled.</li><li>Contrastive learning is able to learn the discriminative embeddings even without labels. But the following problems have to be addressed<ul><li>How to design a heterogeneous contrastive mechanism<ul><li>Different meta-paths (used to capture the long-rang structure in a HIN) represent different semantics.</li><li>Performing contrastive learning only on single meta-path view is actually is actually distant from sufficient.</li></ul></li><li>How to select proper views in a HIN:<ul><li>The selected view should cover both of the local and high-order structures while meta-path is usually used to extract the high-order structure. Meta-path is the combination of multiple relations, so it contains complex semantics, which is regarded as high-order structure.</li><li>Both of the network schema and meta-path structure views should be carefully considered.</li></ul></li><li>How to set a difficult contrastive task<ul><li>A proper contrastive task will further promote to learn a more discriminative embedding.</li><li>Need to make the contrastive learning on two not too similar views. One strategy is to enhance the information diversity in two views, the other is to generate harder negative samples of high quality.</li></ul></li></ul></li><li>There is a lack of methods contrasting across views in HIN so that the high-level factors can be captured.</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923162244368.png" alt="image-20210923162244368" /><figcaption aria-hidden="true">image-20210923162244368</figcaption></figure><h3 id="goal-29">Goal</h3><ul><li>Self-supervised on Heterogeneous graph neural networks (HGNNs), <strong>learn node embeddings</strong>.</li><li>Propose a novel co-contrastive learning mechanism for HGNNs, named HeCo. It employs cross-view contrastive mechanism. Specifically , two views of a HIN, namely network schema and meta-path views are proposed to learn node embeddings.</li><li></li></ul><h3 id="how-31">How</h3><ul><li>Choose network schema and meta-path structure as two views.<ul><li>In network schema, the node embedding is learned by aggregating information from its direct neighbors.</li><li>In meta-path view, the node embedding is learned by passing messages along multiple meta-paths to capture high-order structure.</li></ul></li><li>To make contrast harder, propose a view mask mechanism that hides different parts of network schema and meta-path, respectively.</li><li>Given the high correlation between nodes, define the positive samples of a node in HIN and design a optimization strategy specially.</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923163653685.png" alt="image-20210923163653685" /><figcaption aria-hidden="true">image-20210923163653685</figcaption></figure><h4 id="node-feature-transformation">Node feature transformation</h4><ul><li>First need to project features of all types of nodes into a common latent vector space to avoid features of nodes distributed into different spaces.</li><li>For a node <span class="math inline">\(i\)</span> with type <span class="math inline">\(\phi_i\)</span>, design a type-specific mapping matrix <span class="math inline">\(W_{\phi_i}\)</span> to transform its feature <span class="math inline">\(x_i\)</span> into common space as <span class="math inline">\(h_i=\sigma(W_{\phi_i}\cdot x_i+b_{\phi_i})\)</span>, where <span class="math inline">\(h_i\)</span> is the projected feature of node.</li></ul><h4 id="network-schema-view-guided-encoder">Network schema view guided encoder</h4><ul><li>Now aim to learn the embedding of node <span class="math inline">\(i\)</span> under network schema view. For node <span class="math inline">\(i\)</span>, different types of neighbors contribute differently to its embedding, and so do the different nodes with the same type. Therefore, attention is used in node-level and type-level to hierarchically aggregate messages from other types of neighbors to target node <span class="math inline">\(i\)</span>.</li><li>For node-level (same type) attention, the projected features of nodes <span class="math inline">\(h_i\)</span> are used to calculate the attention map, then applied on projected features. Note that in practice, they randomly sample a part of neighbors (in the same type) for aggregation on one node. In this way, they ensure that every node aggregates the same amount of information from neighbors, and promote the diversity of embeddings in each epoch under this view.</li><li>After getting all type embeddings, they use type-level attention to fuse them together to get the final embedding for node <span class="math inline">\(i\)</span> under network-schema view. The attention here is apply tanh on linear combination of type embeddings.<ul><li>Is <span class="math inline">\(\mathrm{a}_{sc}\)</span> trainable here?</li></ul></li></ul><h4 id="meta-path-view-guided-encoder">Meta-path view guided encoder</h4><ul><li>Each meta-path represents one semantic similarity.</li><li>For one meta-path <span class="math inline">\(\mathcal{P}_n\)</span> from <span class="math inline">\(M\)</span> meta-paths, apply meta-path specific GCN to encode, features will be <span class="math inline">\(h_j^{\mathcal{P}_n}=\frac{1}{d_i+1}h_i+\sum\limits_{j\in N_i^{\mathcal{P}_n}}\frac{1}{\sqrt{(d_i+1)(d_j+1)}}h_j\)</span>, where <span class="math inline">\(h_i,h_j\)</span> are their projected features. For each node, along <span class="math inline">\(M\)</span> meta-path, it will have <span class="math inline">\(M\)</span> embeddings.</li><li>Then to fuse <span class="math inline">\(M\)</span> embeddings for each node, use semantic-level attention to get the final embedding under the meta-path view.<ul><li>Is <span class="math inline">\(\mathrm{a}_{mp}\)</span> trainable here?</li></ul></li></ul><h4 id="view-mask-mechanism">View mask mechanism</h4><ul><li><p>a view mask mechanism that hides different parts of network schema and meta-path views, respectively.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923171508369.png" alt="image-20210923171508369" /><figcaption aria-hidden="true">image-20210923171508369</figcaption></figure></li><li><p><font color="blue">How to select nodes? Still unclear for me.</font></p></li></ul><h4 id="collaboratively-contrastive-optimization">Collaboratively contrastive optimization</h4><ul><li>Feed the obtained features from two views to MLP to map them into the space where contrastive loss is calculated. Both views use the same MLP.</li><li>Positive pairs and negative pairs<ul><li>But consider that nodes are usually highly-correlated because of edges, propose a new positive selection strategy<ul><li>if two nodes are connected by many meta-paths, they are positive samples. Then the selected positive samples can well reflect the local structure of the target node.</li></ul></li><li>Steps<ul><li>First count the number of meta-paths connecting two nodes</li><li>Then sort the node set <span class="math inline">\(S_i\)</span> by this count in descending order and set a threshold <span class="math inline">\(T_{pos}\)</span>. If The count is larger than the threshold, select first <span class="math inline">\(T_{pos}\)</span> nodes from <span class="math inline">\(S_i\)</span> as positive samples of node <span class="math inline">\(i\)</span>.</li><li>Treat all left nodes as negative samples of <span class="math inline">\(i\)</span>.</li></ul></li><li>Different from traditional methods, they consider multiple positive pairs.</li><li>In contrastive loss <span class="math inline">\(\mathcal{L}_i^{sc}\)</span>, for two nodes in a pair, the target embedding is from the network schema view and the embeddings of positive and negative samples are from the meta-path view. This also realize the cross-view self-supervision.</li><li>In contrastive loss <span class="math inline">\(\mathcal{L}_i^{mp}\)</span>, the target embedding is from the meta-path view while the embeddings of positive and negative samples are from the network schema view.</li></ul></li><li>The final loss is the combination of <span class="math inline">\(\mathcal{L}_i^{sc},\mathcal{L}_i^{mp}\)</span>, optimized by back propagation. The <span class="math inline">\(\mathcal{L}_i^{mp}\)</span> is used to perform downstream tasks because nodes of target type explicitly participant into the generation of <span class="math inline">\(z^{mp}\)</span>.</li></ul><h4 id="model-extension">Model extension</h4><ul><li>HeCo_GAN<ul><li>Sample additional negatives from a continuous Gaussian distribution.</li><li>Composed of three components: the proposed HeCo, a discriminatory D and a generator G.<ul><li>Train D and G: first train D to identify the embedding from two views as positives and that generated from G as negatives. Then rain G to generate samples with high quality that fool D.</li><li>Use trained G to generate samples, which can be viewed as the new negative samples with high quality.</li></ul></li></ul></li><li>HeCo_MU<ul><li>Proposed to improve results in supervised learning by adding arbitrary two samples to create a new one.</li><li>Build hard negative samples: get cosine similarities between noes <span class="math inline">\(i\)</span> and nodes from <span class="math inline">\(\mathbb{N}_i\)</span> during calculating, and sort them in the descending order. Then select first top <span class="math inline">\(k\)</span> negative samples as the hardest negatives, and randomly add them to create new <span class="math inline">\(k\)</span> negatives, which are involved in training.</li><li>No learnable parameters in this version.</li></ul></li></ul><h2 id="paper-33-sm-sge-a-self-supervised-multi-scale-skeleton-graph-encoding-framework-for-person-re-identification">Paper 33: <a href="https://arxiv.org/abs/2107.01903">SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</a></h2><p>Codes: https://github.com/Kali-Hac/SM-SGE</p><h3 id="why-31">Why</h3><ul><li>Previous works just learn body and motion features from the body-joint trajectory, whereas lack a systematic way to model body structure and underlying relations of body components beyond the scale of body joints.<ul><li>extract appearance-based features such as body texture and silhouettes from RGB or depth images: but vulnerability to illumination or appearance changes</li><li>skeleton-based models: robust to factors such as view and body shape changes</li></ul></li><li>still an open challenge to extract discriminative body and motion features with 3D skeletons for person Re-ID</li><li>Multi-scale skeleton graphs: many previous methods extract hand-crafted features from skeletons with a single spatial scale and topology</li><li>Multi-scale relation learning: previous works mostly encode body-joint trajectory or pre-defined pose descriptors into a feature vector, rarely explore the inherent relations between different body joints or components.</li><li>Multi-scale skeleton dynamics modeling: Previous methods mostly learn skeleton motion at a fixed scale of body joints, lack the flexibility to capture motion patterns at various levels==&gt; cause a loss of global motion features.</li></ul><h3 id="goal-30">Goal</h3><ul><li>Person re-identification. Propose a Self-supervised Multi-scale Skeleton Graph Encoding (SM-SGE) framework that models human body, component relations, and skeleton dynamics from unlabeled skeleton graphs of various scales to learn an effective skeleton representation for person Re-ID.</li><li>MGRN (multi-scale graph relation network) + MSR (multi-scale skeleton reconstruction), where MSR with two concurrent pretext tasks, namely skeleton subsequence reconstruction task and cross-scale skeleton inference task.</li><li>No need of ID labels during training.</li><li>Effective with the 3D skeletons estimated from RGB videos.</li></ul><h3 id="how-32">How</h3><ul><li>First devise multi-scale skeleton graphs with coarse-to-fine human body partitions, which enables to model body structure and skeleton dynamics at multiple levels.</li><li>To mine inherent correlations between body components in skeletal motion, propose a multi-scale graph relation network to learn structural relations between adjacent body-component nodes and collaborative relations among nodes of different scales.</li><li>Propose a multi-scale skeleton reconstruction mechanism to enable to encode skeleton dynamics and high-level semantics from unlabeled skeleton graphs.</li><li>The input is several clips of 3D skeleton sequence. First train to get representation <span class="math inline">\(\mathrm{H}\)</span>, then take the frozen <span class="math inline">\(\mathrm{H}\)</span> and corresponding ID labels to train a MLP for person Re-ID.</li></ul><h4 id="ms-skeleton-graph-construction">MS skeleton graph construction</h4><ul><li><p>Goal: learn a latent discriminative representation <span class="math inline">\(\mathrm{H}\)</span> from skeleton sequences <span class="math inline">\(\mathrm{S}\)</span> without using any label.</p></li><li><p>regard body joints as the basic components, and merge spatially nearby groups of joints to be a higher level body-component node at the center of their positions. The graphs at joint-scale, part-scale and body-scale are denoted as <span class="math inline">\(\mathcal{G^1,G^2,G^3}\)</span> respectively.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924142204794.png" alt="image-20210924142204794" style="zoom:50%;" /></p></li><li><p>The hyper-joint-scale is implemented by interpolating nodes between adjacent nodes.</p></li></ul><h4 id="msgr">MSGR</h4><ul><li><p>Propose to learn relations of body components from two aspects: structural relations which provide a higher motion correlation than distant pairs, and collaborative relations that considers several action-related body components.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924222135548.png" alt="image-20210924222135548" /><figcaption aria-hidden="true">image-20210924222135548</figcaption></figure></li><li><p>Structural relation learning</p><ul><li>First compute the structural relation between adjacent nodes as follows: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924185808927.png" alt="image-20210924185808927" style="zoom:33%;" /></li><li>Then normalized relations with a temperature-based softmax (T-softmax) function to learn flexible structural relations. The higher value of temperature produces a softer relation distributed over nodes and retains more similar relation information.</li><li>The aggregate features of most relevant nodes to represent the node <span class="math inline">\(i\)</span>, in which both the <span class="math inline">\(v_j\)</span> and the relation matrix <span class="math inline">\(\mathrm{A}_{i,j}^m\)</span> are taken as the input.</li><li>The previous three steps are operated for <span class="math inline">\(P\)</span> times to obtain <span class="math inline">\(P\)</span> different structural relation matrices.</li><li>Then averagely aggregate features learned by these relation matrices to represent each node.</li></ul></li><li><p>Collaborative relation learning</p><ul><li>Unique walking patterns could be represented by the dynamic cooperation among body joints or between different body components.</li><li>Learn collaborative relations from two aspects: single-scale collaborative relations among nodes of the same scale, and cross-scale collaborative relations between a node and its spatially corresponding or motion-related higher level body component.</li><li>Use softmax and the nodes features at different scale to obtain collaborative relation matrix, still the T-softmax. But calculate the inner product of node feature representations to measure the degree of collaboration between two nodes.</li></ul></li><li><p>Multi-scale collaboration fusion</p><ul><li>Each node representation in the <span class="math inline">\(a\)</span>-th scale graph is updated by the feature fusion of collaborative nodes from different graphs as <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924222304571.png" alt="image-20210924222304571" /> where the <span class="math inline">\(\lambda_C\)</span> is the fusion coefficient to fuse collaborative graph node features.</li><li>graph representations of each individual scale is retained to encourage their model to capture skeleton dynamics and pattern information at different levels</li></ul></li></ul><h4 id="multi-scale-skeleton-reconstruction-mechanism">Multi-scale skeleton reconstruction mechanism</h4><ul><li>Pretext tasks for capturing skeleton graph dynamics and high-level semantics from different scales of graphs.</li><li>Skeleton subsequence reconstruction task: MSR aims to reconstruct target multi-scale skeletons corresponding to multi-scale graphs in subsequences, instead of reconstructing the original subsequences.</li><li>Cross-scale subsequence reconstruction task: exploit fine skeleton graph representations to infer 3D positions of coarser body components. E..g., use joint-level graph representations to infer nodes of body-scale skeletons.</li><li>To simultaneously achieve above two pretext tasks, first sample <span class="math inline">\(k\)</span>-length subsequences by randomly discarding <span class="math inline">\((f-k)\)</span> skeletons from the input sequence. The sampling process is repeated for <span class="math inline">\(r\)</span> rounds and each round cover all possible lengths form 1 to <span class="math inline">\(f-1\)</span>.<ul><li>Given a sampled skeleton subsequence, MGRN encodes its corresponding skeleton graphs of each scale into fused graph features as previously mentioned.</li><li>Then, leverage an LSTM to integrate the temporal dynamics of graphs at each scale into effective representations.</li><li>Last, use graph states at the <span class="math inline">\(𝑎^{𝑡ℎ}\)</span> scale and MLP to reconstruct the target skeleton at the <span class="math inline">\(b^{𝑡ℎ}\)</span> scale.</li><li>The reconstruction loss use <span class="math inline">\(\ell_1\)</span> norm.</li></ul></li></ul><h4 id="the-entire-framework">The entire framework</h4><ul><li>For the downstream task, extract encoded graph states (𝒉) learned from the pre-trained framework, and exploit an MLP <span class="math inline">\((g(\cdot))\)</span> to predict the sequence label .</li><li>For the <span class="math inline">\(t^{th}\)</span> skeleton in an input sequence, concatenate its corresponding encoded graph states of four scales as its skeleton-level representation of the sequence. Then train MLP with the frozen <span class="math inline">\(\mathrm{H}_t\)</span> and its label. The predicted label is <span class="math inline">\((g(\mathrm{H}_t))\)</span>.</li></ul><h2 id="paper-34-space-time-correspondence-as-a-contrastive-random-walk">Paper 34: <a href="https://arxiv.org/abs/2006.14613">Space-time correspondence as a contrastive random walk</a></h2><p>Codes: https://github.com/ajabri/videowalk</p><h3 id="why-32">Why</h3><ul><li>Challenge: temporal correspondence: a physical point depicted at position <span class="math inline">\((x,y)\)</span> in frame <span class="math inline">\(t\)</span> might not have any relation to what we find at that same <span class="math inline">\((x,y)\)</span> in frame <span class="math inline">\(t+k\)</span>.</li><li>Cycle-consistency of time is a promising direction.<ul><li>current methods rely on complex and greedy tracking that may lead to local optima, especially when applied recurrently in time.</li></ul></li></ul><h3 id="goal-31">Goal</h3><ul><li>self-supervised approach for learning a representation of visual correspondence from raw video, where correspondence means prediction of links in space-time graph constructed from video.</li><li>Take patches sampled from each frame as nodes, and nodes adjacent in time sahre a directed edge. The long-range correspondence is computed as a walk along the graph.</li><li>Optimize the representation to place high probability along paths of similarity.</li></ul><h3 id="how-33">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926103840998.png" alt="image-20210926103840998" /><figcaption aria-hidden="true">image-20210926103840998</figcaption></figure><ul><li>Turn training videos into palindromes: sequences where the first half is repeated backwards. View each step of the walk as a contrasstive learning problem where the walker's target provides supervision for entire chains of intermediate comparisons.</li></ul><h4 id="contrastive-random-walks-on-video">Contrastive random walks on video</h4><ul><li>An encoder <span class="math inline">\(\phi\)</span> maps nodes to <span class="math inline">\(\ell_2\)</span>-normalized <span class="math inline">\(d\)</span>-dimensional vectors. The similarity function is non-neative affinities, the whole function of encoder is based on <span class="math inline">\(T\)</span>-softmax, and the output is denoted as the stocastic matrix of affinities <span class="math inline">\(A_{t}^{t+1}\)</span>, which describes only the local affinity between the patches of two video frames <span class="math inline">\(\mathrm{q}_t,\mathrm{q}_{t+1}\)</span>. <span class="math inline">\(A_{t}^{t+1}\)</span> is then used to build transition probabilities of a random walk on the whole graph, which relates all nodes in the video as a Markov chain.<ul><li>The step of random Walker Can be viewed as performing tracking by contrastive similarity of neighboring nodes.</li><li>The long-range correspondence as walking multiple steps along the graph is <span class="math inline">\(\bar{A}_{t}^{t+k}=\prod\limits_{i=0}^{k-1}A_{t+i}^{t+i+1}=P(X_{t+k}|X_t)\)</span>.</li><li>They use resnet.</li></ul></li><li>Guiding the walk<ul><li>Aim: encourage the random Walker to follow paths of corresponding patches as it steps through time. ==&gt; <em>construct targets for free by choosing palindromes as sequences for learning (right)</em></li><li>Train: use labels to fit the embedding by maximizing the likelihood that a Walker beginning at a query node at <span class="math inline">\(t\)</span> ends at the target node at time <span class="math inline">\(t+k\)</span>. The loss is cross-entropy.</li><li>The walk is viewed as a chain of contrastive learning problems, providing supervision at every step amounts to maximizing similarity between query and target nodes adjacent in time, while minimizing similarity to all other neighbors.</li><li>By minimizing the loss function, they shift affinity to paths that link the query and target.</li></ul></li></ul><h4 id="ssl">SSL</h4><ul><li><p>Build labels by palindromes, where sequences that are identical when reversed, for which targets are known since the first and last frames are identical. Given a sequence of frames, they form training examples by simply concatenating the sequence with a temporally reversed version of itself, aka <span class="math inline">\((I_t,\cdots,I_{t+k})\rightarrow (I_t,\cdots,I_{t+k},\cdots I_t)\)</span>.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926115930588.png" alt="image-20210926115930588" /><figcaption aria-hidden="true">image-20210926115930588</figcaption></figure></li><li><p>The cycle-consistency objective is then built as <span class="math inline">\(\mathcal{L}_{cyc}^k=\mathcal{L}_{CE}(\bar{A}_t^{t+k}\bar{A}_{t+k}^t,I)=-\sum\limits_{i=1}^{N}\log P(X_{t+2k}=i|X_t=i)\)</span>.</p></li></ul><h4 id="edge-dropout">Edge dropout</h4><ul><li>Consider segments correspondence, where points within a segment might have strong affinity to all other points in the segment.==&gt; inspire a trivial extension, randomly dropping edges from the graph, thereby forcing the Walker to consider alternative paths.</li><li>Implementation: use dropout to the transition matrix <span class="math inline">\(A\)</span> and then re-normalize.</li><li>They argue that edge dropout improve object-centric correspondence.</li></ul><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926121026487.png" alt="image-20210926121026487" style="zoom:45%;" /></p><h2 id="paper-35-spatially-consistent-representation-learning">Paper 35: <a href="https://arxiv.org/abs/2103.06122">Spatially consistent representation learning</a></h2><p>Codes: https://github.com/kakaobrain/scrl</p><h4 id="why-33">Why</h4><ul><li>Previous SSL methods are prone to overlook spatial consistency of local representations and therefore have a limitation in pretraining for localization tasks such as object detection.</li><li>Aggresively cropped views used in existing contrastive methods can minimize representation distances between the semantically different regions of a single image.</li><li>Contrastive SSL aims to obtain discriminative representations based on the semantically positive and negative image pairs.</li><li>Most exisiting contrastive methods explot consistent global representations on a per image basis, they are likely to generate inconsistent local representations with respect to the same spatial regions after image transformations.<ul><li>Lead to performance degradation on localization tasks based on spatial representations.</li></ul></li><li>Similar previous work<ul><li>SwAV: introduce cluster assignment and swapped prediction, also propose multi-crop where they compare multiple smaller patches cropped from the same image, but substantially different in that the spatial consistency on the feature map is not directly considered.</li><li>BYOL removes the necessity of negative pairs and have shown to be more robust against changes in batch size.</li><li>SSL methods to leverage geometric correspondence are limited to dense representation irself that comes with object structure learning.</li></ul></li></ul><h4 id="goal-32">Goal</h4><ul><li>Propose SCRL (spatially consistent representation learning) for multi-object and location-specific tasks.</li><li>Devise a new SSL objectIve that tries to produce coherent spatial representations of a randomly cropped local region according to geometric translations and zooming operations.</li><li>SCRL can possibly sample infinite number of pairs by pooling the variable-sized random regions with bilinear interpolation. VADeR is a specific instance of their methods where the sizes of the pooled box is fixed to a single pixel without a sophisticated pooling technique.</li></ul><h4 id="how-34">How</h4><ul><li>From a positive pair of cropped feature maps, apply RoIAlign to the respective maps and obtain equally-sized local representations. Optimize the encoding network to minimize the distance between these two local representations. Adpat BYOL learning framework.</li></ul><h4 id="spatially-consistent-representation-learning">Spatially consistent representation learning</h4><ul><li>Use two network, the online network parameterized by <span class="math inline">\(\theta\)</span> and target network parameterized by <span class="math inline">\(\xi\)</span>. The target network provides the regression target to train the online network while <span class="math inline">\(\xi\leftarrow\tau\xi+(1-\tau)\theta\)</span> (an exponential moving average with a decay parameter <span class="math inline">\(\tau\)</span>).</li><li><span class="math inline">\(\mathbb{T}_1,\mathbb{T}_2\)</span> denote two sets of image augmention strategies and then generate two augmented views <span class="math inline">\(v_1 =t_1(I),v_2=t_2(I)\)</span>. Then <span class="math inline">\(v_1,v_2\)</span> are respectively fed into the two encoder networks <span class="math inline">\(f_\theta,f_\xi\)</span> (without GAP layer) to obtain spatial feature maps <span class="math inline">\(m_1,m_2\)</span>.</li><li>To find spatial correspondence, first find the intersection regions, use IoU to reject a candidate box.</li><li>Then randomly sample an arbitrary box <span class="math inline">\(B=(x,y,w,h)\)</span> in intersection region, the box has to be translated to the coordinates in each view, denoted as <span class="math inline">\(B_1,B_2\)</span>.<ul><li>Though the size, location, and internal color of each box may be different for each views, the semanttic meaning in the cropped box area does not change between <span class="math inline">\(B_1,B_2\)</span>.</li><li>The box are rectangular box, and to map one box to another box after geometrical transformations, exclude certain affine transformations such as shear operations and rotations.</li><li>To obtain the equally-sized local representations from both boxes, crop the corresponding sample regions, called RoI, on the spatial feature maps and locally pool the cropped feature maps by <span class="math inline">\(1\times 1\)</span> RoIAlign.</li><li>Sample <span class="math inline">\(K\)</span> boxes on a given spatial feature map, one image can generate multiple pairs of local representations <span class="math inline">\(p_i^k\)</span>, where <span class="math inline">\(i\)</span> denotes the view (<span class="math inline">\(i=1,2\)</span>), <span class="math inline">\(k\)</span> is the number of sampled boxes.</li></ul></li><li>In the online network (view 1), then perform the projection from <span class="math inline">\(p_1^k\)</span> to get <span class="math inline">\(z_1^k\)</span>, followed by the prediction <span class="math inline">\(q_\theta(z_1^k)\)</span>, and the target network gets <span class="math inline">\(z_2 ^k\)</span>.</li><li>The spatial consistency loss is MSE between the normalized prediction <span class="math inline">\(\bar{q_\theta(z_1^k)}=\frac{q_\theta(z_1^k)}{\|q_\theta(z_1^k)\|^2}\)</span>and the normalized target prediction <span class="math inline">\(\bar{z_2^k}=\frac{z_2^k}{\|z_2^k\|^2}\)</span>.<ul><li>To symmetrize the loss, also feed <span class="math inline">\(v_2\)</span> to the online network and <span class="math inline">\(v_1\)</span> to the target network respectively.</li></ul></li><li>use resnet as the backbone.</li></ul><h2 id="paper-36-spatiotemporal-contrastive-video-representation-learning">Paper 36: <a href="https://arxiv.org/abs/2008.03800">Spatiotemporal contrastive video representation learning</a></h2><p>Codes: https://github.com/tensorflow/models/tree/master/official/</p><h3 id="why-34">Why</h3><ul><li>Traditional hand-crafted local invariant features for images have their counterparts in videos.</li><li>The long-standing pursuit after temporal cues for self-supervised video representation learning has left selfsupervision signals in the spatial subspace under-exploited for videos.</li><li>Simply applying spatial augmentation independently to video frames actually hurts the learning because it breaks the natural motion along the time dimension</li><li>a pair of positive clips that are temporally distant may contain very different visual content, leading to a low similarity that could be indistinguishable from those of the negative pairs.</li><li>The recent wave of contrastive learning shares a similar loss objective as instance discrimination.</li><li></li></ul><h3 id="goal-33">Goal</h3><ul><li>Propose CVRL (contrastIve video representation learning) to learn spatiotemporal visual representations from unlabeled videos.</li><li>There are two augmented clips used for contrastive learning, augmented by their proposed temporally consistent spatial augmentation method and sampling-based temporal augmented method.</li></ul><h3 id="how-35">How</h3><ul><li>construct positive pairs as two augmented video clips sampled from the same input video.</li><li>CVRL is more effective with larger networks.</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927135417172.png" alt="image-20210927135417172" /><figcaption aria-hidden="true">image-20210927135417172</figcaption></figure><h4 id="video-representation-learning-framework">Video representation learning framework</h4><ul><li>Use InfoNCE loss, which allows the postitive pair <span class="math inline">\((z_i,z_i&#39;)\)</span> to attract mutually while they repel the other items in the mini-batch.</li><li>Components: An encoder maps an input video clips to its representation <span class="math inline">\(z\)</span>, spatiotemporal augmentations to construct positive pairs <span class="math inline">\((z_i,z_i&#39;)\)</span> and the properties they induce, and methods to evaluate the learned representations.</li></ul><h4 id="video-encoder">Video encoder</h4><ul><li>Encode a video sequence using 3D-ResNets as backbones.<ul><li>Only two changes on original 3D-ResNets: the temporal stride of 2 in the data layer, and the temporal kernel size of 5 and stride of 2 in the first CNN layer.</li><li>The video representation will be a 2048-dimensional feature vector.</li></ul></li><li>The add a multi-layer projection head (MLP) onto the backbone to obtain the encoded 128-dimensional feature vector <span class="math inline">\(z\)</span>.</li><li>During evaluation, the MLP is dropped.</li></ul><h4 id="data-augmentation-1">Data augmentation</h4><ul><li><p>Temporal augmentation: The main motivation is that two clips from the same video would be more distinct when their temporal interval is larger.</p><ul><li>If sample temporally distanct clips with smaller probabilities, the contrastive loss would focus more on the temporally close clips, pulling their features closer and imposing less penalty over the clips that are far away in time.==&gt; aligning lower sampling probability on larger temporal intervals.</li><li>Steps: First draw a time interval <span class="math inline">\(t\)</span> from a distribution <span class="math inline">\(P(t)\)</span> over <span class="math inline">\([0，T]\)</span>, then uniformly sample a clip from <span class="math inline">\([0，T-t]\)</span>, followed by the second clip which is delayed by <span class="math inline">\(t\)</span> after the first.</li><li>The descreasing distributions generally perform better than the unifom or increasing ones.</li></ul></li><li><p>Spatial augmentation: With fixed randomness across frames, the 3D video encoder is able to better utilize spatiotemporal cues.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927135237680.png" alt="image-20210927135237680" /><figcaption aria-hidden="true">image-20210927135237680</figcaption></figure></li></ul><h4 id="evaluation">Evaluation</h4><ul><li>evaluate the learned video representations by fixing the weights in the pre-trained video encoder and training a linear classifier on top of it.</li></ul><h2 id="paper-37-sportscap-monocular-3d-human-motion-capture-and-fine-grained-understanding-in-challenging-sports-videos">Paper 37: <a href="https://arxiv.org/abs/2104.11452">SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</a></h2><p>Codes: https://github.com/ChenFengYe/SportsCap</p><h3 id="why-35">Why</h3><ul><li>Most existing action understanding solutions are limited to the pure high-level action assessment, where the abundant 3D motion capture information of sub-motions has been ignored.</li><li>Previous methods on pose and shape estimation do not consider rich semantic information embedded in sports, and the structure constraints within sub-motions.<ul><li>They use the underlying semantic and ordering rules in sport to reduce the complexity of the problem, and use PCA to capture the similarities of poses in each sub-motion and constrain estimated poses in reasonable forms.</li><li>Previous methods rely on 2D poses under-estimate the complex sports actions.</li></ul></li><li>Action parsing<ul><li>Competitive sports are a mixture of long and short dynamics.</li><li>Approaches rely on joint motions ignore the patterns of the human body in certain activities.</li></ul></li><li>Dataset: To their knowledge, the SMART dataset is the only one that provides the fine-grained semantic labels, 2D ad 3D annotated poses, and assessment information.</li></ul><h3 id="goal-34">Goal</h3><ul><li>Monocular 3D Human Motion Capture on sports videos, use the semantic and temporally structured sub-motion prior in the embedding space. Propose a motion embedding module to recover both the implict motion embedding and explicit 3D motion details via a corresponding mapping function and a sub-motion classifier.</li><li>reconstruct both the 3D human motion and the corresponding fine-grained action attributes from monocular professional sports video.</li><li>The approach will provide sub-actions, action types and rotation angles .</li></ul><h3 id="how-36">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926221807606.png" alt="image-20210926221807606" /><figcaption aria-hidden="true">image-20210926221807606</figcaption></figure><ul><li>The fine-grained semantic action attributes are predicted by ST-GCN, use both the original motion embedding stream and the recovered 3D detailed motion stream of the whole video clip.</li><li>The model comprises of two modules<ul><li>Motion embedding module: Use PCA to extract embedding prior, the estimate the per-frame motion embedding parameters (pose, shape) so as to recover the 3D motion details. The embedding module consists of a sub-motion classifier (WS-DAN) , a CNN encoder to regress the embedding and the mapping function from the embedding space to the 3D motion output.</li><li>Actions parsing module: use ST-GCN, input both the implicit pose embedding and the explicit 3D joints. Finally map the predicted attributes to the final action label by a semantic attribute mapping block.</li></ul></li></ul><h4 id="motion-embedding-and-capturing">Motion embedding and capturing</h4><ul><li>Split one action into sub-motions. In each sub-motion, poses exhibit high resemblance across athletes.</li><li>The sub-motion classifier is WS-GAN.</li><li>Then use SMPL (skinned multi-person linear model) to build the motion embedding space, which represents the pose parameters as <span class="math inline">\(\theta\)</span>.<ul><li>Use pose coefficient <span class="math inline">\(\boldsymbol{\alpha}\)</span> to leverage the rich semantic and temporally structural prior of sub-motions in the motion embedding space. <span class="math inline">\(\boldsymbol\theta=\sum\limits_{k=1}^K\alpha_k\boldsymbol{b}_k^m+\boldsymbol{a}^m\)</span>, where <span class="math inline">\(\boldsymbol{a}^m\)</span> is the mean of pose parameters, and <span class="math inline">\(\boldsymbol{\alpha}\)</span> are the pose coefficients.</li></ul></li><li>Then use PCA on pose parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>, the <span class="math inline">\(\boldsymbol{b}_k^m\)</span> in above are PCs, and <span class="math inline">\(\boldsymbol{a}^m\)</span> denotes the mean, and <span class="math inline">\(\alpha\)</span> means the coefficient assigned by std.</li><li>The module consists of a resnet-152 encoder followed by two FCC to regress the pose coefficients <span class="math inline">\(\boldsymbol{\alpha}\)</span> and then reconstruct joint positions<ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926224512111.png" title="fig:" alt="image-20210926224512111" /></li><li>Use this network to estimate pose coefficients, shape parameters and camera parameters from images. Then recover 3D human body meshes from estimated pose and shape parameters of SMPL.</li></ul></li></ul><h4 id="action-parsing">Action parsing</h4><ul><li><p>Tasks include inferring semantic meaningful labels and the action number (code ) from sports videos.</p></li><li><p>Use SAs (semantic attributes, are learnt by ST-GCN ) to represent the semantic meaningful label. The action number represents a valid combination of SAs.</p><ul><li><p>SAs indicate the specific number of a motion (the number of frames for one type of motion?), like the rotation angle, take-off type and so on.</p></li><li><p>For one sequence, there maybe are many types of motions that intersect together?</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926233345530.png" alt="image-20210926233345530" /><figcaption aria-hidden="true">image-20210926233345530</figcaption></figure></li></ul></li><li><p>ST-GCN for extract SAs.</p><ul><li>J- and B-Stream use 2s-AGCN.</li><li>In P-stream, represent pose coefficients as a 1D vector and use 1DCNN with residual to generate features.</li></ul></li><li><p>Semantic attributes mapping block</p><ul><li>Learn the mapping between the extracted ST features and the final action label.</li><li>Goal: partition the whole action sequence in terms of SAs, or to say how individual frames contribute to specific SAs.</li><li>Use two FCs to predict their contributions, then stack the resulting SAs and feed the results to another 2 layer FCs to infer the action number.</li></ul></li></ul><h4 id="multi-task-training">Multi-task training</h4><ul><li>Loss for motion embedding module<ul><li>The prior loss: <span class="math inline">\(\mathcal{L}_{prior}=\|\mathrm{W}(\bar{\alpha}-\hat{\alpha})\|_2\)</span>, where <span class="math inline">\(\bar{\alpha}\)</span> is the mean of pose coefficients in training set, and <span class="math inline">\(\hat{\alpha}\)</span> is the predicted pose coefficients.</li><li>The data loss: On 2D pose, <span class="math inline">\(\mathcal{L}_{data}=\|\mathrm{V(J-\hat{J})}\|_2\)</span>, where <span class="math inline">\(\mathrm{J}\)</span> is the ground truth, and <span class="math inline">\(\hat{\mathrm{J}}\)</span> is the estimated 2D joints, which are projected from estimated 3D points. <span class="math inline">\(\mathrm{V}\)</span> denotes the visibility of the ground truth joint.</li><li>SMPL loss: <span class="math inline">\(\mathcal{L}_{smpl}=\|\theta-\hat{\theta}\|_2+\|\beta-\hat{\beta}\|_2\)</span>, where <span class="math inline">\(\theta,\beta\)</span> are the supervision of pose/shape parameters.</li><li>The final loss is the combination of previously listed 3 losses.</li></ul></li><li>Loss for action parsing module<ul><li>Use the cross-entropy between the predicted and the ground truth attributes.</li><li>The acion labeling task also use the cross-entropy loss.</li><li>The final loss in this module is the combination of three losses.</li></ul></li><li>Training strategy:<ul><li>Stage 1: train a motion embedding module for each sub-motion independently and fix its parameters</li><li>Stage 2: Then train the action attribute prediction and label classification modules in the action parsing module jointly.</li><li>Stage 3: Fine tune the entire network using the combined losses from two modules.</li></ul></li></ul><h2 id="paper-38-ssan-separable-self-attention-network-for-video-representation-learning">Paper 38: <a href="https://arxiv.org/abs/2105.13033">SSAN: Separable Self-Attention Network for Video Representation Learning</a></h2><h3 id="why-36">Why</h3><ul><li>Self-attention succeed in video representation learning due to its ability of modeling long range dependencies. Existing methods build the dependencies merely by the pairwise correlations along spatial and temporal dimensions simultaneously.<ul><li>However, spatial correlations and temporal correlation represent different contextual information of scenes and temporal reasoning.</li><li>Learning spatial contextual information will benefit temporal modeling.</li></ul></li><li>Learning strong and generic video representations is still challenging.<ul><li>Videos contain rich semantic elements within individual frames, but also the temporal reasoning across time.</li><li>The long-range dependencies among pixels cannot be well-cared by CNN.</li><li>RNN does, but sufferes from the high computation cost, and it cannot establish the direct pairwise relationship between positions regardless of their distance.</li></ul></li><li>The correlations from space and time represent different contextual information. The former often relates to scenes and objects, and the latter often relates to temporal reasoning for actions (short-term) and events (long-term).<ul><li>Learning correlations along spatial and temporal dimensions together might capture irrelevant information, leading to the ambiguity for action understanding</li><li>Short-term dependencies should also be considered for capturing episodes of complex activities.</li></ul></li><li>Video learning networks<ul><li>Mainly two branches for video learning architectures: 2D based methods and 3D based methods.</li><li>But 3D based methods suffer from the overhead of parameters and complexity, while 2D based methods need careful temporal feature aggregation.</li></ul></li><li>Action recognition<ul><li>video classification, early works try to use 2D based methods to video, the 3DCNN are used. But huge computation cost.</li><li>3D based methods always take several consecutive frames as input, so that videos with complex actions cannot be well handled.</li><li>2D convolution networks with temporal aggregation achieve significant improvements.</li></ul></li><li>Self-attention<ul><li>The full connections among pixels also introduces irrelevant information.</li><li>The 3D based self-attention can model long dependencies from space and time simultaneously, but such dependencies are first-order correlations which mainly capture the similarity between single pixels, not semantic-level correlations.</li><li>Non-local methods consider more on position-wise correlations but less on channel-wise correlations.</li></ul></li><li>Visual-language learning<ul><li>VideoBERT: propose to use a visual-linguistic model to learning high-level features without any explicit supervision.</li><li>Sun et al.: use contrastive bidirectional transformer (CBT) to perform SSL for video representations.</li><li>UniViLM: propose a joint video and language pre-training scheme by adding generation tasks in the pre-training process.</li><li>These methods only focus on the training of the transformer encoder and decoder, while take the video networks as feature extractors.</li></ul></li></ul><h3 id="goal-35">Goal</h3><ul><li>Propose separable self-attention (SSA) module, which models spatial and temporal correlations sequentially.</li><li>The proposed SSA learn spatial self-attention firstly. The attention maps are then aggregated along temporal dimension and sent to temporal attention module.</li></ul><h3 id="how-37">How</h3><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927200926592.png" alt="image-20210927200926592" style="zoom:67%;" /></p><h4 id="self-attention-in-vision">Self-attention in vision</h4><ul><li>If videos <span class="math inline">\(X\)</span> in <span class="math inline">\(T\)</span> frames, in size <span class="math inline">\(H,W,C\)</span>, then reshape the embedding for query, key and value to size <span class="math inline">\(THW\times C,C\times THW,THW \times C\)</span>, respectively, denoted by <span class="math inline">\(X_q,X_k,X_v\)</span>.</li><li>Then the attention map Y is <span class="math inline">\(Y=softmax(X_q\times X_k)\times X_v\)</span> in size <span class="math inline">\(THW \times C\)</span>.</li><li>Then it is transformed by <span class="math inline">\(1\times 1\times 1\)</span> convolution and added back to the original query feature <span class="math inline">\(X\)</span>. <span class="math inline">\(Z=W_z(Y)+X\)</span>.</li></ul><h4 id="separable-self-attention-module">Separable Self-attention module</h4><ul><li>The spatial and temporal attention are performed sequentially, so that temporal correlations can fully consider the spatial contexts. Second, sptial attention maps exploit as much context information as possible.</li><li>First use 2D <span class="math inline">\(1\times 1\)</span> convolutions to map input feature <span class="math inline">\(X\)</span> into <span class="math inline">\(X_q,X_v,X_k\)</span>.</li><li>After spatial attention, the 4D intermediate attention map <span class="math inline">\(\hat{X}\)</span> (after spatial attention) is then transformed to temporal embeddings <span class="math inline">\(\hat{X}_q,\hat{X}_v\)</span> using <span class="math inline">\(3\times1\times 1\)</span> convolution. The temporal attention still use same <span class="math inline">\(X_v\)</span> (same as what's used in spatial attention ).</li><li>Spatial attention<ul><li>Both position-wise and channel-wise attention, so two similarity matrices <span class="math inline">\(M_s,M_c\)</span> are generated.</li><li>The final spatIAL maps for time <span class="math inline">\(t\)</span> is <span class="math inline">\(\hat{X_t}=(M_s\times X_{v(s)}^t)+(M_s\times X_{v(c)}^t)\)</span>, and the intermediate attention map <span class="math inline">\(\hat{X}=Cat[\hat{X}_0,\cdots,\hat{X}_T]\)</span>.</li></ul></li><li>Temporal attention<ul><li><span class="math inline">\(3\times1\times 1\)</span> convolution alloes temporal fusion on spatial attention maps and builds the short range correlations along temporal dimension.</li></ul></li></ul><h4 id="network-architecture">Network architecture</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927203812177.png" alt="image-20210927203812177" /><figcaption aria-hidden="true">image-20210927203812177</figcaption></figure><ul><li>Choose TSN as the baseline, insert SSA module into different layers to establish separable self-attention network (SSAN).</li></ul><h2 id="paper-39-tdgraphembed-temporal-dynamic-graph-level-embedding">Paper 39: <a href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411953">tdgraphembed: Temporal dynamic graph-level embedding</a></h2><p>Codes: https://github.com/moranbel/tdGraphEmbed</p><h3 id="why-37">Why</h3><ul><li>Existing graph embedding methods focus on capturing the graph's nodes in a static mode and/or not model the graph in its entirety in temporal dynamic mode.</li><li>To the best of their knowledge, the novel task of embedding entire graphs into a single embedding in temporal domains has not been addressed yet.</li><li>Graph representation learning on Static graph context:<ul><li>Matrix factorization: like SVM, LLE, decompose the Laplacian or higher-order adjacency matrix to produce node embedding that preserves the distance between ndoes.</li><li>Random walk based methods: create random path over the graph and use word2vec-like architecture for the node embedding task</li><li>Deep learning based methods: like autoencoders, GCN, GAEs</li></ul></li><li>Unsupervised graph embeddings<ul><li>Graph2vec: embed the entire graph in a static mode and use an analogy between graphs and documents, take the rooted subgraphs around every node as the context terms that make up the document</li><li>Sub2vec, unsupervised, for arbitrary static subgraphs using gone random walk</li><li>UGraphEmbed: graph-level representation, the embeddings of two graphs preserve their graph-graph proximity based on GED measure.</li><li>But they encode the graph without considering the historical contexts.</li></ul></li><li>Temporal node/edge embedding methods<ul><li>generate node embeddings at each time step of the graph using static embedding, and then align them to from a unified representation</li><li>DynGEM: use the learned embeddings from the previous graph to initialize the representation at current time.</li><li>Dyngraph2vec: recurrent layers to learn patterns over sequence of graphs.</li><li>They don't take the fact that the number of nodes changes over time into consideration.</li><li>DyRep: use two time scaling process that captures temporal node interactions ad topological evolution, but only supports the network's growth; DynamicTriad: analyze every two consecutive timesteps to create the embedding. ==&gt; But not graph-level embedding.</li></ul></li></ul><h3 id="goal-36">Goal</h3><ul><li>Temporal dynamic graphs, propose tdGraphEmbed to learn temporal graph-level embedding unsupervisedly, which is an extension of random walk that globally embed both the nodes of the graph and its representation at each step.</li></ul><h3 id="how-38">How</h3><ul><li>The proposed method embeds the entire graph at timestamp <span class="math inline">\(t\)</span> into a single vector <span class="math inline">\(G_t\)</span>. At each timestamp <span class="math inline">\(t\)</span>, it performs <span class="math inline">\(\gamma\)</span> random walks of length <span class="math inline">\(L\)</span> from each node in the graph. Then jointly learn the node embeddings <span class="math inline">\(v\)</span> along with the entire graph embedding <span class="math inline">\(G_t\)</span>.</li><li>The optimization becomes predicting a node embedding given the node's context in the random walks at time <span class="math inline">\(t\)</span> of the graph and the vector <span class="math inline">\(G_t\)</span>.</li></ul><h4 id="random-walk">Random walk</h4><ul><li>Modeling each graph as a sequence of sentences, and then model the graph's sentences using random walks.<ul><li>Denote a random walk initiated at node <span class="math inline">\(v_i\)</span> as <span class="math inline">\(\mathcal{W}_{v_i}\)</span>. Then a random walk is a stochastic process with random variables <span class="math inline">\(\mathcal{W}_{v_i,1},\mathcal{W}_{v_i,2},\cdots,\mathcal{W}_{v_i,k}\)</span> such that <span class="math inline">\(\mathcal{W}_{v_i,k+1}\)</span> is a node chosen at random from the neighbors of node <span class="math inline">\(W_{v_i,k}\)</span>.</li><li>Implement a second order random walk with a return parameter <span class="math inline">\(p\)</span> and an in-out parameter <span class="math inline">\(q\)</span> to guide the walk.<ul><li>Use <span class="math inline">\(p,q\)</span> to adjust the transition probability <span class="math inline">\(\alpha_{pq}(u,x)\)</span> from node <span class="math inline">\(u\)</span> to some node <span class="math inline">\(x\)</span>: <span class="math inline">\(\alpha_{pq}(u,x)=\begin{cases}\frac{1}{p} &amp; if d_{ux}=0\\1 &amp; if d_{ux}=1\\ \frac{1}{q}&amp; if d_{ux}=2\end{cases}\)</span>, where <span class="math inline">\(d_{ux}\)</span> indicates the distance between node <span class="math inline">\(u\)</span> and node <span class="math inline">\(x\)</span>.</li><li>Then this node2vec can bias the random walk closer or further away from the source node, creating different embedding types.</li><li>Precisely, <span class="math inline">\(p&lt;q\)</span> biases the random walk to noes closer to each other, then nodes from the same cluster to be embedded closer and nodes from different regions to be embedded further away; <span class="math inline">\(p&gt;q\)</span> biases the random walk to embed nodes of the same graph characteristics closer together while others are embedded further away.</li></ul></li></ul></li><li>Then the goal is to estimate the likelihood of observing <span class="math inline">\(G_t\)</span> in its entirety given all random walks at time <span class="math inline">\(t\)</span>, defined as <span class="math inline">\(Pr(G_t|(\mathcal{W}_{v_1},\mathcal{W}_{v_2},\cdots,\mathcal{W}_{v_k}))\)</span>.</li></ul><h4 id="framework">Framework</h4><ul><li>Each node is initialized by <span class="math inline">\(\gamma\)</span> random walks of length <span class="math inline">\(\mathcal{L}\)</span>.</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928123213933.png" alt="image-20210928123213933" /><figcaption aria-hidden="true">image-20210928123213933</figcaption></figure><ul><li><p>After the step shown in figure 2, the goal is to predict the next node in a random walk. Take context nodes (sampled from a sliding window <span class="math inline">\(\omega\)</span> over random walks), also known as nodes in the neighborhood of the root node, and denote as <span class="math inline">\(N_s(v_i^t)\)</span>.</p></li><li><p>To learn the representation map <span class="math inline">\(\phi\)</span> that maps a given node and graph to a <span class="math inline">\(d\)</span>-dimensional space, then maximize <span class="math inline">\(\log p(v_i^t|N_s(v_i^t),G_t)\)</span> for each node in <span class="math inline">\(V_t\)</span>, which is defined by softmax. <span class="math inline">\(\log p(v_i^t|N_s(v_i^t),G_t)=\frac{\exp (\phi(v_i^t)\cdot h)}{\sum_{j\in V_t}\exp(\phi(v_j^t)\cdot h)}\)</span>, where <span class="math inline">\(h\)</span> is the combination of mapped node embedding and mapped graph embedding <span class="math inline">\(G_t\)</span>, and map means <span class="math inline">\(\phi\)</span>.</p></li><li><p>The graph’s time index can be thought of as another node which acts as a memory for the global context of the graph.</p></li><li><p>Finally, the objective is</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928124909064.png" alt="image-20210928124909064" /><figcaption aria-hidden="true">image-20210928124909064</figcaption></figure></li></ul><h4 id="tips">Tips</h4><ul><li>To decrease computation cost, use negative sampling for the denominator of softmax.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928130901448.png" title="fig:" alt="image-20210928130901448" /></li></ul><h2 id="paper-40-videomoco-contrastive-video-representation-learning-with-temporally-adversarial-examples">Paper 40: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf">Videomoco: Contrastive video representation learning with temporally adversarial examples</a></h2><h3 id="why-38">Why</h3><ul><li>The evolution of contrastive learning heavily focuses on feature representations from static images while leaving the temporal video representations less touched.<ul><li>Large scale video data is difficult to store in memory.</li><li>Attempts on unsupervised video representation learning focus on proposing pretext tasks related to a sub-property of video content.</li></ul></li><li>MoCo treats the contribution of keys from the queue only based on their representation. But in fact, the longer the keys in the queue, the more different their representations are compared to those of the current input samples.</li></ul><h3 id="goal-37">Goal</h3><ul><li>An extension of MoCo to learn video representation unsupervisedly.</li><li>Two modifications given a video sequence as a sample<ul><li>Introduce a generator to drop out several frames from the sample temporally.==&gt; by dropping during adversarial training, force the input sample to train a temporally robust encoder.</li><li>Use temporal decay to model key attenuation in the memory queue when computing contrastive loss. As the momentum encoder updates after keys enqueue, the representation ability of these keys degrades when we use the current input sample for contrastive learning.</li></ul></li><li>VideoMoCo utilizes color information and performs instance discrimination without bringing empirical pretext tasks.</li></ul><h3 id="how-39">How</h3><ul><li>Introduce adversarial learning to improve the temporal robustness of the encoder<ul><li>Use a generator to adaptively drop out several frames</li><li>And the dropped samples are sent to discriminatory for differentiating.</li><li>After adversarial training, only D is kept to extract temporally robust features.</li></ul></li><li>The sample after dropping out some frames is then taken as a query sample and perform contrastive learning with keys in the memory queue.<ul><li>Model the degradation of keys by proposing a temporal decay. If a key stays longer in the queue, its contribution is less (manipulated by the decay).</li></ul></li></ul><h4 id="moco">MoCo</h4><ul><li>The contrastive loss of MoCo <span class="math inline">\(L_q=-\log \frac{\exp (q\cdot k_+/\tau)}{\sum\limits_{i=0}^K\exp(q\cdot k_i/\tau)}\)</span>, which tends to classify <span class="math inline">\(q\)</span> as <span class="math inline">\(k_+\)</span>. The query <span class="math inline">\(q\)</span> is the representation of an input sample via the encoder network, while the keys <span class="math inline">\(k_i\)</span> are the representations of the other training samples in the queue.</li><li>The queue follows FIFO scheme. Denote the parameters of an encoder as <span class="math inline">\(\theta_q\)</span> and those of a momentum encoder as <span class="math inline">\(\theta_k\)</span>. The momentum encoder is updated as <span class="math inline">\(\theta_k\leftarrow m\theta_k+(1-\theta)\theta_q\)</span>, where <span class="math inline">\(m\)</span> is a momentum coefficient.</li><li>For VideoMoCo, give an input video clip with a fixed number of frames, the model sent it to a generator G and the encoder D to produce <span class="math inline">\(q\)</span>. Meanwhile, reweigh <span class="math inline">\(\exp(q\cdot k_i/\tau)\)</span> by <span class="math inline">\(t^i,t\in(0,1)\)</span>.</li></ul><h4 id="temporally-adversarial-learning">Temporally adversarial learning</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928142136488.png" alt="image-20210928142136488" /><figcaption aria-hidden="true">image-20210928142136488</figcaption></figure><ul><li>G, consisting of ConvLSTM, takes <span class="math inline">\(x\)</span> as input and produces a temporal mask. The mask will help drop out <span class="math inline">\(25\%\)</span> of the frames.</li><li>Regard the encoder of MoCo as the discriminator D, and take <span class="math inline">\(X^{query},x\)</span> as the input of D.</li><li>Tips for training: Initially, we train D without using G and only use contrastive learning. When D is learned to approach a semi-stable state, we train D by involving G. We empirically observe that <strong>utilizing adversarial learning at the initial stage makes D difficult to converge.</strong></li></ul><h4 id="temporal-decay">Temporal decay</h4><ul><li><p>For a key <span class="math inline">\(k_i\)</span> in the queue, set its corresponding temporal decay as <span class="math inline">\(t^i\)</span> where <span class="math inline">\(t\in(0,1)\)</span>, and <span class="math inline">\(i\)</span> gradually increases by 1 and <span class="math inline">\(t^i\)</span> decreases correspondingly.</p></li><li><p>Then with <span class="math inline">\(t^i\)</span>, the real loss function for the discriminator D is</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928143615211.png" alt="image-20210928143615211" style="zoom:50%;" /></p></li></ul><h4 id="visualization">Visualization</h4><ul><li>After obtaining classification scores, use entropy to measure the classifier's confidence when making the prediction. A high entropy value means the classifier is more uncertain on making the current predictions.</li><li>Even though both MoCo and VideoMoCo can resist temporally occluded video sequences, the feature representation of MoCo has become very fragile while that of VideoMoCo does not degrade significantly.</li><li>The features learned from VideoMoCo attend the network to the temporal motions.<ul><li><font color="red">How to calculate attention maps?</font></li></ul></li></ul><h2 id="paper-41-visual-relationship-forecasting-in-videos">Paper 41: <a href="https://arxiv.org/abs/2107.01181">Visual Relationship Forecasting in Videos</a></h2><h3 id="why-39">Why</h3><ul><li>Current efforts mostly focus on detecting the object interactions based on observation rather than reasoning about the spatiotemporal connections among predicates.</li><li>VRF requires additional spatiotemporal modeling among time series. This task emphasizes the importance of visual relationship reasoning.</li><li>Future prediction<ul><li>generating future frames: usually generative models</li><li>predicting future labels or states: multiple methods, VRF can be classified into this group. It performs forecasting on the unknown future frames, and it focuses on the relationship forecasting among a specific pair of <subject-object>.</li></ul></li></ul><h3 id="goal-38">Goal</h3><ul><li>Deal with object interactions in unknown future. Propose VRF (visual relationship forecasting, a new task) in videos to explore the prediction of visual relationships in a reasoning manner.<ul><li>Formally, given a subject-object pair with <span class="math inline">\(H\)</span> existing frames, it aims to predict their future interactions for the next <span class="math inline">\(T\)</span> frames without visual evidence.</li><li>Propose a graph convolutional transformer (GCT) framework, which captures both object-level and frame-level dependencies by ST-GCN (object-level ) and transformer (frame-level) .</li></ul></li></ul><h3 id="how-40">How</h3><ul><li>Propose GCT</li></ul><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928172156560.png" alt="image-20210928172156560" /><figcaption aria-hidden="true">image-20210928172156560</figcaption></figure><h4 id="framework-1">Framework</h4><ul><li>GCT can be divided into three parts: feature representation, object-level reasoning, and frame-level reasoning.</li><li>In feature representation module, use visual feature, semantic feature and spatial features. For spatial reasoning, a spatial graph is constructed for every key fame and the corresponding GCN is taken to model the interactions between the given <subject-object> pairs.</li><li>Then, use a multi-head transformer for frame-level reasoning.</li></ul><h4 id="feature-representation">Feature representation</h4><ul><li>Visual features: Use the bounding boxes of one instance, <span class="math inline">\(b_s,b_{so},b_o\)</span> indicate the subject, predicate and object bounding box. Use Inception-ResNet-V2 as the backbone and extract the features of these bounding boxes from the fully connected layers.</li><li>Spatial features: Get the relative spatial feature of bounding boxes, adopt the idea of box regression.<ul><li><span class="math inline">\(\Delta(b_i,b_j)\)</span> denote the box delta that regresses the bounding box <span class="math inline">\(b_i\)</span> to <span class="math inline">\(b_j\)</span>, <span class="math inline">\(dis(b_i,b_j)\)</span> and <span class="math inline">\(iou(b_i,b_j)\)</span> denote the normalized distance and IoU between <span class="math inline">\(b_i,b_j\)</span>, and the union region of <span class="math inline">\(b_i,b_j\)</span> is denoted as <span class="math inline">\(b_{ij}\)</span>.</li><li>The relative spatial location of subject and object can be defined as: <span class="math inline">\(l_{ij}=[\Delta(b_i,b_j);\Delta(b_i,b_{ij});\Delta(b_j,b_{ij});iou(b_i,b_j);dis(b_i,b_j)]\)</span>.</li></ul></li><li>Semantic features: Adopt a semantic embedding layer to map the object category <span class="math inline">\(c\)</span> into word embedding <span class="math inline">\(s\)</span>.<ul><li>The parameters of object categories are initialized with the pre-trained word representations such as word2vec.</li></ul></li></ul><h4 id="object-level-reasoning">Object-level reasoning</h4><ul><li>The spatiotemporal graph <span class="math inline">\(\mathcal{G}=\{\mathcal{V}_o,\mathcal{V}_p,\mathcal{E}\}\)</span> is constructed, containing an object node set <span class="math inline">\(\mathcal{V}_o\)</span>, a predicate node set <span class="math inline">\(\mathcal{V}_p\)</span>, and an edge set.<ul><li><span class="math inline">\(v_i\in \mathcal{V}_o\)</span> represents an object bounding box <span class="math inline">\(b_i\)</span> in a video fame, corresponding with object category <span class="math inline">\(c_i\)</span>.</li><li><span class="math inline">\(v_i\in \mathcal{V}_p\)</span> represents a predicate bounding box <span class="math inline">\(b_i\)</span> in a video fame, corresponding with predicate category <span class="math inline">\(c_i\)</span>.</li><li>There is an edge between an object node and a predicate node, but no links between two object nodes.</li><li>The objects and predicate nodes in the key frames at different moments are all included in the corresponding node set</li></ul></li><li>Then, perform GCN on the built graph.</li></ul><h4 id="transformer-for-temporal-modeling">Transformer for temporal modeling</h4><ul><li>The encoder layer encode the object-level features outputed by GCN, spatial features and semantic features into continuous representations. The encoder will output <span class="math inline">\(E\)</span> as the feature representation.</li><li>The decoder will use multi-head attention on <span class="math inline">\(E\)</span>, with two multi-head attention layers.</li><li>The final decoder embedding is sent to a FC layer and a softmax to get the final prediction.</li><li>Cross-entropy for optimization.</li></ul><h2 id="paper-42-wasserstein-embedding-for-graph-learning">Paper 42: <a href="https://openreview.net/pdf?id=AAes_3W-2z">Wasserstein embedding for graph learning</a></h2><p>Codes: https://github.com/navid-naderi/WEGL</p><h3 id="why-40">Why</h3><ul><li><p>The research on kernel approaches, perhaps most notably the random walk kernel and the WL kernel, remains an active field of study.</p></li><li><p>Recent works</p><ul><li>One propose a node embedding based on WL kernel, and combine the resulting node embeddings with the Wassertein distance.</li><li>Previous works all suffer from graphs in huge size.<ul><li>On the GCN side, optimization is challenging.</li><li>On the graph kernel side, calculating the matrix of all pairwise similarities can be a burden.</li></ul></li><li>Graph kernels<ul><li>The shortest-path kernels</li><li>The graphlets and WL subtree kernel.</li><li>Others like kernel methods using spectral approaches, assignment-based approaches, and graph decomposition algorithms.</li></ul></li></ul></li><li><p>Wassertein distance</p><ul><li><p>Due to Brenier's theorem , for absolutely continuous probability measures <span class="math inline">\(\mu_i,\mu_j\)</span>, the 2-wassertein distance can be equivalently obtained from</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928181830196.png" alt="image-20210928181830196" /><figcaption aria-hidden="true">image-20210928181830196</figcaption></figure></li><li><p>The mapping <span class="math inline">\(f\)</span> is referred to as a transport map, and the optimal transport map is called the Monger map.</p></li><li><p>But pairwise calculation of the Wassertein distance could be expensive.==&gt; The linear optimal transportation framework between the probability measures to define a Hilbertian embedding, in which the <span class="math inline">\(\ell_2\)</span> distance provides a true metric between the probability measures that approximates <span class="math inline">\(\mathcal{W}_2\)</span>.</p></li></ul></li></ul><h3 id="goal-39">Goal</h3><ul><li>learn graph-level representations, propose WEGL(wassertein embedding for graph learning). Define the similarity between graphs as a similarity function between their node embedding distributions. And use wassertein distance to measure the dissimilarity between node embeddings of different graphs.</li><li>WEGL calculates Monger maps from a reference distribution to each node embedding and, based on these maps, creates a fixed-sized vector representation of the graph.</li><li>WEGL embeds a graph into a Hilbert space, where the <span class="math inline">\(\ell_2\)</span> distance between two embedded graphs provides a true metric between the graphs that approximates their 2-Wassertein distance.</li></ul><h3 id="how-41">How</h3><h4 id="linear-wassertein-embedding">Linear Wassertein embedding</h4><ul><li>The probability distributions are mapped to the tangent space with respect to a fixed reference distribution. The framework below is for isometric Hilbertian embedding of probability measures such that the Euclidean distance between the embedded images approximates <span class="math inline">\(\mathcal{W}_2\)</span>.</li><li>By define <span class="math inline">\(\phi(\mu_i)\)</span> as a function of Jacobian equation <span class="math inline">\(p_0\)</span>, the identity function <span class="math inline">\(id(z)=z\)</span> and the Monge map <span class="math inline">\(f_i\)</span>, where <span class="math inline">\(f_i=\arg\min_{f\in MP(\mu_0,\mu_i)\int_{\mathcal{Z}}\|z-f(z)\|^2d\mu_0(z)}\)</span>. <span class="math inline">\(\phi(\mu_i)\)</span> provides an isometric embedding for probability measures.<ul><li><span class="math inline">\(\|\phi(\mu_i)-\phi(\mu_0)\|_2=\|\phi(\mu_i)\|_2=\mathcal{W}_2(\mu_i,\mu_0)\)</span>, with <span class="math inline">\(\phi(\mu_0)=0\)</span></li><li><span class="math inline">\(\|\phi(\mu_i)-\phi(\mu_j)\|_2\approx\mathcal{W}_2(\mu_i,\mu_j)\)</span>.</li></ul></li><li>Since <span class="math inline">\(\phi(\mu_i)\)</span> provides a linear embedding for the probability measures, it's called the linear Wassertein embedding. It can also be thought as the RKHS embedding of the measure <span class="math inline">\(\mu_i\)</span>.</li><li>For the discrete distributions , the Monge coupling is used.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928191439028.png" title="fig:" alt="image-20210928191439028" /></li><li>The Monger map is the approximated from the optimal transport plan by barycentric projection via <span class="math inline">\(F_i=N(\pi_i^*Z_i)\in\mathbb{R}^{N\times d}\)</span>, and <span class="math inline">\(\phi(Z_i)=(F_i-Z_0)/\sqrt{N}\in\mathbb{R}^{N\times d}\)</span>.<ul><li>Since the barycentric projection, <span class="math inline">\(\phi(\cdot)\)</span> is only pseudo-inverible.</li></ul></li></ul><h4 id="wegl">WEGL</h4><ul><li><p>Applying optimal transport to measure the dissimilarity between two graphs, the entire embedding for each graph <span class="math inline">\(G_i\)</span>, is obtained by composing <span class="math inline">\(\phi(\cdot)\)</span> and <span class="math inline">\(h(\cdot)\)</span>,i.e., <span class="math inline">\(\psi(G_i)=\phi(h(G_i))\)</span> where <span class="math inline">\(h\)</span> is an arbitrary node embedding process.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928192439836.png" alt="image-20210928192439836" /><figcaption aria-hidden="true">image-20210928192439836</figcaption></figure></li><li><p>Node embedding</p><ul><li><p>They use a similar non-parametric propagation/diffusion-based encoder as in <a href="https://arxiv.org/abs/1906.01277">here</a> .</p></li><li><p>Precisely, given node features <span class="math inline">\(\{x_v\}_{v\in\mathcal{V}}\)</span> and scalar edge features <span class="math inline">\(\{w_{uv}\}_{(u,v)\in\mathcal{E}}\)</span>, <span class="math display">\[x_v^{(l)}=\sum\limits_{u\in\mathcal{N}_v\cup\{v\}}\frac{w_{uv}}{\sqrt{\deg(u)\deg(v)}}\cdot x_u^{(l-1)},\forall l\in\{1,\cdots,L\},\forall v\in\mathcal{V}\\z_v=g(\{x_v^{(l)}\}_{l=0}^L\})\]</span></p></li></ul></li></ul><p>where <span class="math inline">\(g(\cdot)\)</span> is a local pooling process on a single node, <span class="math inline">\(z_v\)</span> is the result embedding for each node.</p><ul><li>Calculation of the reference distribution<ul><li>Use k-means on <span class="math inline">\(\cup_{i=1}^MZ_i\)</span> with <span class="math inline">\(N=\lfloor\frac{1}{M}\sum\limits_{i=1}^MN_i\rfloor\)</span> centroids.</li><li>Or one can calculate the Wassertein barycenter of the node embeddings or simply use <span class="math inline">\(N\)</span> samples from a normal distribution.</li></ul></li></ul><h2 id="paper-43-anomaly-detection-in-video-via-self-supervised-and-multi-task-learning">Paper 43: <a href="https://arxiv.org/abs/2011.07491">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning</a></h2><p>Codes: https://github.com/lilygeorgescu/AED-SSMTL. Need to fill the form firstly.</p><h3 id="why-41">Why</h3><ul><li>Previous work on anomaly detection<ul><li>Prediction based, and reconstruction error as the clue.</li><li>Reconstruction error</li><li>The dissimilarity between patches in Siamese network.</li><li>Eliminate the reliance of anomaly detection on context and form the problem as HAR.</li></ul></li><li>Addressing anomaly detection through a single proxy task is suboptimal.</li><li></li></ul><h3 id="goal-40">Goal</h3><ul><li>anomalous event detection in video at object-level.</li><li>Use 3DCNN on three pretext tasks and one distillation task. The tasks are : discrimination of forward/backward moving objects (arrow of time), discrimination of objects in consecutive/intermittent frames, reconstruction of object-specific appearance information. The distillation task is based on teacher-student mechanism.</li><li></li></ul><h3 id="how-42">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211014151336939.png" alt="image-20211014151336939" /><figcaption aria-hidden="true">image-20211014151336939</figcaption></figure><ul><li>YOLOv3 and ResNet-50 as teachers.</li><li>Data preparation: use YOLOv3 to generate an object-centric temporal sequence by simply cropping the corresponding bounding box from frames.</li><li>The anomaly score is the average scores predicted fro each task.</li><li>Four tasks share the 3DCNN together, but each task has its prediction head.</li></ul><h4 id="task-1-arrow-of-time">Task 1: arrow of time</h4><ul><li>For one sequence, create a forward and a backward sample, then train.</li><li>Use cross-entropy</li></ul><h4 id="task-2-motion-irregularity">Task 2: motion irregularity</h4><ul><li>Use one consecutive clip and the other is composed of the centric frame and other sampled frame, with a step sampled from <span class="math inline">\(\{1,2,3,4\}\)</span>.</li><li>Use cross-entropy</li></ul><h4 id="task-3-middle-bounding-box-prediction">Task 3: middle bounding box prediction</h4><ul><li>Use the sequence with the centric frame missing and try to predict the missing one.</li><li>Use L1 loss.</li></ul><h4 id="task-4-model-distillation">Task 4: model distillation</h4><ul><li>Resnet 50 pretrained on ImageNet and YOLOv3 pretrained on MS COCO as the teacher who gives the GT, first YOLOv3 to get the middle box and the prediction <span class="math inline">\(Y_{yolo}\)</span>, then input the middle box to resnet 50 to get the prediction <span class="math inline">\(Y_{res}\)</span>, these two predictions are concatenated together as the GT.</li><li>The shared 3DCNN is the student who use the features before input in resnet 50.</li><li>Use L1 loss.</li></ul><h4 id="joint-loss">Joint loss</h4><ul><li>Summation of the four loss, but the 4th loss is weighted summed to regulate the importance of the knowledge distillation task.</li></ul><h4 id="inference">Inference</h4><ul><li>For an object, the abnormal score is the average from four tasks. The pixel-level anomaly map uses a 3D mean filter. For a certain frame, it is taken as the maximum score in the corresponding anomaly map. The final frame-level anomaly scores are obtained by a temporal Gaussian filter.</li><li>The failures of YOLOv3 are translated into false negatives.<ul><li>By fusing the frame-level and object-level anomaly scores.</li></ul></li></ul><h2 id="paper-44-exponential-moving-average-normalization-for-self-supervised-and-semi-supervised-learning">Paper 44: <a href="https://arxiv.org/abs/2101.08482">Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning</a></h2><p>Codes: https://github.com/amazon-research/exponential-moving-average-normalization</p><h3 id="why-42">Why</h3><ul><li>The BN used in both teachers and students can lead to<ul><li>Cross-sample dependency: caused by the intrinsic property of BN where the output of a sample is dependent on all other samples in the same batch. To solve this, some use layer normalization, MoCo designs ShuffleBN where a mini-batch uses BN statistics from other randomly sampled mini-batch. SimCLR and BYOL use Synchronized BN.</li><li>Model parameter mismatch: parameters in a teachers comes from the student in previous iterations, while the batch-wise BN statistics are instantly collected at current iteration, which will lead to potential mismatch between the model parameters and the BN statistics in the parameter space.</li></ul></li><li>Normalization<ul><li>BN introduces some issues: the large batch sizes for accurate statistics, mismatch between how BN is used during training and inference</li><li>Other normalization<ul><li>Layer normalization (LN) : along the channel and spatial dimension.</li><li>Instance normalization (IN): along only the spatial dimension</li><li>Group normalization (GN): similar to LN, divides the channels into groups.</li><li>MABN: similar with EMAN, but focus on the stability of small batch size training and updates its statistics inside a single network.</li><li>ShuffleBN: a minibatch uses BN statistics from other randomly sampled minibatch.</li></ul></li></ul></li></ul><h3 id="goal-41">Goal</h3><ul><li>Propose a plug-in replacement for BN called EMAN (exponential moving average normalization), for the improvement of student-teacher mechanism.</li><li>EMAN in the teacher side updates statistics by exponential moving average, from the BNN statistics of the student. This design reduces the intrinsic cross-sample dependency of BN and enhances the generalization of the teacher.</li></ul><h3 id="how-43">How</h3><ul><li><p>The EMAN statistics <span class="math inline">\(\mu,\sigma\)</span> in the teacher are exponentially moving averaged from the student BN statistics.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211014165232695.png" alt="image-20211014165232695" /><figcaption aria-hidden="true">image-20211014165232695</figcaption></figure></li></ul><h4 id="ema-teacher">EMA teacher</h4><ul><li><p>Both the student and the teacher use standard BN during training.</p></li><li><p>The teacher parameters <span class="math inline">\(\theta&#39;\)</span> are updated by exponential moving average from the student parameters <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta&#39;:=m\theta&#39;+(1-m)\theta\)</span>, where <span class="math inline">\(m\)</span> is the momentum. SGD for students, but no gradient back propagation through the teacher model.</p></li><li><p>BN</p><ul><li><p>In training, first compute the mean and the variance of the layer inputs for the current batch. Then every sample in the current batch is normalized using the batch-wise statistics, and then an affine transformation with learnable parameters <span class="math inline">\(\gamma,\beta\)</span> is applied <span class="math display">\[\hat{x}=BN(x)=\gamma \frac{x-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2}+\epsilon}+\beta,\]</span> where $$ is a small constant for numerical stability.</p></li><li><p>In inference, though the population statistics should be used instead, but since the additional stage, in many implementations, a more practical way is collecting the proxy statistics <span class="math inline">\(\mu,\sigma^2\)</span> by EMA during training <span class="math display">\[\mu:=\alpha\mu+(1-\alpha)\mu_{\mathcal{B}},\\\sigma^2:=\alpha\sigma^2+(1-\alpha)\sigma_{\mathcal{B}}^2,\\\]</span> then the BN at inference becomes <span class="math inline">\(\hat{x}=BN(x)=\gamma \frac{x-\mu}{\sqrt{\sigma^2}+\epsilon}+\beta,\)</span></p></li></ul></li><li><p>When the teacher is updated by EMA, the standard BN is not well aligned with the model parameters</p><ul><li>The teacher is used to generate pseudo GY to guide the learning of the student, the batch-wise BN will make generated labels be cross-sample dependent.</li><li>Possible mismatch between the model parameters <span class="math inline">\(\theta&#39;\)</span> and batch-wise BN statistics in the teacher model since the non-alignment of iterations.==&gt; lead to non-smoothness in the parameter space.</li></ul></li></ul><h4 id="eman">EMAN</h4><ul><li><p>Compared with formula (2) in pervious section, they use EMAN for the teacher during training (student still uses BN ) where <span class="math inline">\(\hat{x}=EMAN(x)=\gamma \frac{x-\mu&#39;}{\sqrt{\sigma&#39;^2}+\epsilon}+\beta,\)</span> where <span class="math inline">\(\mu&#39;,\sigma&#39;^2\)</span> are also exponentially moving averaged from the student <span class="math inline">\(\mu,\sigma^2\)</span>: <span class="math display">\[\mu&#39;:=m\mu&#39;+(1-m)\mu,\\\sigma&#39;^2:=m\sigma&#39;^2+(1-m)\sigma^2,\\\]</span></p><ul><li>This is a linear transform which is no longer dependent on batch statistics.</li></ul></li><li><p>Although the student is still cross-sample dependent, this is a less serious issue than the cross-sample dependency in the teacher.</p></li></ul><h2 id="paper-45-recognizing-actions-in-videos-from-unseen-viewpoints">Paper 45: <a href="https://arxiv.org/abs/2103.16516">Recognizing Actions in Videos from Unseen Viewpoints</a></h2><h3 id="why-43">Why</h3><ul><li>CNNs are unable to recognize actions/data that are outside of the training data distribution, also unseen viewpoint.</li><li>PoseNet for 3D coordinates, and CalibNet for a limited extrinsic camera matrix.</li><li>But the 3D pose directly from PoseNet may not be the best feature since scale changes, speed of which the action occurs etc.</li><li></li></ul><h3 id="goal-42">Goal</h3><ul><li>introduce a new geometric convolutional layer that can learn viewpoint invariant representations. The core is to learn and represent the intrinsic camera matrix <span class="math inline">\(K\)</span>.</li><li>Estimating 3D poses, then explores using different representations of it for recognition.</li><li></li></ul><h3 id="how-44">How</h3><ul><li><p>Learn global 3D pose and 2d multi-view projections of it for classifying unseen viewpoints.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211017124211442.png" alt="image-20211017124211442" /><figcaption aria-hidden="true">image-20211017124211442</figcaption></figure></li><li><p>The 3d world coordinate system is the same for all videos, thus <span class="math inline">\(R\)</span> is different for each video, depending on the camera viewpoint. <span class="math inline">\(R\)</span> plays the role of aligning features in different scenes, so that the losses are minimized.</p></li><li><p>To enforce the similarity of the 3D representations from the same action and the dissimilarity of 2D view,</p><ul><li><span class="math inline">\(\mathrm{3d\_{loss}}(V,U)=\|F_W(V)-F_W(U)\|_F\)</span>,</li><li><span class="math inline">\(cam_reg(c_1,c_2)=\max(-\|c_1,c_2\|_F,\alpha)\)</span></li></ul></li><li><p>The final loss</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211017150147461.png" alt="image-20211017150147461" /><figcaption aria-hidden="true">image-20211017150147461</figcaption></figure></li></ul><h2 id="paper-46-shot-contrastive-self-supervised-learning-for-scene-boundary-detection">Paper 46: <a href="https://arxiv.org/abs/2104.13537">Shot Contrastive Self-Supervised Learning for Scene Boundary Detection</a></h2><h3 id="why-44">Why</h3><ul><li>A shot: a series of frames captured from the same camera over an interrupted period of time. A scene: a series of shots depicting a semantically cohesive part of a story.</li><li>Scenes are more difficult to be localized.</li><li>Previous relatively simple data augmentation used by SSL cannot encode the complex temporal scene-structure.</li><li>Scene detection: define the locations in videos where different scenes begin and end.</li></ul><h3 id="goal-43">Goal</h3><ul><li>Find scene boundaries. Propose ShotCoL to Learn a shot representation that maximizes the similarity between nearby shots compared to randomly selected shots, and then use the shot representation to detect scene boundary.</li><li>Use the priors: nearby shots tend to have the same set of actors enacting a semantically cohesive story-arch, and therefore are more similar to each other compared with other randomly selected shots.==&gt;consider nearby shots as augmented versions of each other .</li><li>Specifically, given a shot (both the images and audios), they try to: (a) maximize its similarity with its most similar neighboring shot, and (b) minimize its similarity with a set of randomly selected shots.</li></ul><h3 id="how-45">How</h3><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211017221640577.png" alt="image-20211017221640577" /><figcaption aria-hidden="true">image-20211017221640577</figcaption></figure><h4 id="shot-level-representation-learning">Shot-level representation learning</h4><ul><li>Given a full-length input video, use standard shot detection techniques to divide it into its constituent set of shots.</li><li>Comprises of encoder network and momentum contrastive learning to contrast the similarity of the embedded shots.</li><li>Shot encoder network<ul><li>The intra-shot frame dynamics is not as important since scene boundaries depend on inter-shot relationships.<ul><li>Sample <span class="math inline">\(k\)</span> frames uniformly from each shot, and then reshape the 4D tensor to 3D by combining <span class="math inline">\(c\)</span> channels and <span class="math inline">\(k\)</span> frames.</li><li>Advantages: the usage of standard networks and resource efficiency.</li></ul></li><li>Audio modality: use a Wavegram-Longmel CNN which incorporates a 14-layer CNN similar in architecture to the VGG network.</li></ul></li><li>Shot contrastive learning<ul><li>Pretext: given a query shot, first find the positive key as its most similar shot within a neighborhood around the query, and then maximizes the similarity between the query and the positive key, and minimize the similarity of the query with a set of randomly selected shots.</li><li>The positive key is the most similar shot compared with the query one in embedded space (after query encoder).</li><li>The contrastive loss is InfoNCE.</li><li>Momentum contrast: save the embedded keys in a fixed-sized queue as negative keys, and follow FIFO. And a momentum update scheme is used for the key encoder. <span class="math inline">\(\theta_k \leftarrow\alpha\theta_k+(1-\alpha)\theta_q\)</span>.</li></ul></li></ul><h4 id="supervised-learning">Supervised learning</h4><ul><li>Formulate the problem of scene boundary detection as a binary classification problem of determining if a shot boundary is also a scene boundary or not.</li><li>For each shot boundary, consider its <span class="math inline">\(2N\)</span> neighboring shots as a data-point to perform scene boundary detection.</li><li>Use MLP as the classifier of boundary, and take the features extracted from trained shot-level contrastive learning as the input.</li><li>While inference, directly use the features of audio and visual from shot encoder, and the trained MLP.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Paper 1: &lt;a href=&quot;https://arxiv.org/pdf/2104.00323.pdf&quot;&gt;Jigsaw Clustering for Unsupervised Visual Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 2: &lt;a href=&quot;https://arxiv.org/pdf/2104.00240.pdf&quot;&gt;Self-supervised Motion Learning from Static Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 3: &lt;a href=&quot;https://arxiv.org/pdf/2104.00862.pdf&quot;&gt;Self-supervised Video Representation Learning by Context and Motion Decoupling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 4: &lt;a href=&quot;https://arxiv.org/pdf/2104.11487.pdf&quot;&gt;Skip-convolutions for Efficient Video Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 5: &lt;a href=&quot;https://arxiv.org/pdf/2104.09496.pdf&quot;&gt;Temporal Query Networks for Fine-grained Video Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 6: &lt;a href=&quot;https://arxiv.org/pdf/2103.16605.pdf&quot;&gt;Unsupervised disentanglement of linear-encoded facial semantics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 7:&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Bi-GCN_Binary_Graph_Convolutional_Network_CVPR_2021_paper.pdf&quot;&gt;Bi-GCN: Binary Graph Convolutional Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 8: &lt;a href=&quot;https://arxiv.org/pdf/2105.09711.pdf&quot;&gt;An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 9: &lt;a href=&quot;https://arxiv.org/abs/2008.03087&quot;&gt;Cascade Graph Neural Networks for RGB-D Salient Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 10: &lt;a href=&quot;https://arxiv.org/abs/2103.01302&quot;&gt;Coarse-Fine Networks for Temporal Activity Detection in Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 11: &lt;a href=&quot;https://arxiv.org/abs/2104.03851&quot;&gt;CoCoNets: Continuous Contrastive 3D Scene Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 12: &lt;a href=&quot;https://arxiv.org/abs/2104.04015&quot;&gt;CutPaste: Self-Supervised Learning for Anomaly Detection and Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 13: &lt;a href=&quot;https://arxiv.org/abs/2108.03662&quot;&gt;Discriminative Latent Semantic Graph for Video Captioning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 14: &lt;a href=&quot;https://arxiv.org/abs/2108.02183?context=cs&quot;&gt;Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 15: &lt;a href=&quot;https://arxiv.org/abs/2011.10566&quot;&gt;Exploring simple siamese representation learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 16: &lt;a href=&quot;https://arxiv.org/abs/2107.05475&quot;&gt;GiT: Graph Interactive Transformer for Vehicle Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 17: &lt;a href=&quot;https://arxiv.org/abs/2103.01730&quot;&gt;Graph-Time Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 18: &lt;a href=&quot;https://arxiv.org/abs/1910.02370&quot;&gt;Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 19: &lt;a href=&quot;https://arxiv.org/pdf/2107.09787.pdf&quot;&gt;Group Contrastive Self-Supervised Learning on Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 20: &lt;a href=&quot;https://link.springer.com/article/10.1007/s10618-021-00750-y&quot;&gt;Homophily outlier detection in non-IID categorical data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 21: &lt;a href=&quot;https://arxiv.org/abs/2108.02113&quot;&gt;Hyperparameter-free and Explainable Whole Graph Embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 22: &lt;a href=&quot;https://arxiv.org/abs/1908.01000&quot;&gt;Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 23: &lt;a href=&quot;https://arxiv.org/abs/2010.12609&quot;&gt;Iterative graph self-distillation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 24: &lt;a href=&quot;https://arxiv.org/abs/2103.17260&quot;&gt;Learning by Aligning Videos in Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 25: &lt;a href=&quot;https://arxiv.org/abs/2103.13125&quot;&gt;Learning graph representation by aggregating subgraphs via mutual information maximization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 26: &lt;a href=&quot;https://arxiv.org/abs/1802.09612&quot;&gt;Mile: A multi-level framework for scalable graph embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 27: &lt;a href=&quot;https://arxiv.org/pdf/2108.03400.pdf&quot;&gt;Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 28: &lt;a href=&quot;https://arxiv.org/abs/2107.02639&quot;&gt;Multi-Level Graph Contrastive Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 29: &lt;a href=&quot;https://arxiv.org/abs/2104.09856&quot;&gt;Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 30: &lt;a href=&quot;https://arxiv.org/abs/2008.04575&quot;&gt;PiNet: Attention Pooling for Graph Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 31: &lt;a href=&quot;https://arxiv.org/abs/2106.04113&quot;&gt;Self-supervised Graph-level Representation Learning with Local and Global Structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 32: &lt;a href=&quot;https://arxiv.org/abs/2105.09111&quot;&gt;Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 33: &lt;a href=&quot;https://arxiv.org/abs/2107.01903&quot;&gt;SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 34: &lt;a href=&quot;https://arxiv.org/abs/2006.14613&quot;&gt;Space-time correspondence as a contrastive random walk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 35: &lt;a href=&quot;https://arxiv.org/abs/2103.06122&quot;&gt;Spatially consistent representation learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 36: &lt;a href=&quot;https://arxiv.org/abs/2008.03800&quot;&gt;Spatiotemporal contrastive video representation learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 37: &lt;a href=&quot;https://arxiv.org/abs/2104.11452&quot;&gt;SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 38: &lt;a href=&quot;https://arxiv.org/abs/2105.13033&quot;&gt;SSAN: Separable Self-Attention Network for Video Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 39: &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3340531.3411953&quot;&gt;tdgraphembed: Temporal dynamic graph-level embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 40: &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf&quot;&gt;Videomoco: Contrastive video representation learning with temporally adversarial examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 41: &lt;a href=&quot;https://arxiv.org/abs/2107.01181&quot;&gt;Visual Relationship Forecasting in Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 42: &lt;a href=&quot;https://openreview.net/pdf?id=AAes_3W-2z&quot;&gt;Wasserstein embedding for graph learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 43: &lt;a href=&quot;https://arxiv.org/abs/2011.07491&quot;&gt;Anomaly Detection in Video via Self-Supervised and Multi-Task Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 44: &lt;a href=&quot;https://arxiv.org/abs/2101.08482&quot;&gt;Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 45: &lt;a href=&quot;https://arxiv.org/abs/2103.16516&quot;&gt;Recognizing Actions in Videos from Unseen Viewpoints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 46: &lt;a href=&quot;https://arxiv.org/abs/2104.13537&quot;&gt;Shot Contrastive Self-Supervised Learning for Scene Boundary Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Fall Surveys</title>
    <link href="http://yoursite.com/posts/notes/2021-08-23-notes-paper-SSL-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-08-23-notes-paper-SSL-survey.html</id>
    <published>2021-08-23T14:36:39.000Z</published>
    <updated>2021-08-24T19:48:27.279Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="https://arxiv.org/pdf/2006.08218.pdf">Self-supervised Learning: Generative or Contrastive</a></li></ul><a id="more"></a><h2 id="ssl-generative-or-contrastive">SSL: Generative or Contrastive</h2><h3 id="why">Why?</h3><h4 id="supervised-learning"><strong>Supervised Learning</strong></h4><ul><li>relies heavily on expensive manual labeling</li><li>suffers from generalization error, spurious correlations, and adversarial attacks</li><li>The characteristics of different types of falls are not taken into consideration in most of the work on fall detection surveyed. (like age, gender etc.)</li></ul><h4 id="ssl">SSL</h4><ul><li>Training data is automatically labeled by leveraging the relations between different input sensor signals.</li><li>Features<ul><li>Obtain “labels” from the data itself by using a “semi-automatic” process.</li><li>Predict part of the data from other parts.</li></ul></li></ul><h3 id="motivation-of-ssl">Motivation of SSL</h3><h4 id="mainstream-of-methods">Mainstream of methods</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823112440152.png" alt="image-20210823112440152" style="zoom:67%;" /></p><ul><li>For latent distribution <span class="math inline">\(z\)</span>: explicit in generative and contrastive methods, and implicit in GAN</li><li>Discriminator: GANs and contrastive have while generative method does not.</li><li>Objectives: generative methods use a reconstruction loss, the contrastive ones use a similarity metric and the GANs leverage distributional divergence as the loss (JS-divergence, Wasserstein distance )</li></ul><h4 id="hints">Hints</h4><ul><li>Contrastive learning is useful for almost all visual classification tasks: since the contrastive object is modeling the class-invariance between different image instances.</li><li>The art of SSL primarily lies in defining proper objectives for unlabeled data.</li></ul><h4 id="summary-of-papers">Summary of papers</h4><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823114232256.png" alt="image-20210823114232256" /><figcaption aria-hidden="true">image-20210823114232256</figcaption></figure><h4 id="outline-of-this-paper">Outline of this paper</h4><h3 id="generative-ssl">Generative SSL</h3><h4 id="ar-model">AR model</h4><ul><li>Viewed as "Bayes net", where the probability of each variable is dependent on the previous variables.</li><li>objective: in NLP, usually maximizing the likelihood under the forward autoregressive factorization.</li><li>Examples:<ul><li>PixelRNN: lower (right) pixels are generated by conditioning on the upper (left) pixels.</li><li>For 2D images: factorize probabilities according to specific directions, and therefore masked filters</li><li>For raw audio: PixelCNN, wavenet.</li><li>GraphRNN: y decompose the graph generation process into a sequence generation of nodes and edges conditioned on the graph generated so far. The objective is the likelihood of the observed graph generation sequences.</li></ul></li><li>Pros and cons: can model the context dependency well, but the token at each position can only access its context from one direction .</li></ul><h4 id="flow-based-model">Flow-based model</h4><ul><li>Goal: estimate complex high-dimensional densities from data. It designs the mapping between <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> in invertible, but also requires that <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> have the same dimension.</li><li>Examples<ul><li>NICE and RealNVP design affine coupling layer to parameterize <span class="math inline">\(f_\theta\)</span>.</li><li>GLOW: introduces invertible 1 × 1 convolutions and simplifies RealNVP.</li></ul></li></ul><h4 id="ae-model">AE model</h4><h5 id="basic-ae">Basic AE</h5><ul><li>RBM can be regarded as a special AE.</li><li>AE model is usually a feed-forward neural network trained to produce its input at the output layer.</li><li>the linear autoencoder corresponds to the PCA method.</li><li>some interesting structures can be discovered by imposing sparsity constraints on the hidden units</li></ul><h5 id="context-prediction-model-cpm">Context prediction model (CPM)</h5><ul><li>Idea: predict contextual information based on inputs.</li><li>negative sampling is employed to ensure computational efficiency and scalability</li><li>Examples<ul><li>CBOW, Skip-Gram, FastText based on CBOW</li><li>DeepWalk: based on a similar context prediction objective. It treats random walks as the equivalent of sentences.</li><li>LINE: aims to generate neighbors based on current nodes. Use negative sampling to sample multiple negative edges to approximate the objective.</li></ul></li></ul><h5 id="denoising-ae-model">Denoising AE model</h5><ul><li>Intuition: representation should be robust to the introduction of noise.</li><li>Examples<ul><li>MLM (masked language model): randomly mask some of the tokes from the input and then predicts them based on their context information. But <strong>it assumes the predicted tokens are independent if the unmasked tokens are given,</strong> which does not hold in reality.</li><li>Bert: import a unique token to mask some tokens, but also replace the unique token with original words or random words with a small probability.</li><li>SpanBert: mask continuous random spans rather than random tokens adopted by BERT. It trains the span boundary representations to predict the masked spans.</li><li>ERINE: learn entity-level and phrase-level knowledge and further integrates knowledge in knowledge graphs into language models</li><li>GPT-GNN: asks GNN to generate masked edges and attributes.</li></ul></li></ul><h5 id="variational-ae-model">Variational AE model</h5><ul><li><p>Assumes that data are generated from underlying latent representation. The posterior distribution over a set of unobserved variable <span class="math inline">\(Z\)</span> given some data <span class="math inline">\(X\)</span> is approximated by a variational distribution <span class="math inline">\(q(z|x)\approx p(z|x)\)</span>. In variational inference, the ELBO (evidence lower bound) on the log-likelihood of data is maximized during training.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823135504436.png" alt="image-20210823135504436" /><figcaption aria-hidden="true">image-20210823135504436</figcaption></figure><p>The 1st term of ELBO is a regularizer forcing the posterior to approximate the prior. The second term is the likelihood of reconstructing the original input data based on latent variables.</p></li><li><p>Examples</p><ul><li>VQ-VAE: aims to learn discrete latent variables motivated by the fact that many modalities are inherently discrete, such as language, speech, and images. VQ-VAE relies on vector quantization (VQ) to learn the posterior distribution of discrete latent variables.</li><li>VQ-VAE-2: enlarge the scale and enhance the autoregressive priors by a powerful PixelCNN prior</li><li>VGAE: VAE combined with GCN as the encoder, with an objective to reconstruct the adjacency matrix of the graph by measuring node proximity.</li><li>DVNE: s Gaussian node embedding to model the uncertainty of nodes. 2-Wasserstein distance is used to measure the similarity between the distributions for its effectiveness in preserving network transitivity</li><li>vGraph: node representation learning and community detection. Assumed that each node can be generated from a mixture of communities.</li></ul></li></ul><h4 id="hybrid-generative-models">Hybrid generative models</h4><h5 id="arae">AR+AE</h5><ul><li>Examples<ul><li>MADE: modify autoencoder by masking its parameters to respect AR constraints.</li><li>PLM (permutation language model): AR+ auto encoding .</li><li>XLNet: introduces PLM, enables learning bidirectional contexts by <strong>maximizing the expected likelihood over all permutations</strong> of the factorization order. It also integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL into pre-training, which can model long-range dependency better that Transformer.</li></ul></li></ul><h5 id="ae-flow-based-models">AE + flow-based models</h5><ul><li>Examples<ul><li>GraphAF: to molecule graph generation as a sequential decision process. Follow flow-based method, it defines an invertible transformation from a base distribution to a molecular graph structure. Dequantization technique is utilized to convert discrete data into continuous data.</li></ul></li></ul><h4 id="pros-and-cons">Pros and cons</h4><ul><li>Ability: recover the original data distribution without assumptions for downstream tasks.</li><li>Shortcomings<ul><li>far less competitive than contrastive self-supervised learning in some classification scenarios because contrastive learning’s goal naturally conforms the classification objective: MOCO, SimCLR, BYOL, SwAV.</li><li>Point-wise nature of the generative objective has some inherent defects. The MLE is based on all the samples <span class="math inline">\(x\)</span> we hope to model and the context information <span class="math inline">\(c\)</span> is conditionally constrained.<ul><li>Sensitive and conservative distribution: when <span class="math inline">\(p(x|c)\rightarrow 0\)</span>, MLE loss becomes super large, making generative model extremely sensitive to rare samples.</li><li>Low-level abstraction objective: the representation distribution in MLE is mostly modeled at <span class="math inline">\(x\)</span>'s level, while most of the classification tasks target at high-level abstraction.</li></ul></li><li>Generative - contrastive SSL abandons the point-wise objective.</li></ul></li></ul><h3 id="contrastive-ssl">Contrastive SSL</h3><p>The contrastive models show the potential of discriminative models for representation. They aim at "learn to compare" through a NCE (noise contrastive estimation ) objective formatted as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823152108301.png" alt="image-20210823152108301" style="zoom:80%;" /></p><p>With more dissimilar pairs involved, we have the InfoNCE formulated as</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823152312022.png" alt="image-20210823152312022" /><figcaption aria-hidden="true">image-20210823152312022</figcaption></figure><h4 id="context-instance-contrast">Context-instance Contrast</h4><p>Also called as global-local contrast, focusing on modeling the belonging relationship between the local feature of a sample and its global context representation. There are two main types of context-instance contrast:</p><ul><li>PRP (predict relative position): learn relative positions between local components. The global context serves as an implicit requirement for predicting these relations. (such as understanding what an elephant looks like is critical for predicting relative position between its head and tail)</li><li>MI (maximize mutual information): learn the direct belonging relationships between local parts and global context. Ignore relative positions between local parts.</li></ul><h5 id="predict-relative-position">Predict relative position</h5><ul><li>The PRP is also knowns as pretext task, such as jigsaw, rotation. It may also serve as tools to create hard positive samples.</li><li>Examples<ul><li>NSP (next sentence prediction ): for a sentence, the model is asked to distinguish the following and a randomly sampled one. But in RoBERTa, the NSP loss is removed since some argue that NSP may hurt performance.</li><li>ALBERT propose SOP task (sentence order prediction ): two sentences that exchange their position are regarded as a negative sample, making the model concentrate on the semantic meaning’s coherence.</li></ul></li></ul><h5 id="mi">MI</h5><ul><li><p>Target at maximizing the association between two variables. To reduce the computation price, in practice, MI is maximized by maximizing its lower bound with an NCE objective</p></li><li><p>Examples</p><ul><li>Deep InfoMax: maximizing the MI between a local patch and its global context.</li><li>CPC (contrastive predictive coding ): inspired by deep infomax. It maximizes the association between a segment of audio and its context audio. The negative context vectors are taken at the same time.</li><li>AMDIM: enhances the positive association between a local feature and its context. Instead picking a negative image which has the same context, it picks the image which is taken from a different view of the positive image.</li><li><em>CMC</em>: extends AMDIM, which take the image in several different views as the positive samples, while the negative sample is another irrelevant sampled image. It <em>measure the instance-instance similarity rather than context-instance similarity</em>.</li><li>InfoWord: maximize the mutual information between a global representation of a sentence and n-grams in it. The context is induced from the sentence with selected n-grams being masked, and the negative contexts are randomly picked out from the corpus.</li><li>DGI (deep graph InfoMax): take a nodes representation as the local feature and the average of randomly samples 2-hop neighbors as the context. To generate negative samples on one single graph, DGI corrupt the original context by keeping the sub-graph structure and permuting the node features.</li><li>InfoGraph: follow DGI, learn graph-level representation.</li><li>Similar as CMC: <a href="https://arxiv.org/pdf/2006.05582.pdf">paper</a> learns node and graph representations by maximizing MI between node representations of one view and graph representation of another view and vice versa. They find that graph diffusion is the most effective way to yield augmented positive sample pairs in graph learning.</li><li>In <a href="https://arxiv.org/abs/1905.12265">paper</a> , they attempt to unify graph pre-training in two strategies. One is structural prediction at node-level, where they propose context prediction to maximized the MI between the k-hop neighborhood's representations and its context graph. For node-level/graph-level strategy, they propose attribute mask to predict a node's attribute according to its neighborhood, which is a generative objective similar to token masks in bert.</li><li><span class="math inline">\(\mathrm{S}^2\)</span>GRL: separate nodes in the context graph into k-hop context subgraphs and maximizes their MI with target node, respectively. <font color=red>(There are k negative samples?)</font></li></ul></li><li><p>Cons</p><p>Existing graph pre-training work is only applicable for a specific domain, while graph pre-training tends to learn inductive biases across graphs.</p></li></ul><h5 id="improvements">Improvements</h5><ul><li>Some argue that the models above is only loosely connected to MI by showing that <strong><em>an upper bound MI estimator leads to ill-conditioned and lower performance representations.</em></strong>--&gt; <font color="blue">More should be attributed to encoder architecture and a negative sampling strategy related to metric learning.</font></li><li>And therefore in metric learning: <strong><em>perform hard positive sampling while increasing the negative sampling strategy</em></strong>.</li></ul><h4 id="instance-instance-contrast">Instance-instance Contrast</h4><ul><li>Directly studies the relationships between different samples' instance-level local representations as what metric learning does.</li></ul><h5 id="cluster-discrimination">Cluster discrimination</h5><ul><li>The motivation is to pull similar images near in the embedding space.</li><li>Examples<ul><li>Deep Cluster: leverage clustering to yield pseudo labels and asks a discriminator to predict images' labels. In details, K-means to cluster pseudo labels and then the discriminator predicts whether two samples are from the same cluster and back-propagates to the encoder.<ul><li>In DeepCluster, samples are assigned to mutual-exclusive clusters. But LA identifies neighbors separately for each examples.</li><li>DeepCluster optimizes a cross-entropy discriminative loss, while LA employs an objective function that directly optimizes a local soft-clustering metric.</li></ul></li><li>LA (local aggregation): improve the cluster-based method's boundary.<ul><li>LA identifies neighbors separately for each examples.</li><li>LA employs an objective function that directly optimizes a local soft-clustering metric.</li></ul></li><li>VQ-VAE: similar as LA. For the feature matrix encoded from an image, VQ-VAE substitutes each 1-dimensional vector in the matrix to the nearest one in an embedding dictionary.</li><li>ClusterFit: help in the generalization of other pre-trained models. Introduce a cluster prediction fine-tuning stage similar to DeepCluster.</li><li>SwAV: to improve the time-consuming two-stage training. Use online clustering ideas and multi-view data augmentation strategies into the cluster discrimination approach. To reduce time price, they propose an online computing strategy to label the images in different views.</li><li>M3S: in graph learning. Given little labeled data and many unlabeled data, for every stage, M3S first pretrain as DeepCluster does and then compares these pseudo labels with those predicted by the model being supervised trained on labeled data. . Only top-k confident labels are added into a labeled set for the next stage of semi-supervised training.</li></ul></li></ul><h5 id="instance-discrimination">Instance Discrimination</h5><ul><li>The prototype is InstDisc.</li><li>Examples<ul><li>CMC: adopt multiple different views of an image as positive samples and take another one as the negative. But it's constrained by the idea of Deep Infomax, which only samples one negative sample for each positive one.</li><li>MoCo: leverage instance discrimination via momentum contrast, which substantially increases the amount of negative samples.<ul><li>It designs the <strong>momentum contrast learning with two encoders (query and key),</strong> which prevents the fluctuation of loss convergence in the beginning period</li><li>to enlarge negative samples’ capacity, MoCo employs a queue (with K as large as 65536) to save the recently encoded batches as negative samples.</li><li>But the positive sample strategy is too simple: a pair of positive representations come from the same sample without any transformation or augmentation.</li></ul></li><li>PIRL: based on MoCo, adds jigsaw augmentation.</li><li>SimCLR: hard positive sample strategy by introducing data augmentation in 10 forms. The augmentation leverages several different views to augment the positive pairs. To handle the large-scale negative samples problem, it chooses a batch size of N as large as 8196. <strong>Techniques in SimCLR can also further improve MoCo's performance.</strong></li><li>InfoMin: more into augmenting positive samples. They argue that we should select those views with less mutual information for better augmented views in contrastive learning. To do so, they first propose an unsupervised method to minimize mutual information between views, but this will result in a loss of information for predicting labels, say a pure blank view. Then a semi-supervised method is proposed to find views sharing only label information.</li><li>BYOL: discards negative sampling in SSL but achieves an even better result over InfoMin. <em>They argue that negative samples may not be necessary in this process</em>.<ul><li>If we use a fixed randomly initialized network to serve as the key encoder, the representation produced by query encoder would still be improved during training.</li><li>BYOL proposes an architecture with an exponential moving average strategy to update the target encoder just as MoCo does.</li><li>The loss is mean square error, which is robust to smaller batch size.</li><li>The batch size in BYOL is not as critical as what's in MoCo and SimCLR.</li></ul></li><li>SimSiam: study how necessary is negative sampling.<ul><li>They show that the most critical component in BYOL is the stop gradient operation, which makes the target representation stable.</li><li>It converges faster than MoCo, SimCLR, and BYOL with even smaller batch sizes.</li></ul></li><li>ReLIC: argue that contrastive pre-training teaches the encoder to causally disentangle the invariant content and style in an image.<ul><li>They propose to add an extra KL-divergence regularizer between prediction logits of an image's different views.</li></ul></li><li>GCC (graph contrastive coding): leverage instance discrimination as the pretext task for structural information pre-training.<ul><li>For each node, they sample two subgraphs independently by random walks with restart and use top eigenvectors from their normalized graph Laplacian matrices as nodes' initial representations.</li><li>Then they use GNN to encode them and calculate the InfoNCE loss.</li></ul></li><li>GraphCL: studies the data augmentation strategies, propose four different augmentation methods based on edge perturbation and node dropping. They show that the appropriate combination of these strategies can yield even better performance.</li></ul></li></ul><h4 id="ss-contrastive-pre-training-for-semi-supervised-self-training">SS contrastive pre-training for semi-supervised self-training</h4><ul><li>No matter how self-supervised learning models improve, they are still the only powerful feature extractor, and to transfer to the downstream task, we still need labels more or less.</li><li>In self-training, a model is trained on the small amount of labeled data and then yield labels on unlabeled data. Only those data with highly confident labels are combined with original labeled data to train a new model. We iterate this procedure to find the best model.<ul><li>Student-teacher</li><li>The improvements from pre-training and self-training are orthogonal to each other. The model with joint pre-training and self-training is the best.</li></ul></li><li>SimCLR v2 adopts the conclusion above<ul><li>Do SS pre-training as SimCLR v1， with some minor architecture modification and a deeper ResNet.</li><li>Fine tune the last few layers with only 1% or 10% of original ImageNet labels.</li><li>Use the fine-tuned network as teacher to yield labels on unlabeled data to train a smaller student ResNet-50.</li></ul></li></ul><h4 id="pros-and-cons-1">Pros and cons</h4><ul><li>usually light-weighted and perform better in discriminative downstream applications</li><li>Problems remain to be solved<ul><li>Scale to natural language pre-training</li><li>Sampling efficiency: hints from BYOL and SimSiam. The role that negative sampling plays in contrastive learning is still not clear.</li><li>data augmentation: in theory, why data augmentation can boost contrastive learning's performance is still not clear.</li></ul></li></ul><h3 id="generative-contrastive-adversarial-ssl">Generative-Contrastive (Adversarial) SSL</h3><p>Adversarial learning learns to reconstruct the original data distribution rather than the samples by minimizing the distributional divergence.</p><h4 id="generate-with-complete-input">Generate with Complete Input</h4><ul><li>Capturing the sample's complete information.</li><li>To extract the implicit distribution out <span class="math inline">\(p(z)\)</span>,<ul><li>AAE: the generator in GAN is an implicit autoencoder, which can be replaced by an explicit variational autoencoder (VAE).<ul><li>AAE substitutes the KLH divergence function for a discriminative loss.</li></ul></li></ul></li></ul><h4 id="recover-with-partial-input">Recover with Partial Input</h4><ul><li>Provide models with partial input and ask them to recover the rest parts. Similar as masked bert but this works in an adversarial manner.</li><li>Examples<ul><li>Colorization: given one color channel L in an image and predicting the value of two other channels A, B. The encoder and decoder networks can be set to any form of convolutional neural network.</li><li>Inpainting: ask the model to predict an arbitrary part of an image given the rest of it. Then a discriminator is employed to distinguish the inpainted image from the original one.</li><li>SRGAN: follows the same idea in inpainting,</li></ul></li></ul><h4 id="pre-trained-language-model-ptm">Pre-trained Language model (PTM)</h4><ul><li>Focus on maximum likelihood estimation based on pretext task.</li><li>Examples<ul><li>ELECTRA: outperform BERT.<ul><li>The generator is a small masked language model (MLM)</li><li>The discriminator will predict which words are replaced.</li><li>Training steps: first warming-up the generator by MLM pretext task. Then train with the discriminator.</li></ul></li><li>WKLM: perform Replaced Token Detection (RTD) at the entity-level.</li></ul></li></ul><h4 id="graph-learning">Graph learning</h4><ul><li>Adopt adversarial training<ul><li>ANE (adversarial network embedding) designs a generator that is updated in two stages: the generator encodes sampled graph into target embedding and computes traditional NCE with a context encoder like Skip-gram; discriminator will distinguish embedding from the generator and sampled one from a prior distribution.</li><li>GraphGAN: model the link prediction task and follow the original GANs style discriminative objective to distinguish directly at node-level rather than representation-level.</li><li>GraphSGAN: use the adversarial method in semi-supervised graph learning with the motivation that marginal nodes cause most classification errors in the graph. Between clusters, there are density gaps where few samples exist. They prove that we can complete classification theoretically if we generate enough fake samples in density gaps. The generator will generate fake nodes in density gaps during the training.</li></ul></li></ul><h4 id="domain-adaption-and-multi-modality-representation">Domain adaption and multi-modality representation</h4><ul><li>GAN can help on domain adaption: <a href="https://arxiv.org/abs/1505.07818">[1]</a>, <a href="https://arxiv.org/abs/1805.05151">[2]</a>, <a href="https://arxiv.org/abs/1505.07818">[42]</a>, <a href="https://www.researchgate.net/publication/318224334_Adversarial_Representation_Learning_for_Domain_Adaptation">[113]</a>.</li><li>Leverage adversarial sampling to improve the negative samples' quality: <a href="https://aclanthology.org/N18-1133.pdf">[16]</a>, <a href="https://arxiv.org/abs/1809.11017">[138]</a></li></ul><h4 id="pros-and-cons-2">Pros and cons</h4><ul><li>Challenges<ul><li>Limited applications in NLP and graph.</li><li>Easy to collapse</li><li>Not for feature extraction: Contrastive learning is more practical in extraction.</li></ul></li></ul><h3 id="theory-behind-ssl">Theory behind SSL</h3><h4 id="gan">GAN</h4><h5 id="divergence-matching">Divergence matching</h5><ul><li>Different divergence functions leads to different GAN variants. <a href="https://arxiv.org/abs/1606.00709">Paper</a> discusses the effects of various choices of divergence functions.</li></ul><h5 id="disentangled-representation">Disentangled representation</h5><ul><li>GAN shows its superior potential in learning disentangled features empirically and theoretically.</li><li>InfoGAN proposes to learn disentangled representation with DCGAN.<ul><li>Since mutual information is hard to compute, they leverage the variational inference approach to estimate its lower bound.</li></ul></li><li><a href="https://arxiv.org/abs/1811.10597">GAN dissection</a>: apply causal analysis into understanding GAN. They identify the correlations between channels in the convolutional layers and objects in the generated images, and examine whether they are causally-related with the output.</li><li><a href="https://openreview.net/pdf?id=SJxDDpEKvH">Paper</a> examines the channels' conditional independence via rigorous counterfactual interventions over them. They show that in BigGAN, it's possible to disentangle backgrounds and objects.</li></ul><h4 id="maximizing-lower-bound">Maximizing Lower Bound</h4><h5 id="evidence-lower-bound">Evidence lower bound</h5><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824130843214.png" alt="image-20210824130843214" style="zoom:80%;" /></p><p>ELBO is the lower bound of the optimization target KL divergence. VAE maximizes the ELBO to minimize the difference between <span class="math inline">\(q_\phi(z|x),p_\theta(z|x)\)</span>.</p><h5 id="mutual-information">Mutual information</h5><ul><li>Maximizes the MI of the input and its representation with joint density <span class="math inline">\(p(x|y\)</span> and marginal densities <span class="math inline">\(p(x),p(y)\)</span>.</li><li>Examples<ul><li>Deep Infomax maximizes the MI of local and global features and replaces KL-divergence with JS-divergence, which is similar to GAN.</li><li>Instance Discrimination directly optimizes the proportion of gap of positive pairs and negative pairs. One of the commonly used estimators is InfoNCE. And prove that useful to use large negative samples(large values of N. But then the other testify that increasing the number of negative samples does not necessarily help.</li><li>Maximizing the lower bound (MI and ELBO) is not sufficient to learn useful representations.</li><li>MI maximization can be analyzed from the metric learning view. By rewriting the InfoNCE MI as the triplet loss, it is corresponding to the expectation of the multi-class k-pair loss.</li></ul></li></ul><h4 id="contrastive-ss-representation-learning">Contrastive SS representation learning</h4><h5 id="relationship-with-supervised-learning">Relationship with Supervised learning</h5><ul><li><p>How contrastive pre-training benefits supervised learning?</p><ul><li><p><strong><em>SSL cannot learn more than supervised learning, but make it with few labels.</em></strong></p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824134109250.png" alt="image-20210824134109250" /><figcaption aria-hidden="true">image-20210824134109250</figcaption></figure></li></ul></li><li><p>SSL trained neural networks are more robust t adversarial examples, label corruption and common input corruptions. It also benefits OOD detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods.</p></li></ul><h5 id="understanding-contrastive-loss">Understanding Contrastive Loss</h5><ul><li><p>Split the contrastive loss into two terms</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824134816367.png" alt="image-20210824134816367" /><figcaption aria-hidden="true">image-20210824134816367</figcaption></figure><p>where the first term aims at “alignment” and the second aims at “uniformity” of sample vectors on a sphere given the normalization condition. They show that these two terms have a large agreement with downstream tasks.</p><ul><li>They show that by directly optimizing the two loss, it is consistently better than contrastive loss. And both these terms are necessary for a good representation.</li></ul></li><li><p>It's doubtful that whether alignment and uniformity are necessarily in the form of upper two losses. We may still achieve uniformity via other techniques such as exponential moving average, batch normalization, regularization and random initialization.</p></li></ul><h5 id="generalization">Generalization</h5><p>It is unclear why the learned representations should also lead to better performance on downstream tasks.</p><ul><li><a href="https://arxiv.org/abs/1902.09229">Paper</a> propose a conceptual framework to analyze contrastive learning on average classification tasks.<ul><li>Under the context of only 1 negative sample, it is proved that optimizing unsupervised loss benefits the downstream classification tasks.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824142746586.png" title="fig:" alt="image-20210824142746586" /></li><li>They argue that enlarging the number of negative samples does not hold for contrastive learning and shows that it can hurt performance when the negative samples exceed a threshold.</li></ul></li><li>Noise Contrastive Estimation(NCE) [49] explains that increasing the number of negative samples can provably improve the variance of learning parameters</li></ul><h3 id="discussion-and-future-directions">Discussion and future directions</h3><ul><li>Theoretical foundation<ul><li><a href="https://arxiv.org/abs/1902.09229">Paper</a> proposes a conceptual framework to analyze the contrastive objective's function in generalization ability.</li><li><a href="https://arxiv.org/abs/1907.13625">Paper</a> proves that the sampling strategies and architecture design may count more.</li></ul></li><li>Transferring to downstream tasks<ul><li>pre-training task selection problem: By ALBERT, NSP for bert may hurt its performance.</li><li>NAS to design pre-training tasks for a specific downstream task automatically.</li></ul></li><li>Transferring cross datasets (inductive learning)</li><li>Exploring potential of sampling strategies<ul><li>leverage super large amounts of negative samples and augmented positive samples, whose effects are studied in deep metric learning.</li><li>How to further release the power of sampling is still an unsolved and attractive problem.</li></ul></li><li>Early degeneration for contrastive learning<ul><li>the contrastive objectives often get trapped into embedding spaces’ early degeneration problem, which means that the model over-fits to the discriminative pretext task too early, and therefore lost the ability to generalize.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2006.08218.pdf&quot;&gt;Self-supervised Learning: Generative or Contrastive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
  </entry>
  
  <entry>
    <title>Book-Graph Representation Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-05-22-notes-book-grl.html"/>
    <id>http://yoursite.com/posts/notes/2021-05-22-notes-book-grl.html</id>
    <published>2021-05-22T16:10:39.000Z</published>
    <updated>2021-05-30T12:33:53.738Z</updated>
    
    <content type="html"><![CDATA[<p>Book <a href="https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf">Graph Representation Learning</a></p><a id="more"></a><h2 id="chapter-1-introduction">Chapter 1: Introduction</h2><h3 id="graphs">Graphs</h3><ul><li>Types and represent edges<ul><li>Simple graphs: at most one edge between each pair of nodes, no loop and undirected.</li><li>To represent graphs, adjacent matrix can be use. It shows as a symmetric matrix under simple graphs and may not symmetric under directed graphs.</li><li>Heterogeneous and multiplex graphs<ul><li>Heterogeneous graphs: both nodes and graphs are heterogeneous.</li><li>Multiplex: graph can be decomposed in a set of <span class="math inline">\(k\)</span> layers. Every layers corresponds to a unique relation, denoted as intra-layer edge, and those between layers are inter-layer edges. <strong>E.g., the graph of skeleton clips.</strong></li></ul></li></ul></li><li>Feature information<ul><li>If heterogeneous, each types of nodes may have different dimensions of attributes.</li></ul></li><li>Tasks on graphs with machine learning<ul><li>node classification: like whether users are bots or not, aka predict the label of node.<ul><li>Note that the <strong>nodes in a graph</strong> are not independently and identically distributed, which <strong>doesn't satisfy the requirements of i.i.d. in supervised learning.</strong></li><li>To inference the label, one idea is to use <em>homophily</em>, which assume that nodes tend to shared attributes with their neighbors in the graph. The other idea is <em>structural equivalence</em>, which assumes that nodes with similar local neighborhood structures will have similar labels.</li></ul></li><li>Relation prediction<ul><li>Like predicting the missing interactions.</li><li>Similar problems met in node classification (i.i.d.), and requires inductive biases that are specific to the graph domain.</li></ul></li><li>Clustering and community detection<ul><li>infer latent community structures given only the input graph.</li></ul></li><li>Graph classification, regression and clustering.<ul><li>In graph clustering, the goal is to learn an unsupervised measure of similarity between pairs of graphs.</li></ul></li></ul></li></ul><h2 id="chapter-2-background-traditional-approaches">Chapter 2: Background &amp; Traditional Approaches</h2><h3 id="graph-statistics-and-kernel-methods">Graph statistics and kernel methods</h3><h4 id="node-level-statistics">Node-level statistics</h4><ul><li><p>Node degree: simply counts the number of edges incident to a node, which will measure how many neighbors a node has.</p></li><li><p>Node centrality: To measure the importance of a node in a graph.</p><ul><li><p>Eigenvector centrality <span class="math inline">\(e_u\)</span></p><ul><li><p><span class="math inline">\(e_u=\frac{1}{\lambda}\sum\limits_{v\in V} \mathrm{A}[u,v]e_v,\forall u\in \mathcal{V}\)</span>., it measures that satisfies the recurrence in above equation corresponds to an eigenvector of the adjacency matrix.</p></li><li><p>The vector of centrality values is given by the eigenvector corresponding to the largest eigenvalues of <span class="math inline">\(\mathrm{A}\)</span>.</p></li><li><p><strong>The eigenvector centrality ranks the likelihood that a node is visited on a random walk of infinite length on the graph.</strong></p></li><li><p>After <span class="math inline">\(t\)</span> iteration, the eigenvector centrality will contain the number of length-<span class="math inline">\(t\)</span> paths arriving at each node.</p></li></ul></li><li><p>Betweeness centrality</p><ul><li>Measures how often a node lies on the shortest path between two other nodes.</li></ul></li><li><p>Closeness centrality</p><ul><li>Measures the average shortest path length between a node and all other nodes.</li></ul></li><li><p>More: M. Newman. Networks. Oxford University Press, 2018. 1, 12, 13, 108, 109</p></li></ul></li><li><p>The clustering coefficient</p><ul><li><p>Measure the structural distinction using variations of the clustering coefficient, which Measures the proportion of closed triangles in a node's local neighborhood.</p></li><li><p>The local variant of clustering coefficient is calculated by:</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522153036032.png" alt="image-20210522153036032" style="zoom:50%;" /></p></li><li><p>It Measures how tightly clustered a node's neighborhood is.</p></li><li><p>In social and biological sciences, they tend to have far higher clustering coefficients than one would expect if edges were sampled randomly.</p></li></ul></li><li><p>Closed Triangles, Ego Graphs, and Motifs</p><ul><li>The global clustering coefficient. Similar equation but this time count in the node's ego graph.</li><li>A node's ego graph: the subgraph containing that node, its neighbors, and all the edges between nodes in its neighborhood.</li><li>The general version of these ideas is counting arbitrary motifs or graphlets within a node's ego graph. E.g.: triangles, cycles of particular length etc.</li></ul></li></ul><h4 id="graph-level-features-and-graph-kernels">Graph-level features and graph kernels</h4><ul><li><p>Bag of nodes</p><ul><li>Aggregate node-level statistics</li><li>E.g., the histograms, the statistics based on the degrees, centralities and clustering coefficients of the nodes.</li><li>May miss important global properties in graph</li></ul></li><li><p>The Weisfieler-Lehman Kernel</p><ul><li><p>Iterative neighborhood aggregation. Extract node-level features that contain more information than just their local Ego graph, and then to aggregate these features into a representation.</p></li><li><p>The most well-known one is the Weisfieler-Lehman (WL) algorithm.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522162120249.png" alt="image-20210522162120249" style="zoom:57%;" /></p><ul><li>In other words, the WL kernel is computed by measuring the difference between the resultant label sets for two graphs.</li><li>Useful while solving the isomorphism problem: approximate graph isomorphism is to check whether or not two graphs have the same label set after <span class="math inline">\(K\)</span> rounds of the WL algorithm.</li></ul></li></ul></li><li><p>Graphlets and path-base methods</p><ul><li>Graphlets: count the occurrence of different small subgraph structures, but it's computationally difficult.</li><li>Path-based methods: examine the different kinds of paths that occur in the graph. E.g. <a href="https://www.aaai.org/Papers/ICML/2003/ICML03-044.pdf">random walk</a> , the <a href="https://ieeexplore.ieee.org/document/1565664">shortest path kernel</a> that is similar as random walk but uses only the shortest-paths between nodes.</li></ul></li></ul><h3 id="neighborhood-overlap-detection">Neighborhood overlap detection</h3><ul><li>The features mentioned above don't quantify the relationships among nodes, and thus won't work well on relation prediction.</li><li>The simplest neighborhood overlap measure: just counts the number of neighbors that two nodes share.</li><li>Hope that node-node similarity Measures computed on the training edges will lead to accurate predictions about the existence of test edges.</li></ul><h4 id="local-overlap-measures">Local overlap measures</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522180625718.png" alt="image-20210522180625718" style="zoom:80%;" /></p><ul><li>Use the functions of the number of common neighbors two nodes share.<ul><li>Sorensen index: normalized by the sum of the node degrees.</li><li>Salton index that normalizes by the product of the degrees of u and v.</li><li>Jaccard overlap: normalized by the degree of union u and v.</li></ul></li><li>Consider the importance of common neighbours, give more weight to common neighbours that have low degree.<ul><li>Resource Allocation index (RA): counts the inverse degrees of the common neighbours.</li><li>Adamic-Adar index (AA): similar as RA but use the inverse logarithm of the degrees.</li></ul></li></ul><h4 id="global-overlap-measures">Global overlap measures</h4><ul><li><p>Katz Index</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522180701697.png" alt="image-20210522180701697" style="zoom:67%;" /></p><ul><li>The most basic global overlap statistic. Simply count the number of paths of all lengths between a pair of nodes.</li><li>This index is a geometric series of matrices.</li><li>It's strongly biased by node degree which will give higher overall similarity scores when considering high-degree nodes cause they will generally be with more paths.</li></ul></li><li><p>Leicht, Holme, and Newman (LHN) similarity</p><ul><li><p>Considering the ratio between the actual number of observed paths and the number of expected paths between two nodes.</p></li><li><p>To compute expectation, the <em>configuration model</em> is relied on. Under a random configuration model, the likelihood of an edge is simply proportional to the product of the two node degrees. But this heuristic calculation is intractable.</p></li><li><p>To approximate, the number of paths between two nodes grows by the largest eigenvalue of adjacent matrix. (The fact that the largest eigenvalue can be used to approximate the growth in the number of paths).</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522182722632.png" alt="image-20210522182722632" style="zoom:67%;" />, and the solution to the matrix series can be written as <span class="math display">\[\mathrm{S_{LNH}}=2\alpha m\lambda_1 \mathrm{D^{-1}(I-\frac{\beta}{\lambda_1}A)^{-1}D^{-1}}\]</span></p></li></ul></li><li><p>Random walk methods</p><ul><li>rather than exact count of paths over the graph in previous introduced index, consider random walks.</li><li>A measure of importance specific to node u will be obtained since the random walks are continually being teleported back to that node.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522183303798.png" alt="image-20210522183303798" style="zoom:80%;" /></li></ul></li></ul><h3 id="graph-laplacians-and-spectral-methods">Graph Laplacians and spectral methods</h3><p>learning to cluster the nodes in a graph.</p><h4 id="graph-laplacians">Graph Laplacians</h4><ul><li><p>Unnormalized Laplacian</p><ul><li><p><span class="math inline">\(\mathrm{L=D-A}\)</span></p></li><li><p>It's symmetric and positive semi-definite. <span class="math inline">\(\mathrm{L}\)</span> has <span class="math inline">\(|V|\)</span> non-negative eigenvalues.</p></li><li><p><span class="math inline">\(\mathrm{x^TLx}=\sum\limits_{(u,v)\in\mathcal{E}}(\mathrm{x}[u]-\mathrm{x}[v])^2\)</span></p></li><li><p>If the graph contains <span class="math inline">\(K\)</span> connected components, then there exists an ordering of the nodes in the graph such that the Laplacian matrix can be written as <span class="math display">\[\mathrm{L}={\begin{bmatrix}\mathrm{L}_1 &amp;  &amp; &amp; \\&amp; \mathrm{L}_2 &amp; &amp; \\ &amp;  &amp; \ddots&amp; \\  &amp;  &amp; &amp;  \mathrm{L}_K\\\end{bmatrix}},\]</span> where each blocks is a valid graph Laplacian of a fully connected subgraph of the original graph. The spectrum of <span class="math inline">\(\mathrm{L}\)</span> is the union of the eigenvalues of the <span class="math inline">\(\mathrm{L}_K\)</span> matrices and the eigenvectors are the union of the eigenvectors of all the <span class="math inline">\(\mathrm{L}_K\)</span> matrices with 0 values filled at the positions of the other blocks.</p></li></ul></li><li><p>Normalized Laplacians</p><ul><li><span class="math inline">\(\mathrm{L_{sym}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}}\)</span>, while random walk Laplacian is defined as <span class="math inline">\(\mathrm{L_{RW}=D^{-1}L}\)</span>.</li><li>For <span class="math inline">\(\mathrm{L_{sym}}\)</span>, the properties of previous mentioned holds but with the eigenvectors for the 0 eigenvalue scaled by <span class="math inline">\(\mathrm{D^{\frac{1}{2}}}\)</span>. For <span class="math inline">\(\mathrm{L_{RW}}\)</span>, the properties hold exactly.</li></ul></li><li><p>These methods just allow to cluster nodes that are already in disconnected components. The Laplacian can be used to get an optimal clustering of nodes within a fully connected graph.</p></li></ul><h4 id="graph-cuts-and-clustering">Graph cuts and clustering</h4><ul><li><p>Graph cuts</p><ul><li>An optimal cluster means that a partition that minimizes the cut value, which is the count of how many edges cross the boundary between the partition of nodes. Theoretically the methods tend to simply make clusters that consists of a single node.</li><li>One way to solve this is minimizing the <em>Ratio Cut</em>, which penalizes the solution of choosing small cluster sizes.</li><li>Another popular solution is minimize the Normalized Cut (NCut), which enforces that all clusters have a similar number of edges incident to their a nodes.</li></ul></li><li><p>Approximating the RatioCut with the Laplacian Spectrum</p><ul><li><p>The ratio cut minimization problem can be approximated as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524082410207.png" alt="image-20210524082410207" style="zoom:40%;" /><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524082433260.png" alt="image-20210524082433260" style="zoom:50%;" /></p></li><li><p>The formula above is NP-hard, after simplification, it will be :</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524082658389.png" alt="image-20210524082658389" style="zoom:67%;" />, and the solution of this problem is the second smallest eigenvector of <span class="math inline">\(\mathrm{L}\)</span>.</p></li></ul></li><li><p>Generalized spectral clustering</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524104844842.png" title="fig:" alt="image-20210524104844842" /></li></ul></li></ul><h1 id="part-i-node-embeddings">Part I: Node Embeddings</h1><h2 id="chapter-3-neighborhood-reconstruction-methods">Chapter 3 Neighborhood Reconstruction Methods</h2><p>Goal: encode nodes as low-dimensional vectors that summarize their graph position and the structure of their local graph neighborhood.</p><h3 id="an-encoder-decoder-perspective">An encoder-decoder perspective</h3><p>An encoder maps each node in the graph into a low-dimensional vector or embedding, a decoder takes the embeddings and uses them to reconstruct information about each node's neighborhood.</p><h4 id="the-encoder">The encoder</h4><ul><li>Most works rely on <em>shallow embedding</em>, where this encoder function is simply an embedding lookup based on the node ID.</li><li>The generalized encoders are called GNNs.</li></ul><h4 id="the-decoder">The decoder</h4><ul><li>They might predict the neighbors of one given node or one row in the graph adjacency matrix.</li><li>The standard practice is to define pairwise decoders, which can be interpreted as predicting the relationship or similarity between pairs of nodes.</li><li>While reconstruction, the goal is to optimize the encoder and decoder to minimize the reconstruction loss.</li></ul><h4 id="optimizing-an-encoder-decoder-model">Optimizing an encoder-decoder model</h4><ul><li>Usually use SGD, and minimize the disparity of decoded latent distance and real distance.</li></ul><h4 id="typical-encoder-decoder-methods">Typical encoder-decoder methods</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524113343676.png" alt="image-20210524113343676" style="zoom:67%;" /></p><h3 id="factorization-based-approaches">Factorization-based approaches</h3><p>Methods below are classified as factorization methods case their loss functions can be minimized using factorization algorithms like SVD.</p><p>The goal of these methods is to learn embeddings for each node such that the inner product between the learned embedding vectors approximates some deterministic measure of node similarity.</p><p>The inner product methods all employ deterministic measures of node similarity.</p><h4 id="laplacian-eigenmaps-le">Laplacian eigenmaps (LE)</h4><ul><li><p>Builds upon the spectral clustering</p></li><li><p>Formula: <span class="math display">\[\mathcal{L}=\sum\limits_{(u,v)\in\mathcal{D}}\mathrm{DEC}(\mathrm{z}_u,\mathrm{z}_v)\cdot\mathrm{S}[u,v],\]</span> which penalizes the model when very similar nodes have embeddings that are far apart.</p></li><li><p>If the reconstructed <span class="math inline">\(\mathrm{S}\)</span> satisfies the properties of a Laplacian matrix, then the node embeddings that minimize the loss are identical to the solution for a spectral clustering.</p></li></ul><h4 id="inner-product-methods">Inner-product methods</h4><ul><li>More recent work employs inner-product, which assumes that the similarity between two nodes, e.g., the overlap between their local neighborhoods is proportional to the dot product of their embeddings.</li><li>Examples:<ul><li>Graph factorization: <a href="https://dl.acm.org/doi/10.1145/2488388.2488393">Distributed large-scale natural graph factorization</a></li><li>GraRep: <a href="https://dl.acm.org/doi/10.1145/2806416.2806512">Learning graph representations with global structural information</a></li><li>HOPE: <a href="https://dl.acm.org/doi/10.1145/2939672.2939751">Asymmetric transitivity preserving graph embedding</a></li><li>All of them used the mean squared error as loss function. They differ primarily in how they define <span class="math inline">\(\mathrm{S[u,v]}\)</span>. GF uses the adjacency matrix and sets <span class="math inline">\(\mathrm{S=A}\)</span>, GraRep defines <span class="math inline">\(\mathrm{S}\)</span> based on powers of the adjacency matrix and HOPE supports general neighborhood overlap measures.</li></ul></li></ul><h3 id="random-walk-embedding">Random walk embedding</h3><p>Use stochastic measures of node neighborhood overlap.</p><h4 id="deepwalk-and-node2vec">Deepwalk and node2vec</h4><ul><li><p>The two methods differ on the inner-product decoder (the notions of node similarity and neighborhood reconstruction). They all optimize embeddings to encode the statistics of random walks. Mathematically, the goal is to learn embeddings so that <span class="math display">\[\mathrm{DEC(z}_u,\mathrm{z}_v)\triangleq\frac{\mathrm{z}_u^\top\mathrm{z}_v}{\sum\limits_{v_k\in\mathcal{V}}\mathrm{z}_u^\top\mathrm{z}_k}\approx p_{\mathcal{G,T}}(v|u),\label{4}\]</span> where <span class="math inline">\(p_{\mathcal{G,T}}(v|u)\)</span> is the probability of visiting <span class="math inline">\(v\)</span> on a length-<span class="math inline">\(T\)</span> random walk starting at <span class="math inline">\(u\)</span>, and <span class="math inline">\(T\)</span> is the range.</p></li><li><p>To train the random walk embeddings, use the cross-entropy loss.</p></li><li><p>DeepWalk employs a hierarchical softmax to approximate equation above, where the normalizing factor is approximated using <em>negative samples</em> in the <a href="https://arxiv.org/abs/1607.00653">following way</a> <span class="math display">\[\mathcal{L}=\sum\limits_{(u,v)\in\mathcal{D}}-\log(\sigma(\mathrm{z}_u^\top\mathrm{z}_v))-\gamma\mathbb{E}_{v_n\sim P_n{\mathcal{V}}}[\log(-\sigma(\mathrm{z}_u^\top\mathrm{z}_v))]\]</span></p></li><li><p>DeepWalk employ uniformly random walks to define the visiting probability and node2vec use hyperparameters to allow the probabilities to smoothly interpolate between walks.</p></li></ul><h4 id="line">LINE</h4><ul><li><a href="http://dx.doi.org/10.1145/2736277.2741093">Large-scale information network embeddings</a>. Rather than explicitly leverage random walks, it shares ideas from DeepWalk and node2vec. It combines two encoder-decoder objectives.</li><li>It has two objectives, the 1starting is <span class="math inline">\(\mathrm{DEC(z}_u,\mathrm{z}_v)=\frac{1}{1+e^{-\mathrm{z}_u^\top\mathrm{z}_v}}\)</span>, and the 2nd has the same equation as <span class="math display">\[\eqref{4}\]</span>,but takes the KL-divergence to encode two-hop adjacency information.</li><li>Instead of random walks, it explicitly reconstructs 1st and 2nd order neighborhood information.</li></ul><h4 id="additional-variants-of-the-random-walk">Additional variants of the random-walk</h4><ul><li>biasing or modifying the random walks.<ul><li>consider random walks that skip over nodes, <a href="https://arxiv.org/pdf/1605.02115.pdf">Perozzi et al.</a></li><li>define random walks based on the structural relationships between nodes: <a href="http://dx.doi.org/10.1145/3097983.3098061">Ribeiro et al.</a></li></ul></li></ul><h3 id="random-walk-and-matrix-factorization">Random walk and matrix factorization</h3><ul><li>https://dl.acm.org/doi/10.1145/3159652.3159706</li><li>Random walk are closely related to matrix factorization.</li><li>The embeddings learned by DeepWalk are closely related to the spectral clustering embeddings, but DeepWalk embeddings control the influence of different eigenvalues through T.</li><li>The disadvantages of shallow embedding<ul><li>It doesn't share any parameters between noes in the encoder, and therefore statistically and computationally inefficient.</li><li>It doesn't leverage node features in the encoder.</li><li>It's inherently trans-ductive, which means it only generate embeddings for nodes that were present during the training phase.</li></ul></li></ul><h2 id="chapter-4-multi-relational-data-and-knowledge-graphs">Chapter 4: Multi-relational data and knowledge graphs</h2><p>The knowledge graph completion is to predict missing edges in the graph, generally. Below only covers the node embeddings way in graph completion.</p><h3 id="reconstruction-multi-relational-data">Reconstruction multi-relational data</h3><ul><li><p>Note the edges have multiple types, and the input for decoder will be a pair of nodes and types of the edge.</p></li><li><p>One simple way is <a href="https://icml.cc/2011/papers/438_icmlpaper.pdf">RESCAL</a>: <span class="math display">\[\mathrm{DEC}(u,\tau,v)=\mathrm{z}_u^\top\,\mathrm{R}_\tau\mathrm{z}_v,\]</span> where the embeddings <span class="math inline">\(\mathrm{z}\)</span> and relation matrices <span class="math inline">\(\mathrm{R}\)</span> are all learnable.</p></li><li><p>While solving the Reconstruction error, it's like tensor factorization.</p></li><li><p>Nearly all multi-relational embedding methods simply define the similarity measure directly based on the adjacency tensor, or to say they all try to reconstruct immediate neighbors from the low-dimensional embeddings.</p></li></ul><h3 id="loss-functions">Loss functions</h3><h4 id="cross-entropy-with-negative-sampling">Cross-entropy with negative sampling</h4><ul><li>The formula</li></ul><p><span class="math display">\[\mathcal{L}=\sum\limits_{(u,\tau,v)\in\mathcal{E}}-\log(\sigma(\mathrm{DEC(z}_u,\tau,\mathrm{z}_v)))-\gamma\mathbb{E}_{v_n\sim P_{n,u}{\mathcal{V}}}[\log(\sigma(-\mathrm{DEC(z}_u,\tau,\mathrm{z}_{v_n}))],\]</span></p><p>where <span class="math inline">\(P_{n,u}(\mathcal{V})\)</span> denotes a negative sampling distribution. The 1st term denotes the log-likelihood that we predict "true" for an edge that does actually exist in the graph, and the 2nd term is the expected log-likelihood that is correctly predicted "false" for an edge that does not exist in the graph.</p><p>After approximating with Monte Carlo, the loss will be <span class="math display">\[\mathcal{L}=\sum\limits_{(u,\tau,v)\in\mathcal{E}}-\log(\sigma(\mathrm{DEC(z}_u,\tau,\mathrm{z}_v)))-\sum\limits_{v_n\in \mathcal{P}_{n,u}}[\log(\sigma(-\mathrm{DEC(z}_u,\tau,\mathrm{z}_{v_n}))],\]</span> , where <span class="math inline">\(\mathcal{P}_{n,u}\)</span> is a set of nodes sampled from <span class="math inline">\(P_{n,u}(\mathcal{V})\)</span>.</p><ul><li>How to define negative sampling distribution?<ul><li>Simply use a uniform distribution over all nodes, but this may get false negative.</li><li>Sample negative samples that satisfy a predefined type constraints.</li><li>Draw negative samples for both the head node and the tail node of the relation.</li></ul></li></ul><h4 id="max-margin-loss">Max-Margin loss</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524174533295.png" alt="image-20210524174533295" style="zoom:67%;" /></p><p>Contrastive estimation to get the negative sample.</p><h3 id="multi-relational-decoders">Multi-relational decoders</h3><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524233843533.png" alt="image-20210524233843533" style="zoom:80%;" /></p><h4 id="methods">Methods</h4><ul><li>RESCAL</li></ul><p>But it's computationally expensive.</p><ul><li>Translational Decoders<ul><li><p><em><a href="https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf">TransE</a></em>, the likelihood of an edge is proportional to the distance between the embedding of the head node and the tail node, after translating the head node according to the relation embedding. <span class="math display">\[\mathrm{DEC(z}_u,\tau,\mathrm{z}_{v})=-\|\mathrm{z}_u+\mathrm{r}_\tau-\mathrm{z}_{v}\|\]</span></p></li><li><p>Limitation: simplicity.</p></li><li><p>The variants of TransE</p><ul><li><p><a href="">TransX</a></p><p>Import a trainable transformation that depend on the relation <span class="math inline">\(\tau\)</span>. <span class="math display">\[\mathrm{DEC(z}_u,\tau,\mathrm{z}_{v})=-\|g_{1,\tau}(\mathrm{z}_u)+\mathrm{r}_\tau-g_{2,\tau}(\mathrm{z}_{v})\|\]</span></p></li><li><p><a href="https://persagen.com/files/misc/wang2014knowledge.pdf">TransH</a></p><p>Project the entity embeddings onto a learnable relation-specific hyperplane-defined by the normal vector <span class="math inline">\(\mathrm{w}_r\)</span>-before performing translation.</p></li></ul></li></ul></li><li>Multi-Liner dot products<ul><li>Also known as <a href="https://arxiv.org/abs/1412.6575">DistMult</a>.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525081148389.png" alt="image-20210525081148389" style="zoom:80%;" /></li><li>Generalizing the dot-product decoder from simple graphs.</li><li>One <strong>limitation</strong> is that it can only encode symmetric relations while there are many directed graph and thus they are asymmetric.</li></ul></li><li>Complex decoders<ul><li><p><a href="http://proceedings.mlr.press/v48/trouillon16.pdf">ComplEx</a>, augmenting the DistMult by employing complex-valued embeddings. It's defined as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525081551755.png" alt="image-20210525081551755" style="zoom:80%;" /></p></li><li><p><a href="https://arxiv.org/abs/1902.10197">RotatE</a> defies the decoder as rotations in the complex plane as:</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525082511044.png" alt="image-20210525082511044" style="zoom:80%;" /></p><p>each dimension of the relation embedding can be represented as <span class="math inline">\(\mathrm{r}_\tau[i]=e^{i\theta_{r,i}}\)</span> and thus corresponds to a rotation in the complex plane.</p></li></ul></li></ul><h4 id="representational-abilities">Representational abilities</h4><p>The Multi-relational decoders can represent different logical patterns on relations.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525083801306.png" alt="image-20210525083801306" style="zoom:80%;" /></p><ul><li>Symmetry and anti-symmetry<ul><li>Symmetric decoders: DistMult</li><li>Anti-symmetric decoders: TransE</li></ul></li><li>Inversion: implies the existence of another with opposite directionality.</li><li>Compositionality:<ul><li>RESCAL: <span class="math inline">\(\mathrm{R_{\tau_3}=R_{\tau_2}R_{\tau_1}}\)</span></li><li>TransE: <span class="math inline">\(\mathrm{r_{\tau_3}=r_{\tau_2}+r_{\tau_1}}\)</span></li></ul></li></ul><h1 id="part-ii-gnns">Part II: GNNs</h1><h2 id="chapter-5-the-gnn-model">Chapter 5 The GNN model</h2><p>GNNs are more complex than shallow embedding, they will generate representations of nodes that actually depend on the structure of the graph.</p><p>A key desideratum of GNNs is that they should be permutation invariant.</p><h3 id="neural-message-passing">Neural message passing</h3><p>After GNN, the embeddings contain structure-based information and feature-based information. However, the feature based information is in their k-hop neighborhoods.</p><p>The message passing in GNN can be taken as using update and aggregate functions.</p><h4 id="the-basic-gnn">The basic GNN</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525122930060.png" alt="image-20210525122930060" style="zoom:67%;" /></p><ul><li>The message passing in basic GNN framework is like a standard multi-layer perception.</li></ul><h4 id="with-self-loop">With self-loop</h4><ul><li>Add self-loops to omit the explicit update step.</li><li>This can alleviate overfitting, but also limit the expressivity of the GNN, sample the information coming the nodes neighbours cannot be differentiated from the information from the node itself.</li><li>In basic GNN, adding self-loops means sharing parameters between the <span class="math inline">\(\mathrm{W}_{self}\)</span> and <span class="math inline">\(\mathrm{W}_{neigh}\)</span> matrices.</li></ul><h3 id="generalized-neighborhood-aggregation">Generalized neighborhood aggregation</h3><h4 id="neighborhood-normalization">Neighborhood normalization</h4><ul><li><p>Simply normalized the aggregation operation based upon the degrees of the nodes involved: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526114126952.png" alt="image-20210526114126952" style="zoom:50%;" /></p></li><li><p>Symmetric normalization: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526114205704.png" alt="image-20210526114205704" style="zoom:50%;" /></p></li><li><p>Combining the symmetric-normalized aggregation along with the basic GNNs update function results in a first-order approximation of a spectral graph convolution.</p></li><li><p>GCNs</p><ul><li><p>They use symmetric-normalized aggregation as well as the self-loop update approach.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526114617065.png" alt="image-20210526114617065" style="zoom:67%;" /></p></li><li><p>In GNN, the use of normalization can lead to a loss of information.</p></li><li><p>Normalization is most helpful in tasks where noes feature information is far more useful than structural information, or where there is a very wide range of node degrees that can lead to instabilities during optimization.</p></li></ul></li></ul><h4 id="set-aggregation">Set aggregation</h4><p>The embeddings of neighbours, there is no natural ordering of a nodes' neighbours, and any aggregation function we define must thus be permutation invariant.</p><ul><li><p>Set pooling</p><ul><li><p>Define an aggregation function based on permutation invariant, can be implemented by just adding some MLP layers, e.g.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526122458994.png" alt="image-20210526122458994" style="zoom:67%;" /></p><p>this always lead to small increases in performance, though with the risk of overfitting.</p></li><li><p>Or use element-wise maximum or minimum to replace summation.</p></li></ul></li><li><p>Janossy pooling</p><ul><li><p>More powerful, apply a permutation-sensitive function and average the result over many possible permutations. In practice, the permutation-sensitive function is defined to be an LSTM.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526124251769.png" alt="image-20210526124251769" style="zoom:80%;" /></p></li><li><p>If the set of permutations <span class="math inline">\(\prod\)</span> is equal to all possible permutations, then the aggregator is also a universal function approximator for sets. Like simple summation based set pooling.</p></li><li><p>In practice, Janossy pooling employs one of two approaches</p><ul><li>Sample a random subsets of possible permutations during each application of the aggregator, and only sum over that random subset.</li><li>Employ a canonical ordering of the nodes in the neighborhood set, e.g., order the nodes in descending order according to their degree, with ties broken randomly.</li></ul></li><li><p>Janossy-style pooling can improve upon set pooling in a number of synthetic evaluation setups.</p></li></ul></li></ul><h4 id="neighborhood-attention">Neighborhood attention</h4><ul><li><p>Assign an attention weight or importance to each neighbour, which is used to weigh this neighbor's influence during the aggregation step.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526130931074.png" alt="image-20210526130931074" style="zoom:67%;" /></p></li><li><p>Popular variants of attention include the bilinear attention model and variations of attention layers using MLPs.</p></li><li><p>Multi heads methods are popular, which is also known as transformer.</p><ul><li>The basic idea behind transformers is to define neural network layers entirely based n the attention operation. The basic transformer layer is extractly equivalent to a GNNs layer using multi-headed attention if we assume that the GNNs receives a fully connected graph input.</li><li>The time complexity is the square of the number of nodes cause each pairs attention need to be calculated.</li><li>Attention can influence the inductive bias of GNNs.</li></ul></li></ul><h3 id="generalized-update-methods">Generalized update methods</h3><ul><li><p>One popular way is <a href="https://arxiv.org/abs/1706.02216">GraphSAGE</a>, which introduced the idea of generalized Neighborhood aggregation.</p></li><li><p>Over-smoothing of GNN: after several iterations of GNN message passing, the representations for all the nodes in the graph can become very similar to one another.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526135929759.png" alt="image-20210526135929759" style="zoom:80%;" /></p></li><li><p>The measure of how much the initial embedding of node <span class="math inline">\(u\)</span> influences the final embedding of node <span class="math inline">\(v\)</span> in the GNN is proportional to the probability of visiting node <span class="math inline">\(v\)</span> on a length-<span class="math inline">\(k\)</span> random walk starting from node <span class="math inline">\(u\)</span>.</p></li></ul><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526140143590.png" alt="image-20210526140143590" style="zoom:80%;" /></p><p>As <span class="math inline">\(k\rightarrow\infin\)</span>, the influence of every node approaches the stationary distribution of random walks over the graph, meaning that local neighborhood information is lost.</p><ul><li>When using simple GNN models, and especially those with the self-loop update approach-building deeper models can actually hurt performance.</li></ul><h4 id="concatenation-and-skip-connections">Concatenation and skip-connections</h4><p>Like using vector concatenation or skip connection that try to directly preserve information from previous rounds of message passing during the update step.</p><ul><li>Do concatenation to preserve more node-level information during message passing<ul><li>The key intuition is that we encourage the model to disentangle information during message passing--separating the information coming from the neighbors from the current representation of each model.</li></ul></li><li>Do linear interpolation between the previous representation and the representation that was updated based on the neighborhood information.</li><li>In practice, these techniques tend to be most useful for node classification tasks with moderately deep GNNs, and excel on tasks that exhibit homophily.</li></ul><h4 id="gated-updates">Gated updates</h4><p>One way to view the GNN meaning passing is that the aggregation function is receiving an observation from the neighbors, which is then used to update the hidden state of each node. Simply replace the hidden state argument of the RNN update function with the node's hidden state, and replace the observation vector with the message aggregated from the local neighborhood.</p><ul><li><a href="https://arxiv.org/abs/1511.05493">GRU based</a></li><li><a href="https://arxiv.org/abs/1802.03685">LSTM based</a></li></ul><h4 id="jumping-knowledge-connection">Jumping knowledge connection</h4><p>To improve the quality of the final node representations</p><ul><li><a href="https://arxiv.org/abs/1806.03536">Jumping knowledge connections</a>: One simple way is to simply leverage the representations at each layer of message passing, rather than only using the final layer output.</li><li>With max-pooling and LSTM attention layers the result is improved.</li></ul><h3 id="edge-features-and-multi-relational-gnns">Edge features and Multi-relational GNNs</h3><h4 id="relational-gnns">Relational GNNs</h4><ul><li><p><a href="https://arxiv.org/abs/1703.06103">RGCN</a>: relational graph convolutional network</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526152624397.png" alt="image-20210526152624397" style="zoom:67%;" /></p><ul><li>Augment the aggregation function to accommodate multiple relation types by specifying a separate transformation matrix per relation type.</li><li>The Multi-relational aggregation in RGCN is thus analogous to the basic a GNN approach with normalization, but we separately aggregate information across different edge types.</li></ul></li><li><p>Parameter sharing</p><ul><li><p>For RGCN, the increase of number of parameters is caused by that each edge type requires a trainable matrix.</p></li><li><p>To fix this, <a href="https://arxiv.org/abs/1703.06103">Schlichtkrull et al.</a> proposed to share with basis matrices, aka an alternative view of the parameter sharing RGCN approach is that we are learning an embedding for each relation, as well as a tensor that is shared across all relations.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526160241907.png" alt="image-20210526160241907" style="zoom:80%;" /></p></li></ul></li><li><p>Extensions and variations</p><ul><li>define separate aggregation matrices per relation as relational GNNs.<ul><li><a href="http://dx.doi.org/10.1093/bioinformatics/bty294">without parameter sharing</a></li><li>http://dx.doi.org/10.18653/v1/d17-1159</li><li><a href="https://arxiv.org/pdf/1911.06962.pdf">RGCN+attention</a></li></ul></li></ul></li></ul><h4 id="attention-and-feature-concatenation">Attention and feature concatenation</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526161216469.png" alt="image-20210526161216469" style="zoom:80%;" /></p><p>https://www.aclweb.org/anthology/D19-1458/</p><h3 id="graph-pooling">Graph pooling</h3><p>To get <strong>representations in graph-level</strong> cause the goal is to pool together the node embedding in order to learn an embedding of the entire graph.</p><ul><li><p>Set pooling approaches</p><ul><li>One popular way it taking a sum or mean of the node embeddings. This is sufficient for small graphs.</li><li>The other popular way is using a combination of LSTM and attention to pool the node embeddings.<ul><li>The way is like what's done is one-head bert, the output of graph-level representation is the concatenation of output from different timestep.</li></ul></li></ul></li><li><p>Graph coarsening approaches</p><p>To exploit the structure of the graph. One popular strategy to accomplish this is to perform graph clustering or coarseing as a means to pool the node representations.</p><ul><li>Estimate assignment matrices<ul><li>Use spectral clustering (the decomposition of adjacent matrix to estimate assignment matrix (from node representations to graph-level representations.))</li><li><a href="https://arxiv.org/abs/1806.08804">Employ another GNN to predict cluster assignments</a></li></ul></li><li>Use the assignment matrices to coarsen the graph.</li></ul></li></ul><h3 id="generalized-message-passing">Generalized message passing</h3><p>Leverage edge and graph-level information at each stage of message passing</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526164837669.png" alt="image-20210526164837669" style="zoom:67%;" /></p><h2 id="chapter-6-gnns-in-practice">Chapter 6 GNNs in practice</h2><p>optimization, regularization and application-based</p><h3 id="applications-and-loss-functions">Applications and loss functions</h3><ul><li><p>GNNs for node classification</p><ul><li>If fully supervised, use the negative log-likelihood loss and softmax to denote the predicted probability.</li><li>transductive and inductive nodes<ul><li>transductive nodes: nodes used while training but not covered while calculating loss. Semi-supervised means that the GNN is tested on transductive nodes.</li><li>inductive: not used in either the loss computation or the GNN message passing operations during training.</li></ul></li></ul></li><li><p>GNNs for graph classification</p><ul><li>similar loss functions but use graph-level representations.</li></ul></li><li><p>GNNs for relation prediction</p><ul><li>use the pairwise node embedding loss functions.</li></ul></li><li><p>Pretraining</p><ul><li><p>Pre-training the GNN using one of the neighborhood reconstruction losses.</p><ul><li><p>E.g., pre-train a GNN to reconstruct missing edges in the graph before fine-tuning on a node classification loss. However, <strong><a href="https://arxiv.org/abs/1809.10341">Veličković et al</a> found that a randomly initialized GNN is equally strong compared to one pre-trained on a neighborhood reconstruction loss.</strong></p></li><li><p>DGI (deep graph infomax): maximize the mutual information between node embeddings and graph embeddings. The basic idea is the GNN model must learn to generate node embeddings that can distinguish between the real graph and its corrupted counterpart.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526172943470.png" alt="image-20210526172943470" style="zoom:67%;" /></p></li></ul></li></ul></li></ul><h3 id="efficiency-concerns-and-node-sampling">Efficiency concerns and node sampling</h3><ul><li><p>Graph-level implementations</p><ul><li>Use sparse matrix multiplications and add self-loops to avoid redundant computations, but it requires operating on tested entire graph and all node features simultaneously, which may not be feasible due to memory limitations.</li></ul></li><li><p>Sampling and mini-batch</p><p>Work with a subset of nodes during message passing.</p><ul><li>The challenge is we cannot simply Run message passing on a subsets of the nodes in a graph without losing information.</li><li>One way is subsampling node neighbors. First select a set of target nodes for a batch and then to recursively sample the neighbors of these nodes in order to ensure that the connectivity of the graph is maintained. Subsample the neighbors of each node, using a fixed sample size to improve the efficiency of batched tensor operations.</li></ul></li></ul><h3 id="parameter-sharing-and-regularization">Parameter sharing and regularization</h3><p>Like L2 regularization, dropout and layer normalization that both work on GNNs and CNNs.</p><ul><li>Parameter sharing across layers<ul><li>use the same parameters in all the aggregate and update functions in the GNN. It's most effective in GNNs with more than six layers, and is often used in conjunction with gated update functions.</li></ul></li><li>Edge dropout<ul><li>randomly remove edges in the adjacency matrix during training, with the intuition that this will make the GNN less prone to overfitting and more robust to noise in the adjacency matrix.</li><li>The neighborhood subsampling approaches lead to this kind of regularization as a side effect, making it a very common strategy in large-scale GNN applications.</li></ul></li></ul><h2 id="chapter-7-theoretical-motivations">Chapter 7 Theoretical motivations</h2><h3 id="gnns-and-graph-convolutions">GNNs and graph convolutions</h3><p>Generalize the notion of convolutions to general graph structured data.</p><h4 id="convolutions-and-the-fourier-transform">Convolutions and the Fourier transform</h4><ul><li>The Fourier analysis<ul><li>The coefficients of Fourier series tell the amplitude if the complex sinusoidal component <span class="math inline">\(e^{-\frac{i2\pi}{N}k}\)</span>.</li><li>The high-frequency components have a large <span class="math inline">\(k\)</span> and vary quickly while the low-frequency components have small <span class="math inline">\(k\)</span> and vary more slowly.</li></ul></li><li>Translation equivalent: translating a signal and then convolving it by a filter is equivalent to convolving the signal and then translating the result.</li></ul><h4 id="from-time-signals-to-graph-signals">From time signals to graph signals</h4><ul><li>Each point in time <span class="math inline">\(t\)</span> is represented as a node and the edges in graph thus represent how the signal propagates</li><li>Represent operations such as time-shifts using the adjacency and Laplacian matrices of the graph.</li><li>Multiplying a signal by the adjacency matrix propagates signals from node to node, and multiplication by the Laplacian computes the difference between a signal at each node and its immediate neighbors.</li><li>shifts and convolutions on time-varying discrete signals can be represented based on the adjacency matrix and Laplacian matrix of a chain graph.</li><li>The convolution operation matrix satisfies translation equivalence.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210527233655918.png" alt="image-20210527233655918" style="zoom:80%;" />, which means that the convolved signal <span class="math inline">\(\mathrm{Q_hx}[u]\)</span> at each node <span class="math inline">\(u\in\mathcal{V}\)</span> will correspond to some mixture of the information in the node's <span class="math inline">\(N\)</span>-hop neighborhood, with the <span class="math inline">\(\alpha_0,\alpha_1,\cdots,\alpha_N\)</span> controlling the strength of the information coming from different hops. Defining <span class="math inline">\(\mathrm{Q_h}\)</span> in this way guarantees that our filter commutes with the adjacency matrix, satisfying a generalized notion of translation equivariance.</li><li><strong>By stacking multiple message passing layers, GNNs are able to implicitly operate on higher-order polynomials of the adjacency matrix.</strong></li><li>The symmetric normalized Laplacian or symmetric normalized adjacency matrix are usually taken as the convolutional filters cause<ul><li>They have bounded spectrums and thus numerically stable.</li><li>They are simultaneously diagonalizable, which means that they share the same eigenvectors.</li></ul></li></ul><h4 id="spectral-graph-convolutions">Spectral graph convolutions</h4><ul><li><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210527235621190.png" alt="image-20210527235621190" style="zoom:80%;" /></p><p>The Laplace operator (<span class="math inline">\(\Delta\)</span>) tells us the average difference between the function value at a point and function values in the neighboring regions surrounding this point. The Laplacian matrix is regarded as a discrete analog of the Laplace operator since it allows to quantity the difference between the value at a node and the values at that node's neighbors.</p></li><li><p>The eigenfunctions of Laplace operator is corresponds to the complex exponentials, which means the eigenfunctions of <span class="math inline">\(-\Delta e^{2\pi ist}\)</span> are the same complex exponentials that make up the modes of the frequency domain in the Fourier transform, with the corresponding eigenvalue indicating the frequency.</p></li><li><p>Graph convolution can be represented as polynomials of the Laplacian (or one of its normalized variants).</p></li><li><p>Given the graph Fourier coefficients <span class="math inline">\(\mathrm{U^\top f}\)</span> of a signal <span class="math inline">\(\mathrm{f}\)</span> as well as the graph Fourier coefficients <span class="math inline">\(\mathrm{U^\top h}\)</span> of some filter <span class="math inline">\(\mathrm{h}\)</span>, we can compute a graph convolution via element-wise products as <span class="math display">\[  \mathrm{f\star_\mathcal{G}h=U(U^\top f \circ U^\top h)}  \]</span> Then represent convolutions in the spectral domain based on the graph Fourier coefficients <span class="math inline">\(\theta_h=\mathrm{U^\top h \in \mathbb{R}^{|\mathcal{V}|}}\)</span>.</p></li><li><p>One way is to learn a nonparametric filter by <strong>directly optimizing <span class="math inline">\(\theta_h\)</span> (spectral filter)</strong> and defining the convolution as <span class="math inline">\(\mathrm{f\star_\mathcal{G}h=U(U^\top f \circ }\theta_h)=(\mathrm{Udiag}(\theta_h)\mathrm{U}^\top)\mathrm{f}\)</span>. But this way has no real dependency on the structure of the graph and may not satisfy many of the properties that we want from a convolution, e.g. locality.</p></li><li><p>To make sure the spectral filter <span class="math inline">\(\theta_h\)</span> is corresponds to a meaningful convolution on the graph, another way is to parameterize <span class="math inline">\(\theta_h\)</span> based on the eigenvalues of the Laplacian, e.g., a degree <span class="math inline">\(N\)</span> polynomial of the eigenvalues of the Laplacian and thus ensure the filtered signal at each node depends on information in its <span class="math inline">\(k\)</span>-hop neighborhood.</p><p><span class="math inline">\(\mathrm{f\star_\mathcal{G}h}=(\mathrm{Up_N}(\Lambda)\mathrm{U}^\top)\mathrm{f}=p_N(\mathrm{L})\mathrm{f}\)</span>.</p></li><li><p>The filter (e.g., Fourier) coefficients cannot be simply interpreted as corresponding to different frequencies.</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528101813914.png" alt="image-20210528101813914" style="zoom:80%;" /></li><li>The smallest eigenvector of the Laplacian corresponds to a signal that varies from node to node by the least amount on the graph, the second-smallest eigenvector corresponds to a signal that varies the second smallest amount and so on.</li><li>Laplacian eigenvectors can be used to assign nodes to communities so that we minimize the number of edges that go between communities. The <strong>Laplacian eigenvectors define signals that vary in a smooth way across the graph, with the smoothest signals indicating the coarse-gained community structure of the graph.</strong></li></ul></li></ul><h4 id="convolution-inspired-gnns">Convolution inspired GNNs</h4><ul><li><p>Purely convolutional approaches</p><ul><li>Directly optimize <span class="math inline">\(\theta_h\)</span> or parameterize it</li><li>Methods<ul><li><a href="https://arxiv.org/abs/1312.6203">Bruna et al.</a> : Nonparametric spectral filter and parametric spectral filter by a cubic spline.</li><li><a href="https://arxiv.org/abs/1606.09375">Defferrard et al.</a>, parameterize the spectral filter by Chebyshev polynomials. Chebyshev polynomials have an efficient recursive formulation and have various properties that make them suitable for polynomial approximation.</li><li><a href="https://arxiv.org/abs/1901.01484">Liao et al.</a>,: learn polynomials of the Laplacian based on the Lanczos algorithm.</li><li>Beyond real-valued polynomials of the Laplacian (or the adjacency matrix): employ more general parametric rational complex functions of the Laplacian.<ul><li><a href="http://dx.doi.org/10.1109/tsp.2018.2879624">Levie et al.</a>: Cayley polynomials of the Laplacian</li><li><a href="https://arxiv.org/abs/1901.01343">Bianchi et al</a>: ARMA filters.</li></ul></li></ul></li></ul></li><li><p><a href="https://arxiv.org/abs/1609.02907">GCNs</a> and connections to message passing</p><ul><li><p>A basic GCN layer is defined as <span class="math display">\[\mathrm{H}^{(k)}=\sigma (\tilde{\mathrm{A}}\mathrm{H}^{(k-1)}\mathrm{W}^{(k)}),\\\tilde{\mathrm{A}}=\mathrm{(D+I)^{-\frac{1}{2}}(I+A)(D+I)^{-\frac{1}{2}}},\]</span> where <span class="math inline">\(\mathrm{W}^{(k)}\)</span> is a learnable parameter matrix. The model was initially motivated as a combination of a simple graph convolution (based on the polynomial <span class="math inline">\(\mathrm{I+A}\)</span>), with a learnable weight matrix and a nonlinearity.</p></li><li><p>The notion of message passing can be viewed as corresponding to a simple form of graph convolutions combined with additional trainable weights and nonlinearites.</p></li><li><p>Stacking multiple rounds of message passing in a basic GNN is analogous to applying a low-pass convolutional filter, which produces a smoothed version of the input signal on the graph.</p><ul><li>The multiplication <span class="math inline">\(\mathrm{A}^K_{sym}\mathrm{X}\)</span> of the input node features by a high power of the adjacency matrix can be interpreted as convolutional filter based on the lowest-frequency signals of the graph Laplacian. Because multiplying a signal by high powers of <span class="math inline">\(\mathrm{A_{sym}}\)</span> corresponds to a convolutional filter based on the lowest eigenvalues of <span class="math inline">\(\mathrm{L_{sym}}\)</span>, i.e., it produces a low-pass filter.</li><li>The deeper, the convolution filters are simpler.</li></ul></li></ul></li><li><p>GNNs without message passing</p><ul><li><p>Simplify GNNs by removing the iterative message passing process. The models are generally defined as <span class="math display">\[\mathrm{Z=MLP}_\theta(f(\mathrm{A)MLP_\phi(\mathrm{A})})\]</span></p><ul><li><p><a href="https://arxiv.org/pdf/1902.07153.pdf">Wu et al.</a> define <span class="math inline">\(f(\mathrm{A})=\mathrm{\tilde{A}}^k\)</span>, with <span class="math inline">\(\tilde{\mathrm{A}}\)</span> is the symmetric normalized adjacency matrix.</p></li><li><p><a href="https://arxiv.org/abs/1810.05997">Klicpera et al.</a> defines <span class="math inline">\(f\)</span> by analogy to the personalized PageRank algorithm as <span class="math display">\[f(\mathrm{A})=\alpha(\mathrm{I-(1-\alpha)\tilde{A}})^{-1}=\alpha\sum\limits_{k=0}^{\infin}(\mathrm{I-\alpha\tilde{A}})^k,\]</span> cause we often do not need to interleave trainable neural networks with graph convolution layers. We can simply use neural networks to learn feature transformations at the beginning and end of the model and apply a deterministic convolution layer to leverage the graph structure. Like GAT.</p></li></ul></li><li><p>Using the symmetric normalized adjacency matrix with self-loops leads to effective graph convolutions.</p><ul><li><a href="https://arxiv.org/pdf/1902.07153.pdf">Wu et al.</a> proves that adding self-loops shrinks the spectrum of corresponding graph Laplacian by reducing the magnitude of the dominant eigenvalue.</li><li>Intuitively, adding self-loops decrease the influence of far-away noes and makes the filtered signal more dependent on local neighborhoods on the graph.</li></ul></li></ul></li></ul><h3 id="gnns-and-probabilistic-graphical-models">GNNs and probabilistic graphical models</h3><p>View the embeddings for each node as latent variables that are inferred.</p><h4 id="hilbert-space-embeddings-of-distributions">Hilbert space embeddings of distributions</h4><ul><li><p>The density <span class="math inline">\(p(\mathrm{x})\)</span> based on its expected value under the feature map <span class="math inline">\(\phi\)</span> is: <span class="math display">\[\mu_\mathrm{x}=\int_{\mathbb{R}^m}\phi(\mathrm{x})p(\mathrm{x})d\mathrm{x}\]</span> The formula will be injective under the assumption of Hilbert space embeddings of distributions. Then <span class="math inline">\(\mu_\mathrm{x}\)</span> can serve as a sufficient statistics for <span class="math inline">\(p(\mathrm{x})\)</span>. Then any computations we want to perform on <span class="math inline">\(p(\mathrm{x})\)</span> can be equivalently represented as functions of the embedding <span class="math inline">\(\mu_\mathrm{x}\)</span>. One well-known feature map <span class="math inline">\(\phi\)</span> is Gaussian radial basis function.</p></li><li><p>In the context of the connection to GNNs, the takeaway is simply that we can represent distributions <span class="math inline">\(p(\mathrm{x})\)</span> as embeddings <span class="math inline">\(\mu_\mathrm{x}\)</span> in some feature space.</p></li></ul><h4 id="graphs-as-graphical-models">Graphs as graphical models</h4><ul><li><p>The notion of dependence between nodes is viewed as a formal, probabilistic way.</p></li><li><p>Assume that a graph defines a Markov random field,</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528155046247.png" alt="image-20210528155046247" style="zoom:80%;" /></p><ul><li>Intuitively, <span class="math inline">\(\Phi(\mathrm{x}_v,\mathrm{z}_v)\)</span> indicates the likelihood of a node feature vector <span class="math inline">\(\mathrm{x}_v\)</span> given its latent node embedding <span class="math inline">\(\mathrm{z}_v\)</span>, while <span class="math inline">\(\Psi\)</span> controls the dependency between connected nodes.</li><li>Assume that node features are determined by their latent embeddings, and the latent embeddings for connected nodes are dependent on each other.</li></ul></li><li><p>GNNs will try to seek to implicitly learn the <span class="math inline">\(\Psi,\Phi\)</span> by leveraging the Hilbert space embedding idea.</p></li></ul><h4 id="embedding-mean-field-inference">Embedding mean-field inference</h4><p>The goal is to infer latent representations for all the nodes in the graph that can explain the dependencies between the observed node features.</p><ul><li><p>The key step is computing the posterior <span class="math inline">\(p(\{\mathrm{z}_v\}|\{\mathrm{x}_v\})\)</span>, i.e., computing the likelihood of a particular set of latent embeddings given the observed features. But the accurate solution is intractable, one way is to approximate it.</p></li><li><p>One way to approximate the posterior is to employ mean-field variational inference, by which the posterior is approximated as <span class="math inline">\(p(\{\mathrm{z}_v\}|\{\mathrm{x}_v\})\approx q(\{\mathrm{z}_v\})=\prod\limits_{v\in\mathcal{V}}q_v(\mathrm{z}_v)\)</span>. The key intuition in mean-field inference is that we assume that the posterior distribution over the latent variables factorizes into <span class="math inline">\(\mathcal{V}\)</span> independent distributions, one per node.</p><ul><li><p>The standard approach to solve is to minimize the KL divergence between the approximate posterior and the true posterior. But directly minimize it is impossible cause evaluating the <strong>KL divergence</strong> requires knowledge of the true posterior.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528213649200.png" alt="image-20210528213649200" style="zoom:80%;" /></p><ul><li>To minimize the KL divergence easily, one way is to use the techniques from variational inference.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528213729244.png" alt="image-20210528213729244" style="zoom:80%;" /></li><li>The approximate posterior at timestep <span class="math inline">\(t\)</span> for each latent node embedding is a function of the node's feature <span class="math inline">\(\mathrm{z}_x\)</span> and the marginal distributions (marginalized on the node's neighbors.) at previous timestep. Therefore, <strong>using variational inference to infer the posterior is really like message passing.</strong></li><li><strong>The key distinction</strong> is that the mean-field message passing equations operate over distributions rather than embeddings (what used in GNN).</li></ul></li></ul></li><li><p>Another way is try to learn embeddings in an end-to-end way, or to say rather than specifying a concrete probabilistic model, one can simply learn embeddings that could correspond to some probabilistic model.</p><ul><li><a href="https://arxiv.org/abs/1603.05629">Dai et al.</a> define <span class="math inline">\(f\)</span> in an analogous manner to a basic GNN and thus at each iteration the updated Hilbert space embedding for node <span class="math inline">\(v\)</span> is a function of its neighbors' embedding as well as its feature inputs.</li></ul></li></ul><h4 id="gnns-and-pgms-more-generality">GNNs and PGMs more generality</h4><ul><li>Different variants of message passing can be derived based on different approximate inference algorithms. <a href="https://arxiv.org/abs/1603.05629">Dai et al.</a></li><li>How GNNs can be integrated more generally into PGM models.<ul><li><a href="https://arxiv.org/abs/1905.06214">Qu et al.</a></li><li><a href="https://arxiv.org/abs/1906.02111">Zhang et al.</a></li></ul></li></ul><h3 id="gnns-and-graph-isomorphism">GNNs and graph isomorphism</h3><p>The motivation of GNNs based on connections to graph isomorphism testing.</p><h4 id="graph-isomorphism-and-representational-capacity">Graph isomorphism and representational capacity</h4><ul><li>Graph isomorphism<ul><li>The goal of graph isomorphism is to declare whether or not the given two graphs are isomorphic. If so, the graphs are essentially identical.</li><li>Formally, given adjacency matrix <span class="math inline">\(\mathrm{A}_1,\mathrm{A}_2\)</span> and node features <span class="math inline">\(\mathrm{X}_1,\mathrm{X}_2\)</span>, then two graphs are isomorphic if and only if there exists a permutation matrix <span class="math inline">\(\mathrm{P}\)</span> such that <span class="math inline">\(\mathrm{PA_1P^\top=A_2,PX_1=X_2}\)</span></li></ul></li><li>The challenges of graph isomorphism<ul><li>The simple definition</li><li>Testing for graph isomorphism<ul><li>A naive approach to test for isomorphism would involve the optimization problem<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528231925373.png" alt="image-20210528231925373" style="zoom:60%;" /> with the computation complexity <span class="math inline">\(\mathcal{O}(|V|!)\)</span>.</li><li>Therefore, this is regarded as NP-indeterminate. No general polynomial time algorithms are known for this problem.</li></ul></li></ul></li><li>Graph isomorphism and representational capacity<ul><li>Graph isomorphism gives a way to quantify the representational power of different learning approaches. E.g., evaluating the power by asking how useful the representations would be for testing graph isomorphism.</li><li>In practice, no representation learning algorithm is going to be “perfect".</li></ul></li></ul><h4 id="the-weisfieler-lehman-algorithm">The Weisfieler-Lehman algorithm</h4><ul><li><p>The steps of 1-WL</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528233109761.png" alt="image-20210528233109761" style="zoom:80%;" /></p></li><li><p>The WL algorithm is known to converge in at most <span class="math inline">\(|V|\)</span> iterations and is known to known to successfully test isomorphism for a broad class of graph.</p></li><li><p>WL may fail for some graphs e.g. when the graph consists of multi disconnected subgraphs.</p></li></ul><h4 id="gnns-and-the-wl-algorithm">GNNs and the WL algorithm</h4><ul><li><p>GNNs aggregate and update node embeddings using NNs while WL aggregates and updates discrete labels.</p></li><li><p>Formally, the relation between GNNs and WL is</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528233713630.png" alt="image-20210528233713630" style="zoom:80%;" /></p><p>It tells that GNNs are no more powerful than the WL algorithm when we have discrete information as node features.</p></li><li><p>If the WL algorithm assigns the same label to two nodes, then any message-passing GNN will also assign the same embedding to these two nodes.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528234128699.png" alt="image-20210528234128699" style="zoom:80%;" /></p></li><li><p>If we define the message passing updates as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529080919406.png" alt="image-20210529080919406" style="zoom:80%;" /></p><p>then this GNN is sufficient to match the power of the WL algorithm.</p></li><li><p>To make GNN is as powerful as WL, the aggregate and update function need to be injective, which means that they need to map every unique input to a unique output value. But this cannot be satisfied usually if the aggregate functions are using a average of the neighbor embeddings.</p></li><li><p>GIN (graph isomorphism network) has few parameters but is still as powerful as the WL algorithm.</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529084928011.png" alt="image-20210529084928011" style="zoom:80%;" /></li></ul></li><li><p>In summary, MP-GNNs (message passing GNNs) are no more powerful than the WL algorithm.</p></li></ul><h4 id="beyond-the-wl-algorithm">Beyond the WL algorithm</h4><ul><li><p>Relational pooling</p><p>Considering the failure cases of the WL algorithm. Message passing approaches generally fail to identify closed triangles in a graph, which is a critical limitation.</p><ul><li>To address the limitation, Murphy et al. consider MP-GNNs with unique node ID features. They simply add a unique, one-hot indicator feature (node ID) for each node.<ul><li>However, this doesn't solve the problem and rather import a new and equally problematic issue that MP-GNNs is no longer permutation equivariant since the adding of node IDs. Specifically, assigning a unique ID to each node fixes a particular node ordering for the graph, which breaks the permutation equivariance.</li></ul></li><li>The other way is <a href="https://arxiv.org/abs/1903.02541">Relational Pooling (PR) approach</a>, which involves marginalizing over all possible node permutations. In practice, it will sum over all possible permutation matrices recovers the permutation invariance.<ul><li>The limitation are<ul><li>its computation complexity.</li><li>We have no way to characterize how much more powerful that PR-GNNs are.</li></ul></li><li>But RP can achieve strong results using various approximation to decrease the computation cost.</li></ul></li></ul></li><li><p>The <span class="math inline">\(k\)</span>-WL test and <span class="math inline">\(k\)</span>-GNNs</p><p>Improving GNNs by adapting generalizations of the WL algorithm.</p><ul><li><span class="math inline">\(k\)</span>-WL works on subgraphs in size <span class="math inline">\(k\)</span>, which can be used to test graph isomorphism by comparing the multi-sets for two graphs.<ul><li>It introduces a hierarchy of representation capacity. For any <span class="math inline">\(k\ge2\)</span> we have that the <span class="math inline">\((k+1)\)</span>-WL test is strictly more powerful than the <span class="math inline">\(k\)</span>-WL test.</li><li>To intimate this, <a href="http://dx.doi.org/10.1609/aaai.v33i01.33014602">Morris et al</a> develop a <span class="math inline">\(k\)</span>-GNN that is a differentiable and continuous analog of the <span class="math inline">\(k\)</span>-WL algorithm. They learn embeddings associated with subgraphs, rather than noes, and the message passing occurs according to subgraph neighborhoods.</li></ul></li><li>Graph kernel methods based on the <span class="math inline">\(k\)</span>-WL test.</li></ul></li><li><p><a href="https://arxiv.org/abs/1905.11136">Invariant and Equivariant <span class="math inline">\(k\)</span> -order GNNs</a></p><ul><li>MP-GNNs are equivariant to node permutations. And permuting the input to an MP-GNNs simply results in the matrix of output node embeddings being permuted in an analogous way.</li><li>MP-GNNs can also be permutation invariant at the graph level. The pooled graph-level embedding does not change when different node orderings are used.</li><li><a href="https://arxiv.org/abs/1905.11136">Maron et al</a> propose a general form of GNN-like models based on permutation equivariant/invariant tensor operations.<ul><li>For a given input, both equivariant and invariant linear operators on this input will correspond to tensors that satisfy the fixed point in <span class="math inline">\(\mathrm{P}\star\mathcal{L}=\mathcal{L},\forall \mathrm{P}\in \mathcal{P}\)</span>, but the number of channels in the tensor will differ depending on whether it is an equivariant or invariant operator.</li><li>The fixed point can be constructed as a linear combination of a set of fixed basis elements.</li><li>The equivariant linear layers involve tensors that have up to <span class="math inline">\(k\)</span> different channels.</li><li>Constructing <span class="math inline">\(k\)</span>-order invariant models for <span class="math inline">\(k&gt;3\)</span> is generally computationally intractable. The built <span class="math inline">\(k\)</span>-order GNNs are equally powerful as the <span class="math inline">\(k\)</span>-WL algorithm.</li></ul></li></ul></li></ul><h1 id="part-iii-generative-graph-models">Part III Generative Graph models</h1><h2 id="chapter-8-traditional-graph-generation-approaches">Chapter 8 Traditional graph generation approaches</h2><p>The goal of graph generation is to build models that can generate realistic graph structures. The key challenge in graph generation is generating graphs that have certain desirable properties.</p><p>Traditional approaches to graph generation generally involve specifying some kind of generative process, which defines how the edges in a graph are created.</p><p>A more through survey and discussion is <a href="https://global.oup.com/academic/product/networks-9780198805090?cc=us&amp;lang=en&amp;">Newman's (1,12,13,108,109).</a></p><h3 id="erdos-renyi-model">Erdos-Renyi model</h3><ul><li>ER model may be the simplest and most well-known generative model of graphs. It simply assumes that the probability of an edge occurring between any pairs of nodes is equal to <span class="math inline">\(r\)</span>.</li><li>To generate a random ER graph, just simply choose how many nodes we want, set the density parameter <span class="math inline">\(r\)</span> and then use equation to generate the adjacency matrix.</li><li>The downside of the ER model is that it doesn't generate very realistic graphs. The graph properties like degree distribution, existence of community structures, node clustering coefficients and tensors occurrence of structural motifs are not captured.</li></ul><h3 id="stochastic-block-models">Stochastic block models</h3><ul><li>SBMs seek to generate graphs with community structure.</li><li>SBMs are based on blocks, every node has a probability that it belongs to block <span class="math inline">\(i\)</span>, edge probabilities are defined by a block-to-block probability matrix. To generate graph, for each node assign a class (block) by sampling and then sample edges for each pair of nodes.</li><li>By controlling the edge probabilities within and between different blocks, one can generate graphs that exhibit community structure.</li><li>The nodes have a probability <span class="math inline">\(\alpha\)</span> of having an Eden with another node that assigned to the same community and a smaller probability of having an Eden with another node that is assigned to a different community.</li><li>The variations including approaches for bipartite graphs, graphs with node features, as well as approaches to infer SBMs parameters from data.</li><li>However, SBMs is limited in that it fails to capture the structural characteristic of individuals nodes that are present in most real-world graphs. In SBMs, the structure of individual communities is relatively homogeneous in that all the nodes have similar structural.</li></ul><h3 id="preferential-attachment">Preferential attachment</h3><ul><li><p>Preferential attachment (PA) attempts to capture the inhomogeneous of communities (real-world degree distributions). It's built based on the assumption that many real-world graphs exhibit power-law degree distributions.</p></li><li><p>The power law distributions are heavy tailed, which means that a probability distribution goes to zero for extreme values slower than an exponential distribution. It also means that there is a large number of nodes with small degrees but also have a small number of nodes with extremely large degrees.</p></li><li><p>The steps of PA:</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529144054529.png" alt="image-20210529144054529" style="zoom:80%;" /></p></li><li><p>The key idea is that the PA model connects new nodes to existing nodes with a probability that is proportional to the existing nodes' degrees.</p></li><li><p>The generation process of PA is autoregressive, the edge probabilities are defined on an iterative approach.</p></li></ul><h3 id="traditional-applications">Traditional applications</h3><p>Historically, the methods introduced above have been used in two key applications:</p><ul><li>Generating synthetic data for benchmarking and analysis tasks</li><li>Creating Null models<ul><li>We can investigate the extent to which different graph characteristics are probable under different generative models.</li></ul></li></ul><p>The traditional approaches can generate graphs, but they lack the ability to learn a generative model from data.</p><h2 id="chapter-9-deep-generative-models">Chapter 9 Deep generative models</h2><p>Focus on the simple and general variants of VAEs, GANs and autoregressive models. All below will only focus on generating graph structure.</p><h3 id="variational-autoencoder-approaches">Variational autoencoder approaches</h3><ul><li><p>Under VAEs, the key idea behind can be summarized as : the goal is to train a probabilistic decoder model from which one can sample realistic graphs by conditioning on a latent variable. Or to say, the goal is to learn a conditional distribution over adjacency matrices.</p></li><li><p>The target is the decoder that generate graph from latent variable, but encoder and decoder are trained together.</p></li><li><p>The components that required:</p><ul><li><p>A probabilistic encoder model <span class="math inline">\(q_\phi\)</span></p><p>It takes a graph as input. Generally , in VAEs the representation trick with Gaussian random variables is used to design this function.</p></li><li><p>A probabilistic decoder model <span class="math inline">\(p_\theta\)</span></p><p>The decoder takes a latent representation as input and uses this input to specify a conditional distribution over graphs. Specifically, it defines a conditional distribution over the entries of the adjacency matrix.</p></li><li><p>A prior distribution over the latent space</p><p>Usually a standard Gaussian prior is used.</p></li></ul></li><li><p>With these components, the loss is minimizing the evidence likelihood lower bound (ELBO)</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529181330038.png" alt="image-20210529181330038" style="zoom:80%;" />, with the basic idea that seek to maximize the reconstruction ability of the decoder.</p><ul><li>The motivation behind the ELBO is rooted in the theory of variational inference.</li><li>The goals will be satisfied under this optimization method<ul><li>The sampled latent representations encode enough information to allow our decoder to reconstruct the input.</li><li>The latent distribution is as close as possible to the prior. It's important if one wants to generate new graphs after training : they can generate new graphs by sampling from the prior and feeding these latent embeddings to the decoder.</li></ul></li></ul></li></ul><h4 id="node-level-latent">Node-level latent</h4><p>Encoding and decoding graphs based on node embeddings. The key idea is that the encoder generates latent representations for each node in the graph.</p><p>As a generative model, the node-level method is limited.</p><ul><li><a href="https://arxiv.org/pdf/1611.07308.pdf%5D">Kipf et al.</a> proposed VGAE (variational graph autoencoder)<ul><li>Encoder model: can be based on any of the GNN architectures. In particular, two GNNs are used to generate mean (for each node in the input graph) and variance parameters separately. Once they are computed, tensors set of latent node embeddings can be sampled.</li><li>Decoder model: predict the likelihood of all the edges in the graph, given a matrix of sampled node embeddings. In VGAE, thy use a dot-product decoder.</li><li>The reconstruction error is a binary cross-entropy over the edge probabilities.</li></ul></li><li>Limitations: It's limited especially when a dot-product decoder is used. The decoder has no parameters, so the model is not able to generate non-trivial graph structures without a training graph as input.</li><li><a href="http://proceedings.mlr.press/v97/grover19a.html">Grover et al.</a> propose to augment the decoder with an iterative GNN based decoder.</li></ul><h4 id="graph-level-latent">Graph-level latent</h4><p>The encoder and decoder functions are modified to work with graph-level latent representations.</p><ul><li>Encoder model: It can be an arbitrary GNN model augmented with a pooling layer. Again there are two separate GNNs to parameterize the mean and variance of a posterior normal distribution over latent variables.</li><li>Decoder model: One original Graph-VAE model proposed to combine a basic MLP with a Bernoulli distributional assumption. Simply independent Bernoulli distribution for each edge, and the overall log-likelihood objective is equivalent to set of independent binary cross-entropy loss function on each edge.<ul><li>The challenges while implementing with MLP:<ul><li>Have to assume a fixed number of nodes</li><li>We don't know the correct ordering of the rows and columns in <span class="math inline">\(\tilde{A}\)</span> when we are computing the reconstruction loss.</li></ul></li></ul></li><li>Limitations: using graph-level latent representations introduces the issue of specifying node orderings. And with MLP, currently limits the application of the basic graph-level VAE to small graphs with hundred of nodes or less.</li></ul><h3 id="adversarial-approaches">Adversarial approaches</h3><p>VAE suffers from serious limitations--such as the tendency for VAEs to produce blurry outputs in the image domain.</p><ul><li>Some works<ul><li><a href="https://arxiv.org/pdf/1805.11973.pdf">De Cao and Kipf et al.</a> propose one that is similar to the graph-level VAE.<ul><li>The generator is a MLP that generates a matrix of edge probabilities given a seed vector.</li><li>The discrete adjacency matrix is generated by sampling independent Bernoulli variables for each edge.</li><li>The discriminator employ any GNN-based graph classification model.</li></ul></li></ul></li><li>Benefits: GAN-based methods remove the complication of specifying a node ordering in the loss computation as long as the discriminator model is permutation invariant.</li><li>Limitations : GAN-based approaches to graph generation have so far received less attention and success than their variational counterparts.</li></ul><h3 id="autoregressive-methods">Autoregressive methods</h3><p>Both VAE-based approaches and basic GANs that discussed before use simple MLPs to generate adjacency matrices. Autoregressive methods can decode graph structures from latent representations, they will combine GANs and VAEs.</p><h4 id="modeling-edge-dependencies">Modeling edge dependencies</h4><p>Previous it's assumed that the edges are independent for convenience, but this is not true in real world.</p><ul><li><p>In autoregressive model, it's assumed that edges are generated sequentially and that the likelihood of each edge can be conditioned on the edges that have been previously generated.</p></li><li><p>Assume the lower-triangular portion of the adjacency matrix <span class="math inline">\(\mathrm{A}\)</span> is denoted as <span class="math inline">\(\mathrm{L}\)</span>.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210530073545690.png" alt="image-20210530073545690" style="zoom:80%;" /></p></li></ul><h4 id="recurrent-models-for-graph-generation">Recurrent models for graph generation</h4><ul><li><a href="https://arxiv.org/abs/1802.08773">GraphRNN</a> : use a hierarchical RNN to model the edge dependencies.<ul><li>The 1st RNN in this model is a graph-level RNN that is used to model the state of the graph that has been generated so far. It involves a stochastic sampling process to generate the discrete edges. In this way, the graph RNN is able to generate diverse samples of graphs even when a fixed initial embedding is used.</li><li>The 2nd RNN, termed a the node-level RNN, generates the entries of <span class="math inline">\(\mathrm{L}\)</span> lower triangular adjacency matrix in an autoregressive manner. It will take the graph-level hidden state as its input and then sequentially generate the binary values of <span class="math inline">\(\mathrm{L}\)</span>, assuming a conditional Bernoulli distribution for each entry.</li><li>The node level RNN is initialized at each time-step with the current hidden state of the graph-level RNN.</li><li>Both RNNs can be optimized to maximize the likelihood of the training graphs using the teaching forcing strategy, meaning that the ground truth of <span class="math inline">\(\mathrm{L}\)</span> are always used to update the RNNs during training. But computing the likelihood requires the assumption that a particular a ordering over the generated nodes.</li><li>It's more capable of generating grid-like structures, compared to the basic graph-level VAE.</li><li>Limitations: It still generates unrealistic artifacts (e.g. long chains) when trained on samples of grids. It can be difficult to train and scale to large graphs due to the need to backpropagate through many steps of RNN recurrence.</li></ul></li><li><a href="https://arxiv.org/abs/1910.00760">GRAN</a>: generate graphs by using a GNN to condition on the adjacency matrix that has been generated so far. GRAN models dependencies between edges. It maintains the autoregressive decomposition of the generation process.<ul><li>It uses GNNs to model the autoregressive generation process.</li><li>One can model the conditional distribution of each row of the adjacency matrix by running a GNN on the graph that has been generated so far.</li><li>Since there are no node attributes associated with the generated nodes, the input feature matrix <span class="math inline">\(\tilde{\mathrm{X}}\)</span> to the GNN can simply contain randomly sampled vectors.</li><li>The key benefit of GRAN compared with GraphRNN is that it does not need to maintain a long and complex history in a graph-level RNN.</li><li>To use GRAN on large graphs, one improvement is that multiple nodes can be added simultaneously in a single block rather than adding nodes one at a time.</li></ul></li></ul><h3 id="evaluating-graph-generation">Evaluating graph generation</h3><p>Quantitatively compare the different models introduced previouly?</p><ul><li>Currently is to analyze different statistics of the generated graphs and to compare the distribution of statistics for the generated graphs to a test set.</li><li>Compute the distance between the statistic's distribution on the test graph and generated graph using a distributional measure, such as the total variation distance.</li><li>The statistics that are used including degree distributions, graphlets counts, and spectral features with distributional distances computed using variants of the total variation score and the 1st Wassertein distance.</li></ul><h3 id="molecule-generation">Molecule generation</h3><ul><li>The goal of molecule generation is to generate molecular graph structures that are both valid (e.g., chemically stable) and ideally have some desirable properties (e.g., medicinal properties or solubility).</li><li>Domain-specific knowledge for both model design and evaluation.</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Building models that can infer latent graph structures beyond the input graph that we are given is a critical direction for pushing forward graph representation learning.</li><li>Message-passing GNNs are inherently bounded by the WL isomorphism test. They suffer from over-smoothing, being limited to simple convolutional filters, and being restricted to tree-structured computation graphs.</li></ul><h3 id="section"></h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Book &lt;a href=&quot;https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf&quot;&gt;Graph Representation Learning&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
  </entry>
  
  <entry>
    <title>Paper--3D photography on your desk</title>
    <link href="http://yoursite.com/posts/notes/2021-03-04-notes-paper-cv-3dshape.html"/>
    <id>http://yoursite.com/posts/notes/2021-03-04-notes-paper-cv-3dshape.html</id>
    <published>2021-03-05T03:35:00.000Z</published>
    <updated>2021-04-28T23:14:51.990Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.2656&amp;rep=rep1&amp;type=pdf">3D photography on your desk, 1998</a>, <a href="http://www.vision.caltech.edu/bouguetj/ICCV98/">website</a></p><p>Reference: <a href="https://ieeexplore.ieee.org/abstract/document/1087109?casa_token=RZ6-EVaSgrIAAAAA:QwrjPXnkBwNtWBQR_RsNxixV2Q7NZ4qj9DFuPG4PkI3rm8J_6NZQUq4spi3Op-xpmGSEwCNuyA">A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses</a></p><a id="more"></a><h1 id="reference">Reference</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/1087109?casa_token=RZ6-EVaSgrIAAAAA:QwrjPXnkBwNtWBQR_RsNxixV2Q7NZ4qj9DFuPG4PkI3rm8J_6NZQUq4spi3Op-xpmGSEwCNuyA">A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses</a></p><h2 id="camera-calibration">Camera calibration</h2><h3 id="types">Types</h3><ol type="1"><li><p><strong>2D image coordinates to 3D information</strong></p><ul><li><p>The Types of 3D information to be inferred</p><ul><li><p>3D information concerning the location of the object, target, or feature.</p><p>Camera calibration will offer a way of determining a ray in 3D space that the object point must lie on.</p></li><li><p>3D information concerning the position and orientation of the moving camera relative to the target world coordinate system.</p><p>Like robot camera.</p></li></ul></li></ul></li><li><p><strong>3D information to 2D image coordinates</strong></p><ul><li>If given hypothetical 3D location of the object, the 2D image coordinates can be estimated.</li></ul></li></ol><h3 id="requirement">Requirement</h3><p>autonomous, accurate, reasonably efficient, versatile, need only common off-the-shelf camera and lens</p><h3 id="previous">Previous</h3><h4 id="full-scale-nonlinear-optimization">Full-scale nonlinear optimization</h4><ul><li>Advantage: allows easy adaption of any arbitrarily accurate yet complex model for imaging.</li><li>Problems: the requirement of a good initial guess and computer-intensive full-scale nonlinear search</li><li>Approaches<ul><li>Classical approach: accurate cause the large number of unknowns and images in high resolution from rather than solid-state image array like CCD</li><li>Direct linear transformation (DLT): only using linear equations, but pure DLT only works fine without lens distortion. DLT confirms that low-resolution images from like CCD can also be used for accurate calibration.</li><li>Sobel, Gennery, Lowe<ul><li>Sobel: nonlinear equation, 18 parameters</li><li>Gennery: iteratively by minimizing the error of epipolar constraints without using 3D coordinates of calibration points. But it's error-prone.</li></ul></li></ul></li></ul><h4 id="computing-perspective-transformation-matrix-using-linear-equation-solving">Computing perspective transformation matrix using linear equation solving</h4><ul><li>Advantage : no requirement of nonlinear optimization</li><li>Problems: Cannot take lens distortion into consideration; the number of unknowns in linear equations is much larger than the real DoF. If lens distortion is not considered, then the perspective matrix can be solved by OLS.</li><li>If the field of view is narrow and the object distance is large, then ignoring distortion should cause more error.</li></ul><h4 id="two-plane-method">Two-plane method</h4><ul><li>Advantage : only linear equations need be solved</li><li>Problems: the number of unknowns is much larger than DoF; the formula used between 2D and 3D is empirically.</li><li>No restrictions needed for the extrinsic camera parameters, but the relative orientation between the camera coordinate system and the object world coordinate system is required. The nonlinear leans distortion theoretically cannot be corrected.</li></ul><h4 id="geometric-technique">Geometric technique</h4><ul><li>Advantage : no linear search is needed</li><li>Problems: no lens distortion be carried; the requirement of focal length, uncertainty of image scale factor is not allowed</li></ul><h2 id="two-stage-calibration">Two-stage calibration</h2><h3 id="goal">Goal</h3><ul><li>Reduce the number of parameters that need to be estimated by applying a constraint. The constraint is <strong><em>radial alignment constraint.</em></strong></li><li>Radial alignment constraint: a function of the relative rotation and translation between the camera and the calibration points.</li><li>The single-plane calibration points are used so the plane must be parallel to image plane.</li><li>If the DLT-type linear approximation is used, the distortion cannot be ignored unless a very narrow angle lens is used.</li></ul><h3 id="camera-model">Camera model</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306222342885.png" alt="image-20210306222342885" style="zoom:30%;" /></p><ol type="1"><li><p>Rigid body transformation from the object world coordinate system <span class="math inline">\((x_w, y_w, z_w)\)</span> to the camera 3D coordinate system $(x, y, z) $. <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306222752281.png" alt="image-20210306222752281" style="zoom:25%;" />. <span class="math inline">\(R,T\)</span> need to be calibrated. Translation is calibrated before rotation.</p><p><strong>However, how can one know <span class="math inline">\((x_w, y_w, z_w)\)</span>?</strong></p></li><li><p>Transformation from 3D camera coordinate<span class="math inline">\((x, y, z)\)</span> to ideal (undistorted) image coordinate <span class="math inline">\((X_u, Y_u)\)</span>. <span class="math inline">\(X_u=f\frac{x}{z},Y_u=f\frac{y}{z}\)</span>.</p></li><li><p>Calibrate radial lens distortion <span class="math inline">\(k_1,k_2\)</span>. Experimentally only one <span class="math inline">\(k\)</span> of radial lens distortion will work fine. The more will cause numerical instability.</p><p><span class="math inline">\(X_d(1+k_1r^2+k_2r^4+\cdots)=X_u,\\Y_d(1+k_1r^2+k_2r^4+\cdots)=Y_u,\\r=\sqrt{X_d^2+Y_d^2}\)</span>.</p></li><li><p>Real image coordinate <span class="math inline">\((X_d,Y_d)\)</span> to computer image coordinate <span class="math inline">\((X_f,Y_f)\)</span>. <span class="math inline">\(S_x\)</span> is gonna be calibrated.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306224649098.png" alt="image-20210306224649098" style="zoom:40%;" /></p></li></ol><p>Get all four steps together and suppose <span class="math inline">\(C_x=0，C_y=0\)</span>, the final formula between image coordinates <span class="math inline">\((X,Y)\)</span> and real world 3D coordinates <span class="math inline">\((x_w,y_w,z_w)\)</span> is</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306231044495.png" alt="image-20210306231044495" style="zoom:40%;" />.</p><p>Now that given image coordinates <span class="math inline">\((X,Y)\)</span> and real world 3D coordinates <span class="math inline">\((x_w,y_w,z_w)\)</span> , the camera can be calibrated.</p><h3 id="implementation-by-a-monoview-coplanar-set-of-points">Implementation by a monoview coplanar set of points</h3><p>Before started, make sure <span class="math inline">\((x_w,y_w,z_w)\)</span> is out of the field view and not close to the <span class="math inline">\(y\)</span> axis so as to avoid <span class="math inline">\(T_y=0\)</span>.</p><ol type="1"><li><p>Compute 3D Orientation , Position (<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>) and sclerosis factor</p><ul><li><p>Compute the distorted image coordinates <span class="math inline">\((X_d,Y_d)\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307113715726.png" alt="image-20210307113715726" style="zoom:50%;" />, where <span class="math inline">\(C_x,C_y\)</span> are supposed as the center of the image frame (aka they cannot be calibrated), <strong><span class="math inline">\(s_x\)</span> is not calibrated here but from a priori.</strong></p></li><li><p>Compute the five unknowns <span class="math inline">\(T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307115018316.png" alt="image-20210307115018316" style="zoom:33%;" />, this requires at least 6 points.</p></li><li><p>Compute <span class="math inline">\((r_1,\cdots,r_9,T_x,T_y)\)</span> from <span class="math inline">\((T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5)\)</span></p><ul><li><p>Compute <span class="math inline">\(|T_y|\)</span> from <span class="math inline">\((T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5)\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307115902149.png" alt="image-20210307115902149" style="zoom:40%;" /></p></li><li><p>Determine the sign of <span class="math inline">\(T_y\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307121434690.png" alt="image-20210307121434690" style="zoom:40%;" /></p><p>This sign reversal of <span class="math inline">\(T_y\)</span> causes <span class="math inline">\((x,y)\)</span> to become <span class="math inline">\(-(x,y)\)</span>. But because <span class="math inline">\(X_d,x\)</span> have the same sign, <span class="math inline">\(Y_d, y\)</span> have the same sign, then <strong>only one of the two signs for <span class="math inline">\(T_y\)</span> is valid</strong> .</p></li><li><p>Compute the 3D rotation <span class="math inline">\(R\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122232570.png" alt="image-20210307122232570" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122414379.png" alt="image-20210307122414379" style="zoom: 50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122549000.png" alt="image-20210307122549000" style="zoom:50%;" /></p></li></ul><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122644888.png" alt="image-20210307122644888" style="zoom:50%;" /></p></li></ul></li><li><p>Compute effective focal length, distortion coefficients and <span class="math inline">\(z\)</span> position.</p><p>Here, <span class="math inline">\(k_1=0\)</span>.</p><ul><li><p>Compute an approximation of <span class="math inline">\(f, T_z\)</span> by ignoring lens distortion.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122549000.png" alt="image-20210307122549000" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122644888.png" alt="image-20210307122644888" style="zoom:50%;" /></p></li><li><p>Compute exact solution for <span class="math inline">\(f,T_z,k_1\)</span></p><p>Solve <span class="math inline">\((8b)\)</span>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307124032524.png" alt="image-20210307124032524" style="zoom:50%;" />, with <span class="math inline">\(f,T_z,k_1\)</span> as unknowns using standard optimization scheme such as steepest descent. The approximation of <span class="math inline">\(f,T_z\)</span> in previous step can be used as initial guess, and the initial guess of <span class="math inline">\(k_1\)</span> can be taken as zero.</p></li></ul></li></ol><h3 id="implementation-using-monoview-noncoplanar-points">Implementation using monoview noncoplanar points</h3><p>When <span class="math inline">\(s_x\)</span> is unknown . Now a coplanar set of calibration points is required. Now <span class="math inline">\(z_w\)</span> is no longer identical zero.</p><ol type="1"><li><p>Compute 3D Orientation , Position (<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>) and scale factor</p><ul><li><p>Compute the distorted image coordinates <span class="math inline">\((X_d,Y_d)\)</span> <span class="math display">\[X_{di}={d_x}&#39;(X_{fi}-C_x)\\Y_{di}=d_y(Y_{fi}-C_y)\]</span> where <span class="math inline">\(C_x,C_y\)</span> are supposed as the center of the image frame (aka they cannot be calibrated), <strong>here, the real <span class="math inline">\(s_x\)</span> is absorbed into the unknowns for the liner equation in equations below.</strong></p></li><li><p>Compute the seven unknowns <span class="math inline">\(T_y^{-1}s_xr_1,T_y^{-1}s_xr_2,T_y^{-1}s_xr_3,T_y^{-1}s_xT_x,T_y^{-1}s_xr_4,T_y^{-1}s_xr_5,T_y^{-1}s_xr_6\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307132957951.png" alt="image-20210307132957951" style="zoom:30%;" />, this requires at least 8 points.</p></li><li><p>Compute <span class="math inline">\((r_1,\cdots,r_9,T_x,T_y)\)</span> from <span class="math inline">\(T_y^{-1}s_xr_1,T_y^{-1}s_xr_2,T_y^{-1}s_xr_3,T_y^{-1}s_xT_x,T_y^{-1}s_xr_4,T_y^{-1}s_xr_5,T_y^{-1}s_xr_6\)</span></p><ul><li><p>Compute <span class="math inline">\(|T_y|\)</span> from <span class="math inline">\((T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5)\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133504840.png" alt="image-20210307133504840" style="zoom:50%;" /></p></li><li><p>Determine the sign of <span class="math inline">\(T_y\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307121434690.png" alt="image-20210307121434690" style="zoom:40%;" /></p><p>This sign reversal of <span class="math inline">\(T_y\)</span> causes <span class="math inline">\((x,y)\)</span> to become <span class="math inline">\(-(x,y)\)</span>. But because <span class="math inline">\(X_d,x\)</span> have the same sign, <span class="math inline">\(Y_d, y\)</span> have the same sign, then <strong>only one of the two signs for <span class="math inline">\(T_y\)</span> is valid</strong> .</p></li><li><p><strong><em>Determine s_x</em></strong>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133615299.png" alt="image-20210307133615299" style="zoom:33%;" /></p></li><li><p>Compute the 3D rotation <span class="math inline">\(R\)</span></p><p>​ <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133750413.png" alt="image-20210307133750413" style="zoom:50%;" />, <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133819256.png" alt="image-20210307133819256" style="zoom:50%;" /></p></li></ul></li></ul></li><li><p>Compute effective focal length, distortion coefficients and <span class="math inline">\(z\)</span> position.</p><p>Here, <span class="math inline">\(k_1=0\)</span>.</p><ul><li><p>Compute an approximation of <span class="math inline">\(f, T_z\)</span> by ignoring lens distortion.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122549000.png" alt="image-20210307122549000" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122644888.png" alt="image-20210307122644888" style="zoom:50%;" /></p></li><li><p>Compute exact solution for <span class="math inline">\(f,T_z,k_1\)</span></p><p>Solve <span class="math inline">\((8b)\)</span>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307124032524.png" alt="image-20210307124032524" style="zoom:50%;" />, with <span class="math inline">\(f,T_z,k_1\)</span> as unknowns using standard optimization scheme such as steepest descent. The approximation of <span class="math inline">\(f,T_z\)</span> in previous step can be used as initial guess, and the initial guess of <span class="math inline">\(k_1\)</span> can be taken as zero.</p></li></ul></li></ol><h1 id="paper">Paper</h1><p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.2656&amp;rep=rep1&amp;type=pdf">3D photography on your desk, 1998</a>, <a href="http://www.vision.caltech.edu/bouguetj/ICCV98/">website</a></p><h2 id="why">Why?</h2><ul><li>Knowing the 3D shape helps a lot, and the progress and computers and computer graphics also encourage the recovering of 3D shape</li><li>Previous<ul><li>commercial 3D scanners: <strong>accurate</strong> but expensive and bulky. Use motorized transport of temporal object and active (laser, LCD projector) lighting of the scene</li><li>Passive cues contain information on 3D shape: stereoscopic disparity, texture, motion parallax, defocus, shadows, shading and specularities, occluding contours and other surface discontinuities amongst them. Stereoscopic disparity is popular way but it suffers from the requirement of two cameras and failing on untextured faces</li></ul></li></ul><h2 id="goals">Goals</h2><p>Simple and inexpensive approach for extracting the 3D shape of objects</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Illuminate the camera (facing the object) by desk-lamp, then the user moves a pencil in front of the light source casting a moving shadow on the object, the 3D shape of object will be recovered by the spatial and temporal location of the observed shadow.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210304224912139.png" alt="image-20210304224912139" style="zoom:30%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210305085918925.png" alt="image-20210305085918925" style="zoom:40%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307200112887.png" alt="image-20210307200112887" style="zoom: 33%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307200146056.png" alt="image-20210307200146056" style="zoom:33%;" /></p><h3 id="implementation">Implementation</h3><h4 id="calibration"><strong>Calibration</strong></h4><ul><li><p><strong>Camera calibration</strong></p><ul><li><p>recover the intrinsic camera parameters (<span class="math inline">\(f,K,s_x,(u_0,v_0)\)</span>) and the location of the desk plane with respect to camera.</p></li><li><p>The method is from <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1087109">here</a>. But because the object used is planar, the optical center cannot be calibrated exactly, so the optical center is assumed to be the center of the image.</p><ul><li><p>Specifically, this method considering <strong>lens distortion</strong>. And the steps for calibration is</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210305220451049.png" alt="image-20210305220451049" style="zoom:67%;" /></p><ul><li><p><em>Why there are lens distortion</em> ? On some cheap camera, the captured pixel on image is rectangle but not square. This will lead to lens distortion. Under lens distortion, suppose the image point under ideal case is <span class="math inline">\((X_u,Y_u)\)</span>, and after lens distortion the image point is <span class="math inline">\((X_d,Y_d)\)</span>, <span class="math inline">\(K\)</span> is the lens distortion coefficient, then</p><p><span class="math inline">\(r^2={X_u}^2+{Y_u}^2,\\ X_d=X_u(1+Kr^2),Y_d=Y_u(1+Kr^2)\)</span></p></li><li><p>From the image above, <span class="math inline">\(P_u\)</span> is the image coordinates of <span class="math inline">\(P\)</span> in real 3D world under ideal case (no lens distortion ), and <span class="math inline">\(P_d\)</span> is the image coordinates considering lens coordinates. Then the formulas are:</p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307144904684.png" alt="image-20210307144904684" /><figcaption aria-hidden="true">image-20210307144904684</figcaption></figure></li></ul></li></ul></li></ul></li><li><p><strong>lamp calibration</strong> : to determine the 3D location of the point light source <span class="math inline">\(S\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307152240120.png" alt="image-20210307152240120" style="zoom:33%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307152308342.png" alt="image-20210307152308342" style="zoom:30%;" /></p><ul><li>Given the height of pencil: <span class="math inline">\(h\)</span>, the pencil will be orthogonal to the desk.</li><li>Measure the bottom of the pencil (<span class="math inline">\(\bar{b}\)</span>) and the tip of the shadow <span class="math inline">\(\bar{t}_s\)</span> in captured image, then according to the calibrated camera, the coordinates of pencil bottom in 3D world (denoted as <span class="math inline">\(K\)</span>) and the tip of shadow in real 3D world (<span class="math inline">\(T_s\)</span>) can be estimated. Then the tip of pencil in 3D world (<span class="math inline">\(T\)</span>) is estimated by <span class="math inline">\(h\)</span>.</li><li>The light source <span class="math inline">\(S\)</span> will be the intersection of two rays <span class="math inline">\(TT_S\)</span>. Therefore, two position of pencils will help figure the light source out.</li></ul></li></ul><h4 id="spatial-and-temporal-shadow-edge-localization"><strong>Spatial and temporal shadow edge localization</strong></h4><p>Notice temporal shadow will be scanned from the left to the right side of the scene and thus the right edge of the shadow corresponds to the front edge of the temporal profile</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307175819939.png" alt="image-20210307175819939" style="zoom:33%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307175915876.png" alt="image-20210307175915876" style="zoom:30%;" /></p><ul><li><strong>Two steps to be done</strong><ul><li><strong>Localize</strong> the edge of the shadow that is directly projected on the tabletop <span class="math inline">\((\bar{x}_{top}(t),\bar{x}_{bot}(t))\)</span> at every time instant <span class="math inline">\(t\)</span> (every frame).</li><li><strong>Estimate the time</strong> <span class="math inline">\(t_s(\bar{x}_c)\)</span> (shadow time) where the edge of the shadow passes through any given pixel <span class="math inline">\(\bar{x}_c=(x_c,y_c)\)</span> in the image.</li></ul></li><li>Cause the pixels corresponding to regions in the scene are not illuminated by the lamp, so they don't provide any relevant depth information. And that's why only processing pixels with contrast value (<span class="math inline">\(I_{max}(x,y)-I_{min}(x,y)\)</span>) larger than predefined threshold <span class="math inline">\(I_{thresh}\)</span>, which is 70 in this paper.</li><li>Adaptive threshold image <span class="math inline">\(I_{shadow}(x,y)=(I_{max}(x,y)+I_{min}(x,y))/2\)</span></li><li>No spatial filtering is used cause it would generate undesired blending in the final depth estimates.</li><li>But when the light source is not close to an ideal point source, the predefine threshold <span class="math inline">\(I_{shadow}\)</span> (as a mean) is not optimum. The shadow edge profile becomes shallower as the distance between the stick and the surface increase.</li><li>To <strong>do it real-time,</strong> as the images <span class="math inline">\(I(x,y,t)\)</span> are acquired , one needs to update at each frame five different arrays <span class="math inline">\(I_{max}(x,y),I_{min}(x,y),I_{contrast}(x,y),I_{shadow}(x,y),t_{s}(x,y)\)</span>.<ul><li>For one pixel <span class="math inline">\((x,y)\)</span>, the maximum brightness <span class="math inline">\(I_{max}(x,y)\)</span> is collected at the 1st frame</li><li>The <span class="math inline">\(I_{min}(x,y), I_{contrast}(x,y)\)</span> is updated as time going. Once <span class="math inline">\(I_{contrast}(x,y)\)</span> crosses <span class="math inline">\(I_{thresh}\)</span><font color='red'> (<strong><em>larger than 70??</em></strong> which means now there is a shadow on pixel (x,y)?? )</font>, the adaptive threshold <span class="math inline">\(I_{shadow}(x,y)\)</span> starts being computed and updated at every frame. This process goes on till the pixel brightness <span class="math inline">\(I(x,y,t)\)</span> is larger than than <span class="math inline">\(I_{shadow}(x,y)\)</span> at the 1st time. <strong>This time instant is registered as the shadow time <span class="math inline">\(t_s(x,y)\)</span>.</strong></li></ul></li></ul><h4 id="triangulation"><strong>Triangulation</strong></h4><ul><li>The real 3D point <span class="math inline">\(P\)</span> is gonna be the intersection of line <span class="math inline">\(O_c\bar{x}_c\)</span> and the given pixel <span class="math inline">\(\bar{x}_c\)</span>'s shadow plane.</li><li>The shadow time <span class="math inline">\(t_s(\bar{x}_c)\)</span> acts as an index to the shadow plane list <span class="math inline">\(\prod(t)\)</span>. Besides, the final plane <span class="math inline">\(\prod(t_s(\bar{x}_c))\)</span> will from plane <span class="math inline">\(\prod(t_0-1)\)</span> and <span class="math inline">\(\prod{t_0}\)</span> if <span class="math inline">\(t_0-1&lt;t_s{(\bar{x}_c)}&lt;t_0\)</span> and <span class="math inline">\(t_0\)</span> integer.</li><li>After the range data are recovered, a mesh can be used to build the 3D surface.</li></ul><h3 id="noise-sensitivity">Noise sensitivity</h3><ul><li><p>For quantifying the effect of the noise in the measurement data <span class="math inline">\(\{x_{top}(t), x_{bot}(t), t_{s}(\bar{x}_c)\}\)</span> on the final reconstructed scene depth map, he analysis of <strong>the variance of the induced noise on the depth estimation <span class="math inline">\(Z_c\)</span>, aka <span class="math inline">\(\sigma_{Z_c}\)</span>,</strong> will help. In a word, <span class="math inline">\(\sigma_{Z_c}\)</span> can quantify the uncertainties on the depth estimation <span class="math inline">\(Z_c\)</span> at every pixel <span class="math inline">\(\bar{x}_c\)</span>, and also constitute a good indicator of the overall accuracies in reconstruction (since most of the errors are located along the <span class="math inline">\(Z\)</span> direction of the camera frame ).</p></li><li><p>The variance of the induced noise on the depth estimation <span class="math inline">\(Z_c\)</span>, aka <span class="math inline">\(\sigma_{Z_c}\)</span>, is derived by taking the 1st order derivatives of <span class="math inline">\(Z_c\)</span> with respect to the 'new' noisy input <span class="math inline">\(x_{top},x_{bot},\bar{x}_c\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307205307729.png" alt="image-20210307205307729" style="zoom:33%;" />, <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307205329890.png" alt="image-20210307205329890" style="zoom:30%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307205408384.png" alt="image-20210307205408384" style="zoom:33%;" /></p></li><li><p>From the equation <span class="math inline">\(\sigma_{x_c}=\frac{\sigma_I}{|I_x(\bar{x}_c)|}\)</span>, <span class="math inline">\(\sigma_{x_c}\)</span> does not depend on the local shadow speed. Therefore , decreasing the scanning speed would not increase the accuracy . However, <strong>better slow down while scanning when the shadow edge is sharper</strong> so as to get good samples for every pixel. But with slow scanning speed, an appropriate low-pass filter before extraction of <span class="math inline">\(t_{s}(\bar{x}_c)\)</span> is required for good accuracy.</p></li><li><p>Numerically, most of the variations in the variance <span class="math inline">\(\sigma_{Z_c}^2\)</span> are due to the variation of volume <span class="math inline">\(V\)</span> within a single scan. And Therefore the <strong>reconstruction noise is systematically larger in portions of the scene further away from the lamp</strong>.</p><ul><li><p><strong>To avoid the systematic error,</strong> one may take two scans of the same scene with the lamp at two different locations (on the left and right side of the camera say)</p></li><li><p><strong><em>The final depth is estimated as</em></strong> <span class="math display">\[Z_c=w_LZ_c^L+w_RZ_c^R\]</span> where <span class="math inline">\(Z_c^L,Z_c^R\)</span> are the two estimates (from left and right) of the same depth <span class="math inline">\(Z_c\)</span>.</p><ul><li><strong>If they are gaussian distributed,</strong> and independent, then using</li></ul><p><span class="math display">\[w_L=\frac{\sigma_{Z_R}^2}{\sigma_{Z_R}^2+\sigma_{Z_L}^2}=\frac{\alpha^2}{1+\alpha^2},\\w_R=\frac{\sigma_{Z_L}^2}{\sigma_{Z_R}^2+\sigma_{Z_L}^2}=\frac{1}{1+\alpha^2},\\\alpha=\frac{V_L}{V_R}\]</span></p><p>for averaging.</p><p>But this suffers from degradation of the overall final reconstruction cause may the <span class="math inline">\(Z_c^L,Z_c^R\)</span> are not gaussian .</p><ul><li>To avoid the problem mentioned above, another solution is sigmoid. <span class="math display">\[w_L=\frac{1}{1+\exp(-\beta\Delta V)},\\w_R=\frac{1}{1+\exp(\beta\Delta V)},\\\Delta V=\frac{V_L^2-V_R^2}{V_L^2+V_R^2}=\frac{\alpha^2-1}{\alpha^2+1}\]</span> The positive coefficient <span class="math inline">\(\beta\)</span> controls the amount of diffusion between the left and the right regions. As <span class="math inline">\(\beta\)</span> tends to infinity, merging reduces to hard decision: <span class="math inline">\(Z_c=Z_c^L\)</span> if <span class="math inline">\(V_L&gt;V_R\)</span> and <span class="math inline">\(Z_c=Z_c^R\)</span> otherwise. This will help reduce tends estimation error and obtain more coverage of the scene.</li></ul></li><li><p>The global accuracy depends on the scanning . This paper scan vertically, so the average relative depth error <span class="math inline">\(|\frac{\sigma_{Z_c}}{Z_c}|\)</span> is inversely proportional proportion to <span class="math inline">\(|\cos\xi|\)</span>. The best value will be got while <span class="math inline">\(\xi=0, \xi=\pi\)</span>. aka <strong>lamp standing either tot he right (<span class="math inline">\(\xi=0\)</span> ) or to the left (<span class="math inline">\(\xi=\pi\)</span> ) of the camera.</strong></p></li></ul></li></ul><h2 id="issues">Issues</h2><h3 id="point-light-source"><strong>Point light source</strong></h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210414221836181.png" alt="image-20210414221836181" style="zoom:67%;" /></p><h3 id="real-time-implementation"><strong>real-time implementation </strong></h3><p><strong>while estimating the shadow time</strong></p><p>All that one needs to do is update at each frame five different arrays <span class="math inline">\(I_{max}(x; y), I_{min}(x; y), I_{contrast}(x; y),\)</span> <span class="math inline">\(I_{shadow}(x; y)\)</span> and the shadow time <span class="math inline">\(t_s(x; y)\)</span>, as the images <span class="math inline">\(I(x; y; t)\)</span> are acquired. For a given pixel <span class="math inline">\((x; y)\)</span>, the maximum brightness <span class="math inline">\(I_{max}(x; y)\)</span> is collected at the very beginning of the sequence (the first frame), and then, as time goes, the incoming images are used to update the minimum brightness <span class="math inline">\(I_{min}(x; y)\)</span> and the contrast <span class="math inline">\(I_{contrast}(x; y)\)</span>. Once <span class="math inline">\(I_{contrast}(x; y)\)</span> crosses <span class="math inline">\(I_{thresh}\)</span>, the adaptive threshold <span class="math inline">\(I_{shadow}(x; y)\)</span> starts being computed and updated at every frame (and activated). This process goes on until the pixel brightness <span class="math inline">\(I(x; y; t)\)</span> crosses <span class="math inline">\(I_{shadow}(x; y)\)</span> for the first time (in the upwards direction). That time instant is registered as the shadow time <span class="math inline">\(t_s(x; y)\)</span>. In that form of implementation, the left edge of the shadow is tracked instead of the right one, however the principle remains the same.</p><h3 id="shadow-time"><strong>shadow time</strong></h3><ul><li><strong>Function</strong>: works as an index to the shadow plane list so as to using the intersection to locate <span class="math inline">\(P\)</span>.</li><li><strong>Accuracy</strong>: Since <span class="math inline">\(t_s(\bar{x}_c)\)</span> is estimated at sub-frame accuracy, the final plane <span class="math inline">\(\prod(t_s(\bar{x}_c))\)</span> actually results from linear interpolation between the two planes <span class="math inline">\(\prod(t_0-1)\)</span> and <span class="math inline">\(\prod(t_0)\)</span> if <span class="math inline">\(t_0-1 &lt; t_s(\bar{x}_c) &lt; t_0\)</span> and <span class="math inline">\(t_0\)</span> integer.</li></ul><h3 id="accuracy-of-depth-estimation">Accuracy of depth estimation</h3><ul><li>The accuracy increases as the sharpness of image increases.</li><li>Remove the lamp reflector improve the accuracy.</li><li>Decreasing the scanning speed would not increase accuracy.</li><li>To guarantee the accuracy of sharp edges of object, temporal pixel profile must be sufficiently sampled within the transition area of the shadow edge. Therefore, <strong>the sharper the shadow edge, the slower the scanning speed will help.</strong></li><li><span class="math inline">\(\sigma_{Z_c}\)</span> is a good indicator of the overall accuracies while reconstruction, since most of the errors are located along the <span class="math inline">\(Z\)</span> direction of the camera frame.</li><li>As the shadow moves into the opposite direction of the lamp, the absolute value of the volume <span class="math inline">\(|V|\)</span> strictly decreases and making <span class="math inline">\(\sigma_{Z_c}\)</span> larger. Therefore, the reconstruction noise is systematically larger in portions of the scene further away from the lamp.</li></ul><h2 id="experiments">Experiments</h2><h3 id="calibration-accuracies">Calibration accuracies</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307223201885.png" alt="image-20210307223201885" style="zoom:33%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307223230315.png" alt="image-20210307223230315" style="zoom:33%;" /></p><ul><li>For camera calibration,<ul><li>10 images of the checkboard are taken, nearly 90 corners on the checkboard (8*9).</li><li>the relative error of radial distortion is larger than others.</li></ul></li><li>In lamp calibration,<ul><li>collect 10 images of the pencil shadow</li><li><span class="math inline">\(\bar{S}_c\)</span> is the coordinate vector of the light source in the camera frame, points <span class="math inline">\(b,t_s\)</span> were manually extracted from the images.</li><li>The calibration accuracy is about 3mm, which is sufficient for final shape recovery.</li></ul></li></ul><h3 id="scene-reconstructions">Scene reconstructions</h3><ul><li>Planarity of the plane<ul><li>There is a decrease of approximately <span class="math inline">\(6\%\)</span> in residual standard deviation after quadratic warping. The global geometric deformations are negligible compared to local surface noise. It indicates that the errors of calibration for not induce significant global deformations on the final reconstruction.</li><li><strong>Why 0.23mm/5cm?</strong></li></ul></li><li>Geometry of the corner<ul><li>The overall reconstructed structure does not have any major noticeable global deformation.</li><li>The <strong>errors</strong> (which errors? The surface noise?) are the order of 0.3mm in most experiments.</li></ul></li><li>Angle scene<ul><li>With bulb naked, there is a significant improvement in the sharpness of the projected shadow compared the shadow captured with lamp reflector.</li></ul></li></ul><h3 id="outdoor">Outdoor</h3><ul><li>outdoors where the sun may be used as a calibrated light source (given latitude, longitude, and time of day).</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Simple and low cost system to extracting surface shape of objects. It can be used in real time. The accuracies on the final reconstruction are reasonable (at most <span class="math inline">\(1\%\)</span> or 0.5mm noise error)</li><li>it easily scales to larger scenarios indoors and outdoors.</li><li>Future work:<ul><li>like using the sun as the light source if measuring outdoors.</li><li>multiple view integration so as to move freely the object in front ode the camera and lamp between scans.</li><li>Incorporate a geometrical model of extended light source to the shadow edge detection process.</li><li></li></ul></li></ul><h2 id="qa">QA</h2><ul><li><p>Briefly explain, referring to the figure, how to obtain a 3D reconstruction at from a camera, a light source, a stick, and one or two shots.</p><ul><li>preliminaries: calibrated lamp <span class="math inline">\(S\)</span> and camera <span class="math inline">\(O_c\)</span>.</li><li>Slowly scan the target object by the stick and saved as a video.<ul><li>Do the statistics to get <span class="math inline">\(I_{max},I_{min}\)</span> and then define <span class="math inline">\(I_{shadow}\)</span>.</li><li>Check the video, for the image of each object's point (manually defined, such as each pixel of this object) <span class="math inline">\(\bar{x}_c:(x_{c},y_{c})\)</span> in the whole video, use <span class="math inline">\(I_{shadow}\)</span> and image intensity, draw the temporal shadow and locate the <span class="math inline">\(t_s(x_c,y_c)\)</span> (the index of frame).</li></ul></li><li>For each point in object <span class="math inline">\(\bar{x}_c\)</span>:<ul><li>Check the shadow time <span class="math inline">\(t_s(x_c,y_c)\)</span> and pick the corresponding image.</li><li>Set <span class="math inline">\(x_{bot}(t), x_{top}(t)\)</span> on the picked image, and then find its corresponding 3D points <span class="math inline">\(A(t),B(t)\)</span> respectively.</li><li>Define the shadow plane as the plane consists of <span class="math inline">\(S,A(t),B(t)\)</span>.</li><li>The 3D point of <span class="math inline">\(x_c\)</span>, denoted as <span class="math inline">\(P\)</span> is the intersection of shadow plane and ray <span class="math inline">\(O_cx_c\)</span>.</li></ul></li><li>After the 3D coordinates of all interested points <span class="math inline">\(\bar{x}_c\)</span> are detected, 3D reconstruction can be completed.</li></ul></li><li><p>Referring to the figure, explain what the reference points A (t) and B (t) are for.</p><p>Points <span class="math inline">\(A(t)\)</span> and <span class="math inline">\(B(t)\)</span> are the 3D points, and used to define the shadow plane by light source <span class="math inline">\(S\)</span> and these two points.</p></li><li><p>To find the internal and external parameters of the camera, the article offers a method of calibration with a single image of a checkerboard placed on the desktop, but in this case it is necessary know the main point (center of the image). What could we do if we don't know the point and we want to estimate it?</p><ul><li><p><del><em>Method 1：2D-3D calibration</em></del></p><p><del>Use the dodecahedron as the 3D object to calibrate camera. The optical center is the unknown parameter of camera intrinsic matrix <span class="math inline">\(\mathrm{K}\)</span>. Because the DoF of <span class="math inline">\(\mathrm{K}\)</span> is 8, at least 4 pairs of non-coplanar points (2D-3D) are required during calibration.</del></p></li><li><p><em>Method 2：plane calibration</em></p><p>Still, use the checkboard, but this time move the checkboard so as to get at least two images of checkboard with different pose (keep at least one pose on the desk), each image and checkboard pair will generate at least 4 pairs of 2D-3D points. With at least 2 homographies (induced from the images), the IAC, denoted as <span class="math inline">\(\omega\)</span> can be detected and thus <span class="math inline">\(\mathrm{K}\)</span> is detected by Cholesky decomposition cause <span class="math inline">\(\omega=\mathrm{K}^{-T}\mathrm{K}^{-1}\)</span>. The optical center is in <span class="math inline">\(\mathrm{K}\)</span>.</p></li></ul></li><li><p>The article says that if we use two planes perpendicular (<span class="math inline">\(\pi_h\)</span> and <span class="math inline">\(\pi_v\)</span>) rather than a single plane, we do not need calibrate the light source. Why?</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210415102209642.png" alt="image-20210415102209642" style="zoom:30%;" /></p><p>The requirement of light source <span class="math inline">\(S\)</span> is for the detection of shadow plane so as to get the intersection of shadow plane and ray <span class="math inline">\(O_cx_c\)</span>. Three non-coplanar points define a plane. Here, <span class="math inline">\(A(t), B(t)\)</span> are detected by calibrated camera <span class="math inline">\(O_c\)</span> and images of them. Suppose <span class="math inline">\(A(t),B(t)\)</span> is on <span class="math inline">\(\pi_v\)</span> and any one shadow point <span class="math inline">\(S&#39;\)</span> is on <span class="math inline">\(\pi_h\)</span>. Since <span class="math inline">\(\pi_h \perp\pi_v\)</span>, <span class="math inline">\(S&#39;\)</span> is easy to be detected. Armed with these three points <span class="math inline">\(A(t),B(t), S\)</span>, the shadow plane can be located.</p></li><li><p>Why must the light source used be “point”? Explain. <strong>Give an example of a non-point light source that would also be adequate.</strong></p><ul><li><p>If the light source is not close to an ideal point source, the mean value between maximum and minimum brightness may not always constitute the optimal value for the threshold image <span class="math inline">\(I_{shadow}\)</span>. Indeed, the shadow edge profile becomes shallower as the distance between the stick and the surface increases. In addition, it deforms asymmetrically as the surface normal changes. These effects could make the task of detecting the shadow boundary points challenging.</p></li><li><p>naked bulb: like just remove the lamp reflector in paper.</p></li></ul></li><li><p>The article says that if we use a vertical pencil, we can calibrate the light source. Explain how, based on the illustration to the right. State how many images are needed and explain why.</p><ul><li>Given the height of pencil: <span class="math inline">\(h\)</span>, the pencil will be orthogonal to the desk.</li><li>Measure the bottom of the pencil (<span class="math inline">\(\bar{b}\)</span>) and the tip of the shadow <span class="math inline">\(\bar{t}_s\)</span> in captured image, then according to the calibrated camera, the coordinates of pencil bottom in 3D world (denoted as <span class="math inline">\(K\)</span>) and the tip of shadow in real 3D world (<span class="math inline">\(T_s\)</span>) can be estimated. Then the tip of pencil in 3D world (<span class="math inline">\(T\)</span>) is estimated by <span class="math inline">\(h\)</span>.</li><li>The light source <span class="math inline">\(S\)</span> will be the intersection of two rays <span class="math inline">\(TT_S\)</span>. Therefore, two position of pencils will help figure the light source out.</li></ul></li><li><p>Explain the concepts of temporal shadow and spatial shadow and what are their roles in the reconstruction process.</p><ul><li>spatial shadow helps to detect the column index of <span class="math inline">\(A(t),B(t)\)</span>'s images. After finding this, the real position of <span class="math inline">\(A(t),B(t)\)</span> can be detected by the calibrated camera <span class="math inline">\(O_c\)</span> and the shadow plane can be defined also. Then the 3D coordinates can be detected as the intersection of shadow plane and ray <span class="math inline">\(O_cx_c\)</span>.</li><li>temporal shadow works as an index to the shadow plane list so as to using the intersection to locate <span class="math inline">\(P\)</span>.</li></ul></li><li><p>Explain for the following elements of the assembly how it could be modified and to improve the precision of the 3D reconstruction. In each case, justify your answers.</p><ul><li><p>choice of light source:</p><ul><li>point light source, like what mentioned in the paper just remove the lamp reflector. Or can use candle without reflector. May can also use a board with only one small hole in front of the light, but this should be adjusted to make sure all required shadows are created (difficult in practical). <em>The reason is a point light source creates more precise shadows.</em></li><li>may can move the light source further away from the object to make shadow more clear.</li></ul></li><li><p>choice of stick used to shade:</p><p>make sure the edge of stick is sharp (like a square stick), and long enough to cover the target object. Because the sharper the stick, its shadow edge will be more clear and easier to be detected and thus to locate <span class="math inline">\(\bar{x}_{bot},\bar{x}_{top}\)</span>. The long enough stick will generate required shadow.</p></li><li><p>spatial resolution of the camera (image size):</p><p>the higher the spatial resolution, the more detailed the image (more pixels and smaller points), and thus the more detailed the reconstruction. To make sure the high spatial resolution, one can decrease the distance between camera and object or fix the camera but prolong the focal length.</p></li><li><p>temporal resolution of the camera (number of images per second):</p><p>make the scan speed faster will increase the temporal resolution. The sharper the shadow edge, the higher the temporal resolution will help. If the scan speed is too fast, then one can see a blurred shadow that hurts the reconstruction accuracy, but this can be fixed by reducing the exposure.</p></li></ul></li><li><p>The article suggests that the same method can be used outdoors. Based on the illustration to the right, explain how this can be done.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210415103710164.png" alt="image-20210415103710164" style="zoom:35%;" /></p><ul><li>Calibration of light source (sun) and camera<ul><li>The camera calibration can be done by checkboard.</li><li>The calibration of light source: use the latitude, longitude, and cause the sun is very far away from the earth, each ray can be taken as a parallel ray.</li></ul></li><li>Take the video of shadow image and take the note of time of the day. Similarly, calculate <span class="math inline">\(I_{max},I_{min}\)</span> and then calculate <span class="math inline">\(I_{shadow}\)</span>. Draw the temporal profile and denote the shadow time <span class="math inline">\(t_s(x_c,y_c)\)</span> as the time of day.</li><li>For each point in object <span class="math inline">\(\bar{x}_c\)</span>:<ul><li>Check the shadow time <span class="math inline">\(t_s(x_c,y_c)\)</span> and pick the corresponding image.</li><li>Set <span class="math inline">\(x_{bot}(t), x_{top}(t)\)</span> on the picked image, and then find its corresponding 3D points <span class="math inline">\(A(t),B(t)\)</span> respectively.</li><li>Define the shadow plane as the plane consists of <span class="math inline">\(S,A(t),B(t)\)</span>.</li><li>The 3D point of <span class="math inline">\(x_c\)</span>, denoted as <span class="math inline">\(P\)</span> is the intersection of shadow plane and ray <span class="math inline">\(O_cx_c\)</span>.</li></ul></li><li>After the 3D coordinates of all interested points <span class="math inline">\(\bar{x}_c\)</span> are detected, 3D reconstruction can be completed.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.2656&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;3D photography on your desk, 1998&lt;/a&gt;, &lt;a href=&quot;http://www.vision.caltech.edu/bouguetj/ICCV98/&quot;&gt;website&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Reference: &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/1087109?casa_token=RZ6-EVaSgrIAAAAA:QwrjPXnkBwNtWBQR_RsNxixV2Q7NZ4qj9DFuPG4PkI3rm8J_6NZQUq4spi3Op-xpmGSEwCNuyA&quot;&gt;A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="cv" scheme="http://yoursite.com/tags/cv/"/>
    
      <category term="3d shape" scheme="http://yoursite.com/tags/3d-shape/"/>
    
  </entry>
  
  <entry>
    <title>Papers--The reviews of anomaly detection</title>
    <link href="http://yoursite.com/posts/notes/2021-03-03-notes-paper-anomal-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-03-03-notes-paper-anomal-survey.html</id>
    <published>2021-03-04T03:35:39.000Z</published>
    <updated>2021-11-30T04:26:10.279Z</updated>
    
    <content type="html"><![CDATA[<p>Paper I: <a href="https://arxiv.org/pdf/2007.02500.pdf">Deep Learning for Anomaly Detection: A Review</a></p><p>Project II: <a href="http://snap.stanford.edu/class/cs224w-2019/project/26424135.pdf">Graph Level Anomaly Detection</a></p><p>Paper III: <a href="https://www.osti.gov/servlets/purl/1214009">Multi-Level Anomaly Detection on Time-Varying Graph Data</a></p><a id="more"></a><h1 id="paper-i-deep-learning-for-anomaly-detection-a-review">Paper I: <a href="https://arxiv.org/pdf/2007.02500.pdf">Deep Learning for Anomaly Detection: A Review</a></h1><h2 id="why">Why?</h2><ul><li>outlier detection or novelty detection</li><li>Deep anomaly detection aims at learning feature representations or anomaly scores via neural networks for the sake of anomaly detection.</li></ul><h2 id="problem-complexities-and-challenges">Problem complexities and challenges</h2><h3 id="major-problem-complexities">Major problem complexities</h3><ul><li>Unknownness: Anomalies are associated with many unknowns</li><li>Heterogeneous anomaly classes: one class of anomalies may demonstrate completely different abnormal characteristics from another class of anomalies.</li><li>Rarity and class imbalance: it is difficult to collect a large amount of labeled abnormal instances.</li><li>Diverse types of anomaly:<ul><li>Point anomalies: individual instances that are anomalous w.r.t. the majority of other individual instances</li><li>Conditional anomalies: contextual anomalies, also refer to individual anomalous instances but in a specific context.</li><li>Group anomalies: a subset of data instances anomalous as a whole w.r.t. the other data instances.</li></ul></li></ul><h3 id="main-challenges-tackled-by-deep-anomaly-detection">Main challenges tackled by deep anomaly detection</h3><ul><li>CH1: low anomaly detection recall rate: How to reduce false positives and enhance detection recall rates</li><li>CH2: anomaly detection in high-dimensional and/or not-independent data:<ul><li>High-dimensional anomaly detection<ul><li>Performing anomaly detection in a reduced lower dimensional space spanned by a small subsets of original features or newly constructed features.</li><li>But challenges on identifying intricate (e.g., high-order, nonlinear and heterogeneous) feature interactions and couplings.</li></ul></li><li>Guarantee the new feature space preserved proper information for specific detection methods.<ul><li>Due to the aforementioned unknowns and heterogeneities of anomalies.</li></ul></li><li>Detect anomalies from instances that may be dependent on each other.</li></ul></li><li>CH3: data-efficient learning of normality/abnormality<ul><li>Fully supervised anomaly detection is often impractical.</li><li>Unsupervised methods do not have any prior knowledge of true anomalies. They rely heavily on their assumption on the distribution of anomalies.</li><li>Weakly supervised anomaly detection<ul><li>How to learn expressive normality/abnormality representations with a small amount of labeled anomaly data.</li><li>How to learn detection models that are generalized to novel anomalies uncovered by the given labeled anomaly data.</li></ul></li></ul></li><li>CH4： Noise-resilient anomaly detection<ul><li>Large-scale anomaly-contaminated unlabeled data</li><li>The amount of noises can differ significantly from datasets and noisy instances may be irregularly distributed in the data space.</li></ul></li><li>CH5：Detection of complex anomalies<ul><li>The generation from point anomalies to conditional anomalies and group anomalies</li><li>How to incorporate the concept of conditional/group anomalies into anomaly measures/models.</li><li>The detection of anomalies with multiple heterogeneous data sources.</li></ul></li><li>CH6： Anomaly explanation<ul><li>Have anomaly explanation algorithms that provide straightforward clues about why a specific data instance is identified as anomaly.</li><li>A main challenge to well balance the model’s interpretability and effectiveness.</li></ul></li></ul><h2 id="addressing-the-challenges-with-deep-anomaly-detection">Addressing the challenges with deep anomaly detection</h2><h3 id="preliminaries">Preliminaries</h3><ul><li>Deep anomaly detection aims at learning a feature representation mapping function <span class="math inline">\(\phi (·) : X\mapsto Z\)</span> or an anomaly score learning function <span class="math inline">\(\tau (·): X\mapsto R\)</span> in a way that anomalies can be easily differentiated from the normal data instances in the space yielded by the <span class="math inline">\(\phi\)</span> or <span class="math inline">\(\tau\)</span> function.</li></ul><h3 id="categorization-of-deep-anomaly-detection">Categorization of deep anomaly detection</h3><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602121703067.png" alt="image-20210602121703067" style="zoom:70%;" /></p><h2 id="deep-learning-for-feature-extraction">Deep learning for feature extraction</h2><ul><li>The deep learning components work purely as dimensionality reduction only.</li><li>Assumptions: The feature representations extracted by deep learning models preserve the discriminative information that helps separate anomalies from normal instances.</li><li>Research lines<ul><li>Directly use popular pre-trained deep learning models such as VGG etc.<ul><li>An unmasking process: iteratively train a binary classifier to separate one set of video frames from its subsequent video frames in a sliding window, with the most discriminant features removed in each iteration step. The power of unmasking framework relies heavily on the quality of the features.</li><li>Using features extracted from a dynamically updated sampling pool of video frames is found to improve the performance of the framework.</li><li>Pretrained model and fine-tuning.</li></ul></li><li>Explicitly train a deep feature extraction model rather than a pre-trained model for the downstream anomaly scoring.<ul><li>Methods<ul><li>Three separate autoencoder networks are trained to learn low-dimensional features for respective appearance, motion, and appearance-motion joint representations for video anomaly detection.</li><li>Unsupervised classification approaches to enable anomaly scoring in the projected space. Use cluster methods to assign pseudo labels and then do one-vs-the-rest classification. The classification probabilities are used to define frame-wise anomaly scores.</li><li>Graph anomaly detection: learn the representations of graph vertices by minimizing autoencoder-based reconstruction loss and pairwise distances of neighbored graph vertices.</li></ul></li></ul></li></ul></li><li>Advantages<ul><li>A large number of state-of-the-art (pre-trained) deep models and off-the-shelf anomaly detectors are readily available.</li><li>Deep feature extraction offers more powerful dimensionality reduction than popular linear methods.</li><li>It is easy-to-implement given the public availability of the deep models and detection methods.</li></ul></li><li>Disadvantages<ul><li>The fully disjointed feature extraction and anomaly scoring often lead to suboptimal anomaly scores.</li><li>Pre-trained deep models are typically limited to specific types of data.</li></ul></li><li>Challenges Targeted<ul><li>The lower-dimensional space often helps reveal hidden anomalies and reduces false positives (CH2).</li><li>May not preserve sufficient information for anomaly detection as the data projection is fully decoupled with anomaly detection.</li><li>Allows to leverage multiple types of features and learn semantic-rich detection models, and then reduce CH1.</li></ul></li></ul><h2 id="learning-feature-representation-of-normality">Learning feature representation of normality</h2><h3 id="generic-normality-feature-learning">Generic normality feature learning</h3><ul><li>Learns the representations of data instances by optimizing a generic feature learning objective function that is not primarily designed for anomaly detection.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602130709282.png" alt="image-20210602130709282" style="zoom:67%;" /></li></ul><h4 id="autoencoders">Autoencoders</h4><ul><li><p>Aims to learn some low-dimensional feature representation space on which the given data instances can be well reconstructed. The learned feature representations are enforced to learn important regularities of the data to minimize reconstruction errors, anomalies are difficult to be reconstructed from the resulting representations and thus have large reconstruction errors.</p></li><li><p>Assumptions: Normal instances can be better restructured from compressed space than anomalies.</p></li><li><p>Formally,</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602132252366.png" alt="image-20210602132252366" style="zoom:70%;" /></p></li><li><p>Methods</p><ul><li>Sparse AE: encourage sparsity in the activation units of the hidden layer.</li><li>Denoising AE: learning representations that are robust to small variations by learning to reconstruct data from some predefined corrupted data instances rather than original data.</li><li>Contractive AE: takes a step further to learn feature representations that are robust to small variations of the instances around their neighbors. By adding a penalty term based on the Forbenius norm of the Jacobian matrix of the encoder's activations.</li><li>Variational AE: introduces regularization into the representation space by encoding data instances using a prior distribution over the latent space.</li></ul></li><li><p>Implementations</p><ul><li>Replicator NNs</li><li>RandNet: learning an ensemble of AEs</li><li><a href="https://git.io/JfYG5">RDA</a>: motivated by robust PCA, it attempts to iteratively decompose term original data into 2 subsets, normal instance set and anomaly set. This is achieved by adding a sparsity penalty <span class="math inline">\(\ell_1\)</span> or grouped penalty <span class="math inline">\(\ell_{2,1}\)</span> into its RPCA-alike objective function to regularize the coefficients of the anomaly set.</li></ul></li><li><p>For more complex data</p><ul><li>Adapting the network architecture to the type of input data, they embeds the encoder-decoder scheme into the full procedure of these methods, such as CNN-AE, LSTM-AE, Conv-LSTM-AE, GCN-AE etc.</li><li>First use AEs to learn low-dimensional representations of the complex data and then learn to predict these learned representations.<ul><li>denoising AE is combined with RNNs to learn normal patterns of multivariate sequence data,</li></ul></li></ul></li><li><p>Advantages</p><ul><li>The idea of AEs is straightforward and generic to different types of data.</li><li>Different types of powerful AE variants can be leveraged to perform anomaly detection.</li></ul></li><li><p>Disadvantages</p><ul><li>The learned feature representations can be biased by infrequent regularities and the presence of outliers or anomalies in the training data.</li><li>The objective function of the data reconstruction is designed for dimension reduction or data compression, rather than anomaly detection.</li></ul></li><li><p>Challenges</p><ul><li>CH2: attributed graph data etc.</li><li>CH1: reduce false positives</li><li>CH4: RPCA</li></ul></li></ul><h4 id="gans">GANs</h4><p>Some form of residual between the real instance and the generated instance are then defined as anomaly score.</p><ul><li><p>Assumption: Normal data instances can be better generated than anomalies from the latent feature space of the generative network in GANs</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfGgc">AnoGAN</a>: computational inefficiency in the iterative search of latent representation <span class="math inline">\(\boldsymbol{\mathrm{z}}\)</span>.</p><ul><li>To alleviate the inefficiency<ul><li><a href="https://git.io/JfGgG">EBGAN</a> : based on BiGAN, discriminate the pair of instances <span class="math inline">\((\mathrm{x}, 𝐸(\mathrm{x}))\)</span> from the pair <span class="math inline">\((𝐺(\mathrm{z}),\mathrm{z})\)</span>. EBGAN is extended to ALAD by adding two more discriminators with one discriminator trying to discriminate the pair <span class="math inline">\((\mathrm{x}, \mathrm{x})\)</span> from $(,𝐺(𝐸())) $and another one trying to discriminate the pair <span class="math inline">\((\mathrm{z},\mathrm{z})\)</span> from <span class="math inline">\((\mathrm{z}, 𝐸(𝐺(\mathrm{z})))\)</span>.</li><li><a href="https://git.io/JfZRn">fast AnoGAN</a>: share the same spirit of EBGAN.</li><li><a href="https://git.io/JfZ8v">ALAD</a>: the extension of EBGAN</li></ul></li></ul></li><li><p><a href="https://git.io/JfGgn">GANomaly</a>: further improves the generator over the previous work by changing the generator network to an encoder-decoder-encoder network and adding two more extra loss function.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602143617119.png" alt="image-20210602143617119" style="zoom:60%;" /></p></li><li><p>Combined with wassertein GAN or adversarially learn end-to-end one-class classification.</p></li></ul></li><li><p>Advantages</p><ul><li>Demonstrated superior capability in generating realistic instances, especially on images.</li><li>A large number of existing GAN-based models and <a href="https://arxiv.org/abs/1710.07035">theories</a> may be adapted for anomaly detection.</li></ul></li><li><p>Disadvantages</p><ul><li>The training of GANs can suffer from multiple problems, such as failure to converge and mode collapse.</li><li>The generator network can be misled and generates data instances out of the manifold of normal instances.</li><li>The GANs-based anomaly scores can be suboptimal since they are built upon the generator network with the objective designed for data synthesis rather than anomaly detection.</li></ul></li><li><p>Challenges</p><ul><li>CH2， CH1</li></ul></li></ul><h4 id="predictability-modeling">Predictability modeling</h4><p>Learn feature representations by predicting the current data instances using the representations of the previous instances within a temporal window as the context. The prediction errors can be used to define the anomaly scores</p><ul><li><p>Assumptions: Normal instances are temporally more predictable than anomalies</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/Jf4pc">FFP</a>: Using like U-net prediction, popular in video anomaly detection.</p><p>U-Net as the frame generator, and measure the objective loss by intensity, gradient and optical flow. After training, for a given video frame <span class="math inline">\(\mathrm{x}\)</span>, a normalized Peak Signal-to-Noise Ratio based on the prediction difference is used to define the anomaly score.</p><ul><li><a href="https://git.io/Jf4pW">LSA</a></li></ul></li><li><p>Another way is based on autoregressive models that assume each element in a sequence is linearly dependent on the previous elements.</p><ul><li>At the evaluation stage, the reconstruction error and the log-likelihood are combined to define the anomaly score.</li></ul></li></ul></li><li><p>Advantages</p><ul><li>A number of sequence learning techniques can be adapted and incorporated into this approach.</li><li>This approach enables the learning of different types of temporal and spatial dependencies.</li></ul></li><li><p>Disadvantages</p><ul><li>This approach is limited to anomaly detection in sequence data.</li><li>The sequential predictions can be computationally expensive.</li><li>The learned representations may suboptimal for anomaly detection as its underlying objective is for sequential predictions rather than anomaly detection.</li></ul></li><li><p>Challenges</p><ul><li>CH1&amp;CH2</li><li>CH5</li></ul></li></ul><h4 id="self-supervised-classification">Self-supervised classification</h4><ul><li><p>Learns representations of normality by building self-supervised classification models and identifies instances that are inconsistent to the classification models as anomalies.</p></li><li><p>Evaluate the normality of data instances by their consistency to a set of predictive models, with each model learning to predict one feature based on the rest of the other feature.</p></li><li><p>Focuses on image data and builds the predictive models by using feature transformation-based augmented data.</p></li><li><p>Assumptions: Normal instances are more consistent to self-supervised classifiers than anomalies.</p></li><li><p>Methods</p><ul><li><a href="https://git.io/JfZRW">GT</a>: Like horizontal flipping, translations, and rotations. <strong>The classification scores of each test instance w.r.t. different <span class="math inline">\(𝑇_𝑗\)</span> are then aggregated to compute the anomaly score.</strong></li><li><a href="https://git.io/Jf4pl"><span class="math inline">\(E^3\)</span>outlier</a> : Training data contains normal instances only. <a href="https://proceedings.neurips.cc/paper/2019/file/6c4bb406b3e7cd5447f7a76fd7008806-Paper.pdf">UOD</a><ul><li><em>The gradient magnitude induced by normal instances is normally substantially larger than outliers during the training of such self-supervised multiclass classification models;</em></li><li><em>The network updating direction is also biased towards normal instances.</em></li></ul></li></ul><p><strong>Normal instances often have stronger agreement with the classification model than anomalies.</strong></p><p>Negative entropy-based anomaly scores perform generally better than average prediction probability and maximum prediction probability.</p></li><li><p>Advantages</p><ul><li>They work well in both the unsupervised and semi-supervised settings.</li><li><em>Anomaly scoring is grounded by some intrinsic properties of gradient magnitude and its updating.</em></li></ul></li><li><p>Disadvantages</p><ul><li>The feature transformation operations are often data-dependent. And the operations (rotation) only work on images.</li><li>The consistency-scores are derived upon the classification scores rather than an integrated module in the optimization, and thus they may be suboptimal.</li></ul></li><li><p>Challenges</p><ul><li>CH1 &amp; CH2</li><li>CH4</li></ul></li></ul><h3 id="anomaly-measure-dependent-feature-learning">Anomaly measure-dependent feature learning</h3><p>Learning feature representations that are specifically optimized for one particular existing measure. They incorporates an existing anomaly measure <span class="math inline">\(f\)</span> into the feature learning objective function to optimize the feature representations specifically for <span class="math inline">\(f\)</span>.</p><h4 id="distance-based-measure">Distance-based measure</h4><ul><li>Aims to learn feature representations that are specifically optimized for a specific type of distance-based anomaly measures.</li><li>Like DB outliers, <span class="math inline">\(k\)</span>-nearest neighbor distance, average <span class="math inline">\(k\)</span>-nearest neighbor distance, relative distance and random nearest neighbor distance. But they fail to work effectively in high dimensional data.</li><li>Assumption : anomalies are distributed far from their closet neighbors while normal instances are located in dense neighbors.</li><li>Methods<ul><li><a href="https://git.io/JfZRg">REPEN</a>: Random neighbor distance-based: The representations are optimized so that the nearest neighbor distance of pseudo-labeled anomalies in random subsamples is substantially larger than that of pseudo-labeled normal instances. The pseudo labels are generated by some off-the-shelf anomaly detectors. The loss function is built upon the hinge loss.</li><li><a href="https://git.io/RDP">RDP</a>: The other uses the distance between optimized representations and randomly projected representations of the same instances to guide the representation learning. Solving <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602202900471.png" alt="image-20210602202900471" style="zoom:50%;" /> is equivalent to have a knowledge distillation from a random NNs and helps learn the frequency of different underlying patterns in the data.</li><li>At the evaluation stage, the function of <span class="math inline">\(f\)</span> is used to compute the anomaly score.</li></ul></li><li>Advantages<ul><li>The distance-based anomalies are straightforward and well defined with rich theoretical supports in the literature.</li><li>They work in low-dimensional representation spaces and can effectively deal with high-dimensional data that traditional distance-based anomaly measures fail.</li><li>They are able to learn representations specifically tailored for themselves.</li></ul></li><li>Disadvantages<ul><li>The extensive computation involved in most of distance-based anomaly measures</li><li>Their capabilities may be limited by the inherent weaknesses of the distance-based anomaly measures.</li></ul></li><li>Challenges<ul><li>CH1&amp;CH2</li><li>CH3,CH4</li></ul></li></ul><h4 id="one-class-classification-measure">One-class classification measure</h4><p>Aims to learn feature representations customized to subsequent one-class classification-based anomaly detection. Learn a description of a set of data instances to detect whether new instances conform to the training data or not.</p><ul><li><p>One way is to learn representations that are specifically optimized for these traditional one-class classification models, like one-class SVM.</p></li><li><p>Assumption: all normal instances come from a single class and can be summarized by a compact model, to which anomalies do not conform.</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfGgl">AE-1SVM</a>, <a href="https://git.io/JfGgZ">OC_NN</a>: Deep one-class SVM, learn the one-class hyperplane from the neural network-enabled low-dimensional representation space rather than the original input space.</p><p>The formula is <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602205007059.png" alt="image-20210602205007059" style="zoom:33%;" /> Any instances that have $𝑟 − ^_𝑖 $ can be reported as anomalies. It has two benefits:</p><ul><li>It can leverage deep networks to learn more expressive features for downstream anomaly detection</li><li>It can also help remove the computational expensive pairwise distance computation in the kernel functions.</li><li>Based on one-class SVM, one may use a random mapping to map latent representation <span class="math inline">\(\mathrm{z}\)</span> to Fourier features since many kernel functions can be approximated by random Fourier features.</li></ul></li><li><p><a href="https://git.io/JfZRR">Deep SVDD</a>, <a href="https://git.io/JfOkr">Deep SAD</a>: Another way is deep models for SVDD, which aims at Learning a minimum hyperplane characterized by a center <span class="math inline">\(\mathrm{c}\)</span> and a radius <span class="math inline">\(r\)</span> so that the sphere contains all training data instances.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602210101160.png" alt="image-20210602210101160" style="zoom:67%;" /></p><p>It's shown that <span class="math inline">\(\mathrm{c}\)</span> as trainable parameters can lead to trivial solutions. <span class="math inline">\(\mathrm{c}\)</span> can be fixed as the mean of the feature representations yield by performing a single initial forward pass. The key idea is to minimize the distance of labeled normal instances to the center while at the same time maximizing the distance of known anomalies to the center.</p></li></ul></li><li><p>Advantages</p><ul><li>The one-class classification-based anomalies are well studied in the literature and provides a strong foundation of deep one-class classification-based methods.</li><li>The representation learning and one-class classification models can be unified to learn tailored and more optimal representations.</li><li>They free the users from manually choosing suitable kernel functions in traditional one-class model.</li></ul></li><li><p>Disadvantages</p><ul><li>The one-class models may work ineffectively in datasets with complex distributions within the normal class.</li><li>The detection performance is dependent on the one-class classification-based anomaly measure.</li></ul></li><li><p>Challenges : CH1&amp;CH2， CH3</p></li></ul><h4 id="clustering-based-measure">Clustering-based measure</h4><p>Aims at learning representations so that anomalies are clearly deviated from the clusters in the newly learned representation space.</p><ul><li><p>Methods use like cluster size, distance to cluster centers, distance between cluster centers, and cluster membership to define clusters.</p></li><li><p>Assumptions: Normal instances have stronger adherence to clusters than anomalies.</p></li><li><p>Many methods are explored based on the motivation that the performance of clustering methods is highly dependent on the input data.</p></li><li><p>The deep clustering methods typically consist of two modules: performing clustering in the forward pass and learning representations using the cluster assignment as pseudo class labels in the backward pass.</p><ul><li><p>The clustering loss can be initialized with a kmeans loss, a spectral clustering loss, an agglomerative loss or a GMM loss.</p></li><li><p>The auxiliary loss can be an autoencoder-based reconstruction loss to learn robust and/or local structure preserved representations.</p></li><li><p>The cluster assignments in the resulting function is used to compute anomaly scores.</p></li><li><p>The aforementioned deep clustering methods are focused on learning optimal clustering results, but the learned representations may not be able to well capture the abnormality of anomalies. <a href="https://www.researchgate.net/publication/330625995_A_Unified_Unsupervised_Gaussian_Mixture_Variational_Autoencoder_for_High_Dimensional_Outlier_Detection">Papers</a></p><p><a href="https://git.io/JfZR0">DAGMM</a>:</p><ul><li>The cluster loss is GMM loss and the auxiliary loss is autoencoder-based reconstruction loss.</li><li>The auxiliary loss is an an autoencoder-based reconstruction loss, but to learn deviated representations of anomalies.</li></ul></li></ul></li><li><p>Advantages</p><ul><li>A number of deep clustering methods and theories can be utilized to support the effectiveness and theoretical foundation of anomaly detection.</li><li>Learn specifically optimized representations that help spot the anomalies easier than on the original data, especially when dealing with intricate data sets.</li></ul></li><li><p>Disadvantages</p><ul><li>The performance of anomaly detection is heavily dependent on the clustering results.</li><li>The clustering process may be biased by contaminated anomalies in the training data, which in turn leads to less effective representations.</li></ul></li><li><p>Challenges: CH1&amp;CH2， CH4</p></li></ul><h2 id="end-to-end-anomaly-score-learning">End-to-end anomaly score learning</h2><p>Aims at learning scalar anomaly scores in an end-to-end fashion. It has a NN that directly learns the anomaly scores. Methods here won't be limited by the inherent disadvantages of the incorporated anomaly measures. There are two design directions: one focuses on how to synthesize existing anomaly measures and neural network models, while another focuses on devising novel loss functions for direct anomaly score learning.</p><h3 id="ranking-models">Ranking models</h3><p>Aims to directly learn a ranking model, such that data instances can be sorted based on an observable ordinal variable associated with the absolute/relative ordering relation of the abnormality.</p><ul><li><p>Assumptions: There exists an observable ordinal variable that captures some data abnormality.</p></li><li><p>Methods</p><ul><li><p>One line is to devise ordinal regression -based loss functions to drive the anomaly scoring neural network.</p><ul><li>Two-class ordinal regression.<ul><li>The end-to-end anomaly scoring network takes <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{N}\)</span> as inputs and learns to optimize the anomaly scores such that the data inputs of similar behaviors as those in <span class="math inline">\(\mathcal{A(N)}\)</span> receive large (small) scores as close <span class="math inline">\(𝑐_1 (𝑐_2)\)</span> as possible, resulting in larger anomaly scores assigned to anomalous frames than normal frames.</li></ul></li></ul></li><li><p>Weakly-supervision</p><ul><li><p><a href="https://git.io/JfZRz">MIL</a>: the model is optimized to learn larger anomaly scores for the pairs of two anomalies than the pairs with one anomaly or none.</p><p>MIL ranking model, directly learn the anomaly score for each video segment. Its key objective is to guarantee that the maximum anomaly score for the segments in a video that contains anomalies somewhere is greater than the counterparts in a normal video.<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605124035832.png" alt="image-20210605124035832" /></p><ul><li><font color='red'>Why not <span class="math inline">\(\min\)</span> at the last term in the 1st term?</font></li><li>The first term is to guarantee the relative anomaly score order, i.e., the anomaly score of the most abnormal video segment in the positive instance bag is greater than that in the negative instance bag. The last two terms are extra optimization constraints, in which the former enforces score smoothness between consecutive video segments while the latter enforces anomaly sparsity, i.e., each video contains only a few abnormal segments.</li></ul></li></ul></li></ul></li><li><p>Advantages：</p><ul><li>The anomaly scores can be optimized directly with adapted loss functions.</li><li>They are generally free from the definitions of anomalies by imposing a weak assumption of the ordinal order between anomaly and normal instances.</li><li>This approach may build upon well-established ranking techniques and theories from areas like learning to rank.</li></ul></li><li><p>Disadvantages</p><ul><li>At least some form of labeled anomalies are required.</li><li>Methods may not be able to generalize to unseen anomalies cause they are designed to detect labeled anomalies.</li></ul></li><li><p>Challenges:</p><ul><li>CH1&amp;CH2;</li><li>CH3</li><li>CH6</li><li>CH4</li></ul></li></ul><h3 id="prior-driven-models">Prior-driven models</h3><p>Use a prior distribution to encode and drive the anomaly score learning. The prior may be imposed on either the internal module or the learning output of the score learning function.</p><ul><li><p>Assumptions: The imposed prior captures the underlying (ab)normality of the dataset.</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfZRw">DevNet</a>: enforce a prior on the anomaly scores. It uses a Gaussian prior to encode the anomaly scores and enable the direct optimization very well. The deviation loss is built upon contrastive loss.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605131355985.png" alt="image-20210605131355985" /><figcaption aria-hidden="true">image-20210605131355985</figcaption></figure><ul><li>Driven by the deviation loss, it will push the anomaly scores of normal instances as close as possible to 𝜇 while guaranteeing at least 𝑚 standard deviations between 𝜇 and the anomaly scores of anomalies.</li><li>The loss is equivalent to enforcing a statistically significant deviation of the anomaly score of the anomalies from that of normal instances in the upper tail.</li><li>It's also interpretable.</li></ul></li></ul></li><li><p>Advantages</p><ul><li>The anomaly scores can be directly optimized w.r.t. a given prior.</li><li>It provides a flexible framework for incorporating different prior distributions into the anomaly score learning.</li><li>The prior can also result in more interpretable anomaly scores than the other methods.</li></ul></li><li><p>Disadvantages</p><ul><li>It's difficult to design a universally effective prior for all anomaly detection application.</li><li>The efficiency of model depends on how the picked prior fits the underlying distribution.</li></ul></li><li><p>Challenges</p><ul><li>CH1&amp;CH2</li><li>CH1&amp;CH3</li><li>CH4</li></ul></li></ul><h3 id="softmax-likelihood-models">Softmax Likelihood models</h3><p>Learning anomaly scores by maximizing the likelihood of events in the training data. Normal instances are presumed to be high-probability events whereas anomalies are prone to be low-probability events. Tools like NCE are used.</p><ul><li>Assumptions: Anomalies and normal instances are respectively low- and high-probability events.</li><li>Methods<ul><li>Use log negative likelihood. Learning the likelihood function 𝑝 is equivalent to directly optimizing the anomaly scoring function.</li><li>But the original likelihood is computed costly, NCE is used to alleviate. For each instance <span class="math inline">\(\mathrm{x}\)</span>, <span class="math inline">\(k\)</span> noise samples <span class="math inline">\(\mathrm{x}_{1, \cdots,k}\sim Q\)</span> are generated from some synthetic known ‘noise’ distribution <span class="math inline">\(Q\)</span>.</li></ul></li><li>Advantages<ul><li>Different types of interactions can be incorporated into the anomaly score learning process.</li><li>The anomaly scores are faithfully optimized w.r.t. the specific abnormal interactions we aim to capture.</li></ul></li><li>Disadvantages<ul><li>The computation of the interactions can be very costly when the number of features/elements in each data instance is large.</li><li>The anomaly score learning is heavily dependent on the quality of the generation of negative samples.</li></ul></li><li>Challenges<ul><li>CH2&amp;CH5</li><li>CH1</li></ul></li></ul><h3 id="end-to-end-one-class-classification">End-to-end one-class classification</h3><p>Train a one-class classifier that learns to discriminate whether a given instance is normal or not. It does not rely on any existing one-class classification measures. Methods like adversarially learned one-class classification are used. It learns a one-class discriminator of the normal instances so that it well discriminates those instances from adversarially generated pseudo anomalies. The goal is to learn a discriminator and this discriminatory will be directly used as anomaly scorer.</p><ul><li><p>Assumptions :</p><ul><li>Data instances that are approximated to anomalies can be effectively synthesized.</li><li>All normal instances can be summarized by a discriminative one-class model.</li></ul></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfZRw">ALOCC</a>:</p><ul><li>The key idea is to train two deep networks, with one network trained as the one-class model to separate normal instances from anomalies while the other network trained to enhance the normal instances and generate distorted outlier. The generator is based on a denoising AE.</li><li>The outliers are randomly sampled from some classes other than the classes where the normal instances come from.</li><li>But this method may be unavailable in many domains cause the reference outliers are beyond the given data.</li></ul></li><li><p><a href="https://git.io/JfYGb">OCAN</a>, <a href="https://git.io/Jf4pR">FenceGAN</a>: generate fringe data instances based on the given training data and use them as negative reference instances to enable the training of the one-class discriminator.</p><ul><li><p>OCAN: The generator is trained to generate data instances that are complementary to the training data.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605190140300.png" alt="image-20210605190140300" style="zoom:67%;" /></p><p>The 1st two terms are devised to generate low-density instances in the original feature space, and the last term is to help better generate data instances within the original data space.</p><p>The objective of the discriminatory is enhanced with an extrovert conditional entropy loss to enable the detection with high confidence.</p></li><li><p><a href="https://git.io/Jf4pR">FenceGAN</a>: generate data instances tightly lying at the boundary of the distribution of the training data, which is achieved by introducing two loss functions into the generator that enforce the generated instances to be evenly distributed along a sphere boundary of the training data.</p></li><li><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605191312110.png" alt="image-20210605191312110" /><figcaption aria-hidden="true">image-20210605191312110</figcaption></figure><p>The first term is called encirclement loss that enforces the generated instances to have the same discrimination score, ideally resulting in instances tightly enclosing the training data. The second term is called dispersion loss that enforces the generated instances to evenly cover the whole boundary.</p></li></ul></li><li><p><a href="https://git.io/Jf4p0">OCGAN</a>: uniformly distributed instances can be generated to enforce the normal instances to be distributed uniformly across the latent space.</p></li><li><p>An ensemble of generator is used with each generator synthesizing boundary instances for one specific cluster of normal instances.</p></li></ul></li><li><p>Advantages</p><ul><li>Its anomaly classification model is adversarially optimized in an end-to-end fashion.</li><li>It can be developed and supported by the affluent techniques and theories of adversarial learning and one-class classification.</li></ul></li><li><p>Disadvantages</p><ul><li>It is difficult to guarantee that the generated reference instances well resemble the unknown anomalies.</li><li>The instability of GANs may lead to generated instances with diverse quality and consequently unstable anomaly classification performance.</li><li>Its applications are limited to semi-supervised anomaly detection scenarios.</li></ul></li><li><p>Challenges</p><ul><li>CH1 &amp; CH2</li></ul></li></ul><h2 id="algorithms-and-datasets">Algorithms and datasets</h2><h3 id="representative-algorithms">Representative algorithms</h3><ul><li><p>Most methods operate in an unsupervised or semi-supervised mode.</p></li><li><p>Deep learning tricks like data augmentation, dropout and pre-training are under-explored.</p></li><li><p>The network architecture used is not that deep.</p></li><li><p>(leaky) ReLU is the most popular one</p></li><li><p>diverse backbone networks can be used to handle different types of input data.</p><figure><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605192143473.png" alt="image-20210605192143473" /><figcaption aria-hidden="true">image-20210605192143473</figcaption></figure></li></ul><h3 id="datasets-with-real-anomalies">Datasets with Real anomalies</h3><ul><li><a href="https://github.com/GuansongPang/anomaly-detection-datasets">The datasets</a></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605192324925.png" title="fig:" alt="image-20210605192324925" /></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Contributions<ul><li>Problem nature and challenges: some unique problem complexities underlying anomaly detection and the resulting largely unsolved challenges.</li><li>Categorization and formulation: three principled frameworks: deep learning for generic feature extraction, learning representations of normality, and end-to-end anomaly score learning</li><li>Comprehensive literature review</li><li>Future opportunities</li><li>Source codes and datasets</li></ul></li><li>Exploring anomaly-supervisory signals<ul><li>The key issue for these formulations is that their objective functions are generic.</li><li>Explore new sources of anomaly-supervisory signals that lie beyond the widely-used formulations such as data reconstruction and GANs, and have weak assumptions on the anomaly distribution.</li><li>Develop domain-driven anomaly detection by leveraging domain knowledge.</li></ul></li><li>Deep weakly-supervised anomaly detection<ul><li>Leveraging deep neural networks to learn anomaly-informed detection models with some weakly-supervised anomaly signals.<ul><li>Utilize a small number of accurate labeled anomaly examples to enhance detection models.</li></ul></li><li>Unknown anomaly detection: aim to build detection models that are generated from the limited labeled anomalies to unknown anomalies.</li><li>Data-efficient anomaly detection or few-shot anomaly detection: given only limited anomaly examples.</li></ul></li><li>Large scale normality leaning<ul><li>Since it is difficult to obtain sufficient labeled data</li><li>The goal is to first learn transferable pre-trained representation models from large-scale unlabeled data in an unsupervised/self-supervised mode, and then fine-tune detection models in a semi-supervised mode.</li><li>May need to be domain/application-specific.</li></ul></li><li>Deep detection of complex anomalies<ul><li>conditional/group anomalies</li><li>Multimodal anomaly detection</li></ul></li><li>Interpretable and actionable deep anomaly detection<ul><li>The abnormal feature selection methods but may render the explanation less useful</li></ul></li><li>Novel applications and settings<ul><li>Out-of-distribution detection: closely related area. It is generally assumed that fine-grained normal class labels are available during training.</li><li>Curiosity learning: learning a bonus reward function in reinforcement learning with sparse rewards. Augmenting the environment with a bonus reward in addition to the original sparse rewards from the environment.</li><li>non-IID anomaly detection: e.g., the abnormality of different instances/features is interdependent and/or heterogeneous. May be confused with anomaly instances.</li><li>detection of adversarial examples, anti-spoofing in biometric systems, and early detection of rare catastrophic events.</li></ul></li></ul><h1 id="project-ii-graph-level-anomaly-detection">Project II: <a href="http://snap.stanford.edu/class/cs224w-2019/project/26424135.pdf">Graph Level Anomaly Detection</a></h1><h2 id="why-1">Why?</h2><p>Anomaly detection at a graph level rather than node level or links level.</p><h2 id="goals">Goals</h2><p>modeling a comprehensive representation of a graph’s local and high level structural features, as well as a challenging problem because of the unique properties of graph based data, such as long dependencies and size variability.</p><h2 id="previous">Previous</h2><ul><li>You et al. propose an autoregressive approach to graph generation that is trained sequentially on existing graphs and then generates them at inference time by breaking the process into a sequence of node and edge formations</li><li>You et al.’s work in an RL approach to goal-directed molecular graph generation. Using partially observed subgraphs (discussed later) and an action-based generation framework v</li></ul><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li><p>Formulate two unsupervised learning objectives for graph level anomaly detection. Namely, we compare 1) generative modeling for graph likelihood estimation and 2) a novel method based on masked graph representation learning.</p></li><li><p>look to learn meaningful representations over a family of graphs by modeling <strong>one step edge completion problems.</strong></p></li></ul><h3 id="implementation">Implementation</h3><h2 id="experiments">Experiments</h2><h2 id="conclusion-1">Conclusion</h2><h1 id="paper-iii-multi-level-anomaly-detection-on-time-varying-graph-data">Paper III: <a href="https://www.osti.gov/servlets/purl/1214009">Multi-Level Anomaly Detection on Time-Varying Graph Data</a></h1><p><a href="https://www.osti.gov/servlets/purl/1214009">Here</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper I: &lt;a href=&quot;https://arxiv.org/pdf/2007.02500.pdf&quot;&gt;Deep Learning for Anomaly Detection: A Review&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Project II: &lt;a href=&quot;http://snap.stanford.edu/class/cs224w-2019/project/26424135.pdf&quot;&gt;Graph Level Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paper III: &lt;a href=&quot;https://www.osti.gov/servlets/purl/1214009&quot;&gt;Multi-Level Anomaly Detection on Time-Varying Graph Data&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
      <category term="anomaly" scheme="http://yoursite.com/tags/anomaly/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Fall Surveys</title>
    <link href="http://yoursite.com/posts/notes/2021-02-07-notes-paper-fall-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-02-07-notes-paper-fall-survey.html</id>
    <published>2021-02-07T13:46:39.000Z</published>
    <updated>2021-03-05T03:34:36.201Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Fall detection<ul><li>Paper I: <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full">Elderly Fall Detection Systems: A Literature Survey, 2020</a></li><li>Paper II: <a href="https://dl.acm.org/doi/10.1145/2769493.2769540">A survey on vision-based fall detection, 2015</a></li><li>Paper III: <a href="https://www.sciencedirect.com/science/article/pii/S2352864815000681">3D depth image analysis for indoor fall detection of elderly people, 2015</a></li><li>Paper IV: <a href="https://ieeexplore.ieee.org/document/9186685">Deep learning based systems developed for fall detection: a review, 2020</a></li><li>Paper V: <a href="https://ieeexplore.ieee.org/abstract/document/8869737">Implementation of Fall Detection System Based on 3D Skeleton for Deep Learning Technique, 2019</a></li><li>Paper VI: <a href="https://ieeexplore.ieee.org/document/8369778">Human fall-down event detection based on 2D skeletons and deep learning approach</a></li><li></li></ul></li><li>Human activity recognition<ul><li>Paper VII: <a href="https://link.springer.com/content/pdf/10.1007/s11042-020-09004-3.pdf">Vision-based human activity recognition: a survey, 2020</a></li><li>3D and depth data<ul><li>Paper VIII: <a href="https://www.sciencedirect.com/science/article/pii/S0167865514001299">Human activity recognition from 3d data: a review, 2014</a></li><li><a href="https://arxiv.org/abs/1711.08362">RGB-D-based Human Motion Recognition with Deep Learning: A Survey, 2017</a></li></ul></li><li>3D skeleton-based human representations<ul><li><a href="https://arxiv.org/abs/1601.01006">Space-Time Representation of People Based on 3D Skeletal Data: A Review, 2016</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S0031320315004392">3D skeleton-based human action classification: A survey,2015</a></li><li><a href="https://d1wqtxts1xzle7.cloudfront.net/47833181/Crowd_analysis_A_survey20160806-21965-155miur.pdf?1470471444=&amp;response-content-disposition=inline%3B+filename%3DCrowd_analysis_a_survey.pdf&amp;Expires=1614620280&amp;Signature=gO5XOCbzmA4O~6zc1hli7UqnkZmethCye13xIqVW58A~NTeZYwbbxSs3vZsO4E9~73WX7gYBapzo3quA7UV5jFDRfaDQ6v0ds8dA3BDhB5ys2PlxRFWxEmPsfGAPSp7G6inWLRrfw89L2xXRnX-KM1caNEnqcsg18OD9zf8LU3aovB4hXyB0kvMtc2T2FXdg1HdlQbjqVAlZmrcSl2Y98j1Gr4it23BLSbmUmwZpYAtVA4WqUwFihyqQco5XHX3dhJn7eUdKTOc6QdqQ2KumIhXBwnHSR8TOF9StECcxoUlOf9fcrEgRH4tDauMCsVqgCWJkANhI4~lp0nJEPP21fQ__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">Crowd analysis: a survey, 2008</a></li><li><a href="https://ieeexplore.ieee.org/document/6909476">Human action recognition by representing 3d skeletons points in a lie group, CVPR 2014</a></li></ul></li><li>knowledge-based HAR activity recognition<ul><li><a href="https://www.sciencedirect.com/science/article/pii/S0957417416302913">A survey on using domain and contextual knowledge for human activity recognition in video streams.,2016</a></li></ul></li><li>Abnormal HAR<ul><li><a href="https://www.sciencedirect.com/science/article/pii/S0952197618301775">A review of state-of-the-art techniques for abnormal human activity recognition, 2018</a></li><li></li></ul></li></ul></li></ul><a id="more"></a><h1 id="fall-detection">Fall detection</h1><h2 id="paper-i-2020">Paper I: 2020</h2><p><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full">Elderly Fall Detection Systems: A Literature Survey, 2020</a></p><h3 id="why">Why?</h3><h4 id="types-of-falls"><strong>Types of falls</strong></h4><ul><li>Types<ul><li>forward, lateral and backward in <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B35">El-Bendary et al.</a></li><li>forward, backward, left-side, right-side, blinded-forward and blinded-backward in <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B95">Putra et al.</a></li><li>fall lateral left lie on the floor, fall lateral left and sit up from floor, fall lateral right and lie on the floor, fall lateral and left sit up from the floor, fall forward and lie on the floor, and fall backward and lie on the floor in <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B23">Chen et al</a></li></ul></li><li>Elderly people may suffer from longer duration of falls, because of motion with low speed in the activity of daily living</li><li>The characteristics of different types of falls are not taken into consideration in most of the work on fall detection surveyed. (like age, gender etc.)</li></ul><h4 id="previous-work-and-falls">Previous work and falls</h4><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>Paper</th><th>contents</th><th>summary</th></tr></thead><tbody><tr class="odd"><td><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B18">Chaudhuri et al. (2014)</a></td><td>fall detection devices for people of different ages (excluding children) from several perspectives, including background, objectives, data sources, eligibility criteria, and intervention methods.</td><td>most of the studies were based on synthetic data</td></tr><tr class="even"><td><strong><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B131">Zhang et al. (2015)</a></strong></td><td>vision-based fall detection systems and their related benchmark data sets. Methods are divided into four categories, namely <strong>individual single RGB cameras, infrared cameras, depth cameras, and 3D-based methods using camera arrays</strong>. Methods are also divided to rely on the activity/inactivity of the subjects, shape (width-to-height ratio), and motion.</td><td>Non-vision sensors are not included</td></tr><tr class="odd"><td><strong><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B16">Cai et al. (2017)</a></strong></td><td><strong>depth cameras</strong>, reviewed the benchmark data sets acquired by Microsoft Kinect and similar cameras.</td><td>helpful for looking for benchmark data sets</td></tr><tr class="even"><td><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B21">Chen et al. (2017a)</a></td><td>vision- and non-vision-based systems</td><td>fusion of depth cameras and inertial sensor resulted in a system that is more robust</td></tr><tr class="odd"><td><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B50">Igual et al. (2013)</a></td><td>low-cost cameras and accelerometers embedded in smartphones may offer the most sensible technological choice for the investigation of fall detection. They also reported three main challenges: (i) real-world deployment performance, (ii) usability, and (iii) acceptance.</td><td>another option of sensors</td></tr></tbody></table><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210207202446658.png" alt="image-20210207202446658" style="zoom:40%;" /></p><h3 id="goals">Goals</h3><p>provide a literature survey of work conducted on elderly fall detection using sensor networks and IoT in terms of data acquisition, data analysis, data transport and storage, sensor networks and Internet of Things (IoT) platforms, as well as security and privacy.</p><h3 id="how">How?</h3><h4 id="the-components-of-system">The components of system</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210207204832364.png" alt="image-20210207204832364" style="zoom:60%;" /></p><h4 id="sensors">Sensors</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210207230722406.png" alt="image-20210207230722406" style="zoom:50%;" /></p><ul><li>Individual wearable sensors<ul><li>possible choices: accelerometers, gyroscopes, glucometers, pressure sensors, ECG (Electrocardiography), EEG (Electroencephalography), or EOG (Electromyography)</li><li><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B14">Bourke et al. (2007)</a> found that accelerometers are regarded as the most popular sensors for fall detection. Smart phones are more practical compared with wearable sensors.</li></ul></li><li>Individual vision sensors<ul><li>Possible choices: infrared, RGB, RGB-D etc.</li><li>main challenge of vision-based detection is the potential violation of privacy</li><li>Almost use synthetic falling dataset. <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B13">Boulard et al. (2014)</a> has actual fall data and the other by <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B108">Stone and Skubic (2015)</a> has mixed data.</li></ul></li><li>Individual ambient sensors<ul><li>Possible choices: active infrared RFID,pressure, smart tiles, magnetic switches, doppler radar, ultrasonic and microphone.</li></ul></li><li>Subjects<ul><li>simulated data from OpenSim contributed to an increase in performance to the resulting models</li><li>transfer learning which adapt to subjects who were not represented in the training data</li><li>reinforcement learning for different subjects</li></ul></li></ul><h4 id="sensor-fusion">Sensor fusion</h4><ul><li><p>Types</p><p><img src="https://www.frontiersin.org/files/Articles/520978/frobt-07-00071-HTML/image_m/frobt-07-00071-g007.jpg" style="zoom:50%;" /></p></li><li><p>feature fusion is the most popular approach, followed by decision fusion.</p></li></ul><h3 id="trends-challenges">Trends &amp; challenges</h3><h4 id="trends">Trends</h4><ul><li>Sensor fusion</li><li>ML,DL and RL</li><li>With 5G wireless networks</li><li>Data augmentation<ul><li>Personalized data: the historical medical and behavioral data of Individuals (<a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B35">El-Bendary et al. (2013)</a> and <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B84">Namba and Yamada (2018b)</a>)</li><li>Use skeletal models for simulation: <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B79">Mastorakis et al. (2007</a>, <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B78">2018)</a>, applied the skeletal model simulated in Opensim,</li></ul></li><li>Fog computing : Intel RealSense includes a 28 nanometer processor</li></ul><h4 id="challenges">Challenges</h4><ul><li>The rarity of data of real falls : only <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B70">Liu et al. (2014)</a> used a data set with nine real falls along with 445 simulated ones.</li><li>Detection in real-time</li><li>Security and privacy</li><li>Platform of sensor fusion</li><li>Limitation of locations: indoor and outdoor environments</li><li>Scalability and flexibility</li></ul><h2 id="paper-ii-vision">Paper II: vision</h2><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1077.2575&amp;rep=rep1&amp;type=pdf">A survey on vision-based fall detection, 2015</a></p><h3 id="goal">Goal</h3><ol type="1"><li><p>focus on recent vision-based fall detection techniques, including depth-based methods;</p></li><li><p>discuss the publicly available fall datasets.</p></li></ol><h3 id="fall-datasets">Fall datasets</h3><p>[1] Definition and performance evaluation of a robust svm based fall detection solution.</p><p>[2] Multiple cameras fall dataset</p><p>The EDF dataset mentioned in this paper, I cannot find the original paper except for <a href="http://vlm1.uta.edu/~athitsos/publications/zhang_isvc2014.pdf">this one</a> , in which EDF and OCCU are both created. But if it's the paper I list, the description of EDF in this paper is not totally correct.</p><table><colgroup><col style="width: 11%" /><col style="width: 15%" /><col style="width: 10%" /><col style="width: 9%" /><col style="width: 22%" /><col style="width: 30%" /></colgroup><thead><tr class="header"><th>-</th><th>SDUFall</th><th>EDF?</th><th>OCCU</th><th>[1]</th><th>[2]</th></tr></thead><tbody><tr class="odd"><td>camera type</td><td>1 kinetic</td><td>2 kinetics</td><td>2 kinetics</td><td>1 RGB camera</td><td>8 calibrated RGB camera</td></tr><tr class="even"><td>camera viewpoints</td><td>1</td><td>2</td><td>2</td><td>NaN</td><td>8</td></tr><tr class="odd"><td>fall type</td><td>falls with different directions</td><td>eight fall directions</td><td><strong>occluded falls</strong></td><td>falls with different directions</td><td>forward, backward falls, falls from sitting down and loss of balance</td></tr><tr class="even"><td>#falls</td><td>200</td><td>320</td><td>60</td><td>192</td><td>200</td></tr><tr class="odd"><td>actions in daily life?</td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td></tr><tr class="even"><td>#scenarios</td><td>1</td><td>1</td><td>1</td><td>4 (home, coffee room, office, lecture room)</td><td>24</td></tr><tr class="odd"><td>#subjects</td><td>20</td><td>10</td><td>5</td><td></td><td>1</td></tr><tr class="even"><td>resolution</td><td><span class="math inline">\(320\times240\)</span></td><td><span class="math inline">\(320\times240\)</span></td><td></td><td><span class="math inline">\(320\times240\)</span></td><td></td></tr><tr class="odd"><td>frame rate</td><td>30 fps</td><td>25 fps</td><td>30 fps</td><td>25 fps</td><td></td></tr><tr class="even"><td>year</td><td>2014</td><td>2008</td><td>2014</td><td>2012</td><td>2010</td></tr></tbody></table><h3 id="vision-based-fall-detectors">Vision-based fall detectors</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Comment: <span class="keyword">the</span> classification rules <span class="keyword">for</span> <span class="keyword">each</span> section are <span class="keyword">not</span> <span class="keyword">the</span> same.</span><br></pre></td></tr></table></figure><h4 id="using-single-rgb-camera">Using single RGB camera</h4><p>Firstly classify by features, then the tasks of it is mentioned</p><ul><li>Shape-related features : based on width to height aspect ratio of the person.<ul><li>background separation to get silhouette and then use SVM for classification:</li><li>shape deformation from silhouettes and use shape deformation based GMM for classification</li><li>ellipse shape for body modeling and GMM for extracting moving object.</li><li>2 HMMs for classify falls and normal activities</li><li>extracting projection histograms of the segmented body silhouette and then use it as feature vector, complete posture classification by KNN.</li></ul></li><li>Motion pattern<ul><li>human motion analysis (analyzing the energy of the motion active area ) + human silhouette shape variations to detect slip-only and fall events</li><li>applying Integrated Time Motion Image (ITMI) to fall detection. ITMI is the calculated the PCs of typical video clip for representing a motion pattern.</li><li>threshold-based by the last few frames (falling, the magnitude of the fall, the maximum velocity of the fall ) etc.</li><li>extract the 3D head trajectory using a single calibrated camera.</li><li>extract foreground human silhouette via background modeling. Ellipse fitting for human body and analyze silhouette motion by an integrated normalized motion energy image.</li></ul></li><li>Inactivity detection<ul><li>use ceiling-mounted, wide-angle cameras with vertically oriented optical axes to reduce the influences of occlusion.</li><li>use learned models of spatial context in conjunction with a tracker</li><li>STHF descriptor by <a href="https://ieeexplore.ieee.org/document/6916794">Charfi et al</a>.</li></ul></li></ul><h4 id="using-multiple-cameras">Using multiple cameras</h4><p>Classified by tasks</p><ul><li>3D reconstruction<ul><li><strong>volume distribution</strong> along the vertical axis, check <a href="https://scholar.google.ca/scholar?q=Fall+detection+using+body+volume+reconstruction+and+vertical+repartition+analysis&amp;hl=zh-CN&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart">here</a>.</li><li>multiple cameras + a hierarchy of fuzzy logic to detect falls. Use voxel person (linguistic summarization of temporal fuzzy inference curves)</li></ul></li><li>Multiple viewpoint<ul><li>Use LHMM (layered hidden Markov model)</li><li>combining decisions from different camera</li><li>Using the measures of humans heights and occupied areas.</li></ul></li></ul><h4 id="using-depth-cameras">Using depth cameras</h4><ul><li>Using the distance from the top of the person to the floor<ul><li>the distance of human centroid to floor is smaller than threshold and the person does not move for a certain seconds</li><li>Other features like head to floor distance, person area and shape's major length to width</li></ul></li><li>analyzing how a human has moved during the last frames<ul><li>characterize the vertical state of a segmented 3D object for each frame. Then compute a confidence by the features extracted on ground event which are fed into an ensemble of decision trees.</li><li>Bayes network on fall detection, including duration, total head drop, maximum speed, smallest head height and fraction of frames for which the head drops</li><li>calculating the velocity based on the contraction or expansion of the width, height and depth of the 3D bounding box.</li></ul></li><li>human body key joints<ul><li>3D body joints at each frame are extracted by randomized decision tree, and the 3D trajectory of the head joint is used to determine whether the fall action has occurred.</li><li>3D depth for main detection (structure similarity and vertical height of the person), RGB for out-of-the-rage of the depth camera (the width-height ratio of the detected human bounding box is for recognizing different activities).</li><li>an action is represented by a bag of curvature scale space features (BoCSS) of human silhouettes. Or represent an action as Fisher Vector (on CSS). Or check whether the width-height ratio of temporal bounding box is greater than the predefined threshold, if it is, then it's fall. Otherwise check 2D velocity and the 3D centroid information</li></ul></li></ul><h3 id="conclusions">Conclusions</h3><ul><li><p>Methods with cameras</p><table><colgroup><col style="width: 20%" /><col style="width: 38%" /><col style="width: 40%" /></colgroup><thead><tr class="header"><th>method</th><th>merits</th><th>demerits</th></tr></thead><tbody><tr class="odd"><td>single RGB camera</td><td>* no requirement of camera calibration<br />* inexpensive</td><td>* case specific and viewpoint-dependent<br />* occlusion problem</td></tr><tr class="even"><td>calibrated multi-camera systems</td><td>* viewpoint independent <br />* No occlusion</td><td>* careful and time-consuming calibration<br />* repeatedly calibration if any of them moved</td></tr><tr class="odd"><td>depth camera</td><td>* viewpoint independent <br />* No occlusion</td><td>* price</td></tr></tbody></table></li><li><p>Tips for benchmark</p><ul><li>both falls and activities of daily life (ADL) are requried</li><li>include various falls</li><li>include different camera viewpoints: to verify whether the proposed algorithms are viewpoint-independent</li><li>real falls: consider the distribution of volunteers like gender, ages etc.</li></ul></li><li><p>Tips for fall detection</p><ul><li>combine with Other type of data like sound,</li></ul></li></ul><h2 id="paper-iii-3d-depth">Paper III: 3D depth</h2><p><a href="https://www.sciencedirect.com/science/article/pii/S2352864815000681">3D depth image analysis for indoor fall detection of elderly people, 2015</a></p><h3 id="previous">Previous</h3><ul><li><p>Existing fall detection methods</p><ul><li><p>wearable sensor based</p><p>small , cheap, wearable, but high drift</p></li><li><p>ambient sensor-based</p><p>low detection precision of these sensors, and the precision is distance-relied and thus more sensors are required if with a bigger room</p></li><li><p>computer-vision based</p><p>more robust and less intrusive.</p><ul><li>methods with a single RGB camera</li><li>3d-based methods Using multiple cameras</li><li>depth camera</li></ul></li></ul></li><li><p>Usually <strong>shape relative features of human motion analysis and inactivity detection are used as clues for detecting falls.</strong></p></li><li><p>moment functions are powerful while describing the human shape</p><ul><li>On grey or color images an ellipse is more accurate than a bounding box</li><li>depth camera can offer 3D data so as to work on shape analysis</li></ul></li><li><p>Threshold based fall detection</p><ul><li>Features that can be used: head-ground distance gap and head-shoulder distance gap, the orientation of the ellipse of the human object and the motion of the human object</li><li>methods: DT, NNs, SVM, Bayesian Belief Network</li></ul></li></ul><h3 id="goal-1">Goal</h3><p>deals with the fall detection of the single elderly person in the home environments</p><h3 id="how-1">How?</h3><h4 id="idea">Idea</h4><ul><li>extracting silhouette of moving Individual by extracting background frame</li><li>with the horizontal and vertical projection histogram statistics the depth images are converted to disparity map</li><li>coefficients of the human body are calculated to determine the direction of individual</li><li>threshold based: centroids of the human body to the floor plane and the angle between the human body and the floor plane</li></ul><h4 id="method">Method</h4><p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S2352864815000681-gr1.jpg" style="zoom:67%;" /></p><ul><li><p>Data preprocessing</p><ul><li><p>depth data: only adopt kinetic data in trusted range (1.2-3.8m)</p></li><li><p>extracting silhouette of the moving individual: subtracting the median-filtered background depth frame</p></li><li><p>floor plane: disparity map. <strong>Floor plane will be a noticeable slant and thick straight line in the disparity map</strong></p><p>$= $, for kinetic, <span class="math inline">\(f=580\)</span> pixels, <span class="math inline">\(b=7.5\)</span> cm, and <span class="math inline">\(d\)</span> is the distance between one point in the space TPM the center of the kinetic.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210215180234037.png" alt="image-20210215180234037" style="zoom:80%;" /></p><p>The floor equation then is <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210215202222673.png" alt="image-20210215202222673" style="zoom:70%;" /></p></li><li><p>The orientation of human body: after estimating the ellipse of human body on the image plane, then estimate orientation</p></li><li><p>calculate the distance from the centroid of the human body to the floor plane and the angles between the body and the floor plane.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210215215635773.png" alt="image-20210215215635773" style="zoom:70%;" /></p></li></ul></li></ul><h3 id="experiments">Experiments</h3><ul><li>The dataset is based on their self-captured dataset.-- <strong>Cannot find the sharing link of dataset</strong></li><li>Only two clips' trajectories are listed, no accuracy data shown</li></ul><h3 id="conclusions-1">Conclusions</h3><ul><li>the method may be taken as a candidate</li><li>the result of experiments is not convince.</li></ul><h2 id="paper-iv-dl">Paper IV: DL</h2><p><a href="https://ieeexplore.ieee.org/document/9186685">Deep learning based systems developed for fall detection: a review, 2020</a></p><h3 id="previous-1">Previous</h3><ul><li>the reviewed fall detection systems have some general steps, combined with sensing, data processing, fall event recognition and emergency alert system to rescue the victim.</li><li></li></ul><h3 id="goal-2">Goal</h3><p>presenting a summary and comparison of existing state-of-the-art deep learning based fall detection systems to facilitate future development in this field.</p><p>The categorization focuses on how the different principal methods (CNN, LSTM, and AE) handle the event data captured by sensors.</p><h3 id="how-2">How?</h3><p>Methods used for fall detection</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210219211345800.png" alt="image-20210219211345800" style="zoom:67%;" /></p><h4 id="cnn-based-fall-detection-systems">CNN based fall detection systems</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210221105345137.png" alt="image-20210221105345137" style="zoom:67%;" /></p><h5 id="cnn">CNN</h5><ul><li><a href="http://eprints.bournemouth.ac.uk/29421/1/activity-recognition-indoor.pdf">Adhikari et al</a> : based on videos images from RGB-D camera .<ul><li>used their own dataset</li><li>poor sensitivity when the user was in crawling, bending and sitting positions. The system also works in a selected environment. Developed on a single-person scenario</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8302004">Li et al</a>: extract human shape deformation by CNNs<ul><li>used dataset URFD</li><li>not tested on real dataset</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8732857">Yhdego et al.</a>:<ul><li>convert the data from accelerometer to image by continuous wavelet transform</li><li>use transfer learning for these images</li><li>use URFD dataset</li></ul></li><li><a href="https://dl.acm.org/doi/pdf/10.1145/3136755.3136802">Yu et al</a><ul><li>extract human body silhouette by background subtraction. CNN is for preprocessing the silhouette</li><li>codebook background subtraction</li></ul></li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-93659-8_53">Shen et al.</a><ul><li>cloud</li><li>deep-cut NNs to detect the key points of human body, feed key points into NNs</li></ul></li><li><a href="https://pubmed.ncbi.nlm.nih.gov/30959877/">Santos et al</a><ul><li>IoT, fog computing</li><li>used three open datasets</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8551564">Zhou et al</a> : multi-sensor fusion<ul><li>STFT is for extracting the time frequency (TF) micro-motion features</li><li>Two Alexnet and one SSD (single shot multi-box detector) net are for classifying the TF features</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8662651">He et al.</a>: FD-CNN net<ul><li>collected data is mapped into 3-channel RGB bitmap image</li><li>the image plus SisFall and MobiFall datasets for training FD-CNN</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8664624">Sadreazami et al</a> : CNN for time-series data (radar)</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-030-20257-6_22">Sortis et al</a> : raw accelerometer data, CUSUM (cumulative sum) algorithm</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8295206">Lu et al</a>: use optical flow images, pretrained 3D CNN on different dataset, transfer learning. Accurate for single-person detection but may not work well on multi-person detection</li><li><a href="https://ieeexplore.ieee.org/document/8813332">Wang et al</a>: tri-axial accelerometer, gyroscope sensor in the smart insole. CNN for improving the accuracy level (it's used directly on the raw sensor data)</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8796672">Zhang and Zhu</a>: CNN that works on raw 3-axis accelerometer data streams.</li><li><a href="https://ieeexplore.ieee.org/document/8787213">Camerio et al</a>: multi-stream model, takes high-level handcrafted feature generators. CNN for optical flow， RGB and human estimated pose. Use URFD and FDD datasets for training</li><li><a href="https://pubmed.ncbi.nlm.nih.gov/32155936/">Casilari et al</a> :CNN for recognizing the pattern from the tri-axial transportable accelerometer. Using multiple dataset for training. For real-life performance, LSTM is used</li><li><a href="https://www.sciencedirect.com/science/article/pii/S0010482519303816">Espinosa et al</a> : Using UP-fall detection multi-modal dataset. <strong>Only vision-based</strong> .</li></ul><h5 id="d-cnn">1D CNN</h5><ul><li><a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/el.2018.6117">Cho and Yoon</a> : SVD on accelerometer data for 1Data CNN. Works for triaxial acceleration data.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8869737">Tsai and Hsu et al.</a>: feature extraction algorithm for converting the depth image to skeleton information . Only seven highlighted feature points are picked from the skeleton joints. Performs well on NTU RGB+D</li></ul><h5 id="d-cnn-1">3D CNN</h5><ul><li><a href="https://ieeexplore.ieee.org/document/8622342">Ranhnemoonfar et al.</a>: Kinect depth camera, Adam optimizer. SDUFall dataset for training.</li><li><a href="https://ieeexplore.ieee.org/document/8779504">Li et al.</a>: pre-impact fall detection. Pretrained model + new samples for fine tuning.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/7946918">Hwang et al.</a>: 3D-CNN for continuous motion data from depth cameras. Using data augmentation. Using TST fall detection dataset</li><li><a href="https://www.researchgate.net/publication/333852390_Human_Fall_Recognition_using_the_Spatiotemporal_3D_CNN">Kasturi et al.</a>: kinetic camera. the data fed into 3DCNN is a staked cube. Tested on UR fall detection dataset</li></ul><h5 id="fof-cnn">FOF CNN</h5><p>FOF: feedback optical flow convolution</p><ul><li><a href="https://ieeexplore.ieee.org/document/8101471">Hsieh and Jeng</a> : IoT, used Feature Feedback Mechanism Scheme (FFMS) and 3D-CNN. KTH dataset.</li></ul><h4 id="lstm-based-fall-detection-systems">LSTM based fall detection systems</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210221123633848.png" alt="image-20210221123631712" style="zoom:67%;" /></p><h5 id="lstm-with-3d-cnn">LSTM with 3D CNN</h5><ul><li><a href="https://ieeexplore.ieee.org/abstract/document/8256202">Lu et al.</a>: trained an extractor only on kinetic data, LSTM+ spatial visual attention. Use Sports-1M dataset, and fall dataset of Multiple Cameras.</li></ul><h5 id="lstm-with-rnn-and-cnn">LSTM with RNN and CNN</h5><ul><li><a href="https://ieeexplore.ieee.org/document/8369778">Lie et al.</a>: CNN for extracting 2D skeletons from RGB camera, LSTM for classifying actions. Has an online version</li><li><a href="https://ieeexplore.ieee.org/document/8615759">Abobakr et al.</a>: Kinect RGB-D camera. Convolutional LSTM + ResNet for visual feature extraction, LSTM for sequence modeling and logistics regression for fall detection. Use URFD public dataset, can work in real-time</li><li><a href="https://iopscience.iop.org/article/10.1088/1742-6596/1267/1/012044/meta">Xu et al.</a>: IoT, acceleration data from a tri-axial accelerometer as input . Outperfoms SVM+CNN model,</li><li><a href="https://journals.sagepub.com/doi/full/10.1177/1550147717703257">Tao and Yun</a>: body posture and human biomechanics equilibrium, fed depth camera data to RNN+LSTM and extracting 3D skeletons. Then by computing center of mass (COM) positions and the region of base support, falls are detected.</li><li><a href="https://www.hindawi.com/journals/wcmc/2019/9507938/">Ajerla <em>et al.</em></a> : used edge devices like a laptop for computing, MetaMotionR sensor (tri-axial accelerometer), subset of the MobiAct public dataset for training,</li><li><a href="https://www.researchgate.net/publication/332780940_Edge-AI_in_LoRa-based_Health_Monitoring_Fall_Detection_System_with_Fog_Computing_and_LSTM_Recurrent_Neural_Networks">Queralta <em>et al.</em></a>: edge computing and fog computing. LSTM+RNN</li><li><a href="https://core.ac.uk/download/pdf/286564597.pdf">Luna-Perejon <em>et al.</em></a>: LSTM+GRU, accelerometer data as input, SisFall dataset for training.</li><li><a href="https://www.researchgate.net/publication/328310209_Embedded_Real-Time_Fall_Detection_with_Deep_Learning_on_Wearable_Devices">Torti et al.</a>: Micro-Controller Unit (MCU) with Tri-axial accelerometers, LSTM, tensorflow, SisFall dataset</li><li><a href="https://link.springer.com/chapter/10.1007/978-981-10-7419-6_25">Theodoridis <em>et al.</em></a> : acceleration measurements, RNN, URFD dataset + random 3D rotation augmentation for training. Comparing 4 type of model: LSTM-Acc, LSTM-Acc Rot, Acc + SVM-Depth, UFT</li><li><a href="https://www.researchgate.net/publication/326622687_Convolutional_recurrent_neural_networks_for_posture_analysis_in_fall_detection">Hsiu-Yu <em>et al.</em></a> : kinetics, posture Types, LSTM compared with CNN, fusion images as input from RGB images after extracting body shape by GMM and optical flow.</li></ul><h5 id="lstm-with-rcn-and-rnn">LSTM with RCN and RNN</h5><ul><li><a href="https://www.semanticscholar.org/paper/Co-Saliency-Enhanced-Deep-Recurrent-Convolutional-Ge-Gu/37b625459bd85caba627a7516741c50c05117344">Ge <em>et al.</em></a>: kinect, co-saliency-enhanced RCN, input video clips, RCN + (RNN+LSTM) to label the output.</li></ul><h4 id="ae-based-fall-detection-systems">AE based fall detection systems</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210222085226504.png" alt="image-20210222085226504" style="zoom:80%;" /></p><ul><li><a href="https://ieeexplore.ieee.org/document/7485147">Jokanovic <em>et al.</em></a>: time-frequency (TF) analysis. two stacked AE and a softmax layer.</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-95095-2_18">Droghini et al.</a>: acoustic fall detection, DT (downstream threshold) classifier.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8803671">Zhou and Komuro et al.</a>: VAE+3D-CNN residual block, reconstruction error for detecting fall actions. Dataset High-quality simulated fall dataset (HQFD) and Le2i dataset.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8283539">Seyfioglu et al.</a>: 3 layer-CAE. unsupervised pretraining for the convolutional layers, radar</li></ul><h3 id="conclusion">Conclusion</h3><ul><li>Sensors:<ul><li>RGB camera: not privacy and thus cannot be used in bathroom which with highly risk. Hacker</li><li>Depth camera: only depth, but can be used in like bathroom</li><li>Accelerator: cheap and the data captured are easy to be used,</li></ul></li><li>Vision based methods really depend on the background</li></ul><h2 id="paper-v-3d-skeletons">Paper V: 3D skeletons</h2><p><a href="https://ieeexplore.ieee.org/abstract/document/8869737">Implementation of Fall Detection System Based on 3D Skeleton for Deep Learning Technique</a></p><h3 id="previous-2">Previous</h3><ul><li>Kinect: depth sensor + RGB camera + microphone array</li><li>silhouette normalization : sensitive to the background</li><li>high computation price</li></ul><h3 id="goal-3">Goal</h3><p>real-time fall detection</p><h3 id="idea-1">Idea</h3><ul><li><p>Methods: seven highlight feature points+ pruned CNNs</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210226225948892.png" alt="image-20210226225948892" style="zoom:30%;" /></p><ul><li>Foreground segmentation: GMM, depth information</li><li>Labeling: use the area size to determine whether the object is human or not.</li><li>Thinning: Use Zhang Suen's rapid thinning method. Dilation + erosion</li><li>Searching: just Using the joints on arms and head, they <strong>argue that when a falling event will occur, the joints above the waist will have enormous change.</strong></li></ul></li><li><p>Input: 7 3D joints in 30 frames, (30，21).Use conv1d (so as to prune the number of parameters)</p></li></ul><h3 id="experiments-1">Experiments</h3><ul><li><p>Dataset: NTU RGB+D</p></li><li><p>Performance</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210226232235263.png" alt="image-20210226232235263" style="zoom:40%;" /></p></li><li><p>system: 15 frames per second in real-time</p></li></ul><h3 id="conclusion-1">Conclusion</h3><ul><li>depth image for skeleton extraction, conv1d for parameter pruning</li><li>15 frames per second for real-time implementation.</li></ul><h2 id="paper-vi-3d-skeletons">Paper VI: 3D skeletons</h2><p><a href="https://ieeexplore.ieee.org/document/8369778">Human fall-down event detection based on 2D skeletons and deep learning approach</a></p><h3 id="previous-3">Previous</h3><ul><li>Most of the existing skeleton-based action recognition approaches model actions based on well-designed hand-crafted local features.<ul><li>depth value+HMM etc.</li><li>single RGB image + NNs (CNNs, RNNs)</li></ul></li></ul><h3 id="idea-2">Idea</h3><ul><li>CNN for extracting skeletons and LSTM for final detection</li><li>Skelton extractor:<ul><li>input: 2D RGB image (1920*1080)</li><li>DeeperCut skeleton extraction: resnet backbone, output 14 joints (“forehead”, “chin”, and left and right “shoulder”, “elbow”, “wrist”, “hip”, “knee”, “ankle”, the mean of hip, the mean of chin and central-hip. The last two joints are for robustness with respect to background, so as to build a translation-invariant skeleton model).</li></ul></li><li>LSTM:<ul><li>input: 8 frames, try to classify 5 actions. Supervised training. (8*28), where (28=2*14)</li></ul></li></ul><h3 id="experiments-2">Experiments</h3><ul><li>800 training, 255 validation and 250 test. Manually remove the wrong skeletons (incomplete skeletons) by like a threshold (sum of the distances between each joint and the centroid is calculated)</li><li>trigger rule: during the last 30 outputs,<ul><li>current-time output is “lying” and the previous 19 outputs contain more than 14 “danger” statuses, an alarm will be triggered.</li><li>current output belongs to “safe” status and the previous 8 outputs contain over 5 “safe” statuses, then the alarm signal will be reset.</li></ul></li><li><strong>8 frames per second</strong></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210228193517428.png" alt="image-20210228193517428" style="zoom:40%;" /></li></ul><h1 id="human-activity-recognition">Human Activity Recognition</h1><h2 id="paper-vii">Paper VII</h2><p>Paper VII: <a href="https://link.springer.com/content/pdf/10.1007/s11042-020-09004-3.pdf">Vision-based human activity recognition: a survey</a></p><h3 id="goal-4">Goal</h3><p>review and summarize the progress of HAR systems from the computer vision perspective.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210228233502592.png" alt="image-20210228233502592" style="zoom:50%;" /></p><h3 id="har">HAR</h3><ul><li>Related: determining and naming activities using sensory observations<br /></li><li>Goal: labeling the same activity with the same label even when performed by different persons under different conditions or styles</li><li>Methods: usually by an activity detection task, which includes the temporal identification and localization. Formally, the activity recognition task is divided into classification and detection</li><li>HAR systems are influenced by two technologies: contact-based and remote methods.<ul><li>contact-based: require the physical interaction of the user with the command acquisition machine or device. Sensors like accelerometers, multi-touch screens etc. But it is not that popular now, cause the complicated sensors and price to make it easily to be accepted and wore</li><li>remote based: vision, societal trust, no requirement of ordinary users and thus non-intrusive</li></ul></li><li>Contributions:<ul><li>New update on this rarely focused theme</li><li>thoroughly analysis</li><li>multiple classification methods so as to analyze: detection, tracking and classification stages; feature extraction progress; input data modalities; supervision level; evaluation methods;</li></ul></li></ul><h3 id="benchmarks">Benchmarks</h3><ul><li>CONVERSE: complex conversational interactions</li><li>ALSAN:</li></ul><h3 id="previous-4">Previous</h3><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210301221944071.png" alt="image-20210301221944071" /><figcaption aria-hidden="true">image-20210301221944071</figcaption></figure><h3 id="har-approaches">HAR approaches</h3><h4 id="har-approaches-according-to-feature-extraction-process">HAR approaches according to feature extraction process</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210301232609121.png" alt="image-20210301232608959" style="zoom:50%;" /></p><ul><li><p>The main steps of handcrafted-based features:</p><ul><li>foreground detection that corresponds to action segmentation</li><li>feature selection and extraction by an expert</li><li>classification of action represented by the extracted features</li></ul></li><li><p>The spatial and temporal representations of actions</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210301233924987.png" alt="image-20210301233924987" style="zoom:50%;" /></p><ul><li><p>Based on spatial cues (spatial representation )</p><ul><li>body models: use kinematic joint model, can be 3D model or directly recognize on 2D model</li><li>image models: holistic representation of actions that use a regular grid bounded by ROI centered around the person. E.g., silhouettes, contours, motion history images, motion energy images and optical flow.</li><li>spatial statistics: use a set of statistics of local features from the surrounding regions. E.g., calculated spatial-temporal interest points (STIP) of the image and assign each region to a set of features.<ul><li>volume-based: rely on features like texture, color, posture etc. Recognizing actions by the similarity between two action volumes. <strong>Only works fine on simple action or gesture.</strong> Like SIFT.</li><li>trajectory-based: represent joint positions with 2D or 3D points and then track them. The tracked changes in the posture is used for classification. Noise, view and /or illumination changes robust. Like HOG, HOF etc.</li></ul></li></ul></li><li><p>Based on temporal cues (temporal representation )</p><ul><li>action grammars: represent the action as a sequence of moments, and each moments is described by its own appearance and dynamics. Features are grouped into similar configurations called states and temporal transition between these states are learned. Like HMM, CRF, regression models and context-free grammars</li><li>action templates: representing the appearance of temporal blocks of features and dynamics called templates. Take representations of dynamics from several frames. Methods like Fourier Transform, Wavelet representations and trajectories of body parts are templates that cane be used</li><li>temporal statistics: use statistical models to describe the distributions of unstructured features over time.</li></ul></li><li><p>Appearance based approaches: <em>can be classified according to either shape or motion based characteristics.</em></p><p>Use 2D or 3D depth images and are based on shape features, motion features or any combination of both features. One advance is the skeleton-based recognition. Methods of it can be classified as (1) joint locations: consider the skeleton as a set of points; (2) joint angles: assume the human body as a system of rigid connected segments and the movement as an evolution of their spatial configuration.</p><ul><li>shape based methods: local shape features such as contour points, local region, silhouette and geometric features from the Human image or video after foreground segmentation.</li><li>motion based methods: optical flow and motion history volume from extracting action representation. methods like vector quantization of motion descriptors is used for action recognition. It uses histograms of optical flow and classifiers of bag-of-words.</li><li>hybrid methods: shape + motion features</li></ul></li></ul></li><li><p>Feature learning based methods for extracting representations</p><ul><li>traditional approaches<ul><li>dictionary learning: provides a sparse representation of the input data. It's proved that the use of over-complete dictionaries can produce even more compact representations</li><li>genetic programming: search a space of possible solutions without having any prior knowledge and can discover functional relationships between features in data enabling its classification. GP is used to construct holistic descriptors that allow to maximize the performance of action recognition tasks.</li><li>bayesian networks: PGMs, some use PGMs to represent and capture the semantic relationships among action units, and the correlations of the action unit intensities</li></ul></li><li>deep-learning-based methods<ul><li>generative methods: the main goal of these models is to understand the data distribution including the features that belong to each class. Methods like AE, VAE, GANs</li><li>discriminative methods: DNN,RNN, CNN. E.g., propose long-term temporal convolutions + high-quality optical flow</li><li>hybrid models</li></ul></li></ul></li></ul><h4 id="har-approaches-according-to-the-recognition-stages">HAR approaches according to the recognition stages</h4><ul><li>First stage methods (detection)<ul><li>skin color: used to detect the desire body part. May face problems since the chosen color space or when the objects of the scenario whose color is close to that of the skin. <strong>Seems not that robust</strong></li><li><strong>????</strong>Shape: the contours of the body part shape. This kind of methods are independent of the camera view, skin color and conditions of lighting.</li><li>pixel values: appearance is represented as pixel values change between images of a sequence according to the activity</li><li>3d models: build matches between characteristics of the model based on various features of the images. These methods are independent ode the viewpoint</li><li>motion: Using like the difference in brightness of pixels of two successive images</li><li><strong>????</strong>anisotropic-diffusion: based on the extension of the successful anisotropic diffusion based segmentation to the whole video sequence to improve the detection</li></ul></li><li>Second stage methods (tracking)<ul><li><p>temporal based methods: models are used to follow body parts</p><ul><li>features tracking based on the correlation: try to track the regions that contains body parts. These models require the part being tracked remains in the same neighborhood in the successive images. E.g., use 3D information of depth maps; optical flow</li><li>contours based tracking: snakes . initially place a contour close to the ROI (region of interest ), then it's warped in an interactive way using active shape models in each frame to make to snake converge. Sensitive to color intensity variations and smoothing and softening of contours</li></ul></li><li><p>optimal estimation</p><ul><li>evaluate the state of moving systems from series of measures. In HAR, they are used to estimate the movements of the human body. In real-time systems. Limitations against cluttered backgrounds.</li><li>similar to KF</li></ul></li><li><p>particle filters</p><p>following the body parts and their configurations in complex environments. The location of one body part is modelled by a set of particles</p></li><li><p>cam shift</p><p>based on mean shift algorithm (it uses the models of appearance based on density to represent the targets).</p></li></ul></li><li>Third stage methods (classification)<ul><li>SVM: used with kernel like Gaussian, RBF or linear kernel</li><li>Naive Bayesian classifier:</li><li>KNN: sensitive to local fatal structure</li><li>Kmeans: sensitive to data structures.</li><li>mean shift clustering: no requirement of prior knowledge about the number of clusters and no limit of the forms</li><li>machines finite state: states (the static gestures and postures) and transitions (temporal and /or probabilistic constraints). However, the models need to be changed everytime a new gesture appears. Computational expensive.</li><li>HMMs:</li><li>dynamic time wrapping: calculates the distances between each pair of possible points from two signals. Used for estimating and detecting the movement in a video sequence</li><li>NNs</li></ul></li></ul><h4 id="har-approaches-according-to-the-source-of-the-input-data">HAR approaches according to the source of the input data</h4><ul><li>Uni-modal methods<ul><li>space-time methods: time and the 3D representation f the body to locate activities in space. But sensitive to noise and occlusion. Cannot works on complex actions</li><li>stochastic methods: models like Markov Model. The training is difficult cause the amount of parameters.</li><li>rule-based methods: characterize the activity using a set of rules or attributes</li><li>shape-based methods: shape features to represent and recognize activities. Dependent on the viewpoint, occlusion, people clothing and sensitive to lighting variations</li></ul></li><li>Multi-modal methods<ul><li>emotional methods: associate visual and textual features</li><li>behavior methods: recognize the behavior methods.</li><li>social networks-based methods: allow recognition of social events and interactions</li></ul></li></ul><h4 id="har-approaches-according-to-the-ml-supervision-level">HAR approaches according to the ML supervision level</h4><ul><li>supervised: mostly used to classify and recognize short term actions</li><li>unsupervised: outperform on finding spatio-temporal patterns of motion. computationally complex, less accurate and trustworthy.</li><li>semi-supervised: hybrid.</li></ul><h3 id="activities-type">Activities type</h3><p>Divided by the complexity</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210303222430563.png" alt="image-20210303222430563" style="zoom:40%;" /></p><ul><li>elementary human actions: simple atomic activities</li><li>gestures: a language or part of the non-verbal communication which can be employed to express significant ideas or orders</li><li>behaviors: a set of physical actions and reaction of individuals in specific situations</li><li>interactions: reciprocal actions or exchanges between two entities or more</li><li>group actions:</li><li>events: social actions between individuals</li></ul><h4 id="body-parts">Body parts</h4><ul><li>hand gestures: be tracked to <strong>detect the communication between individuals</strong></li><li>foot: detect shifting and movements of people or Other actions</li><li><strong>facial expressions: interpret specific Types of human activities, especially for handicapped or disabled people</strong></li><li>full body: posturers and human actions by the whole body</li></ul><h3 id="the-types-of-input-data">The Types of Input Data</h3><h4 id="image-or-videos">image or videos</h4><ul><li>HAR on static images: when activity is distinguishable compared to others by its characteristic.</li><li>HAR by videos: offer extra information related to prior and post event, then the relation between two successive frames can be established</li></ul><h4 id="single-viewpoint-or-multi-view-acquisition">single viewpoint or multi-view acquisition</h4><ul><li>single view acquisition</li><li>multiple view acquisition</li></ul><h3 id="evaluations">Evaluations</h3><h4 id="validation-means">Validation means</h4><ul><li>One platform, validated on different Types of datasets (differ on actions) acquired by this platform.</li><li>Different platforms, different datasets but all be evaluated so as to test the generalization across datasets</li><li>Compared with results in literature</li></ul><h4 id="datasets-benchmarks">Datasets (Benchmarks)</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210304202438344.png" alt="image-20210304202438344" style="zoom:67%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210304202924893.png" alt="image-20210304202924893" style="zoom:50%;" /></p><h4 id="evaluation-metrics">Evaluation metrics</h4><ul><li><p>sensitivity: also known as TP, recall or probability of detection. It determines the failure of the system to detect actions</p><p><span class="math inline">\(Sensitivity =\frac{TP}{TP+FN}\)</span></p></li><li><p>precision: also known as PPV (positive prediction value). <span class="math inline">\((1-precision)\)</span> determines the probability of the recognizer incorrectly identifying a detected activity</p><p><span class="math inline">\(Precision=\frac{TP}{TP+FP}\)</span></p></li><li><p>Specificity: also known as false positive rate (FPR).</p><p><span class="math inline">\(Specificity=\frac{TN}{TN+FP}\)</span></p></li><li><p>Negative predictive value (NPV): also known as negative precision. It measures the system sensitivity to negative class.</p><p><span class="math inline">\(NPV=\frac{TN}{TN+FN}\)</span></p></li><li><p>F_measure: the harmonic mean of precision and recall. It gives information of tests accuracy. Best with 1 and worst with 0.</p><p>$F_measure=2 $</p></li><li><p>Accuracy: the percentage of correct predictions relative to the total number of samples</p><p><span class="math inline">\(Accuracy =\frac{\#CorrectPredictions}{\#Predictions}\\Accuracy =\frac{TP+TN}{\#samples}\)</span></p></li><li><p>Likelihood ratio: the likelihood of an activity predicted when it matches the ground truth compared to the likelihood when it's predicted wrongly.</p><p><span class="math inline">\(LR+=\frac{Sensitivity}{1-Specificity},\\ LR-=\frac{1-Sensitivity}{Specificity}\)</span></p></li><li><p>AUC: it's 1 if predicted perfectly.</p></li><li><p>Confusion matrix</p></li><li><p>IoU: intersection over Union, also known as Jaccard index or Jaccard similarity coefficient.</p><p><span class="math inline">\(IoU=\frac{AreaOfOverlap}{AreaOfUnion}\)</span></p></li></ul><h3 id="limitations-and-challenges">Limitations and Challenges</h3><h4 id="limitations">Limitations</h4><p>Show various issues that may affect the effectiveness of HAR system.</p><ul><li>specific to the methods used during the various phase of the recognition process.<ul><li>methods based on the form or the appearance<ul><li>like colorimetric segmentation: confuse the objects of the scene and body parts</li><li>variation in appearance or clothing of people</li></ul></li></ul></li><li>related to the acquisition devices, experimentation environment or various applications of the system.<ul><li>light variations: affect the image quality and then features</li><li>perspective change: if data is acquired by single view it would be a big problem<ul><li>self-occlusion: body parts occlude each other</li><li>occlusion of another object</li><li>partial occlusion of human body parts</li></ul></li><li>variety of gestures linked to the complex structure of human activities and the similarity between classes of different actions.<ul><li>due to data association</li></ul></li><li>noise, complex or moving backgrounds and unstructured scenes, and scale variation</li></ul></li><li>rely on their own recorded dataset to test performance<ul><li>call of a benchmark</li></ul></li></ul><h4 id="challenges-1">Challenges</h4><ul><li>the requirement of continuous monitoring and generate reliable answers at the right time</li><li>modeling and analyzing interactions between people and objects with an appropriate level of accuracy</li><li>societal challenges: acceptance by the society, privacy, side effects of installation<ul><li>privacy: HAR systems on smartphones may be a way out</li></ul></li><li>HAR system should be independent on users' age, color, size or capacity to use</li><li>gestures independence and gestures spotting from continuous data streams<ul><li>detect and recognize various gestures under different background conditions</li><li>tolerant with the scalability and growth of gestures</li></ul></li><li>context-aware: improve applications in its domain</li><li>daily life activities: complex videos and hard to be modeled; overlapping of starting and ending time of each particular activity; discrimination between intentional and involuntary actions</li><li>HAR through missed parts of video, recognition more than one activities performed by one person at the same time, early recognition and prediction of actions</li><li>The implementation of DL HAR system: memory constraint, high number of parameters update, collection an fusion of large multi-modal variant data for the training process, deployment of different architectures of DL based methods in smartphones or wearable devices</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Fall detection
&lt;ul&gt;
&lt;li&gt;Paper I: &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full&quot;&gt;Elderly Fall Detection Systems: A Literature Survey, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper II: &lt;a href=&quot;https://dl.acm.org/doi/10.1145/2769493.2769540&quot;&gt;A survey on vision-based fall detection, 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper III: &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2352864815000681&quot;&gt;3D depth image analysis for indoor fall detection of elderly people, 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper IV: &lt;a href=&quot;https://ieeexplore.ieee.org/document/9186685&quot;&gt;Deep learning based systems developed for fall detection: a review, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper V: &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8869737&quot;&gt;Implementation of Fall Detection System Based on 3D Skeleton for Deep Learning Technique, 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper VI: &lt;a href=&quot;https://ieeexplore.ieee.org/document/8369778&quot;&gt;Human fall-down event detection based on 2D skeletons and deep learning approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Human activity recognition
&lt;ul&gt;
&lt;li&gt;Paper VII: &lt;a href=&quot;https://link.springer.com/content/pdf/10.1007/s11042-020-09004-3.pdf&quot;&gt;Vision-based human activity recognition: a survey, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D and depth data
&lt;ul&gt;
&lt;li&gt;Paper VIII: &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0167865514001299&quot;&gt;Human activity recognition from 3d data: a review, 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.08362&quot;&gt;RGB-D-based Human Motion Recognition with Deep Learning: A Survey, 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;3D skeleton-based human representations
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1601.01006&quot;&gt;Space-Time Representation of People Based on 3D Skeletal Data: A Review, 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0031320315004392&quot;&gt;3D skeleton-based human action classification: A survey,2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://d1wqtxts1xzle7.cloudfront.net/47833181/Crowd_analysis_A_survey20160806-21965-155miur.pdf?1470471444=&amp;amp;response-content-disposition=inline%3B+filename%3DCrowd_analysis_a_survey.pdf&amp;amp;Expires=1614620280&amp;amp;Signature=gO5XOCbzmA4O~6zc1hli7UqnkZmethCye13xIqVW58A~NTeZYwbbxSs3vZsO4E9~73WX7gYBapzo3quA7UV5jFDRfaDQ6v0ds8dA3BDhB5ys2PlxRFWxEmPsfGAPSp7G6inWLRrfw89L2xXRnX-KM1caNEnqcsg18OD9zf8LU3aovB4hXyB0kvMtc2T2FXdg1HdlQbjqVAlZmrcSl2Y98j1Gr4it23BLSbmUmwZpYAtVA4WqUwFihyqQco5XHX3dhJn7eUdKTOc6QdqQ2KumIhXBwnHSR8TOF9StECcxoUlOf9fcrEgRH4tDauMCsVqgCWJkANhI4~lp0nJEPP21fQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA&quot;&gt;Crowd analysis: a survey, 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/6909476&quot;&gt;Human action recognition by representing 3d skeletons points in a lie group, CVPR 2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;knowledge-based HAR activity recognition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0957417416302913&quot;&gt;A survey on using domain and contextual knowledge for human activity recognition in video streams.,2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Abnormal HAR
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0952197618301775&quot;&gt;A review of state-of-the-art techniques for abnormal human activity recognition, 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
      <category term="fall" scheme="http://yoursite.com/tags/fall/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Graph Embedded Pose Clustering for Anomaly Detection</title>
    <link href="http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html</id>
    <published>2021-01-15T20:17:13.000Z</published>
    <updated>2021-01-21T22:51:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1912.11850.pdf">Graph Embedded Pose Clustering for Anomaly Detection</a></p><p>Code <a href="https://github.com/amirmk89/gepc">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li><p>Anomaly detection task</p><ul><li>Fine-grained anomaly detection: Detecting abnormal variations of an action: e.g. an abnormal type of walking</li><li>Coarse-grained anomaly detection: Defining normal actions and regard other action as abnormal. Aka there are multiple poses regarded as normal actions, rather than a single normal action.</li></ul></li><li><p>Video anomaly detection</p><ul><li>Reconstructive models: learn a feature representation for each sample and attempt to reconstruct a sample based on that embedding, often using <strong>Autoencoder</strong>. Samples poorly reconstructed are considered anomalous.</li><li>Predictive models: model the current frame based on a set of previous frames, often relying on recurrent neural networks or 3D convolutions. Samples poorly predicted are considered anomalous.</li><li>Reconstructive + predictive models</li><li>Generative models: used to reconstruct, predict or model the distribution of the data, often using Variational Autoencoders (VAEs) or GANs. E.g. the differences in gradient-based features and optical flow.</li></ul></li><li><p>GNNs</p><p>The point is the weighted adjacency matrix.</p><ul><li>Temporal and multiple adjacency extensions. (ST-GCN)</li><li>Graph attention networks. (2s-AGCN)</li></ul></li><li><p>Deep clustering models</p><p>Provide useful cluster assignments by optimizing a deep model under a cluster inducing objective.</p></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Skeleton-based methods make the analysis independent of nuisance parameters such as viewpoint or illumination.</li></ul></li><li>Limitations<ul><li>Traditional RGB-based anomaly detection methods have to consider many trivial information (viewing direction, illumination, background clutter etc.), and those data are sparse in human pose.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Generating action words from skeleton-based graphs and then classify actions into normal and abnormal (anomaly detection). With an aim at it can work both on fine-grained and coarse-grained task.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Map graphs into representation space and then cluster them so as to get action words. At last, Dirichlet process based mixture is used for classifying normal and abnormal.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115162120.png" alt="image-20210115162120660" style="zoom:50%;" /></p><h3 id="data-preparation">Data Preparation</h3><ul><li>Similar skeleton graph as what used in ST-GCN.</li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Backbone: ST-GCN</p></li><li><p>ST-GCAE network</p><ul><li><p>GCN block</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115185527.png" alt="image-20210115185526918" style="zoom:50%;" /></p><p>The block will be used in SAGC</p></li><li><p><strong>SAGC</strong> block</p><p>Each adjacency type is applied with its own GCN, using separate weights.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115190041.png" alt="image-20210115190041541" style="zoom:50%;" /></p><ul><li><p>Adjacency matrices</p><table><colgroup><col style="width: 8%" /><col style="width: 38%" /><col style="width: 28%" /><col style="width: 24%" /></colgroup><thead><tr class="header"><th>matrix</th><th>sharing</th><th>level</th><th>Dimension</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\mathrm{A}\)</span></td><td>fixed and shared by all layers</td><td>body-part connectivity over node relations</td><td><span class="math inline">\([V,V]\)</span>, <span class="math inline">\(V\)</span> is the number of nodes</td></tr><tr class="even"><td><span class="math inline">\(\mathrm{B}\)</span></td><td>individual at each layer, applied equally to all samples</td><td>dataset level keypoint relations</td><td><span class="math inline">\([V,V]\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\mathrm{C}\)</span></td><td>is different for different sample</td><td>sample specific relations</td><td><span class="math inline">\([N,V,V]\)</span>, <span class="math inline">\(N\)</span> is the batch size</td></tr></tbody></table></li></ul></li><li><p>ST-GCAE</p><p>The encoder uses large temporal strides with an increasing channel number to compress an input sequence to a latent vector. The decoder uses temporal up-sampling layers and additional graph convolutional blocks.</p></li></ul></li><li><p><strong>Deep embedded cluster</strong></p><ul><li><p>The input is the embedding from ST-GCAE, denoted as <span class="math inline">\(\mathrm{z}_i\)</span> for sample <span class="math inline">\(i\)</span></p></li><li><p>Soft-assignment --clustering layer</p><p>The probability <span class="math inline">\(p_{ik}\)</span> for the <span class="math inline">\(i\)</span>-th sample to be assigned to the <span class="math inline">\(k\)</span>-th cluster is:</p><p><span class="math inline">\(p_{ik}=Pr(y_i=k|\mathrm{z}_i,\Theta)=\frac{exp(\theta^T_k\mathrm{z}_i)}{\sum\limits_{k&#39;=1}^{K}exp(\theta^T_{k&#39;}\mathrm{z}_i)}\)</span>, where <span class="math inline">\(\Theta\)</span> is the clustering layer’s parameters. (Simple softmax)</p></li><li><p><strong>Optimize clustering layer</strong></p><ul><li>Objective: Minimize the KL-divergence between the current model probability clustering prediction <span class="math inline">\(P\)</span> and a target distribution <span class="math inline">\(Q\)</span>. The target distribution aims to strengthen current cluster assignments by normalizing and pushing each value closer to a value of either 0 or 1.</li><li><strong>EM</strong> style. In expectation step, the entire model is fixed and the target distribution <span class="math inline">\(Q\)</span> is updated. In maximization stage, the model is optimized to minimize the clustering loss <span class="math inline">\(L_{cluster}\)</span>.</li></ul></li></ul></li><li><p>Anomaly classifier--<strong>normality scoring??</strong></p><ul><li>Two types of multimodal distributions. One is at the cluster assignment level; the other is at the soft-assignment vector level.</li><li>DPMM based. Classifier is fitted by soft-assignment vector (e.g., for class <span class="math inline">\(i\)</span> the softmax result) and then it can do inference.</li></ul></li><li><p>Model</p><p>Feeding the embedding from ST-GCAE to clustering layer, then fixing decoder, fine-tune the encoder in ST-GCAE and clustering layer by combined loss. After fine tuning, using DPMM-based classifier for final inference.</p><ul><li><p>Loss function</p><ul><li><p><strong>Reconstruction loss</strong> <span class="math inline">\(L_{rec}\)</span>: <span class="math inline">\(\ell_2\)</span> loss between the original temporal pose graphs and those reconstructed by ST-GCAE, <strong>used in pre-training stage</strong>, for training the whole ST-GCAE.</p></li><li><p><strong>Clustering loss</strong> <span class="math inline">\(L_{cluster}\)</span>, combined with reconstruction loss and used for fine-tuning encoder of ST-GCAE+clustering layer</p><p><span class="math inline">\(L_{cluster}=KL(Q||P)=\sum\limits_i\sum\limits_kq_{ik}\log\frac{q_{ik}}{p_{ik}},\\ q_{ik}=\frac{p_{ik}/(\sum_{i&#39;}p_{i&#39;k})^{\frac{1}{2}}}{\sum_{k&#39;}p_{ik&#39;}/(\sum_{i&#39;}p_{i&#39;k&#39;})^{\frac{1}{2}}}\)</span></p></li><li><p>Combined loss</p><p><span class="math inline">\(L_{combined}=L_{rec}+\lambda\cdot L_{cluster}\)</span>, this loss is for training encoder and clustering layer, which means the decoder is fixed while using it.</p></li></ul></li><li><p>Optimization</p><ul><li>encoder: reconstruction loss + cluster loss</li><li>decoder: reconstruction loss</li><li>clustering layer: cluster loss</li></ul></li></ul></li></ul><h2 id="experiments">Experiments</h2><ul><li><p>Dataset</p><ul><li>ShanghaiTech: 130 abnormal events captured in 13 different scenes with complex lighting conditions and camera angles.<ul><li>training set contains only normal examples</li><li>test set contains both normal and abnormal examples</li><li>2D pose</li></ul></li><li>Kinetics-based: Kinetics-250 and NTU-RGBD. Actions in each set are sampled randomly or meaningfully. In Kinetics dataset, remove actions that focus only on slightly part joints' movements, like hair braiding.<ul><li><em>Few vs. Many</em>: few normal actions (<span class="math inline">\(3\sim5\)</span>) in the training set and many abnormal (<span class="math inline">\(10\sim 11\)</span> hundreds) actions in the test set</li><li><em>Many vs. Few</em>: switch the training set and test set in experiment above.</li></ul></li></ul></li><li><p>Preprocessing</p><ul><li>Pre-extracting 2D pose from ShanghaiTech Campus</li></ul></li><li><p>Input features</p><ul><li>The coordinates of joints</li><li>For ShanghaiTech: The embeddings of the patch around each joint (from one of the pose estimation model's hidden layers)</li></ul></li><li><p>Test Algorithms on coarse-grained (Kinetics and NTU-RGBD)</p><ul><li><p>Autoencoder reconstruction loss: ST-GCAE reached convergence prior to the deep clustering fine-tuning stage.</p></li><li><p>Autoencoder based one-class SVM: fit a one-class SVM using the encoded pose sequence representation</p></li><li><p>Video anomaly detection methods: Train Future frame prediction model and the skeleton trajectory model. Anomaly scores for each video are obtained by averaging the per-frame scores.</p></li><li><p>Classifier softmax scores: supervised baseline. Anomaly score is by either using the softmax vector's max value or by using the Dirichlet normality score</p></li><li><p>Test video in fixed size but with sliding-window if the test video with unknown frames</p></li></ul></li><li><p>Evaluation metrics</p><ul><li>Frame-level score: the maximal score over all the people in the frame</li><li>AUC as the combined score over all frames of one test</li></ul></li><li><p>Summary</p><ul><li><p>On ShanghaiTech (fine-grained): Patches ST-GCAE outstands.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121170448.png" alt="image-20210121170448502" style="zoom:40%;" /></p></li><li><p>On coarse grained dataset, ST-GCAE outperforms, but better on meaningful actions. <em>A good skeleton help ST-GCAE</em> (NTU-RGBD has better detection on skeletons cause the depth data is known.)</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121171357.png" alt="image-20210121171357539" style="zoom:30%;" /></p></li><li><p><em>Failed cases</em>: occlusions, high-speed action like cycling, non-person related abnormal like bursting into a vehicle.</p></li><li><p><strong>Ablation study:</strong> adding some abnormal actions into normal actions</p><ul><li>ST-GACE on NTU-RGBD (<strong><em>only dropping, touching and Rand8 dataset are tested</em></strong>): ST-GCAE loses on average less than <span class="math inline">\(10\%\)</span> of performance when trained with <span class="math inline">\(5\%\)</span> abnormal actions added as noises.</li></ul></li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>The use of embedded pose graphs and a Dirichlet process mixture for video anomaly detection;</li><li>A new coarse-grained setting for exploring broader aspects of video anomaly detection;</li><li>State-of-the-art AUC of 0.761 for the ShanghaiTech Campus anomaly detection benchmark.</li></ul><h2 id="remarks"><font color='blue'>Remarks</font></h2><ul><li>The reconstruction (learning representations of graph) is mixed with clustering in the final loss, will this be good? Won't the trivial information of clustering influence the reconstruction?</li><li>The training set for clustering layer is initialized by the K-Means centroids, won't the initialization methods matter?</li><li><strong>The embeddings of patches around each joint outperforms the simple joint coordinates</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1912.11850.pdf&quot;&gt;Graph Embedded Pose Clustering for Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/amirmk89/gepc&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html</id>
    <published>2021-01-15T01:51:32.000Z</published>
    <updated>2021-01-15T20:20:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/lshiwjx/2s-AGCN">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Convolutional DL based methods manually structure the skeleton as a sequence of joint-coordinate vectors or as a pseudo-image, which is fed into RNNs or CNNs to generate the prediction.</li><li>Skeleton-based action recognition<ul><li>Design handcrafted features to model human body, but they are barely satisfactory.</li><li>DL-based: CNN-based methods are generally more popular than RNN-based methods. But both fail to fully represent the structure of the skeleton data.</li><li>GCN-based: ST-GCN, eliminates the meed for designing handcrafted part assignment or traversal rules.</li></ul></li><li>GNNs<ul><li><strong>Spatial perspective</strong>: directly perform the convolution filters on the graph vertexes and their neighbors, which are extracted and normalized based on manually designed rules.</li><li>Spectral perspective: use the eigenvalues and eigenvectors of the graph Laplace matrices. They perform the graph convolution in the frequency domain with the help of graph Fourier transform.</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>The second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods.</li></ul></li><li>Limitations:<ul><li>Representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependency between correlated joints</li><li>In GCN-based skeleton action recognition, the topology of the graph is set manually and thus may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks.</li><li>ST-GCN: 1) The skeleton graph is heuristically predefined and represents only the physical structure of the human body. 2) The fixed topology of graph limiting the flexibility and capacity to model the multilevel semantic information. 3) One fixed graph structure may not be optimal for all the samples of different action classes. Like hands-related actions and legs-related actions</li></ul></li></ul><h2 id="goals">Goals</h2><p>Propose a improved ST-GCN (graph convolutional based model), so as to use 2nd order information and improve the accuracy of action recognition based on skeletons.</p><p>Make the graph is unique for different layers and samples.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Modification on ST-GCN</p><ul><li>Two types of graphs:<ul><li>Global graph: represents the common pattern for all the data</li><li>Individual graph: represents the unique pattern for each data</li></ul></li><li>Second-order information: the length and directions of bones are formulated as a vector pointing from its source joint to its target joint.</li></ul><h3 id="data-preparation">Data Preparation</h3><ul><li>The structure of the graph follows the work of ST-GCN.</li></ul><h3 id="a-look-at-st-gcn">A look at ST-GCN</h3><p>Graph convolution in ST-GCN: <span class="math inline">\(f_{out}(v_{ti})=\sum\limits_{v_{tj}\in \mathcal{B}_i}\frac{1}{Z_{ti}(v_{tj})}f_{in}(v_{j})\cdot \mathrm{w}(l_{ti}(v_{tj}))\)</span>, follows spatial configuration partitioning.</p><ul><li>Graph convolution in spatial dimension</li></ul><p><span class="math inline">\(f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}_k(f_{in}\mathrm{A}_k)\odot\mathrm{M}_k\)</span>, where <span class="math inline">\(\mathrm{M}_k\)</span> is an <span class="math inline">\(N\times N\)</span> attention map that indicates the importance of each vertex. <span class="math inline">\(\mathrm{A}_k\)</span> <strong>determines whether there are connections between two vertexes and <span class="math inline">\(\mathrm{M}_k\)</span> determines the strength of the connections.</strong></p><ul><li>Graph convolution in temporal dimension: <span class="math inline">\(K_t\times 1\)</span>convolution on the output feature map</li></ul><p><em>The model is calculated based on a predefined graph, which may not be a good choice.</em></p><h3 id="implementation">Implementation</h3><ul><li><p>Adaptive graph convolutional network (<strong>AGCN</strong>)</p><p><strong>BN+9 of adaptive graph convolutional blocks + global average pooling + softmax classifier</strong></p><ul><li><p>Adaptive graph convolutional layer:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142649.png" alt="image-20210115142649202" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115133332.png" alt="image-20210115133332722" style="zoom:50%;" /></p><p><span class="math inline">\(f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}_kf_{in}(\mathrm{A}_k+\mathrm{B}_k+\mathrm{C}_k)\)</span>. The adjacency matrix is now divided into three parts</p><ul><li><p><span class="math inline">\(\mathrm{A}_k\)</span>: same as <span class="math inline">\(N\times N\)</span> adjacency matrix <span class="math inline">\(\mathrm{A}_k\)</span> in ST-GCN, it <em>represents the physical structure of the human body</em>.</p></li><li><p><span class="math inline">\(\mathrm{B}_k\)</span>: An <span class="math inline">\(N\times N\)</span> adjacency matrix. It's trainable. It acts as <span class="math inline">\(\mathrm{M}_k\)</span> (attention mechanism) in ST-GCN, influenced by the connections between two joints and also the strength of the connections.</p></li><li><p><span class="math inline">\(\mathrm{C}_k\)</span>: a similarity matrix calculated by the normalized embedded Gaussian function with vectors embedded by <span class="math inline">\(1\times 1\)</span> convolutional layer.</p><p><span class="math inline">\(\mathrm{C}_k=softmax(\mathrm{f}_{in}^T\mathrm{W}_{\theta k}^{T}\mathrm{W}_{\phi k}\mathrm{f}_{in})\)</span>, where <span class="math inline">\(\mathrm{W}_\theta,\mathrm{W}_\phi\)</span> are the parameters of the embedding functions <span class="math inline">\(\theta,\phi\)</span>, respectively.</p></li></ul></li><li><p>Adaptive graph convolutional block</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115132928.png" alt="image-20210115132928144" style="zoom:33%;" /></p></li></ul></li><li><p>Model: two stream networks</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142748.png" alt="image-20210115142748418" style="zoom:50%;" /></p><ul><li>J-stream: the input data are joints, as what's depicted in AGCN.</li><li>B-stream: the input data are bones.<ul><li>A bone is the vector pointing from its source joint to its target joint. This vector will contain both length and direction of a bone. An empty bone is added so as to make sure the B-stream has similar quantity of input as J-stream.</li></ul></li></ul><p>Finally, the <em>softmax</em> scores of the two streams are added to obtain the fused score and do prediction.</p></li><li><p>Loss function</p><p>Cross-entropy</p></li><li><p>Why does it work?</p><ul><li>Considering bones (2nd information)</li><li>Offers trainable attention matrix <span class="math inline">\(\mathrm{B}_k\)</span> and the similarity evaluation of <span class="math inline">\(\mathrm{C}_k\)</span> to estimate the strength of connection. Both of them offer more possible connections and provide more flexibility.</li></ul></li></ul><h2 id="experiments">Experiments</h2><ul><li>Dataset: Kinetics and NTU-RGBD<ul><li>NTU-RGBD: If the number of bodies in the sample is less than 2, the second body is padded with 0.</li><li>Kinetics: Same data augmentation as done in ST-GCN.</li></ul></li><li>Training: SGD with Nesterov momentum (0.9), batch size is 64. The weight decay is set to 0.0001.<ul><li>NTU-RGBD: learning rate is set as 0.1 and is divided by 10 at the 30th epoch and 40th epoch. The training process is ended at the 50th epoch.</li><li>Kinetics: The learning rate is set as 0.1 and is divided by 10 at the 45th epoch and 55th epoch. Training ends at the 65th epoch.</li></ul></li><li>Evaluation metrics</li><li>NTU-RGBD: top-1 accuracy<ul><li>Kinetics : top-1 and top-5 accuracy</li></ul></li></ul><h3 id="ablation-study">Ablation study</h3><ul><li><p>Adaptive graph convolutional block</p><p>Manually delete one of the graphs and estimate.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115150909.png" alt="image-20210115150909820" style="zoom:33%;" /></p><ul><li>Given each connection, a weight parameter is important, which also proves the importance of the adaptive graph structure</li></ul></li><li><p>Visualization of the learned graphs</p><p>Denote the strength of joints by dot size, the bigger the stronger connection.</p><ul><li><p>A higher layer in AGCN contains higher-level information, comparing the dot size in different layers</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115151421.png" alt="image-20210115151421543" style="zoom:50%;" /></p></li><li><p>The diversity for different sample in the same layer is proved.</p></li></ul></li><li><p>Two-stream framework</p><p>The two-stream method outperforms the one-stream-based methods either the J-stream or the B-stream.</p></li></ul><h3 id="compared-with-sota">Compared with SOTA</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115144443.png" alt="image-20210115144443887" style="zoom:50%;" /></p><ul><li>Question 1: <font color='red'> ResNet helps?</font></li><li>Question 2: <font color='red'>How about compared with methods based on RGB or optical flow ?</font> In paper ST-GCN their model fails to those models.</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>An adaptive graph convolutional network is proposed.</li><li>The second-order information of the skeleton data is explicitly formulated and combined with the first-order information using a two-stream framework, which brings notable improvement for the recognition performance.</li><li>On two large-scale datasets for skeleton-based action recognition, the proposed 2s-AGCN exceeds the SOTA by a significant margin.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf&quot;&gt;Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/lshiwjx/2s-AGCN&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html</id>
    <published>2021-01-13T02:58:39.000Z</published>
    <updated>2021-01-15T18:26:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/yysijie/st-gcn">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Human action recognition: can be solved by appearance, depth, optical flows, and body skeletons.</li><li>Models for graph<ul><li>Recurrent neural networks</li><li>GNNs:<ul><li>Spectral perspective: the locality of the graph convolution is considered in the form of spectral analysis.</li><li><strong>Spatial perspective</strong>: the convolution filters are applied directly on the graph nodes and their neighbors.</li></ul></li></ul></li><li>Skeleton Based Action Recognition<ul><li>Handcrafted feature based methods: design several handcrafted features to capture the dynamics of joint motion. E.g., covariance matrices of joint trajectories, relative positions of joints, rotations and translations between body parts.</li><li>Deep learning methods: recurrent neural networks and temporal CNNs. Many emphasize <strong>the importance of modeling the joints within parts of human bodies.</strong></li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Earlier methods of using skeletons for action recognition simply employ the joint coordinates at individual time steps to form feature vectors, and apply temporal analysis thereon. <strong>They do not explicitly exploit the spatial relationships among the joint</strong>.</li><li>Most existing methods which explore spatial relationship rely on hand-crafted parts or rules to analyze the spatial patterns.</li><li>Traditional CNNs are not suitable for 2D or 3D skeletons (graphs rather than data in grids).</li></ul></li><li>Limitations:<ul><li>Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization.</li><li>Models using hand-crafted parts are difficult to be generalized to others</li><li>These parts used in DL based methods are usually explicitly assigned using domain knowledge, which is not automatic and practical.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Build a better model for dynamics of human body skeletons. Specifically, a new method that can automatically capture the patterns embedded in the <strong>spatial configuration of the joints as well as the temporal dynamics</strong> is required.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><p>Skeletons in frames are connected as natural spatial-temporal graph, then using GCN.</p><h3 id="data-preparation">Data Preparation</h3><ul><li>The feature vector on a node <span class="math inline">\(F(v_{ti})\)</span> consists of coordinate vectors, as well as estimation confidence, of the i-th joint on frame t.</li><li>Construct the spatial temporal graph on the skeleton sequences in two steps. First, the joints within one frame are connected with edges according to the connectivity of human body structure. Then each joint will be connected to the same joint in the consecutive frame.</li><li>Both 18 joints skeleton model or 25 joints skeleton model work fine.</li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Model</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113115442.png" alt="image-20210113115429152" style="zoom:50%;" /></p><p>ResNet mechanism is applied on each ST-GCN unit.</p><table><thead><tr class="header"><th>Layer name</th><th>configuration</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(1\sim3\)</span></td><td>64 channels</td></tr><tr class="even"><td><span class="math inline">\(4\sim6\)</span></td><td>128 channels</td></tr><tr class="odd"><td><span class="math inline">\(7\sim9\)</span></td><td>256 channels, 9 temporal kernel size</td></tr><tr class="even"><td>Global pooling+softmax</td><td></td></tr></tbody></table></li><li><p>Loss function</p><ul><li><p><strong>Sampling function</strong> <span class="math inline">\(\mathbb{p}\)</span> enumerates the neighbors of location <span class="math inline">\(x\)</span>.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113155452.png" alt="image-20210113155452481" style="zoom:40%;" /></p><p>In this paper, <strong>the 1-neighbor set</strong> of joint nodes are used.</p></li><li><p><strong>The filter weights</strong> <span class="math inline">\(\mathrm{w} (v_{ti},v_{tj})\)</span> are shared everywhere on the input image.</p><p>Build a mapping <span class="math inline">\(l_{ti}:B(v_{ti})\rightarrow\{0,\cdots,K−1\}\)</span> which maps a node in the neighborhood to its subset label. The weight function <span class="math inline">\(\mathrm{w}(v_{ti}, v_{tj}):B(v_{ti})\rightarrow R^c\)</span> can be implemented by indexing a tensor of <span class="math inline">\((c,K)\)</span> dimension or</p><p><span class="math inline">\(\mathrm{w}(v_{ti},v_{tj})=\mathrm{w}&#39;(l_{ti}(v_{tj})\)</span></p><ul><li><p>Labeling strategies (the definition of <span class="math inline">\(l_{ti}\)</span>)</p><p><img src="C:%5CUsers%5C10457%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210113195834069.png" alt="image-20210113195834069" style="zoom:50%;" /></p><ul><li><p>Uni-labeling</p><p>Make the whole neighbor set itself as subset. Then feature vectors on every neighboring node will have a inner product with the same weight vector. Formally, <span class="math inline">\(K=1,l_{ti}(v_{tj})=0,\forall i,j\in V\)</span></p></li><li><p>Distance partitioning</p><p><span class="math inline">\(d=0\)</span> refers to the root node itself and remaining neighbor nodes are in the <span class="math inline">\(d =1\)</span> subset. Formally , <span class="math inline">\(K=2,l_{ti}(v_{tj})=d(v_{tj},v_{ti})\)</span></p></li><li><p>Spatial configuration partitioning</p><p>Three subsets: 1) the root node itself; 2)centripetal group and 3) centrifugal group.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113200600.png" alt="image-20210113200600480" style="zoom:33%;" />, where <span class="math inline">\(r_i\)</span> is the average distance from gravity center to joint <span class="math inline">\(i\)</span> over all frames in the training set.</p></li></ul></li><li><p>Learnable edge importance weighting</p><p>One joint appears in multiple body parts should have different importance in modeling the dynamics of these parts. A learnable mask M is added on every layer of spatial temporal graph convolution.</p></li></ul></li><li><p>The <strong>spatial graph convolution</strong></p><p><span class="math inline">\(f_{out}(v_{ti})=\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(\mathbb{p}(v_{ti},v_{tj}))\cdot \mathrm{w}(v_{ti},v_{tj})=\\\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(v_{tj})\cdot \mathrm{w}(l_{ti}(v_{tj}))\)</span>, <span class="math inline">\(Z_{ti}(v_{tj})=|\{v_{tk}|l_{ti}(v_{tk})=l_{ti}(v_{tj})\}|\)</span> is the cardinality of the corresponding subset. It's for balancing the contributions of different subsets to the output.</p></li><li><p><strong>Spatial temporal modeling</strong></p><p>Extend neighbors so as to include temporally connected joints</p><p><span class="math inline">\(B(v_{ti})={v_{qj}|d(v_{tj},v_{ti})\le K,|q-t|\le \lfloor\Gamma/2\rfloor}\)</span>, where <span class="math inline">\(\Gamma\)</span> is temporal kernel size (controls the temporal range to be included in the neighbor graph). Then , the sampling function is <span class="math inline">\(l_{ST}(v_{qj})=l_{ti}(v_{tj})+(q-t+\lfloor\Sigma/2\rfloor)\times K\)</span>.</p></li><li><p>The final convolution formula:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113222927.png" alt="image-20210113222926988" style="zoom:40%;" />, where <span class="math inline">\(\Lambda_j^{ii}=\sum_k(A_j^{ik})+\alpha\)</span>, <span class="math inline">\(\alpha=0.001\)</span> is to avoid empty rows in <span class="math inline">\(A_j\)</span>. To add learnable mask <span class="math inline">\(M\)</span>, displace <span class="math inline">\(A_j\)</span> as <span class="math inline">\(A_j\otimes M\)</span>.</p></li></ul></li><li><p>Why does it work?</p><ul><li>Parts restrict the modeling of joints trajectories within “local regions” compared with the whole skeleton, thus forming a hierarchical representation of the skeleton sequences.</li></ul></li></ul><h2 id="experiments">Experiments</h2><ul><li><p>Dataset:</p><ul><li><p>Kinetics (unconstrained action recognition dataset), provides only raw video clips without skeleton data.</p><ul><li><p>Augmentation: To avoid overfitting,two kinds of augmentation are used to replace dropout layers when training on the Kinetics dataset. 1) affine transformations, 2) sampling part of the frames from the whole frame and testing by a whole frame. <font color=Blue>May that's a way for avoiding the two consecutive frame are too similar?</font></p></li><li><p>Videos to skeletons</p><p>To work on skeletons, openpose is used for extracting. Concretely, resize all videos to the resolution of 340 × 256 and convert the frame rate to 30 FPS. Then OpenPose toolbox is used to estimate the location of 18 joints on every frame of the clips.</p></li><li><p>Final features:</p><p>Finally <strong>the clips are represented by a tensor in shape</strong> <span class="math inline">\((3,T,18,2)\)</span>, where 18 is the number of joints, 2 is the number of people and 3 is the number of features (X,Y,C), C is the confidence.</p></li></ul></li><li><p>NTU-RGBD ( in-house captured action recognition dataset)</p><ul><li>Already annotated with 25 3D joints</li><li>Each clip is guaranteed to have at most 2 subjects</li></ul></li></ul></li><li><p>Training: SGD with a learning rate of 0.01. <span class="math inline">\(lr\)</span> decay by 0.1 after every 10 epochs.</p></li><li><p>Evaluation metrics</p><ul><li><p>Kinetics</p><p>Test on validation set. Using <strong>top-1 and top-5 classification accuracy</strong></p></li><li><p>NTU-RGBD</p><p>Report top-1 recognition accuracy.</p><ul><li><strong>Cross-subject</strong>: Train on one subset of actors and test on the remaining actors.</li><li><strong>Cross-view</strong>: Train on skeletons from camera views 2 and 3, and test on those from camera view 1.</li></ul></li></ul></li></ul><h3 id="ablation-study">Ablation study</h3><p>Applied on Kinetics dataset.</p><ul><li><p>Spatial temporal graph convolution</p><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th>Model</th><th>configuration</th></tr></thead><tbody><tr class="odd"><td><em>baseline-TCN</em></td><td>squeeze Spatial dimension, concatenate all input joint locations to form the input features at each frame <span class="math inline">\(t\)</span>.</td></tr><tr class="even"><td><em>local convolution</em></td><td>The input data are the same format, but with unshared convolution filters.</td></tr></tbody></table></li><li><p>Partition strategies: same as what described before. <em>Distance partitioning*</em> is as intermediate between the distance partitioning and uni-labeling. The filters in this setting only differs with a scaling factor -1, or to say <span class="math inline">\(\mathrm{w}_0=-\mathrm{w}_1\)</span>.</p></li><li><p>Learnable edge importance weighting</p><p>This setting is named as <em>ST-GCN+Imp</em>.</p></li><li><p>Results</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114194501.png" alt="image-20210114194452831" style="zoom:50%;" /></p><ul><li>Better performance of ST-GCN based models could justify the power of the spatial temporal graph convolution in skeleton based action recognition</li><li>Distance partitioning* achieves better performance than uni-labeling, which again demonstrate <strong>the importance of the partitioning with multiple subsets</strong>.</li><li>ST-GCN model with learnable edge importance weights can learn to express the joint importance.</li></ul></li></ul><h3 id="compared-with-sota">Compared with SOTA</h3><p><strong>Model setting: ST-GCN+Learnable weights+Spatial configuration partitioning</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204800.png" alt="image-20210114204800283" style="zoom:30%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204722.png" alt="image-20210114204722008" style="zoom:30%;" /></p><ul><li>Kinetics: ST-GCN is able to outperform previous representative approaches, but under-perform methods in RGB or optical flow.</li><li>NTU-RGBD: No data augmentation before training. It outperforms all other selected candidates.</li><li>The skeleton based model ST-GCN can provide complementary information to RGB and optical flow models.</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Propose ST-GCN, a generic graph-based formulation for modeling dynamic skeletons, which is the first that applies graph-based neural networks for action recognition.</li><li>Propose several principles in designing convolution kernels in ST-GCN to meet the specific demands in skeleton modeling.</li><li>On two large scale datasets for skeleton-based action recognition, the proposed model achieves superior performance as compared to previous methods using hand-crafted parts or traversal rules, with considerably less effort in manual design.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1801.07455.pdf&quot;&gt;Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/yysijie/st-gcn&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Self-supervised Learning on Graphs, Deep Insights and New Directions</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html</id>
    <published>2021-01-12T03:00:00.000Z</published>
    <updated>2021-01-24T16:54:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2006.10141.pdf">Self-supervised Learning on Graphs: Deep Insights and New Directions</a></p><p><a href="https://github.com/ChandlerBang/SelfTask-GNN">Codes</a></p><a id="more"></a><h2 id="why">Why?</h2><ul><li>Nodes in graphs present unique structure information and they are inherently linked indicating not independent and identically distributed (or i.i.d.).</li><li>(SSL) has been introduced in both the image and text domains to alleviate the need of large labeled data by deriving labels for the significantly more unlabeled data.</li><li>To fully exploit the unlabeled nodes for GNNs, SSL can be naturally harnessed for providing additional supervision.</li><li>The challenges of graph to use SSL:<ul><li>graphs are not restricted to these rigid structures.</li><li>each node in a graph is an individual instance and has its own associated attributes and topological structures</li><li>instances (or nodes) are inherently linked and dependent of each other.</li></ul></li></ul><h2 id="goals">Goals</h2><ul><li><p>Focus on advancing GNNs for node classification where GNNs leverage both labeled and unlabeled nodes on a graph to jointly learn node representations and a classifier that can predict the labels of unlabeled nodes on the graph. Aims at gain insights on when and why SSL works for GNNs and which strategy can better integrate SSL for GNNs.</p></li><li><p><strong><em>Focus on semi-supervised node classification task</em></strong></p><p><span class="math inline">\(\min\limits_{\theta}\mathcal{L}_{task}(\theta,\mathrm{A,X},\mathcal{D}_L)=\sum\limits_{(v_i,y_i)\in\mathcal{D}_L}\ell(f_{\theta}(\mathcal{G})_{v_i},y_i)\)</span></p></li></ul><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121192249.png" alt="image-20210121192249653" style="zoom:40%;" /></p><h2 id="previous-work">Previous work</h2><h3 id="examples">Examples</h3><ul><li><a href="https://arxiv.org/abs/1902.11038">Multi-stage self-supervised learning for graph convolutional networks on graphs with few labels</a> utilize the clustering assignments of node embeddings as guidance to update the graph neural networks.</li><li><a href="https://arxiv.org/abs/2003.01604">Self-supervised graph representation learning via global context prediction</a> proposed to use the global context of nodes as the supervisory signals to learn node embeddings.</li></ul><h3 id="basic-pretext-task-on-graphs">Basic pretext task on graphs</h3><h4 id="structure-information-adjacency-matrix-mathrma">Structure information (Adjacency matrix <span class="math inline">\(\mathrm{A}\)</span>)</h4><p>Construct self-supervision information for the unlabeled nodes based on their local structure information, or how they relate to the rest of the graph</p><h5 id="local-structure-information"><strong>Local structure information</strong></h5><ul><li><p>Node property</p><ul><li><p>use node degree as a representative local node property for self-supervision while leaving other node properties (or the combination) as one future work</p></li><li><p>Formally, let <span class="math inline">\(d_i=\sum\limits_{j=1}^{N}\mathrm{A}_{ij}\)</span> denote the degree of <span class="math inline">\(v_i\)</span> and construct the associated loss of the SSL pretext task as</p><p><span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}(f_{\theta&#39;}(\mathcal{G})_{v_i}-d_i)^2\)</span>, where <span class="math inline">\(\mathcal{D}_U\)</span> denote the set of unlabeled nodes and associated pretext task labels in the graph.</p></li><li><p>Assumption: The node property information is related to the specific task of interest.</p></li></ul></li><li><p><strong>EdgeMask</strong></p><ul><li><p>Build pretext task based on the connections between two nodes in the graph. Specifically, <strong>one can first randomly mask some edges and then the model is asked to reconstruct the masked edges</strong>.</p></li><li><p>Formally, first mask <span class="math inline">\(m_e\)</span> edges denotes as the set <span class="math inline">\(\mathcal{M}_e\subset\varepsilon\)</span> and also sample the set <span class="math inline">\(\bar{\mathcal{M}_e}=\{(v_i,v_j)|v_i,v_j\in\mathcal{V},(v_i,v_j)\notin\varepsilon\}\)</span>, <span class="math inline">\(|\bar{\mathcal{M}_e}|=|\mathcal{M}_e|=m_e\)</span> . Then the SSL pretext task is to predict whether there exist a link between a given node pair.</p><p><span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\\\frac{1}{|\mathcal{M}_e|}\sum\limits_{(v_i,v_j)\in\mathcal{M}_e}\ell(f_w(|f_{\theta&#39;}(\mathcal{G})_{v_i}-f_{\theta&#39;}(\mathcal{G})_{v_j}|),1)+\frac{1}{|\bar{\mathcal{M}_e}|}\sum\limits_{(v_i,v_j)\in\bar{\mathcal{M}_e}}\ell(f_w(|f_{\theta&#39;}(\mathcal{G})_{v_i}-f_{\theta&#39;}(\mathcal{G})_{v_j}|),0)\)</span>, where <span class="math inline">\(\ell(\cdot,\cdot)\)</span> is the cross entropy loss, <span class="math inline">\(f_w\)</span> linearly maps to 1-dimension.</p></li><li><p>Expecting to help GNN learn information about local connectivity.</p></li></ul></li></ul><h5 id="global-structure-information"><strong>Global structure information</strong></h5><p>Not only based on the node itself or limited to its immediate local neighborhood, but also considering the position of the node in the graph.</p><ul><li>PairwiseDistance<ul><li>Maintain global topology information through a pairwise comparison. Or to say, pretext task will be able to distinguish/predict the distance between different node pairs.</li><li>The measurements of distance vary.</li><li>If use the shortest path length <span class="math inline">\(p_{ij}\)</span> as a measure of the distance, then for all node pairs <span class="math inline">\(\{(v_i,v_j)|v_i,v_j\in\mathcal{V}\}\)</span>, they are grouped into four categories: <span class="math inline">\(p_{ij}=\{1,2,3,\ge4\}\)</span>, <span class="math inline">\(4\)</span> is because of the computing price and accuracy (the more neighbors , the more unrelated noises are included.) Practically, randomly sample a certain amount of node pairs <span class="math inline">\(S\)</span> used for SSL during epoch. Then the SSL loss is <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|S|}\sum\limits_{(v_i,v_j)\in S}\ell(f_w(|f_{\theta&#39;}(\mathcal{G})_{v_i}-f_{\theta&#39;}(\mathcal{G})_{v_j}|),C_{p_{ij}})\)</span>, where <span class="math inline">\(C_{p_{ij}}\)</span> is the corresponding distance category of <span class="math inline">\(p_{ij}\)</span>.</li></ul></li><li><strong><em>Distance2Clusters</em></strong><ul><li>Predicting the distance from the unlabeled nodes to predefined graph clusters. Thus enforce the representations to learn a global positioning vector of each of the nodes.</li><li>First partitioning the graph to get <span class="math inline">\(k\)</span> clusters <span class="math inline">\(\{C_1,C_2,\cdots,C_k\}\)</span> by METIS graph partitioning methods. Denote the node with highest degree as center node <span class="math inline">\(c_j\)</span> in each cluster.</li><li>Formally, the SSL will optimize <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-d_i\|^2\)</span>, where <span class="math inline">\(\mathrm{d}_i\)</span> is the distance vector between node <span class="math inline">\(v_i\)</span> and each center.</li></ul></li></ul><h4 id="attribute-information-nodes-matrix-mathrmx">Attribute information (Nodes matrix <span class="math inline">\(\mathrm{X}\)</span>)</h4><p>Guide the GNN to ensure certain aspects of node/neighborhood attribute information is encoded in the node embeddings after a SSL attribute-based pretext.</p><ul><li><p>AttributeMask</p><ul><li>Let GNN learn attribute information via pretext</li><li>Randomly mask (e.g. set to zero ) the features of <span class="math inline">\(m_a\)</span> nodes <span class="math inline">\(\mathcal{M}_a\subset\mathcal{V}, |\mathcal{M}_a|=m_a\)</span>, then SSL will try to construct these features. Formally , <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{M}_a|}\sum\limits_{v_i\in\mathcal{M}_a}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-\mathrm{x}_i\|^2\)</span>, where <span class="math inline">\(\mathrm{x}_i\)</span> is the dense features after PCA.</li></ul></li><li><p>PairewiseAttrSim</p><ul><li><p>The similarity two nodes have in the input feature space is not guaranteed in the learned representations due to the GNN aggregating features from the two nodes local neighborhoods.</p></li><li><p>Specifically,</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210122233255.png" alt="image-20210122233252928" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210122233404.png" alt="image-20210122233403875" style="zoom:50%;" /></p></li><li><p>Only constrain the intra-class distance</p></li></ul></li></ul><h3 id="merge-pretext-task-on-graphs">Merge pretext task on Graphs</h3><ul><li><p>Joint Training</p><ul><li><p>Optimize the SSL loss (i.e., <span class="math inline">\(\mathcal{L}_{self}\)</span>) and supervised loss (i.e., <span class="math inline">\(\mathcal{L}_{task}\)</span>) simultaneously.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123174253.png" alt="image-20210123174253225" style="zoom:50%;" /></p></li><li><p>The overall objective is <span class="math inline">\(\min\limits_{\theta,\theta&#39;}\mathcal{L}_{task}(\theta,\mathrm{A,X},\mathcal{D}_L)+\lambda(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)\)</span>, where <span class="math inline">\(\lambda\)</span> is the hyperparameter to control the distribution of self-supervision.</p></li></ul></li><li><p>Two-stage training</p><p>Fine tuning the model which is pretrained on pretext task on downstream dataset.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123181120.png" alt="image-20210123181120517" style="zoom:50%;" /></p></li></ul><h2 id="analysis">Analysis</h2><ul><li><p>Targets: Understand what SSL information works for GNNs, which strategies can better integrate SSL for GNNs, and further analyze why SSL is able to improve GNNs</p></li><li><p>Datasets: Cora, Citeseer, Pubmed</p></li><li><p>Training: Adam, learning rate <span class="math inline">\(0.01\)</span>, <span class="math inline">\(L_2\)</span> regularization <span class="math inline">\(5e-4\)</span>, dropout rate <span class="math inline">\(0.5\)</span>, <span class="math inline">\(128\)</span> hidden units across all self-supervised information and GCN, top-K=bottom-K=<span class="math inline">\(3\)</span>. <span class="math inline">\(\lambda\)</span> in range <span class="math inline">\(\{0,0.001,0.01,0.1,1,5,10,50,100,500,1000\}\)</span>, <span class="math inline">\(m_e,m_a\)</span> in <span class="math inline">\(\{10\%,20\%\}\)</span> the size of <span class="math inline">\(|V|\)</span>.</p></li><li><p><strong>Two-stage training</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123200058.png" alt="image-20210123200058736" style="zoom:50%;" /></p><ul><li>the configuration of one graph convolutional layer for feature extraction, one graph convolutional layer for the adaptation of node classification and one linear layer for the adaptation of pretext task works very well for all three strategies</li><li>In most cases, the strategy of “Tune all" achieves the best performance--&gt; fine tune for downstream task is necessary.</li></ul></li><li><p><strong>SSL for GNNs</strong></p><ul><li><p><em>Joint training vs. Two-stage Training</em></p><p><strong><em>Joint training outperforms the Two-stage training in most settings.</em></strong></p></li><li><p><em>What SSL works for GNNs</em></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123202214.png" alt="image-20210123202214101" style="zoom:28%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123202231.png" alt="image-20210123202231858" style="zoom:30%;" /></p><ul><li>SSL for GNNs will improve the accuracy for downstream task</li><li>Across all datasets , the best performing method is a pretext task developed from <em>global structure information.</em></li><li>self-supervised information from both the structure and attributes have potentials</li><li>For the structure information, the global pretext tasks are likely to provide much more significant improvements compared to the local ones.</li></ul></li><li><p><em>Why SSL Works for GNNs</em></p><ul><li>GCN for node classification is naturally semi-supervised that has explored the unlabeled nodes, those (SSL pretext) failed to improve GCNs is argued resulted in GCN has already learned that information.</li><li>GCN is unable to naturally learn the global structure information and employing pairwise node distance prediction as the SSL task can help boost its performance for the downstream task.</li></ul></li><li><p>The capability of pretext representations maintaining similarity</p><p>The most popular similarity for graph is structural equivalence and regular equivalence (规则的等效节点是那些不一定具有相同邻居但具有自身相似的邻居的节点).</p><ul><li>Authors argue pretext task can maintain these two similarities by changing the definition of task (like nodes attribute task or distance between a pair of nodes can maintain structure similarity and regular equivalence.</li></ul></li></ul></li></ul><h2 id="advanced-pretext-task-on-graphs">Advanced pretext task on graphs</h2><p>Pretext tasks are built with the intuition of adapting the notion of regular equivalence to having neighbors with similar node labels (or regular task equivalence). Specifically, <strong>if every node constructs a pretext vector based on information in regards to the labels from their neighborhood, then two nodes having similar (or dissimilar) vectors will be encouraged to be similar (or dissimilar) in the embedding space.</strong></p><h3 id="proposed-tasks">Proposed Tasks</h3><h4 id="distance2labeled">Distance2Labeled</h4><ul><li>Modify Distance2Cluster. Propose to predict the distance vector from each node to the labeled nodes (i.e., <span class="math inline">\(\mathcal{V}_L\)</span>) as the pretext task. <em>For class <span class="math inline">\(c_j\in\{1，\cdots,K\}\)</span> and unlabeled node <span class="math inline">\(v_i\in\mathcal{V}_U\)</span>, the distance vector <span class="math inline">\(\mathrm{d}_i\)</span> for node <span class="math inline">\(v_i\)</span> is defined as three shortest path length (average, minimum, maximum) from <span class="math inline">\(v_i\)</span> to all labeled nodes in class <span class="math inline">\(c_i\)</span>.</em><br /></li><li>Formally, the objective is <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-d_i\|^2\)</span>.</li></ul><h4 id="contextlabel">ContextLabel</h4><ul><li>Considering the sparsity of labels, use similarity based function which utilize structure , attributes , and the current labeled nodes to <strong>construct a neighbor label distribution context vector <span class="math inline">\(\bar{\mathrm{y}}_i\)</span></strong> for each nodes as follows: <span class="math inline">\(f_s({\mathrm{A,X},\mathcal{D}_L,\mathcal{V}_U})\rightarrow\{\bar{\mathrm{y}}_i|v_i\in\mathcal{V}_U\}\)</span>. Specifically , the <span class="math inline">\(c\)</span>-th item of <span class="math inline">\(\bar{\mathrm{y}}\)</span> is: <span class="math inline">\(\bar{\mathrm{y}}_{ic}=\frac{|\mathcal{N}_{\mathcal{V}_L}(v_i,c)|+|\mathcal{N}_{\mathcal{V}_U}(v_i,c)|}{|\mathcal{N}_{\mathcal{V}_L}(v_i)|+|\mathcal{N}_{\mathcal{V}_U}(v_i)|},c=1,\cdots,K\)</span>. (For the neighbors of node <span class="math inline">\(v_i\)</span> (including unlabeled and labeled neighbors), the ratio of neighbors in class <span class="math inline">\(c\)</span>)</li><li>Formally, the objective is <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U)=\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-\mathrm{y}_i\|^2\)</span></li><li>The labels of nodes (aka <span class="math inline">\(f_s\)</span>) can be generated by LP (Label propagation ) or ICA (Iterative Classification Algorithm). But these will import weak labels that are too noisy.</li></ul><h4 id="ensemblelabel">EnsembleLabel</h4><ul><li>Ensemble various functions <span class="math inline">\(f_s\)</span>. <span class="math inline">\(\bar{y}_i=\arg\max_c\sigma_{LP}(v_i)+\sigma_{ICA}(v_i),c=1,\cdots,K\)</span></li><li>The objective is the same as ContextLabel method.</li></ul><h4 id="correctedlabel">CorrectedLabel</h4><ul><li>Enhance ContextLabel by iteratively improving the context vectors. GNN <span class="math inline">\(f_{\theta}\)</span> is trained on both the original (e.g., <span class="math inline">\(\bar{\mathrm{y}}_i\)</span>) and corrected (e.g., <span class="math inline">\(\hat{\mathrm{y}}_i\)</span>) context distributions.</li><li>Formally, the loss is <span class="math inline">\(\mathcal{L}_{self}(\theta&#39;,\mathrm{A,X},\mathcal{D}_U,\hat{\mathcal{D}}_U)=\\\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-\bar{\mathrm{y}}_i\|^2+\alpha(\frac{1}{|\mathcal{D}_U|}\sum\limits_{v_i\in\mathcal{D}_U}\|f_{\theta&#39;}(\mathcal{G})_{v_i}-\hat{\mathrm{y}}_i\|^2)\)</span>, where the 1st and second terms are to fit the original and corrected context distributions respectively, and <span class="math inline">\(\alpha\)</span> controls the contribution from the corrected context distribution. <span class="math inline">\(\hat{y}_i=\arg\max_c\frac{1}{p}\sum\limits_{l=1}^p\cos(f_{\theta&#39;}(\mathcal{G})_{v_i},\mathrm{z}_{cl}),c=1,\cdots,K\)</span>. Where <span class="math inline">\(p\)</span> indicates the prototype nodes in top-<span class="math inline">\(p\)</span> largest <span class="math inline">\(\rho\)</span> values, indicating <strong>the nodes if the measurements (<span class="math inline">\(\rho\)</span>) of their neighbors' label similarity is in top-<span class="math inline">\(p\)</span>.</strong>. Concretely, the similarity of labels' similarity is defined as <span class="math inline">\(\rho_i=\sum\limits_{j=1}^m\mathrm{sign}(\mathrm{S}_{ij}-S_c)\)</span>, where <span class="math inline">\(\mathrm{S}_{ij}\)</span> is the cosine similarity between two nodes based on their embeddings, <span class="math inline">\(S_c\)</span> indicating a constant value (which is selected as the value rank in top <span class="math inline">\(40\%\)</span> in <span class="math inline">\(\mathrm{S}\)</span>).</li><li>In other words, the average similarity between <span class="math inline">\(v_i\)</span> and <span class="math inline">\(p\)</span> prototypes is used to represent the similarity between <span class="math inline">\(v_i\)</span> and the corresponding class, and then assign the class <span class="math inline">\(c\)</span> having the largest similarity to <span class="math inline">\(v_i\)</span>.</li></ul><h3 id="experiments-for-evaluating">Experiments for evaluating</h3><ul><li><p>Experiment settings</p><ul><li>Datasets: Cora, Citeseer, Pubmed and Reddit</li><li>Model: 2-layer GCN as the backbone, with hidden units of 128, <span class="math inline">\(L_2\)</span> regularization <span class="math inline">\(5e−4\)</span>, dropout rate <span class="math inline">\(0.5\)</span> and learning rate <span class="math inline">\(0.01\)</span>. For the SSL loss, the hidden representations from the first layer of GCN are fed through a linear layer to solve SSL pretext task. Jointly train SSL and GCNs. <span class="math inline">\(\lambda\)</span> ranges in <span class="math inline">\(\{1, 5, 10, 50, 100, 500\}\)</span>. <span class="math inline">\(\alpha\)</span> ranges in <span class="math inline">\(\{0.5, 0.8, 1, 1.2, 1.5\}\)</span>.</li><li>Measurements: the average accuracy with standard deviation.</li></ul></li><li><p>Analysis</p><ul><li><p>Performance comparison: Though they argue the performance exist, but seems not that significant. They summarize: <strong>label correction can better</strong> extend label information to unlabeled nodes than ensemble, but it's much less inefficient. A tradeoff must be taken in.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124113911.png" alt="image-20210124113911642" style="zoom:50%;" /></p></li><li><p>Fewer Labeled Samples</p><ul><li><p>By randomly sampling 5 or 10 nodes per class for training and the same number of nodes for validation.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124114147.png" alt="image-20210124114147761" style="zoom:50%;" /></p></li><li><p><strong>SelfTask achieves even greater improvement when the labeled samples are fewer and consistently outperforms the state-of-the-art baselines.</strong></p></li><li><p><font color='red'>Why with fewer samples per class SelfTask can be even better?</font></p></li></ul></li><li><p>Parameter Analysis</p><ul><li><p>Only the sensitivity of the best model <em>SelfTaskCorrectedLabel-ICA</em> is evaluated. Vary <span class="math inline">\(\lambda\)</span> in the range of <span class="math inline">\(\{0, 0.1, 0.5, 1, 5, 10, 50, 100\}\)</span> and <span class="math inline">\(\alpha\)</span> from 0 to 2.5 with an interval of 0.25.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124114646.png" alt="image-20210124114646842" style="zoom:50%;" /></p></li><li><p>The performance of this model first increases with the increase of <span class="math inline">\(\lambda\)</span>, which controls the contribution of SSL pretext task.</p></li><li><p>The using of correction is confirmed.</p></li><li><p>They don't report sensitivity on other datasets, which should have been in supplementary.</p></li></ul></li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Present detailed empirical study to understand when and why SSL works for GNNs and which strategy can better work with GNNs.</li><li>Propose a new direction SelfTask to build advanced pretext tasks which further exploit task-specific self-supervised information, and demonstrate that our advanced method achieves state-of-the-art performance.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2006.10141.pdf&quot;&gt;Self-supervised Learning on Graphs: Deep Insights and New Directions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ChandlerBang/SelfTask-GNN&quot;&gt;Codes&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Predicting What You Already Know Helps, Provable Self-Supervised Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html</id>
    <published>2021-01-11T20:00:00.000Z</published>
    <updated>2021-01-12T19:38:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2008.01064.pdf">Predicting What You Already Know Helps: Provable Self-Supervised Learning</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li><p>Pretext tasks</p><ul><li><strong>Reconstruct images from corrupted versions or just part it</strong>: including denoising auto-encoders, image inpainting, and split-brain autoencoder</li><li><strong>Using visual common sense</strong>, including predicting rotation angle, relative patch position, recovering color channels, solving jigsaw puzzle games, and discriminating images created from distortion.</li><li><strong>Contrastive learning</strong>: learn representations that <strong>bring similar data points closer</strong> while pushing randomly selected points further away or <strong>maximize a contrastive-based mutual information lower bound</strong> between different views</li><li><strong>Create auxiliary tasks</strong>: The natural ordering or topology of data is also exploited in video-based, graph-based or map-based self-supervised learning. For instance, the pretext task is to determine the correct temporal order for video frames.</li></ul></li><li><p>Theory for self-supervised learning: contrastive learning</p><ul><li>Contrastive learning may not work when conditional independence holds only with additional latent variables</li></ul><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th style="text-align: left;">Theory</th><th style="text-align: left;">Limitations</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Shows shows guarantees for contrastive learning representations on linear classification tasks using a class conditional independence assumption</td><td style="text-align: left;">Not handle approximate conditional independence</td></tr><tr class="even"><td style="text-align: left;">Contrastive learning representations can linearly recover any continuous functions of the underlying topic posterior under a topic modeling assumption for text</td><td style="text-align: left;">The <strong>assumption of independent sampling of words</strong> that they exploit is <strong>strong</strong> and <strong>not generalizable to other domains</strong> like images</td></tr><tr class="odd"><td style="text-align: left;">Studies contrastive learning on the hypersphere through intuitive properties like alignment and uniformity of representations</td><td style="text-align: left;">No connection made to downstream tasks</td></tr><tr class="even"><td style="text-align: left;">A mutual information maximization view of contrastive learning</td><td style="text-align: left;">Some issues point by paper [45]</td></tr><tr class="odd"><td style="text-align: left;">Explain negative sampling based methods use the theory of noise contrastive estimation</td><td style="text-align: left;"><strong>guarantees are only asymptotic and not for downstream tasks.</strong></td></tr><tr class="even"><td style="text-align: left;">Conditional independence assumptions and redundancy assumptions on multiple views are used to analyze co-training</td><td style="text-align: left;">not for downstream task</td></tr></tbody></table></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Forming the pretext tasks:<ul><li>Colorization: can be interpreted as <span class="math inline">\(p(X_1,X_2|Y)=p(X_1|Y)\times p(X_2|Y)\)</span>, aka <span class="math inline">\(X_1,X_2\)</span> are independently conditioned on <span class="math inline">\(Y\)</span></li><li>Inpainting: <span class="math inline">\(p(X_1,X_2|Y,Z)=p(X_1|Y,Z)\times p(X_2|Y,Z)\)</span>,aka the inpainted <span class="math inline">\(X_2\)</span> is conditionally independent of <span class="math inline">\(X_2\)</span> (the remainder) given <span class="math inline">\(Y,Z\)</span>.</li></ul></li><li>The only way to solve the pretext task is to first implicitly predict <span class="math inline">\(Y\)</span> and then predict <span class="math inline">\(X_2\)</span> from <span class="math inline">\(Y\)</span></li></ul></li><li>Limitations:<ul><li>The underlying principles of self-supervised learning are still mysterious since it is a-priori unclear why predicting what we already know should help.</li></ul></li></ul><h2 id="goals">Goals</h2><p><strong><em>What conceptual connection between pretext and downstream tasks ensures good representations?</em></strong></p><p><strong><em>What is a good way to quantify this?</em></strong></p><h2 id="how">How?</h2><h3 id="notations">Notations</h3><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th style="text-align: left;">Symbol</th><th style="text-align: left;">Meaning</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><span class="math inline">\(\mathbb{E}^L[Y|X]\)</span></td><td style="text-align: left;">the best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span></td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(\Sigma_{XY|Z}\)</span></td><td style="text-align: left;">partial covariance matrix between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z\)</span></td></tr><tr class="odd"><td style="text-align: left;"><span class="math inline">\(X_1,X_2\)</span></td><td style="text-align: left;">the input variable and the target random variable for the pretext tasks</td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(Y\)</span></td><td style="text-align: left;">label for the downstream task</td></tr><tr class="odd"><td style="text-align: left;"><span class="math inline">\(P_{X_1X_2Y}\)</span></td><td style="text-align: left;">the joint distribution over <span class="math inline">\(\mathcal{X}_1 \times \mathcal{X}_2 \times \mathcal{Y}\)</span></td></tr></tbody></table><h3 id="idea">Idea</h3><ul><li>Under approximate condition independence (CI) (quantified by the norm of a certain partial covariance matrix), show similar sample complexity improvements.</li><li>Testify pretext task helps when CI is approximately satisfied in text domain.</li><li>Demonstrate on a real-world image dataset that a pretext task-based linear model outperforms or is comparable to many baselines.</li></ul><h3 id="formalize-ssl-with-pretext-task">Formalize SSL with pretext task</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111171426.png" alt="image-20210111171415185" style="zoom:50%;" /></p><p>It will be estimated by:</p><ul><li><strong>approximation erro</strong>r:<img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173318.png" alt="image-20210111173318840" style="zoom:33%;" />, where <span class="math inline">\(f^*=\mathbb{E}[Y|X_1]\)</span> is the optimal predictor for the task</li><li><strong>estimation error</strong>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173520.png" alt="image-20210111173520930" style="zoom:25%;" />, it's the difference between Predicting <span class="math inline">\(Y\)</span> directly by <span class="math inline">\(X_1\)</span> and Predicting by the representations from pretext task</li></ul><h2 id="experiments">Experiments</h2><ul><li></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks</li><li>Quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) <strong>allows us to learn representations that can solve the downstream task with drastically reduced sample complexity</strong> by just training a linear layer on top of the learned representation.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2008.01064.pdf&quot;&gt;Predicting What You Already Know Helps: Provable Self-Supervised Learning&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Colorful Image Colorization</title>
    <link href="http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html</id>
    <published>2021-01-10T22:15:39.000Z</published>
    <updated>2021-01-12T21:19:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1603.08511.pdf">Colorful Image Colorization</a></p><p>Code <a href="http://richzhang.github.io/colorization/">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>Predicting colors in free way: taking the image’s <span class="math inline">\(L\)</span> channel as input and its <span class="math inline">\(ab\)</span> channels as the supervisory signal--&gt; but tend to look desaturated, one explanation is using loss functions that encourage conservative predictions</p><ul><li><p>Non-parametric methods: given an input grayscale image, first define one or more color reference images. Then, transfer colors onto the input image from analogous regions of the reference image(s).</p></li><li><p>Parametric methods: learn prediction functions from large datasets of color images at training time, posing the problem as either regression onto continuous color space or classification of quantized color values. --&gt; Work in this paper is also classification task.</p></li><li><p>Concurrent work on colorization</p><table><colgroup><col style="width: 10%" /><col style="width: 37%" /><col style="width: 45%" /><col style="width: 6%" /></colgroup><thead><tr class="header"><th>Paper</th><th style="text-align: center;">loss</th><th>CNNs</th><th>Dataset</th></tr></thead><tbody><tr class="odd"><td>Larsson et al.</td><td style="text-align: center;">un-rebalanced classification loss</td><td>hypercolumns on a VGG</td><td>ImageNet</td></tr><tr class="even"><td>Iizuka et al.</td><td style="text-align: center;">regression loss</td><td>two-stream architecture in which fuse global and local features</td><td>Places</td></tr><tr class="odd"><td>This paper</td><td style="text-align: center;">classification loss, with rebalanced rare classes,</td><td>a single-stream, VGG-styled network with added depth and dilated convolutions</td><td>ImageNet</td></tr></tbody></table></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Even in gray images, the semantics of the scene and its surface texture provide ample cues for many regions in each image</li><li>Color prediction is inherently multimodal --&gt; sparks for a loss tailored to their work</li></ul></li><li>Limitations:<ul><li>Loss only cares Euclidean distance: If an object can take on a set of distinct ab values, the optimal solution to the Euclidean loss will be the mean of the set. In color prediction, this averaging effect favors grayish, desaturated results. Additionally, if the set of plausible colorizations is non-convex, the solution will in fact be out of the set, giving implausible results.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Design colorization based pretext task to get a good image semantic representations: <strong>produce a plausible colorization that could potentially fool a human observer</strong></p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li><strong>Produce vibrant colorization</strong>: Predict a distribution of possible colors for each pixel. Then, re-weight the loss at training time to emphasize rare colors. This encourages the model to exploit the full diversity of the large-scale data on which it is trained. Lastly, produce a final colorization by taking the annealed mean of the distribution.</li><li>Evaluate synthesized images: set up a “colorization Turing test”.</li></ul><h3 id="data-preparation">Data Preparation</h3><ul><li>Quantize the <span class="math inline">\(ab\)</span> output space into bins with grid size <span class="math inline">\(10\)</span> and keep the <span class="math inline">\(Q = 313\)</span> values which are in-gamut. Then this is the label <span class="math inline">\(Z\)</span> of each pixel. Formally, denote the raw label as <span class="math inline">\(Y\)</span>, then <span class="math inline">\(Z = H^{−1}_{gt} (Y)\)</span>, which converts ground truth color <span class="math inline">\(Y\)</span> to vector <span class="math inline">\(Z\)</span>.</li></ul><h3 id="implementation">Implementation</h3><ul><li><p>Model</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110175555.png" alt="image-20210110175554683" style="zoom:30%;" /></p></li><li><p>Loss function</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210112160157.png" alt="image-20210110180438146" style="zoom:33%;" /></p><p>where <span class="math inline">\(v(·)\)</span> is a weighting term that can be used to re-balance the loss based on color-class rarity.</p><ul><li><p>Re-balancing</p><p>The distribution of <span class="math inline">\(ab\)</span> values in natural images is strongly biased towards values with low <span class="math inline">\(ab\)</span> values.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110233506.png" alt="image-20210110233506301" style="zoom:33%;" />, where <span class="math inline">\(\tilde{p}\)</span> the empirical probability of colors in the quantized ab space <span class="math inline">\(p\in \Delta Q\)</span> from the full ImageNet training set and smooth the distribution with a Gaussian kernel <span class="math inline">\(G_{\sigma}\)</span>. <span class="math inline">\(\lambda=\frac{1}{2}, \sigma=5\)</span> worked well.</p></li></ul></li><li><p>Inferring point estimates</p><p>map probability distribution <span class="math inline">\(\hat{Z}\)</span> to color values <span class="math inline">\(\hat{Y}\)</span> with function <span class="math inline">\(\hat{Y} = H(\hat{Z})\)</span></p></li></ul><p>They interpolate by re-adjusting the temperature <span class="math inline">\(T\)</span> of the softmax distribution, and taking the mean of the result. Lowering the temperature <span class="math inline">\(T\)</span> produces a more strongly peaked distribution, and setting <span class="math inline">\(T\rightarrow 0\)</span> results in a 1-hot encoding at the distribution mode. They find that <span class="math inline">\(T=0.38\)</span> captures the vibrancy of the mode while maintaining the spatial coherence of the mean.</p><h2 id="experiments">Experiments</h2><ul><li>Dataset: ImageNet</li><li>Base models</li></ul><table style="width:100%;"><colgroup><col style="width: 11%" /><col style="width: 41%" /><col style="width: 47%" /></colgroup><thead><tr class="header"><th>Model Name</th><th style="text-align: center;">Loss</th><th>Train</th></tr></thead><tbody><tr class="odd"><td>Ours(full)</td><td style="text-align: center;">classification loss</td><td>from scratch with kmeans initialization, ADAM solver for about 450K iterations. <span class="math inline">\(\beta_1 = .9, \beta_2 = .99\)</span>, and weight decay = <span class="math inline">\(10^{−3}\)</span> . Initial learning rate was <span class="math inline">\(3 × 10^{−5}\)</span> and dropped to <span class="math inline">\(10^{−5}\)</span> and <span class="math inline">\(3 × 10^{−6}\)</span> when loss plateaued, at 200k and 375k iterations, respectively.</td></tr><tr class="even"><td>Ours(class)</td><td style="text-align: center;">classification loss withou rebalancing (<span class="math inline">\(\lambda=1\)</span>)</td><td>similar training protocol as Ours(full)</td></tr><tr class="odd"><td>Ours(L2)</td><td style="text-align: center;">L2 regression loss</td><td>same training protocol</td></tr><tr class="even"><td>Ours(L2,ft)</td><td style="text-align: center;">L2 regression loss</td><td>fine tuned from our full classification with rebalancing network</td></tr><tr class="odd"><td>Larsson et al.</td><td style="text-align: center;"></td><td>CNN method</td></tr><tr class="even"><td>Dahl</td><td style="text-align: center;">L2 regression loss</td><td>a Laplacian pyramid on VGG features</td></tr><tr class="odd"><td>Gray</td><td style="text-align: center;">--</td><td>every pixel is gray, with <span class="math inline">\((a, b) = 0\)</span></td></tr><tr class="even"><td>Random</td><td style="text-align: center;">--</td><td>Copies the colors from a random image from the training set</td></tr></tbody></table><h3 id="colorization-quality">Colorization quality</h3><ul><li>AMT: participants confirm their results. They argue that their work produce a more prototypical appearance for those are poorly white balanced</li><li>Semantic interpretability (VGG classification): Are the results realistic enough colorizations to be interpretable to an off-the-shelf object classifier? They check it by by feeding their fake colorized images to a VGG network that was trained to predict ImageNet classes from real color photos.<ul><li>The result is <span class="math inline">\(3.4\%\)</span> lower than Larsson's.</li><li>Without any additional training or fine-tuning, one can improve performance on grayscale image classification, simply by colorizing images with our algorithm and passing them to an off-the-shelf classifier.</li></ul></li><li>Raw accuracy (AuC):<ul><li>L2 metric can achieve accurate colorizations, but has difficulty in optimization from scratch</li><li>class-rebalancing in the training objective achieved its desired effect</li></ul></li><li>Compared with others<ul><li>LEARCH: On SUN dataset, authors have <span class="math inline">\(17.2\%\)</span> on AMT task while LEARCH has <span class="math inline">\(9.8\%\)</span></li></ul></li></ul><h3 id="cross-channel-encoding-as-ssl-feature-learning">Cross-channel encoding as SSL Feature learning</h3><ul><li>Datasets: ImageNet, PASCAL (fine tuned after training on ImageNet)</li><li>Backbone: AlexNet</li><li>Settings<ul><li>ImageNet: fixing the extractor and retrain the classifier (softmax layer) by labels</li><li>PASCAL: : (1) keeping the input grayscale by disregarding color information (Ours (gray)) and (2) modifying conv1 to receive a full 3-channel <span class="math inline">\(Lab\)</span> input, initializing the weights on the <span class="math inline">\(ab\)</span> channels to be zero (Ours (color)).</li></ul></li><li>Summary<ul><li>For ImageNet, there is a <span class="math inline">\(6\%\)</span> performance gap between color and grayscale inputs. Except for the 1st layer, representations from other deeper layers catch and outperform most methods, indicating that <strong>solving the colorization task encourages representations that linearly separate semantic classes in the trained data distribution</strong></li><li>On PASCAL, when conv1 is frozen, the network is effectively only able to interpret grayscale images.</li></ul></li></ul><h3 id="the-properties-of-network">The properties of network</h3><ul><li><p>Is it exploiting low-level cues?</p><p>Given a grayscale Macbeth color chart as input, it was unable to recover its colors. On the other hand, given two recognizable vegetables that are roughly <strong>isoluminant</strong>, <strong>the system is able to recover their color</strong>.</p></li><li><p>Does it learn multimodal color distributions ?</p><p>Take effective dilation ( the spacing at which consecutive elements of the convolutional kernel are evaluated, relative to the input pixels, and is computed by the product of the accumulated stride and the layer dilation) as the measurement. Through each convolutional block from conv1 to conv5, the effective dilation of the convolutional kernel is increased. From conv6 to conv8, the effective dilation is decreased</p></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Designing an appropriate objective function that handles the multimodal uncertainty of the colorization problem and captures a wide diversity of colors</li><li>Introducing a novel framework for testing colorization algorithms, potentially applicable to other image synthesis tasks</li><li>Setting a new high-water mark on the task by training on a million color photos.</li><li>Introduce the colorization task as a competitive and straightforward method for self-supervised representation learning, achieving state-of-the-art results on several benchmarks.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1603.08511.pdf&quot;&gt;Colorful Image Colorization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;http://richzhang.github.io/colorization/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Revisiting Self-Supervised Visual Representation Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html</id>
    <published>2021-01-09T21:21:00.000Z</published>
    <updated>2021-01-12T19:33:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf">Revisiting Self-Supervised Visual Representation Learning</a></p><p>Code <a href="https://github.com/google/revisiting-self-supervised">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>robotics: the result of interacting with the world, and the fact that multiple perception modalities simultaneously get sensory inputs are strong signals for pretext</li><li>videos: the synchronized cross-modality stream of audio, video, and potentially subtitles, or of the consistency in the temporal dimension</li><li>image datasets:<ul><li>Patch-based methods: E.g.: predicting the relative location of image patches; "jigsaw puzzle"</li><li>Image-level classification tasks:<ul><li>RotNet, create class labels by clustering images, image inpaiting, image colorization, split-brain and motion segmentation prediction;</li><li>Enforce structural constraints on the representation space: an equivariance relation to match the sum of multiple tiled representations to a single scaled representation; predict future patches in via autoregressive predictive coding</li><li>Combining multiple pretext task: E.g. extend the “jigsaw puzzle” task by combining it with colorization and inpainting; Combining the jigsaw puzzle task with clustering-based pseudo labels ( Jigsaw++) ; make one single neural network learn all of four different SSL methdos in a multi-task setting; combined the selfsupervised loss GANs objective</li></ul></li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>Expensive labeled data for supervised task</li></ul></li><li>Limitations:<ul><li>Previous works mostly concentrate on pretext task, but didn't pay much attention to the choice of backbones etc.</li></ul></li></ul><h2 id="goals">Goals</h2><p><strong>An optimal CNN architecture for pretext task</strong>, investigating the influence of architecture design on the representation quality.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>a comparison of different self-supervision methods using a unified neural network architecture, but with the goal of combining all these tasks into a single self-supervision task</li></ul><h3 id="implementation">Implementation</h3><h4 id="family-of-cnns">Family of CNNs</h4><ul><li><p>variants of ResNet:</p></li><li><p><strong>ResNet50</strong>, the output before task-specific logits layer is named as <span class="math inline">\(pre-logits\)</span>. explore <span class="math inline">\(k \in \{4, 8, 12, 16\}\)</span>, resulting in pre-logits of size <span class="math inline">\(2048, 4096, 6144\)</span> and <span class="math inline">\(8192\)</span> respectively. <span class="math inline">\(k\)</span> is the widening factor.</p></li><li><p><strong>ResNet v1</strong>: ???batch normalization (BN) right after each convolution and before activation???</p></li><li><p><strong>ResNet v2</strong>: ?</p></li><li><p><strong>ResNet (-)</strong>: without ReLU preceding the global average pooling</p></li><li><p>a batch-normalized <strong>VGG</strong> architecture since VGG is structurally close to AlexNet. BN between CNN and activation, VGG19.</p></li><li><p><strong>RevNets</strong>: stronger invertibility guarantees so as to compare with ResNets. The residual unit used here is equivalent to double application of the residual unit.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110133429.png" alt="image-20210110133429332" style="zoom:50%;" />, check <a href="https://papers.nips.cc/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf">here</a> for details. Apart from this slightly complex residual unit, others are the same as ResNet.</p></li></ul><h4 id="family-of-pretext-tasks">Family of pretext tasks</h4><ul><li><strong>Rotation</strong>: same as RotNet, <span class="math inline">\(\{0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}\}\)</span></li><li><strong>Exemplar</strong>: triplet loss</li><li><strong>Jigsaw</strong>: recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches was performed. Patches are sampled with a random gap between them. Each patch is then independently converted to grayscale with probability <span class="math inline">\(\frac{2}{3}\)</span> and normalized to zero mean and unit standard deviation. Extract final image representations by averaging representations of 9 cropped patches.</li><li><strong>Relative patch location</strong>: predicting the relative location of two given patches of an image. Extract final image representations by averaging representations of 9 cropped patches.</li></ul><h4 id="evaluation-of-the-quality-of-learned-representations">Evaluation of the quality of learned representations</h4><ul><li>Idea: <strong>Using learned representations for training a linear logistic regression model to solve multiclass image classification tasks</strong> (downstream tasks). All representations come from pre-logits level.</li><li>Details: the linear logistic regression model is trained by L-BFGS. But for comparison, using SGD with momentum and use data augmentation during training.</li></ul><h2 id="experiments">Experiments</h2><ul><li>Datasets</li></ul><table><colgroup><col style="width: 12%" /><col style="width: 15%" /><col style="width: 72%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Train</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>ImageNet</td><td style="text-align: center;">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr><tr class="even"><td>Places 205</td><td style="text-align: center;">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr></tbody></table><h3 id="pretext-cnns-downstream">Pretext? CNNs? Downstream?</h3><ul><li><p>Pretext and its preferred CNN architecture: <strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110151545.png" alt="image-20210110151545316" style="zoom:30%;" /></p></li><li><p>The generalization of representations from pretext tasks: each pretext task can be generalized to other dataset. Check the trendings in figure 2.</p></li><li><p>Optimal CNNs for Pretext and downstream tasks: not consistent. But after selecting the right architecture for each self-supervision and increasing the widening factor, models significantly outperform previously reported results.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155158.png" alt="image-20210110155157874" style="zoom:45%;" /></p></li><li><p><strong>Better performance on the pretext task does not always translate to better representations</strong>: Performance on pretext cannot be used to reliably select the model architecture.</p></li></ul><h3 id="cnns-architecture">CNNs architecture</h3><ul><li>Skip-connection: For VGG, representations deteriorate towards the end of the network cause models specialize to the pretext task in the later layers. ResNet prevent this deterioration. They argue that this is because <strong>ResNet’s residual units being invertible under some conditions</strong> and confirm this by RevNet.</li><li>Depth of CNNs: For residual architectures, the pre-logits are always best.</li><li>Model-width and representation size:<ul><li>whether the increase in performance is due to increased network capacity or to the use of higher-dimensional representations, or to the interplay of both? To answer it, authors disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer.</li><li><strong>Model-width and representation size both matter independently, and larger is always better.</strong></li><li>SSL techniques are likely to <strong>benefit from using CNNs with increased number of channels</strong> across wide range of scenarios, even under low-data regime.</li></ul></li></ul><h3 id="evaluate-the-quality-of-representations">Evaluate the quality of representations</h3><ul><li><strong>A linear model is adequate</strong>: MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged</li></ul><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155830.png" alt="image-20210110155829820" style="zoom:50%;" /></p><ul><li>To train <strong>linear model, SGD optimization hyperparameters:</strong> very long training (≈ 500 epochs) results in higher accuracy</li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Most affect performance in the fully labeled setting, may significantly affect performance in the selfsupervised setting.</li><li>the quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</li><li>Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations</li><li>The evaluation procedure, where a linear model is trained on a fixed visual representation using stochastic gradient descent, is sensitive to the learning rate schedule and may take many epochs to converge</li><li><strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong>--&gt;<strong>pretext tasks for self-supervised learning</strong> should not <strong>be considered</strong> in isolation, but <strong>in conjunction with underlying architectures</strong>.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&quot;&gt;Revisiting Self-Supervised Visual Representation Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/google/revisiting-self-supervised&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Representation Learning by Predicting Image Rotations</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html</id>
    <published>2021-01-09T04:11:12.000Z</published>
    <updated>2021-01-12T20:56:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1803.07728.pdf">Unsupervised Representation Learning by Predicting Image Rotations</a></p><p>Codes <a href="https://github.com/gidariss/FeatureLearningRotNet">here</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>How to get <strong>a high-level image semantic representation using unlabeled data</strong></p><ul><li>SSL: defines an annotation free pretext task, has been proved as good alternatives for transferring on other vision tasks. E.g.: colorize gray scale images, predict the relative position of image patches, predict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames.</li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li><strong>the attention maps are equivariant w.r.t. the image rotations, check appendix A.</strong></li></ul></li><li>Limitations<ul><li>supervised feature learning has the main limitation of requiring intensive manual labeling effort</li></ul></li></ul><h2 id="goals">Goals</h2><p>Provide a "self-supervised" formulation for image data, a <strong>self defined supervised task involving predicting the transformations used for image.</strong> The model won't have access to the initial image.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>Concretely, define the geometric transformations as the image rotations by 0, 90, 180, and 270 degrees. Thus, the ConvNet model is trained on the 4-way image classification task of recognizing one of the four image rotations.</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li><p>2D image Rotation</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109124726.png" alt="image-20210109124726420" style="zoom:40%;" /></p><table><thead><tr class="header"><th>Operations</th><th style="text-align: center;">Implementation</th></tr></thead><tbody><tr class="odd"><td>+90</td><td style="text-align: center;">transpose then flip vertically</td></tr><tr class="even"><td>+180</td><td style="text-align: center;">flip vertically then flip horizontally</td></tr><tr class="odd"><td>+270</td><td style="text-align: center;">flip vertically then transpose</td></tr></tbody></table></li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109123815.png" alt="image-20210109123815880" style="zoom:45%;" />, where <span class="math inline">\(F(\cdot)\)</span> is the predicted probability of the geometric transformation with label <span class="math inline">\(y\)</span> and <span class="math inline">\(\theta\)</span> are the learnable parameters of model <span class="math inline">\(F(\cdot)\)</span>, and <span class="math inline">\(g(X_i|y)\)</span> is the transformed image with transformation <span class="math inline">\(y\)</span>.</p></li><li><p><strong>Why does it works?</strong></p><ul><li>To work for this pretext task, extractor has to <strong>understand the concept of the objects depicted in the image</strong>. Models must learn to localize salient objects in the image, recognize their orientation and object type, and then relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images.</li><li>Easy to be implemented by flipping and transpose, no chance for importing low-level visual artifacts so as to avoid trivial features (which have no practical value)</li><li>Operations are easy to be recognized manually.</li></ul></li></ul><h2 id="experiments">Experiments</h2><h3 id="cifar-object-recognition">CIFAR: object recognition</h3><ul><li>Dataset:</li></ul><table><thead><tr class="header"><th>Datasets</th><th>Preprocess</th></tr></thead><tbody><tr class="odd"><td>CIFAR-10</td><td>Rotations</td></tr></tbody></table><ul><li>Training: SGD with batch size 128, momentum 0.9, weight decay <span class="math inline">\(5e−4\)</span> and <span class="math inline">\(lr\)</span> of 0.1. We drop the learning rates by a factor of 5 after epochs 30, 60, and 80. 100 epochs. Each time feeding with all 4 images.</li><li>Summary<ul><li><em>The learned feature hierarchies</em>: convnet with different number of layers. <strong>Representations from the 2nd block</strong> are good, and <strong>increasing the total depth</strong> of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers.</li><li><em>The quality of the learned features w.r.t. the number of recognized rotations</em>: 4 discrete rotations outperform.</li><li><em>Compared with previous work</em> : almost the same as the NIN supervised model. Fine-tuned the unsupervised learned features further improves the classification performance.</li><li><em>Correlation between object classification task and rotation prediction task</em>: The representations from pretext make classifier converge faster compared with the classifier trained from scratch.</li><li><em>Semi-supervised setting</em>: pretrained on the whole dataset without labels, then fine-tuned on a small labeled subset. It exceeds the supervised model when the number of examples per category drops below 1000.</li></ul></li></ul><h3 id="others-classification-object-detection-segmentation">Others: classification, object detection , segmentation</h3><ul><li><p>Dataset: ImageNet, Places, and PASCAL VOC.</p><table><colgroup><col style="width: 24%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th>Task</th><th>Datasets</th></tr></thead><tbody><tr class="odd"><td>Classification</td><td>Pretrained on ImageNet, then test on ImageNet, Places, and PASCAL VOC.</td></tr><tr class="even"><td>Object detection</td><td>PASCAL VOC</td></tr><tr class="odd"><td>Object segmentation</td><td>PASCAL VOC</td></tr></tbody></table></li><li><p>Backbones: AlexNet without local response normalization units, dropout units, or groups in the colvolutional layers while it includes batch normalization units after each linear layer</p></li><li><p>Pretrained: on ImageNet, SGD with batch size 192, momentum 0.9, weight decay <span class="math inline">\(5e − 4\)</span> and <span class="math inline">\(lr\)</span> of 0.01. Learning rates are dropped by a factor of 10 after epochs 10, and 20 epochs. Trained in total for 30 epochs.</p></li><li><p>Summary:</p><ul><li><p>ImageNet classification task: surpasses all the other unsupervised methods by a significant margin, narrows the performance gap between unsupervised features and supervised features.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154626.png" alt="image-20210109154626599" style="zoom:50%;" /></p></li><li><p>Transfer learning evaluation on PASCAL VOC: fine tuning, used weight rescaling proposed by Krahenbuhl et al.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154831.png" alt="image-20210109154831162" style="zoom:60%;" /></p></li><li><p>Places classification task: the learnt features are evaluated w.r.t. their generalization on classes that were “unseen” during the unsupervised training phase</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154953.png" alt="image-20210109154952935" style="zoom:50%;" /></p></li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Propose a new self-supervised task that is very simple and at the same time.</li><li>Rotationsod under various settings (e.g. semi-supervised or transfer learning settings) and in various vision tasks (i.e., CIFAR-10, ImageNet, Places, and PASCAL classification, detection, or segmentation tasks).</li><li>They argue this self-supervised formulation demonstrates state-of-the-art results with dramatic improvements w.r.t. prior unsupervised approaches, and narrows the gap between unsupervised and supervised feature learning.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1803.07728.pdf&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Codes &lt;a href=&quot;https://github.com/gidariss/FeatureLearningRotNet&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Visual Representation Learning by Context Prediction</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html</id>
    <published>2021-01-08T21:22:12.000Z</published>
    <updated>2021-01-12T19:33:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf">Unsupervised Visual Representation Learning by Context Prediction</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><p>How to get <strong>a good image representation</strong></p><ul><li>The latent variables of an appropriate generative model. --&gt; generative models<ul><li>But given an image, inferring the latent structure is <strong>intractable</strong> for even relatively simple models --&gt; to fix, use <strong>sampling</strong> to perform approximate inference.</li></ul></li><li>An embedding that can discriminate the semantics in images by distances of them. -- create a supervised "pretext" task. But hard to tell whether the predictions themselves are correct.<ul><li>Reconstruction-based: E.g., denoising autoencoders (reconstruction ), sparse autoencoders (reconstruction + sparsity penalty )</li><li>Context prediction: "skip-gram" to "filling the blank" task, and convert the prediction task to discriminate task like discriminating between real images vs. images where one patch has been replaced by a random patch from elsewhere in the dataset. But not hard enough for high-level representations</li><li>Discover object categories using hand-crafted features and various forms of clustering. But they will lose shape information. To keep more shape information, some take contour extraction or defining similarity metrics.</li><li>Video-based: since the identity of objects remains unchanged -</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>the difficulties for generalizing CNNs models on Internet -scale datasets</li><li>context has proven to be a powerful source of automatic supervisory signal for learning representations --&gt; context can be regarded as a 'pretext' task to force the model to learn a good word embedding</li><li>current reconstruction-based algorithms struggle with low-level phenomena, like stochastic textures, making it hard to even measure whether a model is generating well.</li></ul></li><li>Limitations:<ul><li>generative models are rather efficiently on smaller datasets but burden on high-resolution natural images</li><li>Some are too simple for extracting high-level representations</li><li>Hard to tell whether the model has obtained good representations.</li></ul></li></ul><h2 id="goals">Goals</h2><p>Provide a "self-supervised" formulation for image data, a supervised task involving predicting the context for a patch.</p><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>Hypothesis: Doing well on predicting patches' positions requires understanding scenes and objects--&gt; a good visual representation</li><li>Concretely, sample random pairs of patches in one of eight spatial configurations, and present each pair to a machine learner. The algorithm must then guess the position of one patch relative to the other.</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li><p>Two patches are fed into network</p></li><li><p>Given an image, one patch will be sampled uniformly, then according to the position of this sampled patch, then 2nd patch will be sampled randomly from the eight possible neighboring locations.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108181759.png" alt="image-20210108181757911" style="zoom:33%;" /></p></li><li><p>including a gap between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</p></li><li><p>For some images ( chromatic aberration), after solving the relative location task (like by detecting the separation between green and magenta (red + blue). ), this problem will be relaxed.</p><ul><li>Shift green and magenta toward gray</li><li>Color dropping : randomly drop 2 of the 3 color channels from each patch and replace them by gaussian noise.</li></ul></li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Siamese network based on AlexNet. But not all layers share weights, LRN (local response normalization ) layers won't.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108183110.png" alt="image-20210108183110662" style="zoom:33%;" /></p></li><li><p><strong>Why does it works?</strong></p><ul><li>Avoid 'trivial' shortcuts (like boundary patterns or textures continuing between patches)--&gt; including a gap (up to 48 pixels) between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</li><li>Enhancing performance on images with chromatic aberration.</li></ul></li></ul><h2 id="experiments">Experiments</h2><p>Pre-Training: SGD+BN+high momentum, 4 weeks on K40 GPU.</p><table><colgroup><col style="width: 8%" /><col style="width: 28%" /><col style="width: 63%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Resizing</th><th>Preprocess</th></tr></thead><tbody><tr class="odd"><td>ImageNet</td><td style="text-align: center;"><span class="math inline">\(150K\sim450K\)</span> total pixels</td><td>1. sample patches at resolution <span class="math inline">\(96\times 96\)</span><br />2. mean subtraction, projecting or dropping colors, and randomly downsampling some patches to as little as 100 total pixels, and then upsampling it.</td></tr></tbody></table><h3 id="ability-on-semantic">Ability on semantic</h3><p>Does it get similar representations for patches with similar semantics?</p><ul><li><p>check nearest neighbors by normalized correlation of <span class="math inline">\(fc6\)</span>'s output. Compared with results from random initialized model and ImageNet AlexNet.</p></li><li><p>Summary</p><ul><li>in a few cases, random (untrained) ConvNet also does reasonably well</li><li>the representations from proposed model often capture the semantic information</li></ul></li></ul><h3 id="learnability-of-chromatic-aberration">Learnability of Chromatic Aberration</h3><ul><li>Patches displayed similar aberration tend to be predicted at the same location.</li><li>The effect of color projection operation is canceled for this kind of images.</li></ul><h3 id="object-detection">Object detection</h3><ul><li>Dataset : VOC 2007</li><li>Train: fine-tune the pretrained model (model is slight different with the previous one considering the image size in VOC) on VOC 2007.</li><li>Test: output from <span class="math inline">\(fc7\)</span> is taken.</li><li>Summary<ul><li>Pre-trained model outperforms the one trained from scratch</li><li>Obtained the best result on VOC 2007 without using labels</li><li>Robustness of the representations for one object in different datasets: acceptable</li></ul></li></ul><h3 id="visual-data-mining">Visual data mining</h3><ul><li>Task : aims to use a large image collection to discover image fragments which happen to depict the same semantic objects</li><li>Specification for this task: sample a constellation of four adjacent patches from an image, after finding the top 100 images which have the strongest matches for all four patches, then use a type of geometric verification to filter away the images where the four matches are not geometrically consistent. Finally, rank the different constellations by counting the number of times the top 100 matches geometrically verify.</li><li>To define the geometric verification: first compute the best-fitting square <span class="math inline">\(S\)</span> to the patch centers (via least-squares), while constraining that side of <span class="math inline">\(S\)</span> be between 2/3 and 4/3 of the average side of the patches. Then compute the squared error of the patch centers relative to <span class="math inline">\(S\)</span> (normalized by dividing the sum-of-squared-errors by the square of the side of <span class="math inline">\(S\)</span>). The patch is geometrically verified if this normalized squared error is less than 1.</li><li>Test: VOC 2011, Street View images from Paris</li><li>Summary<ul><li>The discovery of birds and torsos is good</li><li>The gains in terms of coverage, suggesting increased invariance for learned features</li><li>The pretext task is difficult: for a large fraction of patches within each image, the task is almost impossible</li><li>Limitations: some loss of purity, and cannot currently determine an object mask automatically (although one could imagine dynamically adding more sub-patches to each proposed object).</li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>instance-level supervision appears to improve performance on category-level tasks</li><li>The proposed model is sensitive to objects and the layout of the rest of the image</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf&quot;&gt;Unsupervised Visual Representation Learning by Context Prediction&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
    <link href="http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html</id>
    <published>2021-01-06T16:17:12.000Z</published>
    <updated>2021-01-12T20:58:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1406.6909.pdf">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</a></p><a id="more"></a><h2 id="why">Why?</h2><h3 id="previous-work">Previous work</h3><ul><li>Supervised : labeled data with a specific CNN<ul><li>directly penalizing the derivative of the output with respect to the magnitude of the transformations, but will be sensitive to the magnitude of the applied transformation.</li></ul></li><li>Unsupervised: learning invariant representations<ul><li>Directly modeling the input distribution and are hard for jointly training multiple layers of a CNN<ul><li><strong>autoencoders</strong>: denoising auto encoders, say reconstruct data from randomly perturbed input samples; or learn representations from videos by enforcing a temporal slowness constraint on the feature representation learned by a linear autoencoder.</li><li>invariant to local transformations</li></ul></li><li>most aims at regularization of the latent representation</li></ul></li><li>Semi-supervised<ul><li>Regularization supervised algorithms by unlabeled data: self-training, entropy regularization</li></ul></li></ul><h3 id="summary">Summary</h3><ul><li>Observations<ul><li>the features learned by one network often generalize to new datasets</li><li>a network can be adapted to a new task by replacing the loss function and possibly the last few layers of the network and fine-tuning it to the new problem</li></ul></li><li>Limitations:<ul><li>the need for huge labeled datasets to be used for the initial supervised training</li><li>the transfer becomes less efficient the more the new task differs from the original training task</li></ul></li></ul><h2 id="goals">Goals</h2><p>a more general extractor using unlabeled data. The extractor should satisfy two requirements:</p><ul><li>there must be at least one feature that is similar for images of the same category <span class="math inline">\(y\)</span> (invariance);</li><li>there must be at least one feature that is sufficiently different for images of different categories (ability to discriminate)</li></ul><h2 id="how">How?</h2><h3 id="idea">Idea</h3><ul><li>creating an auxiliary task + invariant features to transformations</li></ul><h3 id="implementation">Implementation</h3><h4 id="data-preparation">Data preparation</h4><ul><li>Do random selected transformation (from a predefined family of transformations) for sampled patches (regions containing considerable gradients so that sample a patch with probability proportional to mean squared gradient magnitude within the patch )</li><li>The family of transformations<ul><li>translation</li><li>scaling</li><li>rotation</li><li>contrast: PCA and HSV</li><li>color: works on HSV space</li><li>blur etc.</li></ul></li><li>Before feeding into model, do normalization (subtract the mean of each pixel over the whole resulting dataset)</li><li>Labeling: all transformed patches from the same seed patch are labeled by the same index</li></ul><h4 id="learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function :</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155621.png" alt="image-20210107155618997" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155911.png" alt="image-20210107155854389" style="zoom:30%;" /></p><ul><li><p>After transformations, the loss for a whole class (augmented by the same seed patch ) can be taken as</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107162118.png" alt="image-20210107162118193" style="zoom:50%;" />, notice the 2nd and the 4th can be canceled.</p><ul><li>The 1st term: enforces correct classification of the average representation <span class="math inline">\(\mathbb{E}_\alpha[g(T_\alpha x_i)]\)</span> for a given input sample</li><li>The 2nd term: a regularizer enforcing all $ h(T_x_i)$ to be close to their average value, i.e., the feature representation is sought to be approximately invariant to the transformations $ T_$, note the convergence to global minimum is listed at appendix.</li></ul></li></ul></li><li><p><strong>Why does it works?</strong></p><ul><li>Previous works mostly focus on modeling the input distribution <span class="math inline">\(p(x)\)</span>, based on the assumption that a good model of <span class="math inline">\(p(x)\)</span> contains information about the category distribution <span class="math inline">\(p(y|x)\)</span>. Therefore, to get the invariance, one will do regularization of the latent representation and obtain representation by reconstruction .</li><li>Their work does not directly model the input distribution <span class="math inline">\(p(x)\)</span> but learns a representation that discriminates between input samples. They argue that this <strong>allows more DOF to model the desired variability of a sample and avoid task-unnecessary reconstruction.</strong></li><li>However, their work will <strong>fail on color-relied task</strong></li></ul></li></ul><h2 id="experiments">Experiments</h2><h3 id="classification">Classification</h3><ul><li><p>Datasets: <strong>STL-10</strong>, CIFAR-10, Caltech-101 and Caltech-256. report mean and standard deviation</p><table style="width:100%;"><colgroup><col style="width: 6%" /><col style="width: 24%" /><col style="width: 35%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>Datasets</th><th style="text-align: center;">Resizing</th><th>Train</th><th>Test</th></tr></thead><tbody><tr class="odd"><td>STL-10</td><td style="text-align: center;">64c5-64c5-128f</td><td>10 pre-defined folds of the training data</td><td>fixed test set</td></tr><tr class="even"><td>CIFAR-10</td><td style="text-align: center;">resize from <span class="math inline">\(32\times 32\)</span> to <span class="math inline">\(64\times 64\)</span></td><td>1. whole training set<br />2. 10 random selections of 400 training samples per class</td><td>1. results on CIFAR-10<br />2. average results on 10 sets.</td></tr><tr class="odd"><td>Caltech-101</td><td style="text-align: center;">to <span class="math inline">\(150\times 150\)</span></td><td>30 random samples per class</td><td>not more than 50 samples per class</td></tr><tr class="even"><td>Caltech-256</td><td style="text-align: center;"><span class="math inline">\(256\times 256\)</span></td><td>randomly selected 30 samples per class</td><td>those except for training</td></tr></tbody></table></li><li><p>backbones of network</p><table><thead><tr class="header"><th>Network</th><th style="text-align: center;">Structure</th><th>Training</th></tr></thead><tbody><tr class="odd"><td>small</td><td style="text-align: center;">64c5-64c5-128f</td><td>1.5 days, SGD with fixed momentum of 0.9</td></tr><tr class="even"><td>medium</td><td style="text-align: center;">64c5-128c5-256c5-512f</td><td>4 days, SGD with fixed momentum of 0.9</td></tr><tr class="odd"><td>large</td><td style="text-align: center;">92c5-256c5-512c5-1024f</td><td>9 days, SGD with fixed momentum of 0.9</td></tr></tbody></table></li><li><p>Training: learning rate starts at 0.01， then when there was no improvement in validation error, decreased the learning rate by a factor of 3. All networks are trained on one Titan</p></li><li><p>Test features: one-vs-all linear SVM.</p></li><li><p>Summary</p><ul><li>with increasing feature vector dimensionality and number of labeled samples, training an SVM becomes less dependent on the quality of the features</li><li>Relation of <strong>the number of surrogate classes</strong> : <strong>sampling too many, too similar images for training can even decrease the performance of the learned features</strong>. ( the discriminative loss is no longer reasonable with too many similar surrogate classes.) --&gt; fix: e.g. clustering the output features then do augmentation for clusters and feed these augmented classes as surrogate data</li><li>Relation of <strong>the number of samples per surrogate class</strong> : around 100 samples is sufficient</li><li>Relation of <strong>types of transformations </strong> : each time remove a group of transformations and check how the performance is decreased , e.g. scaling, rotation etc. Translations, color variations and contrast variations are significantly more important. For the matching task, using blur as an additional transformation improves the performance.</li><li>Relation of <strong>Influence of the dataset </strong>: the learned features generalize well to other datasets</li><li>Relation of <strong>Influence of the Network Architecture on Classification Performance</strong>: Classification accuracy generally improves with the network size</li></ul></li></ul><h3 id="descriptor-matching">Descriptor matching</h3><ul><li>Task: Matching of interest points</li><li>Datasets: by Mikolajczyk et al., augmented by applying 6 different types of transformations with varying strengths to 16 base images from Flickr. In addition to the transformations used before, also change the lighting and blur .</li><li>Backbones: 64c7s2-128c5-256c5-512f, named as Exemplar-CNN-blur</li><li>Training: use unlabeled images from Flickr for training</li><li>Test and measurements: prediction is <span class="math inline">\(TP\)</span> if <span class="math inline">\(IOU\ge 0.5\)</span>. Compared with SIFT and Alexnet</li><li>Summary<ul><li>Optimum patch size (or layer in CNNs): SIFT is based on normalized finite differences, and thus very robust to blurred edges caused by interpolation. In contrast, for the networks, especially for their lower layers, there is an optimal patch size. They argue that features from higher layers have access to larger receptive fields and, thus, can again benefit from larger patch sizes.</li><li>A loss function that focuses on the invariance properties (rather than class-specific features) required for descriptor matching yields better results.</li><li>Features obtained with the unsupervised training procedure outperform the features from AlexNet on both datasets</li></ul></li></ul><h2 id="conclusion">Conclusion</h2><ul><li>Pretty good on tasks: object classification , descriptor matching</li><li>emphasizes the value of data augmentation in general and suggests the use of more diverse transformations.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1406.6909.pdf&quot;&gt;Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>notes-DAPPER</title>
    <link href="http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html"/>
    <id>http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html</id>
    <published>2019-10-05T16:52:21.000Z</published>
    <updated>2021-01-12T19:34:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see <a href="https://github.com/nansencenter/DAPPER">here</a>.</p><p>This notes keep records of the problems encountered while using DAPPER.</p><a id="more"></a><ol type="1"><li>Pre-setting <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.</span><br></pre></td></tr></table></figure> Copy mkl_*.dll, libiomp5md.dll and <em>libiomp5md.pdb</em> from directory "./Library/bin" to the root directory of python exe.</li></ol><p>Reference from <a href="https://blog.csdn.net/supertangcugu/article/details/89790617">here</a>.</p><ol start="2" type="1"><li>Manual</li></ol><p>Manual <a href="https://dapper.readthedocs.io/en/latest/implementation.html">online</a></p><p>For EnKF: Nx-by-N (维度数<em>样本数) For ndarrays: N-by-Nx (样本数</em>维度数)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see &lt;a href=&quot;https://github.com/nansencenter/DAPPER&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This notes keep records of the problems encountered while using DAPPER.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="da" scheme="http://yoursite.com/tags/da/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks and Remote Sensing, July 26th, 2019.</title>
    <link href="http://yoursite.com/posts/talks/2019-07-26-talk-gan-rs.html"/>
    <id>http://yoursite.com/posts/talks/2019-07-26-talk-gan-rs.html</id>
    <published>2019-07-26T17:33:39.000Z</published>
    <updated>2021-02-16T18:17:05.386Z</updated>
    
    <content type="html"><![CDATA[<p>Some works using GANs handle the problems in remote sensing.</p><p>Check <a href="/assets/slides/GAN/GANRS.pdf">slide</a> for more details.``</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Some works using GANs handle the problems in remote sensing.&lt;/p&gt;
&lt;p&gt;Check &lt;a href=&quot;/assets/slides/GAN/GANRS.pdf&quot;&gt;slide&lt;/a&gt; for more deta
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="remote sensing images" scheme="http://yoursite.com/tags/remote-sensing-images/"/>
    
  </entry>
  
  <entry>
    <title>Notes of Mathematics</title>
    <link href="http://yoursite.com/posts/notes/2019-06-19-notes-math.html"/>
    <id>http://yoursite.com/posts/notes/2019-06-19-notes-math.html</id>
    <published>2019-06-19T13:40:09.000Z</published>
    <updated>2021-04-28T23:26:01.555Z</updated>
    
    <content type="html"><![CDATA[<h2 id="泛函">泛函</h2><center><img src="/assets/img/Rules/lines.png" width=500"></center><h3 id="不动点定理">不动点定理</h3><ol type="1"><li><p>不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。</p><p><a href="https://zhuanlan.zhihu.com/p/33885648">链接</a></p></li></ol><h3 id="紧性的利用">紧性的利用</h3><ul><li>证明存在性</li><li>在无限维空间中“模仿”有限维的欧式空间</li></ul><p>紧集是为了模仿描述欧式空间中的有界闭集合么？ 紧=相对紧+闭</p><h3 id="流形manifolds">流形(Manifolds)</h3><ol type="1"><li>概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。</li></ol><h3 id="支撑集support">支撑集(Support)</h3><ol type="1"><li>概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合</li></ol><a id="more"></a><h2 id="数学分析">数学分析</h2><h3 id="lipschitz连续">Lipschitz连续</h3><ol type="1"><li>若存在一个常数K，使得定义域内的任意两点x1,x2满足： $ |f(x_1)-f(x_2)|=K|x_1-x_2|$ 则称函数为Lipschitz连续函数。此性质限定了f的导函数的绝对值不超过K，规定了函数的最大局部变动幅度。</li></ol><h2 id="statistical-learning-theory">Statistical learning theory</h2><h3 id="introduction">Introduction</h3><p>Determine how well a model performs on unseen data</p><h3 id="preliminary">Preliminary</h3><ul><li><p>Markov Inequality</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210155914107.png" alt="image-20210210155914107" style="zoom:67%;" />, in the order of <span class="math inline">\(\mathcal{O}(\frac{1}{deviation})\)</span></p></li><li><p>Chebyshev's Inequality</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210155723575.png" alt="image-20210210155723575" style="zoom:67%;" />, in the order of <span class="math inline">\(\mathcal{O}(\frac{1}{deviation^2})\)</span></p></li><li><p>Generic Chernoff's Bound</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210160010328.png" alt="image-20210210160010328" style="zoom:67%;" /></p></li><li><p>Hoeffding's Inequality</p><ul><li><p>Hoeffding's lemma</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210160051227.png" alt="image-20210210160051227" style="zoom:67%;" /></p></li><li><p>Hoeffding's Inequality</p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210160116965.png" alt="image-20210210160116965" /><figcaption aria-hidden="true">image-20210210160116965</figcaption></figure><p>Hoeffding's inequality is useful to bound the probability of the gap between an empirical value and the true expectation of an average of <strong>bounded</strong> random variables.</p></li></ul></li><li><p>McDiarmid's Inequality</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210181355241.png" alt="image-20210210181355241" style="zoom:50%;" /></p><ul><li>A concentration inequality.</li><li>This bound is useful because if we prove that an algorithm is β stable then we will have this property on a specific function.</li></ul></li></ul><h3 id="pac">PAC</h3><p>Check <a href="http://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-7-notes.pdf">here</a> for PAC, VC, uniform bound and others.</p><ul><li><p><strong>PAC Learning (agnostic PAC learnable)</strong>--finite hypothesis class</p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210155242200.png" alt="image-20210210155242200" /><figcaption aria-hidden="true">image-20210210155242200</figcaption></figure><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210090621996.png" alt="image-20210210090621996" style="zoom:70%;" /></p><ul><li>Consider a simple linear classifier with 2 weights <span class="math inline">\(\vec{w} = (w_1;w_2)\)</span>, which are stored using a 32 bit floats. This implies that the hypothesis class is finite with $|| = 2^{32} $.</li><li>This theorem <strong>works on finite hypothesis class</strong>, and answers that for a hypothesis class <span class="math inline">\(\mathcal{H}\)</span>, to make the generation gap is smaller than <span class="math inline">\(\epsilon\)</span> with at least <span class="math inline">\(1-\delta\)</span>, the required samples complexity.</li><li><em>Once a hypothesis class is PAC learnable, with high probability the training set is <span class="math inline">\(\epsilon\)</span>-representative.</em></li><li>Note Suppose <span class="math inline">\(\mathcal{H}\)</span> is PAC learnable, there <strong>is not a unique function</strong> <span class="math inline">\(m_{\mathcal{H}}\)</span> that satisfies the requirements given in the definition of PAC learnability.</li><li>Finite classes are PAC Learnable, also agnostic PAC Learnable.</li></ul></li><li><p><strong>Uniform convergence</strong></p><ul><li>Formalize that over all hypothesis in <span class="math inline">\(\mathcal{H}\)</span>, the empirical risk is close to the true risk. This will <strong>make sure the ERM to work.</strong></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210161725993.png" alt="image-20210210161725993" style="zoom:80%;" /></li></ul></li><li><p>VC dimension</p><ul><li><p>measure the complexity of hypothesis class other than cardinality.</p></li><li><p><strong>Shattering</strong></p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210163136908.png" alt="image-20210210163136908" /><figcaption aria-hidden="true">image-20210210163136908</figcaption></figure></li><li><p><strong>VC Dimension</strong></p><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210163157194.png" alt="image-20210210163157194" /><figcaption aria-hidden="true">image-20210210163157194</figcaption></figure><p>Informally VC dimension is the maximum number of distinct points that a hypothesis in <span class="math inline">\(\mathcal{H}\)</span> can correctly classify every possible labeling with zero error.</p><ul><li>With infinite VC dimension, the hypothesis class won't be PAC learnable.</li><li>There exist hypothesis classes with uncountable cardinality but finite VC dimension.</li><li>For every two hypothesis classes if <span class="math inline">\(\mathcal{H}_0 \subset \mathcal{H}\)</span> then <span class="math inline">\(VCdim(\mathcal{H}_0) \leq VCdim(\mathcal{H})\)</span>.</li></ul></li></ul></li></ul><h3 id="occams-bound">Occam's bound</h3><ul><li><p>The PAC bound can be treated as Occam's bound with a uniform prior .</p></li><li><p>Occam's bound will put a distribution over the countably infinite hypothesis class <span class="math inline">\(\mathcal{H}\)</span> that is independent of dataset <span class="math inline">\(S\)</span> we will receive. In doing so we will <strong>get bounds on the generalization gap that no longer depend on the size of the hypothesis class</strong>, <span class="math inline">\(|\mathcal{H}|\)</span>. These bounds now become variable depending on how we weigh each individual hypothesis <span class="math inline">\(h\)</span>, i.e. <span class="math inline">\(P(h)\)</span>.</p></li><li><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210090847116.png" alt="image-20210210090847116" style="zoom:80%;" /></p><ul><li><p><strong>Regularizers offers higher probability assigned to <span class="math inline">\(\vec{w}\)</span> near the origin and thus a tighter bound,</strong> it won't influence the algorithm (loss). Specifically , when an <span class="math inline">\(\ell_2\)</span> regularization term is added to the learning algorithm, it adds concavity to the loss function.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210092846655.png" alt="image-20210210092846655" style="zoom:80%;" /></p></li></ul></li><li><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210091000849.png" alt="image-20210210091000849" style="zoom:80%;" />, where <span class="math inline">\(D(Q\|P)\)</span> is the KL divergence which serves as a complexity measure.</p><ul><li><p>It tells that with a good posterior that is close to the prior, then the KL-divergence will become smaller and out bound will be tighter. <strong>Even though it's tight, the bound is tight for hypothesis that we may not care about, e.g. tight on the bound with respect to <span class="math inline">\(P\)</span> prior on hypothesis.</strong></p></li><li><p><em>Note that the posterior is after applying the prior <span class="math inline">\(P\)</span> on hypothesis, and seeing the data, then one can get this posterior <span class="math inline">\(Q\)</span>.</em></p></li><li><p><strong>Why posterior?</strong></p><p>different choices of prior and posterior hypotheses can be made, each resulting in a new bound without us touching the algorithm.</p></li><li><p>The dropout PAC-Bayes is a lower bound on the PAC-Bayes bound that becomes tight when the dropout factor is 0.</p></li></ul></li></ul><h3 id="stability-generalization">Stability, Generalization</h3><ul><li><p>PAC learning and Occam's bound work as algorithm-agnostic bounds.</p></li><li><p>Stability</p><ul><li><p>Hint: a change in data distribution does not change the predictions.</p></li><li><p>Definition: <strong>uniform stability</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210183949427.png" alt="image-20210210183949427" style="zoom:67%;" /></p><p>An algorithm with this property can be understood as one that produces a hypothesis such that the loss function <span class="math inline">\(\ell\)</span> is not drastically affected by perturbing the dataset in this manner.</p></li><li><p><font color='blue'>EMR with regularization is <span class="math inline">\(\beta\)</span>-stable.</font></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210211826259.png" alt="image-20210210211826259" style="zoom:50%;" /></p><ul><li><em>If we perturb the data by a single element, we learn <span class="math inline">\(\mathcal{A}\)</span> that can become arbitrarily close for large <span class="math inline">\(n\)</span>.</em></li></ul></li><li><p><font color='blue'>SGD is stable</font></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210214207908.png" alt="image-20210210214207908" style="zoom:57%;" /></p><ul><li><strong>It holds for a finite number of steps <span class="math inline">\(T\)</span></strong></li><li></li></ul></li></ul></li><li><p><strong>Defect</strong>:</p><p><span class="math inline">\(D[h_S]=R[h_S]-\hat{R}_S[h_S]\)</span></p><ul><li><p>Defect <span class="math inline">\(D[h_S]\)</span> for a hypothesis <span class="math inline">\(h_S\)</span> derived from an algorithm after seeing the dataset <span class="math inline">\(S\)</span> is defined as the difference between the population risk and the empirical risk.</p></li><li><p><strong>It's expectation is not zero.</strong></p></li><li><p><strong>The expectation value of defect can be bounded under certain conditions.</strong></p><p><font color='blue'>If $ $ is a <span class="math inline">\(\beta\)</span>-uniformly stable algorithm, then <span class="math inline">\(-\beta\le \mathbb{E}[D[h_S]]\le\beta\)</span>. </font></p></li><li><p><font color='blue'>Let <span class="math inline">\(\mathcal{A}\)</span> be a <span class="math inline">\(\beta\)</span>-uniformly stable learning algorithm with respect to a loss function <span class="math inline">\(\ell:\mathcal{Y\times Y}\rightarrow [0，M]\)</span>. <strong>The absolute difference of the defect calculated on a dataset <span class="math inline">\(S\)</span> and on a perturbed version of the dataset</strong> <span class="math inline">\(S^{i,z}\)</span> is bounded by <span class="math inline">\(|D[h_S]-D[h_{S^{i,z}}]|\le 2\beta +\frac{M}{n}\)</span>. </font></p></li></ul></li><li><p><font color='blue'>For a <span class="math inline">\(\beta\)</span>-uniformly stable algorithm, the relationship between the empirical and the population risk is <span class="math inline">\(\mathbb{E}[R[h_S]]\le\mathbb{E}_S[\hat{R}_S[h_S]]+\beta\)</span>. </font></p><ul><li>It's a bound on the expectation value of the population risk. But this bound does not hold for all possible <span class="math inline">\(h_S\)</span>.</li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210205331186.png" alt="image-20210210205331186" style="zoom:40%;" /></li></ul></li><li><p><font color='blue'>For a <span class="math inline">\(\beta\)</span>-uniformly stable algorithm <span class="math inline">\(\mathcal{A}\)</span> with respect to a loss function <span class="math inline">\(\ell:\mathcal{Y\times Y}\rightarrow [0，M]\)</span> and a hypothesis <span class="math inline">\(h_S\)</span> with <span class="math inline">\(|S|=n\)</span>. <strong>The relationship between the empirical and the population risk holds with probability <span class="math inline">\(1-\delta\)</span></strong>: <span class="math inline">\(R[h_S]\le \hat{R}_S[h_S]+\beta +(n\beta+\frac{M}{2})\sqrt{\frac{2\log\frac{2}{\delta}}{n}}\)</span>. </font></p><ul><li>The last term is a concentration Inequality (McDiramid's)</li><li>For this bound, as <span class="math inline">\(n\)</span> goes up, it becomes less tight.</li></ul></li></ul><h2 id="convex-optimization">Convex Optimization</h2><p>Book <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>, <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></p><table><colgroup><col style="width: 45%" /><col style="width: 54%" /></colgroup><thead><tr class="header"><th>Terminology</th><th>Definition</th></tr></thead><tbody><tr class="odd"><td>Convex function: origin</td><td><span class="math inline">\(f(\theta x+(1-\theta)y)\le \theta f(x)+(1-\theta)f(y)\)</span></td></tr><tr class="even"><td>Convex function: 1st order (once differential )</td><td>$f(y)f(x)+f(x)^T (y-x) $</td></tr><tr class="odd"><td>Convex function: 2nd order (twice differential )</td><td><span class="math inline">\(\nabla^2f(x)\succeq0\)</span>, the eigenvalues won't be negative</td></tr><tr class="even"><td><span class="math inline">\(L\)</span>-Lipschitz</td><td><span class="math inline">\(\|f(x)-f(y)\|\le L\|x-y\|\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\beta\)</span>-smooth: <span class="math inline">\(\beta\)</span>-Lipschitz on gradient</td><td><span class="math inline">\(\|\nabla f(x)-\nabla f(y)\|\le \beta\|x-y\|\Rightarrow \nabla^2f(x)\preceq\beta\mathrm{I}\)</span></td></tr><tr class="even"><td><span class="math inline">\(\alpha\)</span>-strong convex, limited the domain mostly.</td><td><span class="math inline">\(f(y)-f(x)\le\nabla f(x)^T (y-x)-\frac{\alpha}{2}\|y-x\|^2\Rightarrow \nabla^2f(x)\succeq\alpha\mathrm{I}\)</span></td></tr></tbody></table><center><strong>Table: Optimizer in different conditions</strong></center></br> For each optimizer, from the top line downwarding, the rate of convergence is increasing. The optimal step size is gotten by minimizing the bound.</center><table><colgroup><col style="width: 9%" /><col style="width: 18%" /><col style="width: 18%" /><col style="width: 18%" /><col style="width: 18%" /><col style="width: 18%" /></colgroup><thead><tr class="header"><th>Optimizer</th><th>Condition</th><th>Converge rate</th><th>Optimal step size</th><th>Sub-optimal gap</th><th>Bounds of the gap</th></tr></thead><tbody><tr class="odd"><td>GD after <span class="math inline">\(T\)</span> steps</td><td>L-Lipschitz convex</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{\sqrt{T}})\)</span></td><td><span class="math inline">\(\gamma=\frac{\|x_1-x^*\|_2}{L\sqrt{T}}\)</span></td><td><span class="math inline">\(f(\frac{1}{T}\sum\limits_{k=1}^{T}x_k)-f(x^*)\)</span></td><td><span class="math inline">\(\le\frac{\|x_1-x^*\|L}{\sqrt{T}}\)</span>, <br />the initial point matters</td></tr><tr class="even"><td>GD</td><td><span class="math inline">\(\beta\)</span>-smooth<br />+convex</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{T})\)</span></td><td><span class="math inline">\(\gamma=\frac{1}{\beta}\)</span>, <br />constant and independent of <span class="math inline">\(T\)</span></td><td><span class="math inline">\(f(x_k)-f(x^*)\)</span><br />Notice the average on all samples can be erased</td><td><span class="math inline">\(\le\frac{2\beta\|x_1-x^*\|^2}{k-1}\)</span>, <br /><span class="math inline">\(k\)</span> means the <span class="math inline">\(k\)</span> step. <br />Bound depends on initial point</td></tr><tr class="odd"><td>Projected subGD after <span class="math inline">\(T\)</span> steps</td><td><span class="math inline">\(\alpha\)</span>-strong<br /> + <span class="math inline">\(L\)</span>-Lipschitz</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{T})\)</span></td><td><span class="math inline">\(\gamma_k=\frac{2}{\alpha (k+1)}\)</span>,<br /> diminish at every step</td><td><span class="math inline">\(f(\sum\limits_{k=1}^T\frac{2k}{T(T+1)}x_k)-f(x^*)\)</span></td><td><span class="math inline">\(\le\frac{2L^2}{\alpha (T+1)}\)</span></td></tr><tr class="even"><td>GD</td><td><span class="math inline">\(\lambda\)</span>-strong <br />+ <span class="math inline">\(\beta\)</span>-smooth</td><td><span class="math inline">\(\mathcal{O}(\exp{(-T)})\)</span></td><td><span class="math inline">\(\gamma=\frac{2}{\lambda+\beta}\)</span></td><td><span class="math inline">\(f(x_{t+1})-f(x^*)\)</span><br />Notice the average on all samples can be erased</td><td><span class="math inline">\(\le\frac{\beta}{2}\exp{(-\frac{4t}{\kappa+1})}\|x_1-x^*\|^2\)</span>,<br />$ =<span class="math inline">\(. &lt;br /&gt;\)</span>k$ is the same meaning of <span class="math inline">\(t\)</span>,<br /> rather than the <span class="math inline">\(\kappa\)</span> in denominator.</td></tr><tr class="odd"><td>Polyak (heavy ball)</td><td>Quadratic loss</td><td><span class="math inline">\(\mathcal{O} ((\frac{\sqrt\kappa-1}{\sqrt\kappa+1})^t)\\\approx\exp(-\frac{C}{\sqrt\kappa})\)</span>,<br /><span class="math inline">\(\kappa=\frac{h_{max}}{h_{min}}\)</span></td><td><span class="math inline">\(\gamma^*=\frac{(1+\sqrt\mu)^2}{h_{max}}\\=\frac{(1-\sqrt\mu)^2}{h_{min}}\)</span></td><td><span class="math inline">\(\left\|\left[ \begin{matrix} x_{t+1}-x*\\ x_t-x^* \end{matrix} \right]\right\|_2\)</span></td><td><span class="math inline">\(\le\mathcal{O} (\rho(A)^T)=\mathcal{O} (\sqrt{\mu}^T)\)</span>.<br /> <span class="math inline">\(\mu\)</span> is the curvature</td></tr><tr class="even"><td>Nesterov NAG</td><td><span class="math inline">\(\beta\)</span>-smooth</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{T^2})\)</span></td><td></td><td><span class="math inline">\(f(y_t)-f(x^*)\)</span></td><td></td></tr><tr class="odd"><td>Nesterov NAG</td><td><span class="math inline">\(\alpha\)</span>-strong <br />+<span class="math inline">\(\beta\)</span>-smooth</td><td><span class="math inline">\(\mathcal{O}(\exp(-\frac{T}{\sqrt{\kappa}}))\)</span></td><td></td><td><span class="math inline">\(f(y_t)-f(x^*)\)</span></td><td><span class="math inline">\(\le\frac{\alpha+\beta}{2}\|x_k -x^*\|^2\exp(-\frac{t-1}{\sqrt\kappa})\)</span></td></tr><tr class="even"><td>SGD</td><td><span class="math inline">\(L\)</span>-Lipschitz by<br /> <span class="math inline">\(\|\tilde{g}(x)\|\le L\)</span> with prob. 1</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{\sqrt T})\)</span><br />If wants a <span class="math inline">\(\epsilon\)</span>-tolerance, <span class="math inline">\(T\ge\frac{B^2L^2}{\epsilon^2}\)</span></td><td><span class="math inline">\(\gamma=\frac{B}{L\sqrt{T}}\)</span></td><td><span class="math inline">\(\mathbb{E}[f(\bar{x})]-f(x^*)\)</span>, where <br /><span class="math inline">\(x^*\in\arg\min_{x:\|x\|&lt;B}f(x)\)</span></td><td><span class="math inline">\(\le\frac{BL}{\sqrt T}\)</span></td></tr><tr class="odd"><td>SGD</td><td><span class="math inline">\(\alpha\)</span>-strong+ <span class="math inline">\(\mathbb{E}\|\tilde{g}(x)\|_*^2\le B^2\)</span> (kind of <span class="math inline">\(B\)</span>-smooth)</td><td><span class="math inline">\(\mathcal{O}(\frac{1}{T})\)</span></td><td><span class="math inline">\(\gamma=\frac{2}{\alpha(s+1)}\)</span></td><td><span class="math inline">\(f(\sum\limits_{s=1}^t\frac{2s}{t(t+1)}x_s)-f(x^*)\)</span></td><td><span class="math inline">\(\le\frac{2B^2}{\alpha(t+1)}\)</span></td></tr></tbody></table><h3 id="estimate-sequence"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&amp;rep=rep1&amp;type=pdf">Estimate sequence</a></h3><ul><li><p>Definition</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210129143844.png" alt="image-20210129143844647" style="zoom:67%;" /></p></li><li><p>Properties</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131115340.png" alt="image-20210131115338221" style="zoom:50%;" /></p><p>for any sequence <span class="math inline">\(\{\lambda_k\}\)</span>, satisfying <span class="math inline">\((2.2.2)\)</span> we can derive <strong>the rate of convergence of the minimization process directly from the rate of convergence of the sequence <span class="math inline">\(\{\lambda_k\}\)</span>.</strong></p><p>Note: below in estimate sequence, all <span class="math inline">\(L\)</span> means <span class="math inline">\(L\)</span>-smooth function</p></li><li><p>How to form an estimate sequence?</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131150937.png" alt="image-20210131150935371" style="zoom:40%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131151028.png" alt="image-20210131151028446" style="zoom:40%;" /></p></li><li><p>How to ensure <span class="math inline">\((2.2.2)\)</span>?</p><ul><li><p>Method 1: Do scheme (2.2.6), it will generate a sequence <span class="math inline">\(\{x_k\}_{k=0}^\infin\)</span> such that <span class="math inline">\(f(x_k)-f^*\le \lambda_k[f(x_0)-f^*+\frac{\gamma_0}{2}\|x_0-x^*\|^2]\)</span>. It will make sequence satisfy <span class="math inline">\((2.2.2)\)</span></p></li><li><p>Method 2: Using gradient step</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210201094020.png" alt="image-20210201094020408" style="zoom:50%;" /></p></li></ul></li><li><p>Some cases</p><ul><li><font color='orange'>If in the scheme (2.2.6) we choose <span class="math inline">\(\gamma_0\ge\mu\)</span>, then <span class="math inline">\(\lambda_k\le\min\{(1-\sqrt{\frac{\mu}{L}})^k,\frac{4L}{(2\sqrt{L}+k\sqrt{\gamma})^2}\}\)</span>.</font></li><li><font color='blue'>If in the scheme (2.2.6) we choose <span class="math inline">\(\gamma_0=L\)</span>, then this scheme generates a sequence <span class="math inline">\(\{x_k\}_{k=0}^{\infin}\)</span> such that <span class="math inline">\(f(x_k)-f^*\le L\min \{(1-\sqrt{\frac{\mu}{L}})^k,\frac{4}{(k+2)^2}\}\|x_0-x^*\|^2\)</span>. This  means that it's optimal for the class <span class="math inline">\(\mathcal{S}_{\mu,L}^{1，1}(R^n)\)</span> with <span class="math inline">\(\mu\ge0\)</span>. </font></li><li><font color='blue'>If in the scheme <span class="math inline">\((2.2.8)\)</span> we choose <span class="math inline">\(\alpha_0\ge \sqrt{\frac{\mu}{L}}\)</span>, then this scheme generates a sequence <span class="math inline">\(\{x_k\}_{k=0}^{\infin}\)</span> such that <span class="math inline">\(f(x_k)-f^*\le \min \{(1-\sqrt{\frac{\mu}{L}})^k,\frac{4L}{(2\sqrt{L}+k\sqrt{\gamma_0})^2}\}[f(x_0)-f^*+\frac{\gamma_0}{2}\|x_0-x^*\|^2]\)</span>, where <span class="math inline">\(\gamma_0=\frac{\alpha_0(\alpha_0L-\mu)}{1-\alpha_0}\)</span>. Here <span class="math inline">\(\alpha_0\ge \sqrt{\frac{\mu}{L}}\)</span> is equivalent to <span class="math inline">\(\gamma_0\ge\mu\)</span>. </font></li><li></li></ul></li></ul><h3 id="convex-sets">Convex sets</h3><h4 id="affine-sets">Affine sets</h4><ul><li>Affine sets:<ul><li>Definition : A set <span class="math inline">\(C\subseteq R^n\)</span> is affine if the line through any two distinct points in <span class="math inline">\(C\)</span> lies in <span class="math inline">\(C\)</span>, aka for any <span class="math inline">\(x_1 ,x_2\in C\)</span> and <span class="math inline">\(\theta\in R\)</span>, one has <span class="math inline">\(\theta x_1+(1-\theta)x_2\in C\)</span>. It indicates that the <span class="math inline">\(C\)</span> contains the linear combination of any two points in <span class="math inline">\(C\)</span>.</li><li>Induction: If <span class="math inline">\(C\)</span> is an affine set, <span class="math inline">\(x_1，\cdots,x_k\in C\)</span> and <span class="math inline">\(\theta_1+\cdots+\theta_k=1\)</span>, then the point <span class="math inline">\(\theta_1 x_1+\cdots+\theta_kx_k\)</span> also belongs to <span class="math inline">\(C\)</span>.</li></ul></li><li>Affine hull<ul><li>Definition: the set of all affine combinations of points in some set <span class="math inline">\(C\subseteq R^n\)</span>, denoted as <span class="math inline">\(\mathrm{aff}C\)</span></li><li>It's the smallest affine set that contains <span class="math inline">\(C\)</span></li></ul></li><li>Affine dimension<ul><li>Definition: as the dimension of its affine hull.</li><li>E.g.: <span class="math inline">\(\{x\in R^2|x_1 ^2+x_2^2=1\}\)</span>, the affine dimension is 2.</li></ul></li></ul><h4 id="convex-sets-1">Convex sets</h4><ul><li>Definition: If every point in the set can be seen by every other point, along an unobstructed straight path between them, where unobstructed means lying in the set.</li><li>Every affine set is also convex.</li><li>Convex hull, denotes as <span class="math inline">\(\mathrm{conv} C\)</span>, is the set of all convex combinations of points in <span class="math inline">\(C\)</span>. It is always convex, and it's the smallest convex set that contains <span class="math inline">\(C\)</span></li><li>More generally, suppose <span class="math inline">\(p: R^n \rightarrow R\)</span> satisfies <span class="math inline">\(p(x)\ge0\)</span> for all <span class="math inline">\(x\in C\)</span> and <span class="math inline">\(\int_Cp(x)dx=1\)</span>, where <span class="math inline">\(C\subseteq R^n\)</span> is convex, then <span class="math inline">\(\int_Cp(x)x dx \in C\)</span>, if the integral exists.</li></ul><h3 id="convex-functions">Convex functions</h3><h4 id="convex-functions-1">Convex functions</h4><ul><li><p>Definition</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117165609.png" alt="image-20210117165609361" style="zoom:50%;" /></p></li><li><p>All affine function are both convex and concave.</p></li><li><p><span class="math inline">\(f\)</span> is convex if and only if for all <span class="math inline">\(x\in \mathrm{dom}f\)</span> and all <span class="math inline">\(v\)</span>, the function <span class="math inline">\(g(t)=f(x+tv)\)</span> is convex.</p></li></ul><h4 id="extended-value-extensions">Extended-value extensions</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117170246.png" alt="image-20210117170246166" style="zoom:50%;" /></p><h4 id="first-order-conditions">First-order conditions</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117171410.png" alt="image-20210117171410378" style="zoom:50%;" /></p><p>The inequality (3.2) states that for a convex function, the first-order Taylor approximation is in fact a global <em>underestimator</em> of the function. Conversely, if the first-order Taylor approximation of a function is always a global <em>underestimator</em> of the function, then the function is convex.</p><ul><li>The inequality (3.2) shows that if <span class="math inline">\(\nabla f(x) = 0\)</span>, then for all <span class="math inline">\(y \in \mathrm{dom} f, f(y) ≥ f(x)\)</span>, i.e., <span class="math inline">\(x\)</span> is a global minimizer of the function <span class="math inline">\(f\)</span>.</li></ul><h4 id="second-order-conditions">Second-order conditions</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117173919.png" alt="image-20210117173919136" style="zoom:67%;" /></p><h4 id="more-constraints-on-convex-function">More constraints on convex function</h4><h5 id="beta-smooth"><span class="math inline">\(\beta\)</span>-smooth</h5><ul><li>Definition: a continuously function <span class="math inline">\(f\)</span> is <span class="math inline">\(\beta\)</span>-smooth if the gradient <span class="math inline">\(\nabla f\)</span> is <span class="math inline">\(\beta\)</span>-Lipschitz, that is <span class="math inline">\(\|\nabla f(x)-\nabla f(y)\|\le\beta\|x-y\|\)</span>.</li><li><strong><em>If <span class="math inline">\(f\)</span> is twice differentiable then this is equivalent to the eigenvalues of the Hessians being smaller than <span class="math inline">\(\beta\)</span> at any point.</em></strong> , <span class="math inline">\(\nabla^2f(x)\preceq\beta \mathrm{I}_n,\forall x\)</span>.</li><li><font color='green'>smoothness removes dependency from the averaging scheme</font><br /></li><li><font color='cyan'>If extend the <span class="math inline">\(\beta\)</span>-smooth to multi power, it's called <a href="https://en.wikipedia.org/wiki/H">Holder condition</a></font><br /></li><li>The bigger your function changes in gradients, the upper you have to explore.</li></ul><h5 id="alpha-strong-convexity"><span class="math inline">\(\alpha\)</span>-strong convexity</h5><p>Strong convexity can significantly speed-up the convergence of first order methods.</p><ul><li><p>Definition</p><p>We say that <span class="math inline">\(f:\mathcal{X}\rightarrow\mathbb{R}\)</span> is a <span class="math inline">\(\alpha\)</span><em>-strongly convex</em> if it satisfies the following improved subgradient inequality:</p><p><span class="math inline">\(f(x)-f(y)\le\nabla f(x)^T(x-y)-\frac{\alpha}{2}\|x-y\|^2\)</span>. A large value of <span class="math inline">\(\alpha\)</span> will lead to a faster rate. A <span class="math inline">\(\alpha\)</span><strong>-strong convexity for twice differential function <span class="math inline">\(f\)</span> can also be interpreted as <span class="math inline">\(\nabla^2f(x)\succeq\alpha \mathrm{I}_n,\forall x\)</span></strong>.</p></li><li><p>Strong convexity plus <span class="math inline">\(\beta\)</span>-smoothness will lead to the gradient descent with a constant step-size achieves a linear rate of convergence, precisely the oracle complexity will be <span class="math inline">\(O(\frac{\beta}{\alpha}\log(1/\varepsilon)), \beta\ge\alpha\)</span>. In some sense strong convexity is a dual assumption to smoothness, and in fact this can be made precise within the framework of Fenchel duality.</p></li><li><p><span class="math inline">\(\alpha\)</span> can often be reviewed as large as the sample size. Thus reducing the number of step from <strong>sample size</strong> to <span class="math inline">\(\sqrt{\mathrm{sample \quad size}}\)</span> (cause <span class="math inline">\(\kappa=\frac{\beta}{\alpha}\)</span> for <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strong function, and in basic gradient descent algorithm, to reach <span class="math inline">\(\epsilon\)</span>-accuracy, it requires <span class="math inline">\(\mathcal{O}(\kappa\log(\frac{1}{\epsilon}))\)</span>, and for Nesterov's Accelerated Gradient Descent attains the improved oracle complexity of <span class="math inline">\(\mathcal{O}(\sqrt{\kappa}\log(\frac{1}{\epsilon}))\)</span>) can be a huge deal, especially in large scale applications.</p></li></ul><h4 id="examples-of-convex-functions">Examples of Convex functions</h4><ul><li><em>Norms</em></li><li><em>Max function</em></li><li><em>Quadratic-over-linear function</em> <span class="math inline">\(\frac{x^2}{y}\)</span></li><li><em>Log-sum-exp</em>: <span class="math inline">\(\log(e^{x_1}+\cdots+e^{x_n})\)</span>, which is regarded as a differentiable approximation of the max function</li><li><em>Geometric mean</em>: <span class="math inline">\((\prod\limits_{i=1}^{n}x_i)^{1/n}\)</span>, concave</li><li><em>Log-determinant</em>: <span class="math inline">\(\log\det X\)</span>, concave</li></ul><p>For proofs, check Chapter <span class="math inline">\(3.1.5\)</span> of <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>.</p><h4 id="sublevel-sets">Sublevel sets</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181258.png" alt="image-20210117181258728" style="zoom:50%;" /></p><h4 id="epigraph">Epigraph</h4><p>A function is convex if and only if its epigraph is a convex set.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181415.png" alt="image-20210117181415039" style="zoom:50%;" /></p><h4 id="jensens-inequality-and-extensions">Jensen's inequality and extensions</h4><p>Once a function is convex, then you can get</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117185801.png" alt="image-20210117185801678" style="zoom:50%;" /> the simplest version of it is <span class="math inline">\(f(\frac{x+y}{2})\le\frac{f(x)+f(y)}{2}\)</span>.</p><h4 id="holders-inequality">Holder's inequality</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190336.png" alt="image-20210117190336823" style="zoom:50%;" /></p><h4 id="operations-that-preserve-convexity">Operations that preserve convexity</h4><ul><li><p>Nonnegative weighted sums: <span class="math inline">\(f=w_1f_1+\cdots+w_mf_m\)</span></p></li><li><p>Composition with an affine mapping: <span class="math inline">\(g(x)=f(Ax+b)\)</span>. If <span class="math inline">\(f\)</span> is convex, so is <span class="math inline">\(g\)</span>; if <span class="math inline">\(f\)</span> is concave, so is <span class="math inline">\(g\)</span>.</p></li><li><p>Pointwise maximum and suprenum: <span class="math inline">\(f(x)=\max\{f_1(x),f_2(x)\}\)</span> and <span class="math inline">\(f(x)=\max\{f_1(x),\cdots,f_m(x)\}\)</span></p></li><li><p>Composition: <span class="math inline">\(f(x)=h(g(x))\)</span></p><ul><li><p>Scalar composition</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190943.png" alt="image-20210117190943043" style="zoom:50%;" /></p></li><li><p>Vector composition <span class="math inline">\(f(x)=h(g(x))=h(g_1(x),\cdots,g_k(x))\)</span></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117191112.png" alt="image-20210117191112754" style="zoom:50%;" /></p></li><li><p>Minimization</p></li></ul></li></ul><h3 id="typical-numerical-optimization">Typical Numerical optimization</h3><h4 id="gradient-descent">Gradient descent</h4><p>The basic principle behind <strong>gradient descent</strong> is to make a small step in the direction that minimizes the local first order Taylor approximation of <span class="math inline">\(f\)</span> (also known as the steepest descent direction). This kind of methods will <strong>obtain an oracle complexity <em>independent of the dimension</em>.</strong></p><p><span class="math inline">\(x_{t+1}=x_t-\eta\nabla f(x_t)\)</span></p><p>Taking <span class="math inline">\(f(w)=\frac{1}{2}w^TAw-b^Tw,w\in\mathbb{R}^n\)</span> into consideration, suppose <span class="math inline">\(A\)</span> is symmetric and invertible, then <span class="math inline">\(A=Q\Lambda Q^T,\Lambda=(\lambda_1,\cdots,\lambda_n),\lambda_1\le\lambda_2\le\cdots\le\lambda_{n-1}\le\lambda_n\)</span>.</p><ul><li><p>All errors are not made equal. Indeed, there are different kinds of errors, <span class="math inline">\(n\)</span> to be exact, one for each of the eigenvectors of <span class="math inline">\(A\)</span>.</p><p><span class="math inline">\(f(w^k)-f(w^*)=\sum(1-\alpha\lambda_i)^{2k}\lambda_i[x_i^0]^2\)</span></p></li><li><p><strong>Denote the condition number <span class="math inline">\(\kappa=\frac{\lambda_n}{\lambda_1}\)</span>, then the bigger the <span class="math inline">\(\kappa\)</span> is, the slower gradient descent will be</strong>, cause the condition number is a direct description of pathological curvature.</p></li><li><p><strong>The optimal step-size causes the first and last eigenvectors to converge at the same rate.</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124212843.png" alt="image-20210124212843050" style="zoom:50%;" /></p></li></ul><h4 id="projected-gradient-descent">Projected gradient descent</h4><ul><li><p>Subgradient</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222307.png" alt="image-20210117222306873" style="zoom:40%;" /></p></li><li><p>Projected subgradient descent</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222725.png" alt="image-20210117222725673" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117223015.png" alt="image-20210117222937543" style="zoom:40%;" /></p></li></ul><h4 id="gradient-descent-with-momentum-polyaks-momentum"><a href="https://distill.pub/2017/momentum/">Gradient descent with momentum</a> : Polyak's Momentum</h4><ul><li><p>Sometimes SGD fail with a reason of pathological curvature of objective. (like valley, trench)</p></li><li><p>Momentum modify gradient descent by adding a short-term memory</p><p><span class="math inline">\(y_{t+1}=\beta y_t+\nabla f(x_t)\\x_{t+1}=x_{t}-\alpha y_{t+1}\)</span>.</p><p>When <span class="math inline">\(\beta=0\)</span>, it's gradient descent.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124224747.png" alt="image-20210124224747333" style="zoom:50%;" /></p></li><li><p>Momentum allows us to crank up the step-size up by a factor of 2 before diverging.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124225628.png" alt="image-20210124225628085" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124225657.png" alt="image-20210124225656929" style="zoom:50%;" /></p></li><li><p>Optimize over <span class="math inline">\(\beta\)</span>: The critical value of <span class="math inline">\(\beta = (1 - \sqrt{\alpha \lambda_i})^2\)</span> gives us a convergence rate (in eigenspace <span class="math inline">\(i\)</span>) of <span class="math inline">\(1 - \sqrt{\alpha\lambda_i}\)</span>. A square root improvement over gradient descent, <span class="math inline">\(1-\alpha\lambda_i\)</span>! Alas, this only applies to the error in the <span class="math inline">\(i^{th}\)</span> eigenspace, with <span class="math inline">\(\alpha\)</span> fixed.</p></li><li><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124231402.png" alt="image-20210124231402777" style="zoom:50%;" /></p></li><li><p><strong>Failing</strong>: there exist strongly-convex and smooth functions for which, by choosing carefully the hyperparameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> and the initial condition <span class="math inline">\(x_0\)</span>, the heavy-ball method fails to converge.</p></li></ul><h4 id="nesterovs-accelerated-gradient-descent">Nesterov’s Accelerated Gradient Descent</h4><ul><li><p><a href="https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/">Some notes of it</a></p></li><li><p>Iterations: starting at an arbitrary initial point <span class="math inline">\(x_1=y_1\)</span></p><p><span class="math display">\[y_{s+1}=x_s-\frac{1}{\beta}\nabla f(x_s),\\x_{s+1}=(1+\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})y_{s+1}-\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}y_s\]</span></p></li><li><p><font color='blue'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex, then Nesterov's gradient descent satisfies for <span class="math inline">\(t\ge0\)</span>, <span class="math inline">\(f(y_t)-f(x^*)\le\frac{\alpha+\beta}{2}\|x_1-x^*\|^2\exp{(-\frac{t-1}{\sqrt{\kappa}})}\)</span> .</font></p></li><li><p><strong>Converge in <span class="math inline">\(\mathcal{O}(\frac{1}{T^2})\)</span> for smooth case, and <span class="math inline">\(\mathcal{O}(\exp(-\frac{T}{\sqrt{\kappa}}))\)</span>, it guaranteed convergence for quadratic functions (and not piece-wise quadractic)</strong></p></li></ul><h4 id="stochastic-gradient-descent">Stochastic gradient descent</h4><ul><li><p>Cases: one is <span class="math inline">\(\mathbb{E}_{\xi}\nabla_x\ell(x,\xi)\in\part f(x)\)</span>, where <span class="math inline">\(\xi\)</span> is sampled. This method cannot be reproduced; the other is directly minimize <span class="math inline">\(f(x)=\frac{1}{m}\sum\limits_{i=1}^{m}f_i(x)\)</span>, here gradient is reported as <span class="math inline">\(\nabla f_I(x)\)</span>, where <span class="math inline">\(I\in[m]\)</span>, this method can be reproduced.</p></li><li><p>Non-smooth stochastic optimization</p><ul><li>Definition: there exists <span class="math inline">\(B&gt;0\)</span> such that <span class="math inline">\(\mathbb{E}\|\tilde{g}(x)\|_*^2\le B^2\)</span> for all <span class="math inline">\(x\in\mathcal{X}\)</span></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202153046.png" alt="image-20210202153043826" style="zoom:67%;" /></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202153117.png" alt="image-20210202153116696" style="zoom:67%;" /></li></ul></li><li><p>Smooth stochastic optimization and mini-batch SGD</p><ul><li><p>Definition: there exists <span class="math inline">\(\sigma&gt;0\)</span> such that <span class="math inline">\(\mathbb{E}\|\tilde{g}(x)-\nabla f(x)\|_*^2\le \sigma^2\)</span> for all <span class="math inline">\(x\in\mathcal{X}\)</span>.</p></li><li><p><font color='orange'>smoothness does not bring any acceleration for a general stochastic oracle , while in exact orale case it does.</font></p></li><li><p><font color='blue'>Stochastic smooth optimization converge in <span class="math inline">\(\frac{1}{\sqrt{t}}\)</span>. Deterministic smooth optimization converge in <span class="math inline">\(\frac{1}{t}\)</span></font></p></li><li><p><font color='blue'>Mini-batch SGD converges between <span class="math inline">\(\frac{1}{\sqrt{t}}\)</span> and  <span class="math inline">\(\frac{1}{t}\)</span>.</font></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202154653.png" alt="image-20210202154653243" style="zoom:67%;" /></p></li><li></li></ul></li></ul><h4 id="relations">Relations</h4><h5 id="generalization">Generalization</h5><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124232634.png" alt="image-20210124232634072" style="zoom:50%;" /></p><h5 id="the-momentum-sgd-and-adaptive-optimizers">The momentum SGD and Adaptive optimizers</h5><p><a href="https://arxiv.org/pdf/1706.03471.pdf">YellowFin and the Art of Momentum Tuning</a></p><ul><li>Summary: hand-tuning a single learning rate and momentum makes it competitive with Adam. The proposed YellowFin (an automatic fine tuner for momentum and learning rate in SGD), can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing.</li><li>Adaptive optimizers: like Adam, AdaGrad, RmsProp</li><li>For details of paper check <a href="">here</a></li></ul><h3 id="dimension-free-convex-optimization">Dimension-free convex optimization</h3><h4 id="projected-subgradient-descent-for-lipschitz-functions">Projected subgradient descent for Lipschitz functions</h4><ul><li><p><strong>Theorem</strong></p><p>Assume that <span class="math inline">\(\mathcal{X}\)</span> is contained in an Euclidean ball centered at <span class="math inline">\(x_1\in \mathcal{X}\)</span> and of radius <span class="math inline">\(R\)</span>. Assume that <span class="math inline">\(f\)</span> is such that for any <span class="math inline">\(x\in \mathcal{X}\)</span> and of any <span class="math inline">\(g\in\part f(x)\)</span> (assume <span class="math inline">\(\part f(x)\ne \emptyset\)</span>) one has <span class="math inline">\(\|g\|\le L\)</span>. (This implies that <span class="math inline">\(f\)</span> is L-Lipschitz on <span class="math inline">\(\mathcal{X}\)</span>, that is <span class="math inline">\(\|f(x)-f(y)\|\le L\|x-y\|\)</span>)</p><p><font color='blue'><strong>The projected subgradient descent method with <span class="math inline">\(\eta=\frac{R}{L\sqrt{t}}\)</span> satisfies</strong> <span class="math inline">\(f(\frac{1}{t}\sum\limits_{s=1}^{t}x_s)-f(x^*)\le\frac{RL}{\sqrt{t}}\)</span></font></p><ul><li><p>Proof</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117225400.png" alt="image-20210117225359857" style="zoom:40%;" /></p></li><li><p><font color='blue'><strong>The rate is unimprovable from a black-box perspective.</strong></font></p></li></ul></li></ul><h4 id="gradient-descent-for-smooth-functions">Gradient descent for smooth functions</h4><h5 id="theorems-under-unconstrained-cases">Theorems under unconstrained cases</h5><p>In this section all <span class="math inline">\(f\)</span> is a convex and <span class="math inline">\(\beta\)</span>-smooth function on <span class="math inline">\(\mathbb{R}^n\)</span>.</p><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let <span class="math inline">\(f\)</span> be convex and <span class="math inline">\(\beta\)</span>-smooth function on <span class="math inline">\(\mathbb{R}^n\)</span>. Then gradient descent with <span class="math inline">\(\eta=\frac{1}{\beta}\)</span> satisfies <span class="math inline">\(f(x_t)-f(x^*)\le\frac{2\beta\|x_1-x^*\|^2}{t-1}\)</span> .</font></p><p>For the proof check <span class="math inline">\(3.2\)</span> in <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a>.</p></li><li><p>Gradient descent attains a much faster rate in <span class="math inline">\(\beta\)</span>-smooth situation than in the non-smooth case of the previous section.</p></li><li><p>The Definition of smooth convex functions</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117233245.png" alt="image-20210117233244773" style="zoom:50%;" /></p></li></ul><h5 id="the-constrained-cases">The constrained cases</h5><p>This time <em>consider the projected gradient descent algorithm <span class="math inline">\(x_{t+1}=\prod_{\mathcal{X}}(x_t-\eta\nabla f(x_t))\)</span></em></p><ul><li><strong>Lemma</strong></li></ul><p><font color='orange'>Let <span class="math inline">\(x,y\in \mathcal{X},x^+=\prod_{\mathcal{X}}(x-\frac{1}{\beta}\nabla f(x))\)</span> and <span class="math inline">\(g_{\mathcal{X}}(x)=\beta (x-x^+)\)</span> , then the following holds true: <span class="math inline">\(f(x^+)-f(y)\le g_{\mathcal{X}}(x)^T(x-y)-\frac{1}{2\beta}\|g_{\mathcal{X}}(x)\|^2\)</span>.</font></p><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let <span class="math inline">\(f\)</span> be convex and <span class="math inline">\(\beta\)</span>-smooth function on <span class="math inline">\(\mathcal{X}\)</span>. Then projected gradient descent with <span class="math inline">\(\eta=\frac{1}{\beta}\)</span> satisfies <span class="math inline">\(f(x_t)-f(x^*)\le\frac{3\beta\|x_1-x^*\|^2+f(x_1)-f(x^*)}{t}\)</span> .</font></p><h4 id="strong-convexity">Strong convexity</h4><h5 id="strongly-convex-and-lipschitz-functions"><strong>Strongly convex and Lipschitz functions</strong></h5><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210118152328.png" alt="image-20210118152328268" style="zoom:50%;" /></p><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(L\)</span>-Lipschitz and <span class="math inline">\(\alpha\)</span>-strongly convex  on <span class="math inline">\(\mathcal{X}\)</span>. Then projected gradient descent with <span class="math inline">\(\eta_s=\frac{2}{\alpha(s+1)}\)</span> satisfies <span class="math inline">\(f(\sum\limits_{s=1}^{t}\frac{2s}{t(t+1)}x_s)-f(x^*)\le\frac{2L^2}{\alpha(t+1)}\)</span> .</font></p><p><em>The combination of <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(L\)</span>-Lipschitz means that function has to be constrained in a bounded domain.</em></p><h5 id="strongly-convex-and-smooth-functions"><strong>Strongly convex and smooth functions</strong></h5><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex on <span class="math inline">\(\mathcal{X}\)</span>, then projected gradient descent with <span class="math inline">\(\eta=\frac{1}{\beta}\)</span> satisfies for <span class="math inline">\(t\ge0\)</span>, <span class="math inline">\(\|x_{t+1}-x^*\|^2\le\exp(-\frac{t}{\kappa})\|x_a -x^*\|^2\)</span> .</font></p><p>The intuition of changing <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>: If increasing <span class="math inline">\(\beta\)</span>, the upper bound will be decreased, and if increasing <span class="math inline">\(\alpha\)</span>, the lower bound will be increased. <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121112619.png" alt="image-20210121112605020" style="zoom:33%;" /></p></li><li><p><strong>Lemma</strong></p><p><font color='orange'>Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex on <span class="math inline">\(\mathbb{R}^n\)</span>, then for all <span class="math inline">\(x,y\in \mathbb{R}^n\)</span>, one has <span class="math inline">\((\nabla f(x)-\nabla f(y))^T(x-y)\ge\frac{\alpha\beta}{\alpha+\beta}\|x-y\|^2+\frac{1}{\beta+\alpha}\|\nabla f(x)-\nabla f(y)\|^2\)</span> .</font></p></li><li><p><strong>Theorem</strong></p><p><font color='blue'> Let <span class="math inline">\(f\)</span> be <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex  on <span class="math inline">\(\mathbb{R}^n\)</span>, <span class="math inline">\(\kappa=\frac{\beta}{\alpha}\)</span> as the condition number. Then gradient descent with <span class="math inline">\(\eta=\frac{2}{\beta+\alpha}\)</span> satisfies <span class="math inline">\(f(x_{t+1})-f(x^*)\le\frac{\beta}{2}\exp(-\frac{4t}{\kappa+1}\|x_1-x^*\|^2)\)</span>   </font></p></li></ul><h4 id="lower-bound----black-box">Lower bound -- black box</h4><ul><li><p>A black-box procedure is a mapping from "history" to the next query point, that is it maps (<span class="math inline">\(x_a ,g_1,\cdots,x_t,g_t\)</span>) (with <span class="math inline">\(g_s\in\part f(x_s)\)</span>) to <span class="math inline">\(x_{t+1}\)</span>. To simplify, make the following assumption on the black-box procedure: <span class="math inline">\(x_1=0\)</span> and for any <span class="math inline">\(t\ge0，x_{t+1}\)</span> is in the linear span of <span class="math inline">\(g_1,\cdots,g_t\)</span>, that is <span class="math display">\[x_{t+1}\in\mathrm{Span}(g_t ,\cdots,g_t)\tag{3.15}\label{eq315}\]</span></p></li><li><p><strong>Theorem</strong></p><ul><li><font color='blue'> Let <span class="math inline">\(t\le n,L,R&gt;0\)</span>. There exists a convex and <span class="math inline">\(L\)</span>-Lipschitz function <span class="math inline">\(f\)</span> such that for any black-box procedure satisfying  <span class="math inline">\(\eqref{eq315}\)</span>, <span class="math inline">\(\min\limits_{1\le s\le t}f(x_s)-\min\limits_{x\in B_2(R)}f(x_s)\ge\frac{RL}{2(1+\sqrt{t})}\)</span>, where <span class="math inline">\(B_2(R)=\{x\in\mathbb{R}^n:\|x\|\le R\}\)</span> . There also exists an <span class="math inline">\(\alpha\)</span>-strongly convex and <span class="math inline">\(L\)</span>-Lipschitz function <span class="math inline">\(f\)</span> such that for any black-box procedure satisfying <span class="math inline">\(\eqref{eq315}\)</span>, <span class="math inline">\(\min\limits_{1\le s\le t}f(x_s)-\min\limits_{x\in B_2(\frac{L}{2\alpha})}f(x_s)\ge\frac{L^2}{8\alpha t}\)</span></font></li><li><font color='blue'> Let <span class="math inline">\(t\le (n-1)/2,\beta&gt;0\)</span>. There exists a <span class="math inline">\(\beta\)</span>-smooth convex function <span class="math inline">\(f\)</span> such that for any black-box procedure satisfying <span class="math inline">\(\eqref{eq315}\)</span>, <span class="math inline">\(\min\limits_{1\le s\le t}f(x_s)-f(x^*)\ge\frac{3\beta}{32}\frac{\|x_1-x^*\|^2}{(t+1)^2}\)</span>.</font></li><li><font color='blue'> Let <span class="math inline">\(\kappa\ge 1\)</span>. There exists a <span class="math inline">\(\beta\)</span>-smooth and <span class="math inline">\(\alpha\)</span>-strongly convex function <span class="math inline">\(f:\ell_2\rightarrow \mathbb{R}\)</span> with <span class="math inline">\(\kappa=\frac{\beta}{\alpha}\)</span> such that for any <span class="math inline">\(t\ge1\)</span> and black-box procedure satisfying <span class="math inline">\(\eqref{eq315}\)</span> one has <span class="math inline">\(f(x_t)-f(x^*)\ge\frac{\alpha}{2}(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})^{2(t-1)}\|x_1-x^*\|^2\)</span>. Note that for large values of the condition number <span class="math inline">\(\kappa\)</span> one has <span class="math inline">\((\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})^{2(t-1)}\approx\exp(-\frac{4(t-1)}{\sqrt{\kappa}})\)</span></font></li></ul></li></ul><h2 id="references">References</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/103359560">【凸优化笔记5】-次梯度方法（Subgradient method）</a></li><li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></li><li><a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></li><li>'Understanding Analysis' by Stephen Abbott. It's a nice and light intro to analysis</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;泛函&quot;&gt;泛函&lt;/h2&gt;
&lt;center&gt;
&lt;img src=&quot;/assets/img/Rules/lines.png&quot; width=500&quot;&gt;
&lt;/center&gt;
&lt;h3 id=&quot;不动点定理&quot;&gt;不动点定理&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33885648&quot;&gt;链接&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;紧性的利用&quot;&gt;紧性的利用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;证明存在性&lt;/li&gt;
&lt;li&gt;在无限维空间中“模仿”有限维的欧式空间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;紧集是为了模仿描述欧式空间中的有界闭集合么？ 紧=相对紧+闭&lt;/p&gt;
&lt;h3 id=&quot;流形manifolds&quot;&gt;流形(Manifolds)&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;支撑集support&quot;&gt;支撑集(Support)&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>About</title>
    <link href="http://yoursite.com/posts/uncategorized/2019-06-13-about.html"/>
    <id>http://yoursite.com/posts/uncategorized/2019-06-13-about.html</id>
    <published>2019-06-13T17:02:18.000Z</published>
    <updated>2021-04-28T23:27:52.930Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to Mia Feng's Blog!</p><p>I'm doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in Montreal, Canada. I'm interested in data analysis, GCNs, transfer learning, and anomaly detection.</p><p>I got the bachelor degree from Wuhan University, China, and then I did my master at the National University of Defense Technology, China. During my studies, I've processed spatial-temporal data, and financial data. I also worked as an internship of fintech in Meituan-Dianping. The main areas I have learned including machine learning, geographical information system, software engineering, data assimilation, and finance. Currently I am learning something about computational neuroscience. I want to learn more about transfer learning and visualization and explanation of neural networks.</p><p>Have a look at my <a href="https://github.com/skaudrey/cv/blob/master/cv.pdf">resume</a> for more information.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to Mia Feng&#39;s Blog!&lt;/p&gt;
&lt;p&gt;I&#39;m doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in M
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>rl-intro</title>
    <link href="http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html</id>
    <published>2019-05-26T17:02:18.000Z</published>
    <updated>2021-01-12T20:56:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver. <a id="more"></a> ## Why RL?</p><h3 id="the-difference-between-supervised-learning-and-rl">The difference between supervised learning and RL</h3><ul><li><p>task directed from fixed data sets vs goal directed learning from interaction</p></li><li><p>characteristics:</p><ul><li>RL<ul><li>trial-and-error search</li><li>delayed reward</li></ul></li><li>ML<ul><li>generalization methods: regularization, data augmentation</li><li>real-time loss</li></ul></li></ul></li><li><p>methods:</p><ul><li>RL<ul><li>exploit: get reward</li><li>exploration: make better action selection in the future.</li></ul></li><li>ML<ul><li>discriminative and generative: parameterized and semi-parameterized</li><li>supervised and unsupervised: depends on whether have labeled data</li></ul></li></ul></li></ul><center><img src="/assets/img/RLIntro/RLML.png" width=400"></center><h3 id="the-capacity-of-rl">The capacity of RL</h3><ul><li>sequential decision maker facing unknown or known environment</li><li>works for non i.i.d. data</li></ul><center><img src="/assets/img/RLIntro/RLCapacity.png" width=300"></center><h2 id="whats-rl">What's RL?</h2><h3 id="the-agent-environment-interaction">The agent-environment interaction</h3><center><img src="/assets/img/RLIntro/RLEA.png" width=300"></center><ul><li>At each step t the agent:<ul><li>Executes action at</li><li>Transform to state St</li><li>Receives scalar reward rt</li></ul></li><li>The environment:<ul><li>Receives action at</li><li>Transform to state St</li><li>Emits scalar reward rt+1</li></ul></li><li>t increments at env. step</li></ul><h3 id="the-elements-of-rl">The elements of RL</h3><ul><li><p>Policy: agent's behaviour function, mostly is a PDF mapping state to action</p><ul><li><p>Deterministic policy</p><p><span class="math display">\[      a = \pi\left(S\right)  \]</span></p></li><li><p>Stochastic policy</p><p><span class="math display">\[      \pi\left(a|S\right)=\mathbb{P}\left(A_t=a|S_t=s\right)  \]</span></p></li></ul></li><li><p>Value function: how good is each state and/or action, the scalar value is also named reward.</p><p><span class="math display">\[      v_{\pi}\left(s\right)=\mathbb{E}\left(R_{t+1}+\gamma R_{t+1}+{\gamma}^2 R_{t+2}+\cdots|S_t=s\right)  \]</span></p><p>Mostly, to make algorithm converge, a final state will be rewarded 0, and other non-final states are rewarded as a minus value.</p></li><li><p>Model: agent's representation of the environment, they can be modeled by TKinter, gym etc.</p><ul><li><p>e.g.: models in assimilation, maze</p><p><span class="math display">\[      \mathcal{P}_{ss&#39;}^{a}=\mathbb{P}\left(S_{t+1}=s&#39;|S_t=s,A_t=a\right)\\      \mathcal{R}_{s}^{a}=\mathbb{E}\left(R_{t+1}=s&#39;|S_t=s,A_t=a\right)  \]</span></p></li><li><p>unknown environment can be stimulated by sampling ### Classification of RL #### What you want</p></li><li><p>Value Based: No Policy (Implicit), Value Function</p></li><li><p>Policy Based: Policy, No Value Function</p></li><li><p>Actor Critic: Policy, Value Function #### What you knew</p></li><li><p>Model-free: Policy and/or Value Function, No Model</p></li><li><p>Model-based: Policy and/or Value Function, Model</p></li></ul></li></ul><h2 id="how-to-rl">How to RL?</h2><h3 id="markov-process----to-simplify">Markov process -- to simplify</h3><h4 id="markov-process">Markov Process</h4><p><span class="math display">\[    \mathbb{P}\left(S_{t+1}\right)=\mathbb{P}\left(S_{t+1}|S_1,\cdots,S_t\right)\\    \mathcal{P}_{ss&#39;}=\mathbb{P}\left(S_{t+1}=s&#39;|S_t=s\right)\]</span></p><h4 id="markov-rewarded-process">Markov Rewarded Process</h4><p><span class="math display">\[    \langle S,\mathcal{P},\mathcal{R},\gamma\rangle\]</span> solve the reward from state at time t to the final state, which can be also solved by adding immediate reward and discounted value of successor state.</p><p><span class="math display">\[    v\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)=\mathbb{E}\left(R_{t+1}+\gamma v\left(S_{t+1}|S_t=s\right)\right)\]</span></p><h4 id="markov-decision-process">Markov Decision Process</h4><p><span class="math display">\[    \langle S,\mathcal{A},\mathcal{P},\mathcal{R},\gamma\rangle\]</span></p><p>Sequential decision making. * state-value function <span class="math display">\[    v_{\pi}\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)\]</span></p><ul><li>action-value function <span class="math display">\[  q_{\pi}\left(s,a\right)=\mathbb{E}_{\pi}\left(G_t|S_t=s,A_t=a\right)\]</span></li></ul><h3 id="valuepolicy">Value？Policy？</h3><h4 id="policy-iteration">Policy Iteration</h4><center><img src="/assets/img/RLIntro/policyitr.png" width=300"></center><h4 id="value-iteration">Value Iteration</h4><center><img src="/assets/img/RLIntro/valueitr.png" width=300"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>gnn</title>
    <link href="http://yoursite.com/posts/notes/2019-05-24-gnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-24-gnn.html</id>
    <published>2019-05-24T22:17:30.000Z</published>
    <updated>2019-09-23T09:27:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list the generation and improvement of graph neural networks. # Why Use * Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,</p><h1 id="basic-lines">Basic lines</h1><p>Contents in this block mainly comes from paper <a href="https://arxiv.org/pdf/1812.08434.pdf">Graph Neural Networks: A Review of Methods and Applications</a> and</p><a id="more"></a><h2 id="history">History</h2><h3 id="the-proposal-of-gnns">The proposal of GNNs</h3><p>learn a target node’s representation by propagating neighbor information via recurrent neural architectures in an iterative manner until a stable fixed point is reached Computation expensive * <a href="https://www.researchgate.net/publication/4202380_A_new_model_for_earning_in_raph_domains">A new model for learning in graph domains</a> * Big Question: processing the graph without losing topological information * reason<br />Traditional preprocessing methods for graphs dropped topological information, and thus leads to poor performance and generalization. * background RNN can only handle graph-level problems; Traditional methods dropped topological information.</p><ul><li>the approximation capability of GNN, <a href="https://www.researchgate.net/publication/23763868_Computational_Capabilities_of_Graph_Neural_Networks">Computational Capabilities of Graph Neural Networks</a> under mild generic conditions, most of the practically useful functions on graphs can be approximated in probability by GNNs up to any prescribed degree of accuracy.</li><li><a href="https://ieeexplore.ieee.org/document/4773279">Neural network for graphs: A contextual constructive approach</a></li><li><a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a> ### Go to GNNs #### Spectral-based Graph difficult to parallel or scale to large graphs,cause they need to load the whole graph into the memory. relies on eigen-decomposition of the Laplacian matrix.</li><li>Spectral GNN, <a href="https://arxiv.org/pdf/1312.6203v3.pdf">Spectral networks and locally connected networks on graphs</a></li><li>ChebNet, <a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a>，codes <a href="https://github.com/mdeff/cnn%20graph">here</a>.</li><li>1st ChebNet, <a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a>, code <a href="https://github.com/tkipf/gcn">here</a>. localized in space, but the computation requirement grow exponentially, to reduce it, sampling methods are proposed. See <a href="https://arxiv.org/pdf/1801.10247.pdf">FASTGCN</a>, <a href="https://arxiv.org/pdf/1710.10568.pdf">reduce variance</a> and <a href="https://arxiv.org/pdf/1809.05343v1.pdf">adaptive sampling</a> for details.</li><li>AGCN, calculate a pairwise distance of nodes to construct a residual graph, see <a href="https://arxiv.org/pdf/1801.03226.pdf">Adaptive Graph Convolutional Neural Networks</a> for details.</li></ul><h4 id="spatial-based-graph-convolution">Spatial-based Graph Convolution</h4><p>has gained more attention * <a href="https://arxiv.org/pdf/1706.02216.pdf">Inductive representation learning on large graphs</a> * <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model cnns</a> * <a href="https://arxiv.org/pdf/1605.05273.pdf">Learning convolutional neural networks for graphs</a> * <a href="https://arxiv.org/abs/1808.03965">Large-scale learnable graph convolutional networks</a> [1],[4] used sampling strategy the common way is to stack multiple graph convolution layer together. ##### Recurrent-based Spatial GCNs update a node's representation recursively until a stable fixed point is reached * GNNs, <a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a> * GGNNs, used GRU, <a href="https://www.aclweb.org/anthology/D14-1179">Learning phrase representations using rnn encoder-decoder for statistical machine translation</a>, codes <a href="https://github.com/yujiali/ggnn">here</a>. * Stochastic Steady-state Embedding (SSE), updates the node latent representations stochastically in an asynchronous fashion, <a href="http://proceedings.mlr.press/v80/dai18a/dai18a.pdf">Learning steady-states of iterative algorithms over graphs</a>, codes <a href="https://github.com/Hanjun-Dai/steady%20state%20embedding">here</a>.</p><h5 id="composition-based-spatial-gcns">Composition-based Spatial GCNs</h5><ul><li>Message Passing Neural Networks (MPNNs), <a href="https://arxiv.org/pdf/1704.01212.pdf">Neural Message Passing for Quantum Chemistry</a></li><li>GraphSage, <a href="https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf">Inductive representation learning on large graphs</a>, codes <a href="https://github.com/williamleif/GraphSAGE">here</a>. ###### Miscellaneous Variants of Spatial GCNs</li><li>Diffusion Convolution Neural Networks (DCNN), the hidden node representation is get by independently convolving inputs with power series or transition probability matrix, <a href="https://arxiv.org/pdf/1511.02136.pdf">Diffusion-convolutional neural networks</a></li><li>Build GCN into a standard grid to do CNN, <a href="http://proceedings.mlr.press/v48/niepert16.pdf">Learning convolutional neural networks for graphs</a>, but it ignored the node information.</li><li>Large-scale Graph Convolution Networks (LGCN), <a href="https://arxiv.org/pdf/1808.03965.pdf">Large-scale learnable graph convolutional networks</a>, still using standard grid, but it also collects nodes' information and draw subgraph for mini-batch training. Codes <a href="https://github.com/divelab/lgcn/">here</a>.</li><li>Mixture Model Network (MoNet), <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model CNNs</a>, introduce pseudo-coordinates and weight functions to let the weight of a node’s neighbor be determined by the relative position (pseudo-coordinates) between the node and its neighbor.</li><li><a href="https://arxiv.org/abs/1802.00910">Geniepath: Graph neural networks with adaptive receptive paths</a>, everages gating mechanisms to control the depth and breadth of a node's neighborhood.</li><li><a href="https://persagen.com/files/misc/zhuang2018dual.pdf">Dual graph convolutional networks for graph-based semi-supervised classification</a>, one for global representation the other for local representation.</li><li><a href="https://arxiv.org/pdf/1811.10435.pdf">On filter size in graph convolutional networks</a>, introduce a hyperparameter to influence the receptive field size of a node.</li></ul><h4 id="pooling-module">Pooling module</h4><ul><li><a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a></li><li>pooling by rearranging vertices into meaningful order, <a href="https://arxiv.org/pdf/1506.05163.pdf">Deep convolutional networks on graph-structured data</a></li><li><a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf">An end-to-end deep learning architecture for graph classification</a></li><li>DIFFPOOL pools nodes hierarchically by learning a cluster assignment matrix in each layer to get a cluster embedding, which can be combined with any standard GCN module, <a href="https://arxiv.org/pdf/1806.08804.pdf">Hierarchical graph representation learning with differentiable pooling</a></li></ul><h3 id="graph-attention-networks">Graph attention networks</h3><p>For sequence-based tasks, in total, assigning attention weights to different neighbors when aggregating feature information, ensembling multiple models according to attention weights, and using attention weights to guide random walks. * <a href="https://arxiv.org/pdf/1710.10903.pdf">Graph attention networks</a>, (GAT), multi-head weights. Codes <a href="https://github.com/PetarV-/GAT">here</a>. * <a href="https://arxiv.org/pdf/1803.07294.pdf">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</a>， also use multi-head, but it use a self attention mechanism to compute a different head for each head. * <a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">Graph classification using structural attention</a>, Graph Attention Model (GAM), adaptively visiting a sequence of important nodes. * <a href="https://arxiv.org/pdf/1710.09599.pdf">Watch your step: Learning node embeddings via graph attention</a>, factorize the co-occurrence matrix with differentiable attention weights. ### Graph Autoencoders had to handle the problem caused by the sparsity of adjacency matrix. * <a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf">Deep neural networks for learning graph representations</a>, uses the stacked denoising auto-encoders to reconstruct PPMI matrix. Codes <a href="https://github.com/ShelsonCao/DNGR">here</a> * <a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">Structural deep network embedding</a>, preserve nodes first-order proximity (drive representations of adjacent nodes close to each other) and second-order proximity (a node's neighbourhood information) jointly. Codes <a href="https://github.com/suanrong/SDNE">here</a> * <a href="https://arxiv.org/pdf/1611.07308.pdf">Variational graph auto-encoders</a>, Graph Auto-Encoder (GAE), combined with GCN firstly. Codes <a href="https://github.com/limaosen0/Variational-Graph-Auto-Encoders">here</a> * <a href="https://shiruipan.github.io/pdf/CIKM-17-Wang.pdf">Mgae: Marginalized graph autoencoder for graph clustering</a>, reconstruct node's hidden state. * <a href="https://www.ijcai.org/proceedings/2018/0362.pdf">Adversarially regularized graph autoencoder for graph embedding</a>, using GANs to regularize the graph auto-encoders, recover adjacency matrix. Codes <a href="https://github.com/Ruiqi-Hu/ARGA">here</a> * <a href="https://www.kdd.org/kdd2018/accepted-papers/view/learning-deep-network-representations-with-adversarially-regularized-autoen">Learning deep network representations with adversarially regularized autoencoders</a>, recover node sequences rather than adjacency matrix. * <a href="http://pengcui.thumedialab.com/papers/NE-RegularEquivalence.pdf">Deep recursive network embedding with regular equivalence</a>, codes <a href="https://github.com/tadpole/DRNE">here</a> ### Graph Generative Networks Not scalable to large graphs. * <a href="https://arxiv.org/pdf/1802.08773.pdf">Graphrnn: A deep generative model for graphs</a>, graph-level RNN + node-level RNN, use breadth-first-search (BFS) to sequence the nodes and Bernoulli assumption for edge generation. Codes <a href="https://github.com/snap-stanford/GraphRNN">here</a>. * <a href="https://arxiv.org/pdf/1803.03324.pdf">Learning deep generative models of graphs</a>, utilize spatial-based GCNs to obtain a hidden representation of an existing graph. * <a href="https://arxiv.org/pdf/1805.11973.pdf">Molgan: An implicit generative model for small molecular graphs</a>, RL+GAN+GCN * <a href="https://arxiv.org/pdf/1803.00816.pdf">Net-gan: Generating graphs via random walks</a>, combines LSTM with Wasserstein GAN to generate graphs from a random-walk-based approach. As for random walk, see <a href="http://leogrady.net/wp-content/uploads/2017/01/grady2004multilabel.pdf">here</a>. * <a href="https://arxiv.org/pdf/1809.02630.pdf">Constrained generation of semantically valid graphs via regularizing variational autoencoders</a> ### Graph Spatial-Temporal Networks</p><ul><li><a href="https://arxiv.org/pdf/1612.07659.pdf">Structured sequence modeling with graph convolutional recurrent networks</a></li><li><a href="https://arxiv.org/pdf/1707.01926.pdf">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</a>, can work forwardly or backwardly. Codes <a href="https://github.com/liyaguang/DCRNN">here</a>.</li><li><a href="https://arxiv.org/pdf/1709.04875.pdf">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</a>, codes <a href="https://github.com/VeritasYin/STGCN_IJCAI-18">here</a>.</li><li><a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial temporal graph convolutional networks for skeleton-based action recognition</a>, extend the temporal flow as graph edges, and then assign each a label to each edge. Codes <a href="https://github.com/yysijie/st-gcn">here</a>.</li><li><a href="https://arxiv.org/pdf/1511.05298.pdf">Structural-rnn: Deep learning on spatio-temporal graphs</a>, aims at predicting nodes' labels at each time, has nodeRNN and edgeRNN, and split nodes and edges into semantic groups. Codes <a href="https://github.com/asheshjain399/RNNexp">here</a>.</li></ul><h2 id="main-methodologies----graph-embedding">Main Methodologies -- Graph Embedding</h2><h3 id="matrix-factorization">Matrix Factorization</h3><ul><li><a href="https://www.ijcai.org/proceedings/2018/0493.pdf">Discrete network embedding</a></li><li><a href="https://shiruipan.github.io/pdf/ICDM-18-Yang.pdf">Binarized attributed network embedding</a> ### Random Walks</li><li><a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf">Deepwalk: Online learning of social representations</a></li></ul><h2 id="problems">Problems</h2><ul><li>Does going deeper always work in GNNs?</li><li>How to select representative receptive field for a node?</li><li>How to work on large graphs?</li><li>How to handle dynamic and heterogeneous graph structures?</li></ul><h1 id="papers">Papers</h1><h2 id="introduction">Introduction</h2><ul><li>M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, "Geometric deep learning: going beyond euclidean data,"IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 18–42, 2017 ## others ### <a href="https://arxiv.org/pdf/1905.02296v1.pdf">Are Graph Neural Networks Miscalibrated?</a></li></ul><h3 id="estimating-node-importance-in-knowledge-graphs-using-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.08865.pdf">Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks</a></h3><p>GENI, a GNN-based method designed to deal with distinctive challenges involved with predicting node importance in KGs.</p><h3 id="understanding-attention-in-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.02850.pdf">Understanding attention in graph neural networks</a></h3><p>We aim to better understand attention over nodes in graph neural networks and identify factors influencing its effectiveness. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 40% in some of our classification tasks</p><h3 id="are-graph-neural-networks-miscalibrated"><a href="https://arxiv.org/pdf/1905.02296.pdf">Are Graph Neural Networks Miscalibrated?</a></h3><p>Graph Neural Networks (GNNs) have proven to be successful in many classification tasks, outperforming previous state-of-the-art methods in terms of accuracy</p><h3 id="graph-convolutional-networks-with-eigenpooling"><a href="https://arxiv.org/pdf/1904.13107.pdf">Graph Convolutional Networks with EigenPooling</a></h3><p>To apply graph neural networks for the graph classification task, approaches to generate the  from node representations are demanded. Experimental results of the graph classification task on <span class="math inline">\(6\)</span> commonly used benchmarks demonstrate the effectiveness of the proposed framework.</p><h3 id="pan-path-integral-based-convolution-for-deep-graph-neural-networks"><a href="https://arxiv.org/pdf/1904.10996.pdf">PAN: Path Integral Based Convolution for Deep Graph Neural Networks</a></h3><p>Experimental results show that the path integral based graph neural networks have great learnability and fast convergence rate, and achieve state-of-the-art performance on benchmark tasks.</p><h3 id="attacking-graph-based-classification-via-manipulating-the-graph-structure"><a href="https://arxiv.org/pdf/1903.00553.pdf">Attacking Graph-based Classification via Manipulating the Graph Structure</a></h3><p>We evaluate our attacks and compare them with a recent attack designed for graph neural networks. Results show that our attacks 1) can effectively evade graph-based classification methods; 2) do not require access to the true parameters, true training dataset, and/or complete graph; and 3) outperform the existing attack for evading collective classification methods and some graph neural network methods</p><h3 id="deep-learning-in-bioinformatics-introduction-application-and-perspective-in-big-data-era"><a href="https://arxiv.org/abs/1903.00342">Deep learning in bioinformatics: introduction, application, and perspective in big data era</a></h3><p>After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures</p><h3 id="constant-time-graph-neural-networks"><a href="https://arxiv.org/pdf/1901.07868.pdf">Constant Time Graph Neural Networks</a></h3><p>Recent advancements in graph neural networks (GNN) have led to state-of-the-art performance in various applications including chemo-informatics, question answering systems, and recommendation systems, to name a few</p><h3 id="a-comprehensive-survey-on-graph-neural-networks"><a href="https://arxiv.org/pdf/1901.00596.pdf">A Comprehensive Survey on Graph Neural Networks</a></h3><p>We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories</p><h3 id="graph-transformation-policy-network-for-chemical-reaction-prediction"><a href="https://openreview.net/pdf?id=r1f78iAcFm">Graph Transformation Policy Network for Chemical Reaction Prediction</a></h3><p>To this end, we propose Graph Transformation Policy Network (GTPN) -- a novel generic method that combines the strengths of graph neural networks and reinforcement learning to learn the reactions directly from data with minimal chemical knowledge. Evaluation results show that GTPN improves the top-1 accuracy over the current state-of-the-art method by about 3% on the large USPTO dataset</p><h3 id="contextualized-non-local-neural-networks-for-sequence-learning"><a href="https://arxiv.org/pdf/1811.08600.pdf">Contextualized Non-local Neural Networks for Sequence Learning</a></h3><p>Recently, a large number of neural mechanisms and models have been proposed for sequence learning, of which self-attention, as exemplified by the Transformer model, and graph neural networks (GNNs) have attracted much attention. Specifically, we propose contextualized non-local neural networks (CN<span class="math inline">\(^{\textbf{3}}\)</span>), which can both dynamically construct a task-specific structure of a sentence and leverage rich local dependencies within a particular neighborhood.</p><h3 id="automated-theorem-proving-in-intuitionistic-propositional-logic-by-deep-reinforcement-learning"><a href="https://arxiv.org/pdf/1811.00796.pdf">Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning</a></h3><p>Using the large volume of augmented data, we train highly accurate graph neural networks that approximate the value function for the set of the syntactic structures of formulas. Within the specified time limit, our prover solved 84% of the theorems in a benchmark library, while <span class="math inline">\(\texttt{tauto}\)</span> was able to solve only 52%.</p><h3 id="pileup-mitigation-at-the-large-hadron-collider-with-graph-neural-networks"><a href="https://arxiv.org/pdf/1810.07988.pdf">Pileup mitigation at the Large Hadron Collider with Graph Neural Networks</a></h3><p>We present a classifier based on Graph Neural Networks, trained to retain particles coming from high-transverse-momentum collisions, while rejecting those coming from pileup collisions. This model is designed as a refinement of the PUPPI algorithm, employed in many LHC data analyses since 2015</p><h3 id="weisfeiler-and-leman-go-neural-higher-order-graph-neural-networks"><a href="https://arxiv.org/pdf/1810.02244.pdf">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</a></h3><p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. The following work investigates GNNs from a theoretical point of view and relates them to the <span class="math inline">\(1\)</span>-dimensional Weisfeiler-Leman graph isomorphism heuristic (<span class="math inline">\(1\)</span>-WL). We show that GNNs have the same expressiveness as the <span class="math inline">\(1\)</span>-WL in terms of distinguishing non-isomorphic (sub-)graphs</p><h3 id="multitask-learning-on-graph-neural-networks---learning-multiple-graph-centrality-measures-with-a-unified-network"><a href="https://arxiv.org/pdf/1809.07695.pdf">Multitask Learning on Graph Neural Networks - Learning Multiple Graph Centrality Measures with a Unified Network</a></h3><p>Graph neural networks (GNN), consisting of trained neural modules which can be arranged in different topologies at run time, are sound alternatives to tackle relational problems which lend themselves to graph representations. The proposed model achieves <span class="math inline">\(89\%\)</span> accuracy on a test dataset of random instances with up to 128 vertices and is shown to generalise to larger problem sizes</p><h3 id="meta-gnn-on-few-shot-node-classification-in-graph-meta-learning"><a href="https://arxiv.org/pdf/1905.09718.pdf">Meta-GNN: On Few-shot Node Classification in Graph Meta-learning</a></h3><p>However, there are very few works applying meta-learning to non-Euclidean domains, and the recently proposed graph neural networks (GNNs) models do not perform effectively on graph few-shot learning problems. Additionally, Meta-GNN is a general model that can be straightforwardly incorporated into any existing state-of-the-art GNN</p><h3 id="mr-gnn-multi-resolution-and-dual-graph-neural-network-for-predicting-structured-entity-interactions"><a href="https://arxiv.org/pdf/1905.09558.pdf">MR-GNN: Multi-Resolution and Dual Graph Neural Network for Predicting Structured Entity Interactions</a></h3><p>In recent years, graph neural networks have become attractive. Experiments conducted on real-world datasets show that MR-GNN improves the prediction of state-of-the-art methods.</p><h3 id="revisiting-graph-neural-networks-all-we-have-is-low-pass-filters"><a href="https://arxiv.org/pdf/1905.09550.pdf">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</a></h3><p>In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property</p><h3 id="multi-hop-reading-comprehension-across-multiple-documents-by-reasoning-over-heterogeneous-graphs"><a href="https://arxiv.org/pdf/1905.07374.pdf">Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs</a></h3><p>We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based model (single model) achieves state-of-the-art result.</p><h3 id="ipc-a-benchmark-data-set-for-learning-with-graph-structured-data"><a href="https://arxiv.org/pdf/1905.06393.pdf">IPC: A Benchmark Data Set for Learning with Graph-Structured Data</a></h3><p>The data set, named IPC, consists of two self-contained versions, grounded and lifted, both including graphs of large and skewedly distributed sizes, posing substantial challenges for the computation of graph models such as graph kernels and graph neural networks</p><h1 id="datasets">Datasets</h1><ul><li><a href="https://www.aminer.cn/citation">Citation Networks</a>: <a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/cora_raw.zip">Cora</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/citeseer.zip">Citeseer</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/pubmed.zip">Pubmed</a>,<a href="https://www.aminer.cn/citation">DBLP</a></li><li><a href="http://networkrepository.com/soc_BlogCatalog.php">Social Networks</a>: <a href="http://socialcomputing.asu.edu/datasets/BlogCatalog">BlogCatalog</a>,<a href="https://github.com/linanqiu/reddit-dataset">Reddit</a>,<a href="http://www.trustlet.org/downloaded_epinions.html">Epinions</a></li><li>Chemical/Biological Graphs: <a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI1.zip">NCI-1</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI109.zip">NCI-9</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/MUTAG.zip">MUTAG</a>, D&amp;D,<a href="https://github.com/bigdata-ustc/QM9nano4USTC">QM9</a>,<a href="https://tripod.nih.gov/tox21/challenge/data.jsp">Tox21</a>,<a href="http://snap.stanford.edu/graphsage/ppi.zip">PPI</a>.</li><li>Unstructured Graphs: convert <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, <a href="http://www.mattmahoney.net/dc/textdata.html">Wikipedia</a> or News Groups into graphs.</li><li>Others: <a href="https://pan.baidu.com/s/14Yy9isAIZYdU__OYEQGa_g#list/path=%2F">METR-LA</a>, <a href="https://grouplens.org/datasets/movielens/1m/">Movies-Lens1M</a>, NELL.</li></ul><p>Thanks for the links given <a href="https://www.jianshu.com/p/67137451b67f">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list the generation and improvement of graph neural networks. # Why Use * Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,&lt;/p&gt;
&lt;h1 id=&quot;basic-lines&quot;&gt;Basic lines&lt;/h1&gt;
&lt;p&gt;Contents in this block mainly comes from paper &lt;a href=&quot;https://arxiv.org/pdf/1812.08434.pdf&quot;&gt;Graph Neural Networks: A Review of Methods and Applications&lt;/a&gt; and&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>UNet proposed by Olaf Ronneberger etc.</title>
    <link href="http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html</id>
    <published>2019-05-23T21:18:21.000Z</published>
    <updated>2021-01-12T19:33:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>The notes of paper <a href="https://arxiv.org/abs/1505.04597v1">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>.</p><a id="more"></a><h2 id="big-question-classification-in-pixel-level-and-thus-image-segmentation">Big Question: classification in pixel level and thus image segmentation</h2><h3 id="reason">reason</h3><ul><li>single label for a image is not enough to support segmentation. ### background</li><li>why focus on it<ul><li>biomedical images: like cells segmentation. Add: also appropriate for other entity segmentation</li></ul></li><li>how have been done:<ul><li>the development of deeper CNNs</li><li>using CNNs segmenting electron microscopy images</li></ul></li><li>what have been missed:<ul><li>computation efficiency and redundancy: slow cause every patch require a running of network; patch overlapping</li><li>difficult trade-off for localization and the usage of context.</li></ul></li></ul><h2 id="methods">Methods</h2><h3 id="for-what">For what?</h3><p>Cells segmentation.</p><h3 id="framework-of-methods">Framework of Methods</h3><p>Convolution2D + deconvolution (upsampling 2D). The output of one downsampling layer is contracted as part of the input of the corresponding symmetric upsampling layer.</p><h3 id="novelty">Novelty</h3><ul><li>data augmentation randomly elastic deformations: shift, rotation, gray value, random elastic deformations are the most important</li><li>replace pooling by upsampling.</li><li>No fully connection layers.</li><li>weighted the loss of touching objects (cells).</li></ul><h2 id="details">Details</h2><h3 id="weighted-map-to-segment-overlapped-cells">weighted map to segment overlapped cells</h3><p>According to the paper, they pre-compute the weight map for each ground truth segmentation to compensate the different frequency of pixels.</p><h3 id="data-augmentation">data augmentation</h3><p>Smooth deformations using random displacements vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation.</p><h2 id="abstract">Abstract</h2><p>The main idea in abstract are contracted NNs and data augmentation so that the new NNs can get reasonable results by fewer images.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The notes of paper &lt;a href=&quot;https://arxiv.org/abs/1505.04597v1&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>paper--Faster RCNN</title>
    <link href="http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster%20rcnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster rcnn.html</id>
    <published>2019-05-19T15:17:12.000Z</published>
    <updated>2021-01-12T20:48:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Some understanding about the details in Faster RCNN, based on the codes in tensorflow.</p><a id="more"></a><h2 id="big-question">Big Question</h2><h3 id="reason">reason</h3><h3 id="background">background</h3><h2 id="region-proposal-network">Region Proposal Network</h2><h3 id="some-numbers">Some numbers</h3><ul><li>The number of anchor boxes for one anchor target <span class="math inline">\(k = scale \times ratios\)</span>,</li><li>The number of anchor boxes for one feature layer (which has <span class="math inline">\(W \times H\)</span> grids), will get $ W H k $ anchor boxes. Every grid in the feature map (the output of a popular CNN without FC layers) will have <span class="math inline">\(k\)</span> anchor boxes.</li><li>Not like the ROI method, the size of features are fixed, but anchor boxes are rescaled by <span class="math inline">\(k\)</span> regressors.</li></ul><h2 id="experiments">Experiments</h2><h3 id="prove">prove</h3><ul><li>The top-ranked RPN proposals are accurate.</li><li>NMS does not harm the detection mAP and may reduce false alarms.</li></ul><h2 id="construct">Construct</h2><h3 id="add-loss">Add loss</h3><h3 id="problems-using-it-processing-typhoon-data">Problems using it processing typhoon data</h3><ul><li>Does NMS lead to loss of typhoon? Not really, the texture of typhoon is obvious in image.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Some understanding about the details in Faster RCNN, based on the codes in tensorflow.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Positioning Data of FY4 AGRI.</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html</id>
    <published>2019-04-26T22:53:05.000Z</published>
    <updated>2019-04-26T15:31:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.</p><p>There are two ways for positioning, one is querying the lookup table given by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">NSMC</a>, the other is calculating by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">formulas</a>.</p><p>However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days. <a id="more"></a></p><h2 id="querying-the-lookup-table.">Querying the lookup table.</h2><p>There are two errors in the files.</p><ul><li>The first 8 bytes denote latitude, and the next 8 bytes are reserved for longitude.</li><li>The data are stored as little-endian data.<center><img src="/assets/img/FY4-AGRI/lookup.png" width="400"></center></li></ul><h2 id="calculating-by-formulas">Calculating by formulas</h2><p>The formulas are OK, but the constant variable <span class="math inline">\(\lambda_D\)</span> should be measured in rad before being used.</p><center><img src="/assets/img/FY4-AGRI/formulas.png" width="400"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.&lt;/p&gt;
&lt;p&gt;There are two ways for positioning, one is querying the lookup table given by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;NSMC&lt;/a&gt;, the other is calculating by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;formulas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="Sensor" scheme="http://yoursite.com/tags/Sensor/"/>
    
  </entry>
  
  <entry>
    <title>A Tex Template of Cornell Notes</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2019-04-26T15:34:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these. <a id="more"></a></p><h2 id="preliminaries">Preliminaries</h2><p>Before starting, you need install * CTex</p><h2 id="install-missed-packages-of-ctex">Install missed packages of CTex</h2><h3 id="download-the-required-packages">Download the required packages</h3><p>Search <a href="https://www.ctan.org/pkg">here</a>.</p><h3 id="unzip-and-compile-manually-if-needed.">Unzip and compile manually if needed.</h3><p>Unzip the downloaded file and jump to the directory after unzip. If need compile, run <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  pdflatex ***.ins</span><br></pre></td></tr></table></figure> ### Install Copy the compiled file folder to the CTex path: ~/CTex/CTex/tex/latex</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ texhash --admin</span><br></pre></td></tr></table></figure>The notes will be generated like this:<center><img src="/assets/img/CornellNotes.png" width="400"></center><h1 id="acknowledgement">Acknowledgement</h1><p>Thank <a href="https://blog.csdn.net/Myriad_Dreamin/article/details/83384110">this blog</a>.</p><h1 id="resources">Resources</h1><p>Tex file can be found <a href="https://github.com/skaudrey/skaudrey.github.io/tree/master/assets/notes/Cornell">here</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>Understand and debug the codes of GCN proposed by Thomas N. Kipf</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2021-01-27T14:45:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1609.02907.pdf">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>.</p><a id="more"></a><h2 id="big-question-semi-supervised-classification-of-graph-data">Big Question: semi-supervised classification of graph data</h2><ul><li>reason<ul><li>computation effective: semi-supervision</li><li>the complex of graphs, the information of nodes and edges are not structural information.</li></ul></li><li>background<ul><li>the improvement of GCNs: spectral GCNs</li></ul></li></ul><h2 id="key-points">Key points</h2><h3 id="the-approximation-of-spectral-graph-convolution">The approximation of spectral graph convolution</h3><p>The lines in paper had confused me at first before I ran the codes.</p><p>The difference of graph convolution and valina convolution is the input, as the input is a graph rather than data in same dimension, the key point is how to convert data represented by node and graph to a tensor in fixed dimension.</p><p>To solve it, Thomas maps the graph into a spectral space and also, to be computational efficient, approximate the infinite coefficients by second-order Chebyshev polynomial formulas.</p><p>After those approximation, it is input into the whole network with features.</p><h3 id="build-model">Build model</h3><p>Actually, except the complicated preprocess to represent graph G into a sparse tensor, the other step are not that complex, just the similar as what a convolution layer do. <span class="math display">\[Z = f\left(\mathbf{X},A\right)=softmax\left(\hat{A}ReLU\left({\hat{A}XW^{\left(0\right)}}\right)W^{\left(1\right)}\right)\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Build up personal blog</title>
    <link href="http://yoursite.com/posts/notes/2019-01-28-hexo+killy+github%20pages=blog.html"/>
    <id>http://yoursite.com/posts/notes/2019-01-28-hexo+killy+github pages=blog.html</id>
    <published>2019-01-28T18:33:39.000Z</published>
    <updated>2021-02-07T15:48:06.557Z</updated>
    
    <content type="html"><![CDATA[<p>This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github. <a id="more"></a></p><h2 id="preliminaries">Preliminaries</h2><p>Before starting, you need: * node.js+npm</p><pre><code>Get node.js from [here](https://pan.baidu.com/s/1kU5OCOB#list/path=%2Fpub%2Fnodejs). Check [here](https://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000/00143450141843488beddae2a1044cab5acb5125baf0882000) for more info of node.</code></pre><ul><li><p>hexo Install by npm:</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure></p></li><li><p>git,</p></li><li><p>an account of Github,</p></li></ul><p>and configure the ssh-key on your device.</p><h2 id="build-blog">Build blog</h2><h3 id="initialize-hexo-with-hexo">Initialize hexo with hexo</h3><p>Create a local folder as your root directory, such as "blog", and go to the directory in your terminal and initialize it by hexo. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br><span class="line">$ <span class="built_in">cd</span> blog</span><br></pre></td></tr></table></figure> Then initialize this directory with npm.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="link-hexo-with-github">Link hexo with Github</h3><p>Set deployment tool,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>and initialize the remote repository for your blog on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br><span class="line">$ git add *</span><br><span class="line">$ git commit -m <span class="string">&quot;init commit&quot;</span></span><br></pre></td></tr></table></figure><p>Change the deployment in file "_config.yml" like:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line">deploy: </span><br><span class="line">    <span class="built_in">type</span>: git </span><br><span class="line">    repo: git@github.com:jack/jack.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure><p>Tips: The name of your hosting repository should be "[githubname].github.io", such as "jack.github.io". And mind the blankspaces while rewriting file "_config.yml". ### Generate static files Do it before you push it on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><h1 id="see-your-pages">see your pages</h1><p>Click https://[githubname].github.io/, such as https://jack.github.io/.</p><h2 id="customization">Customization</h2><h3 id="change-theme">change theme</h3><p>I picked theme yilia. Configuration should be done as bellow:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ git <span class="built_in">clone</span> https://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br></pre></td></tr></table></figure><p>Change the default theme defined in "_config.yml" under root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: yilia</span><br></pre></td></tr></table></figure><h3 id="upload-your-avatar">upload your avatar</h3><p>New a folder under the "source" directory, I named it assets. I also new the "img" folder for pictures. Put you avatar picture here. Then reconfigure the _config.yml file beneath theme "yilia"'s folder, which is:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avatar: /assets/img/avatar.jpg</span><br></pre></td></tr></table></figure><h3 id="classify-your-posts-by-categories-rather-than-tags-in-default">classify your posts by categories rather than tags in default</h3><p>Now take your eye away from file "_config.yml" under theme yilia, open the file "_config.yml" under the root directory of your blog. You need to configure category_map, for instance,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  about: about</span><br><span class="line">  notes: notes</span><br><span class="line">  projects: projects</span><br><span class="line">  papers: papers</span><br><span class="line">  talks: talks</span><br><span class="line">  meetings: meetings</span><br></pre></td></tr></table></figure><p>Each pair of it can be different, it is just a mapping, such as:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  parole: talks</span><br><span class="line">  关于我: about</span><br></pre></td></tr></table></figure><h3 id="change-the-naming-rule-of-a-new-post">change the naming rule of a new post</h3><p>The default naming rule of hexo is YYYY/MM/DD/[post name], which leads to a hyper-link without html suffix. I change it as html. It can be accomplished by configure the _config.yml in root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permalink: posts/:category/:year-:month-:day-:title.html</span><br></pre></td></tr></table></figure><h3 id="truncate-the-post-in-home-list-when-it-is-too-long.">Truncate the post in home list when it is too long.</h3><p>You need to add</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure><p>after where you want to truncate in a post. And configure the __config.yml_ which is under themes' folder.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The truncate signal while post is too long.</span></span><br><span class="line">excerpt_link: <span class="string">&quot;more&quot;</span></span><br></pre></td></tr></table></figure><h3 id="support-latex">Support Latex</h3><p>Check <a href="https://www.jianshu.com/p/5623c5e35c93">here</a> for details.</p><p>Tips: if things don't work try restart, regenerate and redeploy.</p><h1 id="tips">Tips</h1><p>You can debug pages locally by</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>, which is convenient before deployment.</p><h1 id="acknowledgement">Acknowledgement</h1><p>Thank <a href="https://www.cnblogs.com/wumz/p/8030244.html">Mauger</a>, and <a href="https://github.com/litten/hexo-theme-yilia">litten</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github.
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>Clouds detection of infrared hyperspectral data based on logistic.</title>
    <link href="http://yoursite.com/posts/projects/2019-01-28-lr.html"/>
    <id>http://yoursite.com/posts/projects/2019-01-28-lr.html</id>
    <published>2019-01-28T18:33:39.000Z</published>
    <updated>2021-02-16T18:18:55.095Z</updated>
    
    <content type="html"><![CDATA[<p>This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed with what clouds release, and they exist in more than 90% IFOVs, cloudy IFOVs have to be kicked off in order to get clean data.</p><p>Therefore, a new feature construction method is proposed for infrared hyperspectral data, such as what IASI releases. Concretely, four channels of IASI are picked, namely channel 921, channel 386, channel 306 and channel 241. They are picked because of physical characteristics. And then, cloudy IFOVs are detected by logistic regression.</p><p>The recall, auc and accuracy of this new method carried on IASI data was more than 0.95 when detecting IFOVs of sea, while the result of land's IFOVs was less than it. After adding surface emissivity features, the auc of it increased by aroud 5%, and recall of it grew by 10% approximately.</p><p>Codes are available <a href="https://github.com/skaudrey/cloud">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed 
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="logistic" scheme="http://yoursite.com/tags/logistic/"/>
    
  </entry>
  
  <entry>
    <title>HCR--Compress and Resonstruct Hyperspectral Data.</title>
    <link href="http://yoursite.com/posts/projects/2018-12-28-hcr.html"/>
    <id>http://yoursite.com/posts/projects/2018-12-28-hcr.html</id>
    <published>2018-12-28T18:33:39.000Z</published>
    <updated>2021-02-16T18:19:17.158Z</updated>
    
    <content type="html"><![CDATA[<p>This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression and reconstruction. The numerous infrared hyperspectral data are overloaden for computing resources currently. Taking IASI, an atmosphere detector on satellite Metop launched by European Organization for the Exploitation of Meteorological Satellites (EUMETSAT), as an example, it has 8461 channels, which can detect atmosphere vertically in details. To process these data more efficiently, compressing them and then reconstructing is required.</p><p>Considering their high correlation in spectral and spatial dimension, a new compressing and reconstructing network HCR is proposed. Concretely, the radiation brightness values are gridded so that one value at specific location is recongnized as a color value at this pixel. After normalizing by batch normalization, HCR compresses by convolution and reconstructs by deconvlution.</p><p>Carrying on IASI data, the RMSE of this new method was decreased by 5% at least compared with the result of principle component analysis (PCA) in the same compression ratio. The compression kernels encode tempetature information and reconstruct it. In reconstruction, the kernels' weights for likewise data are similar.</p><p>Codes are available <a href="https://github.com/skaudrey/hyp">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression an
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="cnn" scheme="http://yoursite.com/tags/cnn/"/>
    
      <category term="compress" scheme="http://yoursite.com/tags/compress/"/>
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
  </entry>
  
  <entry>
    <title>What Can Artificial Intelligence Do in Data Assimilation? Dec. 9th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2018-12-09-talk-mlutility.html"/>
    <id>http://yoursite.com/posts/talks/2018-12-09-talk-mlutility.html</id>
    <published>2018-12-09T18:33:39.000Z</published>
    <updated>2021-02-16T18:16:19.620Z</updated>
    
    <content type="html"><![CDATA[<p>This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my lab, and most of them haven't learn much about AI. The talk would like to show them what can AI do these days, and help them figure out what else can AI do in data assimilation.</p><p>Slides are avaliable <a href="/assets/slides/mlDo/mlUtility.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my l
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="Data assimilation" scheme="http://yoursite.com/tags/Data-assimilation/"/>
    
  </entry>
  
  <entry>
    <title>Computing in the 21th Century &amp; Asia Faculty Summit held by Microsoft, Nov., 2018.</title>
    <link href="http://yoursite.com/posts/meetings/2018-11-23-microsoft.html"/>
    <id>http://yoursite.com/posts/meetings/2018-11-23-microsoft.html</id>
    <published>2018-11-23T18:33:39.000Z</published>
    <updated>2021-02-16T18:18:38.701Z</updated>
    
    <content type="html"><![CDATA[<p>This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is <a href="https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda">here</a>.</p><p>Babysitting AI and computational neuroscience impressed me a lot. <a id="more"></a> # computing neuroscience: robots with feelings.</p><p>Prof. Lenore Blue's keynote is about computational neuroscience. They try to let robots feel pain, and to simulate the long and short term encoding happening in our brains.</p><p>What she talked reminds me of something I had read before.</p><p>According to the book <em>Psychology</em> written by Daniel Schacter, our brains do encode information into long and short codes, and the short form is possibly trasformed into a long one. Even though we don't mean to encode or memorize something sometimes, encoding still occurs unconsciously.</p><p>Reviewing is one useful way for recalling these information. Moreover, if you are in the similar environment in which you encoded the codes before, you will have higher possibility to recall it. However, the <strong>encoding error</strong> happend during reviewing is more, and that's why a detective should try to get full information while inquiring evidences from witnesses at the 1st time.</p><p>Also, our brains are more sensible to pictorial information compared with text information. So, if you try to make each thing you want to keep code as a photo, you can boost the capability of memorizing.</p><h1 id="yoshu-bengio-beyond-i.i.d.-and-babysitting-ai">Yoshu Bengio: Beyond i.i.d. and babysitting AI</h1><p>One of the basic assumption that makes generation possible is independent identically distributed assumption. However, influenced by observing equipments, imbalanced samples and others, the distributions of training data and test data are not always the same. Hence, Prof. Bengio's team proposed that all data are sampled from the same system rather than same distribution. As for weather of two different seasons, data desciping them are sampled from the same atmospheric circulation system, but they don't distribute identically. Bengio said they tend to initialize this system with diversed initial conditions, and the result of this distribution will be taken as what the data set follows. It makes sense.</p><p><em>One thing that troubles me is, how can I model the system and figure out the initial conditions? For things with obvious physical rules, it is easy, and even the model's codes are open-acssessed online. What if the one I don't know? How to make this idea works in common situations?</em></p><p>CNN is renowned as its power in representation learning, which encodes a variety of information into vectors. Our brains also work like this. Nontheless, what they learn are supervised, and the utility of binary network, the simplification of network structure all demostrate that the captured information are redundant. The model learned is fragile, too. After adding some noises into an image, even though the image dosen't change visually for us humans, model can not tell what it is as before. It all comes from the uncontrolling unsupervision. Babysitting AI aims at modeling with environmental information and other information, so that leading AI models.</p><h1 id="andrew-c.-yao-the-advent-of-quantum-computing">Andrew C. Yao: The Advent of Quantum Computing</h1><p>The quantum computing will offer exponential speedup for crypto-code breaking, simulation of quantum physical systems, simulation of materials, chemistry, and biology, nonlinear optimization, ML and AI. It will break through the bottleneck of computing.</p><p>Its implementation is like crystallography. In terms of crystallography, you take an X-ray photo for a crystal and then compute its structure. For quantum computing, instead of taking a real photo, you just need to collect a polynomial number of sample points. By wave-particle duality, this single photo can recreate the raw image probabilistically.</p><p>According to Andrew, dimond qubits are in the highest possibility to be used in our laptops.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is &lt;a href=&quot;https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Babysitting AI and computational neuroscience impressed me a lot.
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="applications" scheme="http://yoursite.com/tags/applications/"/>
    
      <category term="tendency" scheme="http://yoursite.com/tags/tendency/"/>
    
  </entry>
  
  <entry>
    <title>The Introduction of Infrared Hyperspectral Data and Kernel PCA, June 5th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2018-06-05-talk-hyp.html"/>
    <id>http://yoursite.com/posts/talks/2018-06-05-talk-hyp.html</id>
    <published>2018-06-05T17:33:39.000Z</published>
    <updated>2021-02-16T18:16:35.981Z</updated>
    
    <content type="html"><![CDATA[<p>Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:</p><ul><li>high spectral correlation,</li><li>high spatial correlation,</li><li>and sparsity,</li></ul><p>and they casue a trouble during processing. <a id="more"></a> This talk explained why they are highly correlated but also sparse. Kernel PCA for compressing was also tested.</p><p>Check <a href="/assets/slides/hyp/hypCompression.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;high spectral correlation,&lt;/li&gt;
&lt;li&gt;high spatial correlation,&lt;/li&gt;
&lt;li&gt;and sparsity,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and they casue a trouble during processing.
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="compression" scheme="http://yoursite.com/tags/compression/"/>
    
      <category term="reconstruction" scheme="http://yoursite.com/tags/reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Weather processes interpolation based on GPR</title>
    <link href="http://yoursite.com/posts/projects/2018-05-17-gpr.html"/>
    <id>http://yoursite.com/posts/projects/2018-05-17-gpr.html</id>
    <published>2018-05-17T17:33:39.000Z</published>
    <updated>2021-02-16T18:19:54.655Z</updated>
    
    <content type="html"><![CDATA[<p>This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale dependencies of weather processes. Weather processes with and without cyclones are discussed, and two interpolation methods are proposed. Check <a href="http://www.mdpi.com/2073-4433/9/5/194/pdf">paper</a> for more information. Codes are available <a href="https://github.com/skaudrey/gpml">here</a>.</p><h1 id="reference">Reference</h1><pre><code>Carl Edward Rasmussen. Gaussian process for Machine Learning.</code></pre><h1 id="acknowledgement">Acknowledgement</h1><pre><code>Thanks for the opening source toolbox GAUSSIAN PROCESS REGRESSION AND CLASSIFICATION Toolbox version 4.0, programmed by Carl et al.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale depen
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
  </entry>
  
  <entry>
    <title>Multivariate Interpolation of Wind Fields Based on Gaussian Process Regression, Jan. 24th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2018-01-24-talk-gpr.html"/>
    <id>http://yoursite.com/posts/talks/2018-01-24-talk-gpr.html</id>
    <published>2018-01-24T18:33:39.000Z</published>
    <updated>2021-02-16T18:16:51.349Z</updated>
    
    <content type="html"><![CDATA[<p>This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check the <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html">projects' introduction</a> and <a href="https://github.com/skaudrey/gpml/">github</a> for more details.</p><p>Slides are avaliable <a href="/assets/slides/gpr/windInterpolation.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check th
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
      <category term="interpolation" scheme="http://yoursite.com/tags/interpolation/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2017-09-11-notes-fourier-GCN.html"/>
    <id>http://yoursite.com/posts/talks/2017-09-11-notes-fourier-GCN.html</id>
    <published>2017-09-11T17:33:39.000Z</published>
    <updated>2021-02-16T18:18:02.480Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.</p><p>Check <a href="/assets/slides/notes/GCN/fourier-GCN.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.&lt;/p&gt;
&lt;p&gt;Check &lt;a hre
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2017-09-11-talk-da.html"/>
    <id>http://yoursite.com/posts/talks/2017-09-11-talk-da.html</id>
    <published>2017-09-11T17:33:39.000Z</published>
    <updated>2021-02-16T18:17:17.766Z</updated>
    
    <content type="html"><![CDATA[<p>Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it from other forms of machine learning, image analysis, and statistical methods. This talk discussed the basic ideas of machine leaning, and compared it with machine learning. It is given after I came back from Harbin's summer school in Agust, 2017. After this talk, I began to throw myself into studying machine learning.</p><p>Check <a href="/assets/slides/D.A/pres.pdf">slide</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>The summer school held in Harbin, Aug. 2017.</title>
    <link href="http://yoursite.com/posts/meetings/2017-08-22-harbin.html"/>
    <id>http://yoursite.com/posts/meetings/2017-08-22-harbin.html</id>
    <published>2017-08-22T17:33:39.000Z</published>
    <updated>2021-02-16T18:19:33.454Z</updated>
    
    <content type="html"><![CDATA[<p>I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check <a href="http://mss2017.hit.edu.cn/showSubjectDominWebSite.do">here</a> for more info. I gave a talk to the students in my lab after returning, <a href="https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html">here</a> are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation. <a id="more"></a> The themes given by those professors are listed below.</p><ul><li><p>Prof. Francois</p><p>Research Area：Variational data assimilation (VAR), especially 4DVAR.</p><p>Keynotes：The direvation of adjoint models, sensitivity analysis and the introduction of image assimilation. See the <a href="/assets/notes/harbin/François%20meeting%20minutes.pdf">minutes file</a> for details.</p></li><li><p>Prof. Jordan</p><p>Research Area：Statistical Learning</p><p>Keynotes: Summarize popular machine learning algorithms, and prove the convergence etc. See the <a href="/assets/notes/harbin/Jordan%20meeting%20minutes.pdf">minutes file</a> for details.</p></li><li><p>Prof. Jurgen</p><p>Research Area：AI, DL</p><p>Keynotes: The introduction of utilizing AI. Check more from his <a href="http://people.idsia.ch/~juergen/">home page</a>.</p></li><li><p>Prof. Ma</p><p>Research Area：Compression sensing.</p><p>Keynotes: The introduction of compression sensing and its applications.</p></li><li><p>Prof. Cai</p><p>Research Area：Statistical inference.</p><p>Keynotes: Statistical inference in high-dimensions. No slides, the minutes file is <a href="/assets/notes/harbin/Tony%20meeting%20minutes.pdf">here</a>.</p></li></ul><p>Slides and records are available in the <a href="https://pan.baidu.com/s/1jGj07koiMIV-MOf17N_jeg">baidu network disk</a> with password 6o2x. Enjoy yourself.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check &lt;a href=&quot;http://mss2017.hit.edu.cn/showSubjectDominWebSite.do&quot;&gt;here&lt;/a&gt; for more info. I gave a talk to the students in my lab after returning, &lt;a href=&quot;https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html&quot;&gt;here&lt;/a&gt; are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation.
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="summer school" scheme="http://yoursite.com/tags/summer-school/"/>
    
      <category term="mathematics" scheme="http://yoursite.com/tags/mathematics/"/>
    
  </entry>
  
  <entry>
    <title>The naive implementation of some popular machine learning algorithms.</title>
    <link href="http://yoursite.com/posts/projects/2017-06-28-ml-implement.html"/>
    <id>http://yoursite.com/posts/projects/2017-06-28-ml-implement.html</id>
    <published>2017-06-28T17:33:39.000Z</published>
    <updated>2021-02-16T18:18:21.140Z</updated>
    
    <content type="html"><![CDATA[<p>Naive implementations of some M.L. algorithms, which are updated continuously.</p><p>The algorithms that have been implemented are listed as follows:</p><ul><li>Logistic Regression,</li><li>SVM solved by SMO,</li><li>K-Means，</li><li>GMM solved by EM,</li><li>Perceptron，</li><li>Naive Bayes,</li><li>LeNet-Keras，</li><li>MLP-Numpy solved with BP,</li><li>MCMC sampling.</li></ul><p>Codes are available <a href="https://github.com/skaudrey/ml_algorithm">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Naive implementations of some M.L. algorithms, which are updated continuously.&lt;/p&gt;
&lt;p&gt;The algorithms that have been implemented are list
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="naive" scheme="http://yoursite.com/tags/naive/"/>
    
  </entry>
  
</feed>
