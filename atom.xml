<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mia&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/2d5354ebc5a8c2413323ef55a6c6d252</icon>
  <subtitle>Je marche lentement, mais je ne recule jamais.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-09-13T17:01:56.563Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Mia Feng</name>
    <email>skaudreymia@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/posts/notes/2021-08-27-notes-paper-cvpr2021-ssl-graph.html"/>
    <id>http://yoursite.com/posts/notes/2021-08-27-notes-paper-cvpr2021-ssl-graph.html</id>
    <published>2021-08-27T15:16:00.000Z</published>
    <updated>2021-09-13T17:01:56.563Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Paper 1: <a href="https://arxiv.org/pdf/2104.00323.pdf">Jigsaw Clustering for Unsupervised Visual Representation Learning</a></li><li>Paper 2: <a href="https://arxiv.org/pdf/2104.00240.pdf">Self-supervised Motion Learning from Static Images</a></li><li>Paper 3: <a href="https://arxiv.org/pdf/2104.00862.pdf">Self-supervised Video Representation Learning by Context and Motion Decoupling</a></li><li>Paper 4: <a href="https://arxiv.org/pdf/2104.11487.pdf">Skip-convolutions for Efficient Video Processing</a></li><li>Paper 5: <a href="https://arxiv.org/pdf/2104.09496.pdf">Temporal Query Networks for Fine-grained Video Understanding</a></li><li>Paper 6: <a href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></li><li>Paper 7: <a href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></li><li>Paper 8: <a href="https://arxiv.org/pdf/2105.09711.pdf">An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</a></li><li>Paper 9: <a href="https://arxiv.org/abs/2008.03087">Cascade Graph Neural Networks for RGB-D Salient Object Detection</a></li><li>Paper 10: <a href="https://arxiv.org/abs/2103.01302">Coarse-Fine Networks for Temporal Activity Detection in Videos</a></li><li>Paper 11: <a href="https://arxiv.org/abs/2104.03851">CoCoNets: Continuous Contrastive 3D Scene Representations</a></li><li>Paper 12: <a href="https://arxiv.org/abs/2104.04015">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</a></li><li>Paper 13: <a href="https://arxiv.org/abs/2108.03662">Discriminative Latent Semantic Graph for Video Captioning</a></li><li>Paper 14: <a href="https://arxiv.org/abs/2108.02183?context=cs">Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</a></li><li>Paper 15: <a href="https://arxiv.org/abs/2011.10566">Exploring simple siamese representation learning</a></li><li>Paper 16: <a href="https://arxiv.org/abs/2107.05475">GiT: Graph Interactive Transformer for Vehicle Re-identification</a></li><li>Paper 17: <a href="https://arxiv.org/abs/2103.01730">Graph-Time Convolutional Neural Networks</a></li><li>Paper 18: <a href="https://arxiv.org/abs/1910.02370">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</a></li><li>Paper 19: <a href="https://arxiv.org/pdf/2107.09787.pdf">Group Contrastive Self-Supervised Learning on Graphs</a></li><li>Paper 20: <a href="https://link.springer.com/article/10.1007/s10618-021-00750-y">Homophily outlier detection in non-IID categorical data</a></li><li>Paper 21: <a href="https://arxiv.org/abs/2108.02113">Hyperparameter-free and Explainable Whole Graph Embedding</a></li><li>Paper 22: <a href="https://arxiv.org/abs/1908.01000">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</a></li><li>Paper 23: <a href="https://arxiv.org/abs/2010.12609">Iterative graph self-distillation</a></li><li>Paper 24: <a href="https://arxiv.org/abs/2103.17260">Learning by Aligning Videos in Time</a></li><li>Paper 25: <a href="https://arxiv.org/abs/2103.13125">Learning graph representation by aggregating subgraphs via mutual information maximization</a></li><li>Paper 26: <a href="https://arxiv.org/abs/1802.09612">Mile: A multi-level framework for scalable graph embedding</a></li><li>Paper 27: <a href="https://arxiv.org/pdf/2108.03400.pdf">Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</a></li><li>Paper 28: <a href="https://arxiv.org/abs/2107.02639">Multi-Level Graph Contrastive Learning</a></li><li>Paper 29: <a href="https://arxiv.org/abs/2104.09856">Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</a></li><li>Paper 30: <a href="https://arxiv.org/abs/2008.04575">PiNet: Attention Pooling for Graph Classification</a></li><li>Paper 31: <a href="https://arxiv.org/abs/2107.02039">Power Law Graph Transformer for Machine Translation and Representation Learning</a></li><li>Paper 32: <a href="https://arxiv.org/abs/2106.04113">Self-supervised Graph-level Representation Learning with Local and Global Structure</a></li><li>Paper 33: <a href="https://arxiv.org/abs/2105.09111">Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</a></li><li>Paper 34: <a href="https://arxiv.org/abs/2107.01903">SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</a></li><li>Paper 35: <a href="https://arxiv.org/abs/2006.14613">Space-time correspondence as a contrastive random walk</a></li><li>Paper 36: <a href="https://arxiv.org/abs/2103.06122">Spatially consistent representation learning</a></li><li>Paper 37: <a href="https://arxiv.org/abs/2008.03800">Spatiotemporal contrastive video representation learning</a></li><li>Paper 38: <a href="https://arxiv.org/abs/2104.11452">SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</a></li><li>Paper 39: <a href="https://arxiv.org/abs/2105.13033">SSAN: Separable Self-Attention Network for Video Representation Learning</a></li><li>Paper 40: <a href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411953">tdgraphembed: Temporal dynamic graph-level embedding</a></li><li>Paper 41: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf">Videomoco: Contrastive video representation learning with temporally adversarial examples</a></li><li>Paper 42: <a href="https://arxiv.org/abs/2107.01181">Visual Relationship Forecasting in Videos</a></li><li>Paper 43: <a href="https://openreview.net/pdf?id=AAes_3W-2z">Wasserstein embedding for graph learning</a></li></ul><a id="more"></a><h2 id="Paper-1-Jigsaw-Clustering-for-Unsupervised-Visual-Representation-Learning">Paper 1: <a href="https://arxiv.org/pdf/2104.00323.pdf">Jigsaw Clustering for Unsupervised Visual Representation Learning</a></h2><h3 id="Previous-pretext-task">Previous pretext task</h3><ul><li>Intra-image tasks: including colorization and jigsaw puzzle, design a transform of one image and train the network to learn the transform.<ul><li>Since only the training batch itself is forwarded each time, they name these methods as single -batch methods.</li><li>Can be achieved using only one image’s information, limiting the learning ability of feature extractors.</li></ul></li><li>Inter-image tasks<ul><li>require the network to discriminate among different images.</li><li>Try to reduce the distance between representations of positive pairs and enlarge the distance between representations of negative samples.</li><li>since each training batch and its augmented version are forwarded simultaneously, methods are named as dual-batches methods.</li></ul></li><li>The way to design an efficient single-batch based method with similar performance to dual-batches methods is still an open problem.</li></ul><h3 id="Ideas">Ideas</h3><p>They propose a framework for efficient training of unsupervised models using Jigsaw clustering, which combines advantages of solving jigsaw puzzles and contrastive learning, and makes use of both intra- and inter-image information to guide feature extractor.</p><h4 id="Jigsaw-Clustering-task">Jigsaw Clustering task</h4><ul><li>Every image in a batch is split into different patches. They are randomly permuted and stitched to form a new batch for training.</li><li><strong>Goal</strong> : recover the disrupted parts back to the original images.</li><li>The patches are permuted in a batch</li><li>The network has to distinguish between different parts of one image and identifies their original positions to recover the original image from multiple montage input images.</li><li><em><strong>Why works?</strong></em><ul><li>Discriminating among different patches in one stitched image forces the model to <em>capture instance-level information inside an image</em>. This level of feature is missing in general in other contrastive learning methods.</li><li>Clustering different patches from multiple input images helps the model <em>learn image-level features across images.</em></li><li>arranging every patch to the correct location requires detailed location information, which was considered in single-batch methods.</li></ul></li></ul><h3 id="How">How?</h3><ul><li><p>Batches</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827135623377.png" alt="image-20210827135623377" style="zoom:80%;" /><p>Each batch will have $n$ images, and after splitting <strong>(patches split in images have a level of overlap)</strong>, there will be $n\times m\times m$ patches, the patches are latterly stitched into $n$ images. The cluster branch will cluster the $n\times m\times m$ patches into $n$ classes so as to define which original image that one patch comes from.</p><ul><li>Using montage images as input instead of every single patch is noteworthy, since directly using small patches as input leads to the solution with only global information.</li><li>the input images form only one batch with the same size as the original batch, which costs half of resource during training compared with recent methods.</li><li><em><strong>The choice of $m$ affects the difficulty of the task</strong></em>. They show that $m=2$ is good.</li></ul></li><li><p>Network</p><ul><li>The logits is in size $n\times m \times m$. (in location branch)</li></ul></li><li><p>Loss function</p><ul><li><p>The target of clustering is pulling together objects from the same class and pushing away patches from different classes.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827151850175.png" alt="image-20210827151850175"></p></li><li><p>The loss function of location branch is simply cross-entropy loss.</p></li><li><p>The final objective is the weighted summation of the two losses mentioned above. But in their experiments, when the two loss are simply summed, they get the best result.</p></li></ul></li></ul><h2 id="Paper-2-Self-supervised-Motion-Learning-from-Static-Images">Paper 2: <a href="https://arxiv.org/pdf/2104.00240.pdf">Self-supervised Motion Learning from Static Images</a></h2><h3 id="Why">Why</h3><ul><li>To well distinguish actions, correctly locating the prominent motion areas is of crucial importance.</li></ul><h3 id="Previous-work">Previous work</h3><ul><li>Motion learning by architectures: two-stream networks and 3D convolutional networks. The two-stream networks extract motions representations explicitly from optical flows, while 3D structures apply convolutions on the temporal dimension or space-time cubics to extract motion cues implicitly.</li><li>Self-supervised image representation learning: patch-based approaches , image-level pretext tasks such as image inpainting, image colorization, motion segment prediction and predicting image rotations.</li><li>Self-supervised video representation learning: extend patch-based context prediction to spatial-temporal scenarios, e.g., spatio-temporal puzzles, video cloze procedure and frame/clip order prediction; learn representations by predicting future frames; generate supervision signals, such as speed up prediction and play back rate prediction.</li></ul><h3 id="How-2">How</h3><h4 id="Idea">Idea</h4><ul><li>Learn <strong>M</strong>otion from <strong>S</strong>tatic <strong>I</strong>mages (MoSI), take images as our data source, and generate deterministic motion patterns.</li><li>Given the desired direction and the speed of the motions, MoSI generates pseudo motions from static images. By correctly classifying the direction and speed of the movement in the image sequence, models trained with MoSI is able to well encode motion patterns.</li><li>Furthermore, a static mask is applied to the pseudo motion sequences. This produces inconsistent motions between the masked area and the unmasked one, which guides the network to focus on the inconsistent local motions</li></ul><h4 id="Motion-learning-from-static-images">Motion learning from static images</h4><h5 id="Pseudo-motions">Pseudo motions</h5><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172717585.png" alt="image-20210827172717585"></p><ul><li><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172910767.png" alt="image-20210827172910767"></p></li><li><p>Label pool:</p><ul><li>For each label, a non-zero speed only exists on one axis.</li></ul></li><li><p>Pseudo motion generation</p><ul><li><p>To generate the samples with different speeds, the moving distance from the start to the end of the Pseudo sequences are need.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827174808696.png" alt="image-20210827174808696"></p></li><li><p>The start location is randomly sampled from a certain area which ensures the end location is located completely within the source image.</p></li><li><p>For label $(x, y) = (0, 0)$, where the sampled image sequence is static on both axis, the start location is selected from the whole image with uniform distribution.</p></li></ul></li><li><p>Classification</p><ul><li>each batch contains all transformed image sequences generated from the same source image</li><li>The model is trained by cross entropy loss.</li></ul></li></ul><h5 id="Static-masks">Static masks</h5><ul><li>The static masks creates local motion patterns that are inconsistent with the background.</li><li>introduce static masks as the second core component of the proposed MoSI since the model may possibly focus on just several pixels.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181344187.png" alt="image-20210827181344187"></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181534056.png" alt="image-20210827181534056"></li><li>the model is now required not only to recognize motion patterns, but also to spot where the motion is happening</li></ul><h5 id="Implementation">Implementation</h5><ul><li>Data preparations: the source images need to be first sampled from the videos in the video datasets.</li><li>Augmentation: randomize the location and the size of the unmasked area. In addition, randomize the selection of the background frames in the MoSI.</li></ul><h2 id="Paper-3-Self-supervised-Video-Representation-Learning-by-Context-and-Motion-Decoupling">Paper 3: <a href="https://arxiv.org/pdf/2104.00862.pdf">Self-supervised Video Representation Learning by Context and Motion Decoupling</a></h2><h3 id="Why-2">Why</h3><ul><li>What to learn ?<ul><li>Context representation can be used to classify certain actions, but also leads to background bias.</li><li>Motion representation</li></ul></li><li>Problem<ul><li>The source of supervision: video in compressed format (such as MPEG-4) roughly decouples the context and motion information in its I-frames and motion vectors.<ul><li>I-frames can represent relatively static and coarse-grained context information, while motion vectors depict dynamic and fine-grained movements</li></ul></li></ul></li></ul><h3 id="Previous-work-2">Previous work</h3><ul><li>SS video representation learning<ul><li>video specific pretext tasks: estimating video playback rates, verifying temporal order of clips, predicting video rotations, solving space-time cubic puzzles, and dense predictive coding.</li><li>Contrastive learning<ul><li>clips from the same video are pulled together while clips from different videos are pushed away.</li><li>employ adaptive cluster assignment, where the representation and embedding clusters are simultaneously learned.</li><li>But they may suffer from the context bias problem.</li></ul></li><li>mutual supervision across modalities</li><li>DSM: enhance the learned video representation by decoupling the scene and the motion. It simply changes the construction of positive and negative pairs in contrastive learning.</li></ul></li><li>Action recognition in compressed videos<ul><li>Video compression techniques (e.g., H.264 and MPEG4) usually store only a few key frames completely, and reconstruct other frames using motion vectors and residual errors from the key frames.</li><li>Some methods directly build models on the compressed data.<ul><li>One replace the optical flow stream in two-stream action recognition models with a motion vector stream.</li><li>CoViAR: use all modalities, including I-frames, motion vectors and residuals.</li></ul></li></ul></li><li>Motion prediction<ul><li>deduce the states of an object in a near future.</li><li>Typical models: RNNs, Transformers, and GNNs.</li></ul></li></ul><h3 id="Goal">Goal</h3><p>Design a self-supervised video representation learning method that jointly learns motion prediction and context matching.</p><ul><li>The context matching task aims to give the video network a rough grasp of the environment in which actions take place. It casts a NCE loss between global features of video clips and I-frames, where clips and I-frames from the same videos are pulled together, while those from different videos are pushed away.</li><li>The motion prediction task requires the model to predict pointwise motion dynamics in a near future based on visual information of the current clip.<ul><li>They use pointwise contrastive learning to compare predicted and real motion features at every spatial and temporal location $(x,y,t)$, which will lead to more stable pretraining and better transferring performance.</li><li>It works as a strong regularization for video networks, and it can also be regarded as an auxiliary task clearly improves the performance of supervised action recognition.</li></ul></li></ul><h3 id="How-3">How</h3><ul><li>Data: compressed videos, to be exactly, MPEG-2 Part2, where every I-frames is followed by 11 consecutive P-frames.</li><li>Methods: context matching task for coarse-grained and relatively static context representation, and a motion prediction task for learning fine-grained and high-level motion representation.<ul><li>context matching<ul><li>where (video clip, I-frame) pairs from the same videos are pulled together, while pairs from different videos are pushed away</li><li></li></ul></li><li>Motion prediction<ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827225903818.png" alt="image-20210827225903818"></li><li>Only feature points corresponding to the same video $i$ and at the same spatial and temporal position $(x, y, t)$ are regarded as positive pairs, otherwise they are regarded as negative pairs.</li><li>The input and output for Transformer is considered as a 1-D sequence.</li><li>Some findings<ul><li>Predicting future motion information leads to significantly better video retrieval performance compared with estimating current motion information;</li><li>Matching predicted and “groundtruth” motion features using the pointwise InfoNCE loss brings better results than directly estimating motion vector values;</li><li>Different encoder-decoder networks lead to similar results, while using Transformer performs slightly better.</li></ul></li></ul></li></ul></li></ul><h2 id="Paper-4-Skip-convolutions-for-Efficient-Video-Processing">Paper 4: <a href="https://arxiv.org/pdf/2104.11487.pdf">Skip-convolutions for Efficient Video Processing</a></h2><h3 id="Why-3">Why</h3><ul><li>Leverage the large amount of redundancies in video streams and save computations</li><li>The spiking nets is  lack of efficient training algorithms</li><li>Residual frames provide a strong prior on the relevant regions, easing the design of effective gating functions</li></ul><h3 id="Previous-work-3">Previous work</h3><ul><li>Efficient video models<ul><li>feature propagation, which computes the expensive backbone features only on key-frames.</li><li>interleave deep and shallow backbones between consecutive frames: methods are mostly suitable for global prediction tasks where a single prediction is made for the whole clip.</li></ul></li><li>Efficient image models: The reduction of parameter redundancies<ul><li>model compression: Skip-Conv leverages temporal redundancies in activations.</li><li>conditional computations in developing efficient models for images.</li></ul></li></ul><h3 id="Goal-2">Goal</h3><p>To speed up any convolutional network for inference on video streams.  Considering a video as a series of changes across frames and network activations, denotes as residual frames. They reformulate standard convolution to be efficiently computed over such residual frames by limiting the computation only to the regions with significant changes while skipping the others. The important residuals are learned by a gating function.</p><h3 id="How-4">How</h3><ul><li>The contributions<ul><li>a simple reformulation of convolution, which computes features on highly sparse residuals instead of dense video frames</li><li>Two gating functions, Norm gate and Gumbel gate, to effectively decide whether to process or skip each location, where Gumbel gate is trainable.</li></ul></li></ul><h4 id="Skip-Convolutions">Skip Convolutions</h4><ul><li><p>Convolutions on residual frames</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901091727352.png" alt="image-20210901091727352"></li><li>for every kernel support filled with zero values in $\mathrm{r}<em>t$, the corresponding output will be trivially zero, and the convolution can be skipped by copying values from $\mathrm{z}</em>{t−1}$ to $\mathrm{z}_{t}$.</li><li><em>Introduce a gating function for each convolutional layer to predict a binary mask indicating which locations should be processed, and taking only $\mathrm{r}_t$ as input.</em> $\mathrm{r}_t$ as input will provide a strong prior to the gating function.</li></ul></li><li><p>Gating functions</p><ul><li><p>Norm gate: decides to skip a residual if its magnitude (norm) is small enough, not learnable</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901093537848.png" alt="image-20210901093537848"></li><li>indicate regions that change significantly across frames, but not all changes are equally important for the final prediction.</li></ul></li><li><p>Gumbel gate, trainable with the convolutional kernels.</p><ul><li><p>A higher efficiency can be gained by introducing a higher</p></li><li><p>pixel-wise Bernoulli distributions by applying a sigmoid function. During training, sample binary deisions from the Bernoulli distribution.</p></li><li><p>Employ the Gumbel reparametrization and a straight-through gradient estimator in order to backpropagate through the sampling procedure.</p></li><li><p>The Gating parameters are learned jointly with all model parameters by minimizing $\mathcal{L}<em>{task}+\beta \mathcal{L}</em>{gate}$.</p></li><li><p>The gating loss is defined as the average multiply-accumulate (MAC) count needed to process $T$ consecutive frames as</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901130744146.png" alt="image-20210901130744146" style="zoom:80%;" /></li><li><p>train the model over a fixed-length of frames and do inference iteratively on an binary number of frames.</p></li><li><p>By simply adding a downsampling and an unsampling  function on the predicted gates, the Skip-conv can be extended to generate structured sparsity. This structure will enable more efficient implementation with minimal effect on performance.</p></li></ul></li></ul></li><li><p>Generalization and future work</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901141700865.png" alt="image-20210901141700865"></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901142059270.png" alt="image-20210901142059270"></li></ul></li></ul><h2 id="Paper-5-Temporal-Query-Networks-for-Fine-grained-Video-Understanding">Paper 5: <a href="https://arxiv.org/pdf/2104.09496.pdf">Temporal Query Networks for Fine-grained Video Understanding</a></h2><p><a href="http://www.robots.ox.ac.uk/~vgg/research/tqn/">http://www.robots.ox.ac.uk/~vgg/research/tqn/</a></p><h3 id="Why-4">Why</h3><ul><li>For finer-grain classification which depends on subtle differences in pose, the specific sequence, duration and number of certain subactions, it requires reasoning about events at varying temporal scales and attention to fine details.</li><li>the constraints imposed by finite GPU memory. To overcome this, one way is to use pretrained features, but this relies on good initializations and ensures a small domain gap. Another solution focuses on extracting key frames from untrimmed videos.</li><li>VQA (visual question and answering)<ul><li>Have queries which attends to relevant features for predicting the answers.</li><li>The problem in this paper is more interested in a common set of queries shared across the whole dataset.</li></ul></li></ul><h3 id="Goal-3">Goal</h3><ul><li><p>Fine-grained classification of actions in untrimmed videos.</p></li><li><p>Propose a Transformer-based video network,  namely the Temporal Query Network (TQN) for fine-grained action classification, which will take a video and a predefined set of queries as input and output responses for each query, where the response is query dependent.</p></li><li><p>The queries act as “experts” that are able to pick out from the video the temporal segments required for their response.</p><ul><li>Pick out relevant temporal segments and ignore irrelevant segments.</li><li>Since only relevant segments will help in classification, the excessive temporal aggregation may lose the signal in the noise.</li></ul></li><li><p>Introduce a stochastically updated feature bank to solve memory constraints.</p><ul><li>features from densely sampled contiguous temporal segments are cached over the course of training,</li><li>only a random subset of these features is computed online and backpropagated through in each training iteration</li></ul></li></ul><h3 id="How-5">How</h3><p>Train with weak supervision, meaning that at training time the temporal location information for the response is not proposed.</p><h4 id="TQN">TQN</h4><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901152032964.png" alt="image-20210901152032964" style="zoom:50%;" /><ul><li>Identifies rapidly occurring discriminative events in untrimmed videos and can be trained given only weak supervision.</li><li>Achieves by learning a set of permutation-invariant query vectors corresponding to predefined queries about events and their attributes, which are transformed into response vectors using Transformer decoder layers attending to visual features extracted from a 3DCNN backbone.</li><li>Given an untrimmed video, first visual features for <em>contiguous non-overlapping clips of 8 frames</em> are extracted using a 3D ConvNet.</li><li>multiple layers of a parallel non-autoregressive Transformer decoder</li><li>The training loss is a multi-task combination of individual classifier losses, which are Softmax cross-entropy, where the labels are the ground-truth attribute for the label query.</li></ul><h4 id="Stochastically-updated-feature-bank">Stochastically updated feature bank</h4><ul><li>The memory bank caches the clip-level 3DCNN visual features.</li><li>In each training iteration, a fixed number $n_{online}$ of randomly samples consecutive clips are forwarded through the visual encoder, while the remaining $t-n_{online}$ clip features are retrieved from the memory bank.</li><li>The two sets mentioned above are then combined and input into the TQN decoder for final prediction and backpropagation.</li><li>During inference, all features are computed online without the memory bank.</li><li>Advantages: fixed number of clips to reduce the memory price. Also enables to extend temporal context and promotes diversity in each mini-batch as multiple different videos can be included instead of just a single long video.</li></ul><h4 id="Factorizing-categories-into-attribute-queries">Factorizing categories into attribute queries</h4><ul><li>This factorization unpacks the monolithic category labels into their semantic constituents</li><li>The categories are factorized into multiple queries that with several attributes respectively.</li></ul><h4 id="Implementation-2">Implementation</h4><ul><li>Use S3D as visual backbone, operating on non-overlapping contiguous video clips of 8 frames.</li><li>The decoder consists of 4 standard post-normalization Transformer decoder layers, each with 4 attention heads.</li><li>The visual encoder is pre-trained on Kinetics-400.</li></ul><h2 id="Paper-6-Unsupervised-disentanglement-of-linear-encoded-facial-semantics">Paper 6: <a href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></h2><h3 id="Why-5">Why</h3><ul><li>Sampling along the linear-encoded representation vector in latent space will change the associated facial semantics accordingly.</li><li>Current frameworks that maps a particular facial semantics to a latent representation vector relies on training offline classifiers with manually labeled datasets. Therefore they require artificially defined semantics and provide the associated labels for all facial images. If training with labeled facial semantics:<ul><li>They demand extra effort on human annotations for each new attributes proposed</li><li>Each semantics is defined artificially</li><li>unable to give any insights on the connections among different semantics</li></ul></li><li>Previous work<ul><li>Synthesizing faces by GAN, which changes the target attribute but keep other information ideally unchanged.<ul><li>The comprehensive design of loss functions.</li><li>the involvement of additional attribute features</li><li>the architecture design</li></ul></li><li>To achieve meaningful representations, one should always introduce either supervision or inductive biases to the disentanglement method<ul><li>Inductive bias: rise from the symmetry of natural objects and the 3D graphical information.</li><li>Reconstruct the face images by carefully remodeling the graphics of camera principal, which makes it possible to decompose the facial images into environmental semantics and other facial semantics.</li><li>It’s unable to generate realistic faces and perform pixel-level face editing on it.</li></ul></li></ul></li></ul><h3 id="Goal-4">Goal</h3><ul><li>Photo-realistic images synthesizing, minimize the demand for human annotations</li><li>Capture linear-encoded facial semantics.</li></ul><h3 id="How-6">How</h3><p>With a given collection of coarsely aligned faces, a GAN is trained to mimic the overall distribution of the data. Then use the faces that the trained GAN generates as training data and trains a 3D deformable face reconstruction method. A mutual reconstruction strategy stabilizes the training significantly. Then they keep a record of the latent code from StyleGAN and apply linear regression to disentangle the target semantics in the latent space.</p><h4 id="Decorrelating-latent-code-in-StyleGAN">Decorrelating latent code in StyleGAN</h4><ul><li><p>Enhance the disentangled representation by decorrelating latent codes.</p><ul><li><p>In order to maximizes the utilization of all dimensions, they use Pearson correlation coefficient to zero and variance of all dimension.</p></li><li><p>Introduce decorrelation regularization via a loss function</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901225711236.png" alt="image-20210901225711236"></p></li><li><p>The mapping network is the only one to update with the new loss.</p></li></ul></li></ul><h4 id="Stabilized-training-for-3D-face-reconstruction">Stabilized training for 3D face reconstruction</h4><ul><li><p>Use the decomposed semantics to reconstruct the original input image with the reconstruction loss</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901230258180.png" alt="image-20210901230258180"></li><li>The 3D face reconstruction algorithm struggles to estimate the pose of profile or near-profile faces.</li><li>The algorithm tries to use extreme values to estimate the texture and shape of each face independently, which deviate far away from the actual texture and shape of the face. To solve this, the mutual reconstruction strategy is proposed to prevent the model from using extreme values to fit individual reconstruction, and the model learns to reconstruct faces with a minimum variance of the shape and texture among all samples.</li></ul></li><li><p>During training, they swap the albedo and depth map between two images with a probability  $\epsilon$ to perform the reconstruction with the alternative loss.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902113314940.png" alt="image-20210902113314940"></p></li><li><p>simply concatenate the two images channel-wise as input to the confidence network</p></li></ul><h4 id="Disentangle-semantics-with-linear-regression">Disentangle semantics with linear regression</h4><ul><li>The Ultimate goal of disentangling semantics is to find a vector in StyleGAN, such that it only takes control of the target semantics.</li><li>Semantic gradient estimation<ul><li>It’s observed that with StyleGAN, many semantics can be linear-encoded. Therefore, the gradient is now independent of the input latent code.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902120453628.png" alt="image-20210902120453628"></li></ul></li><li>Semantic linear regression<ul><li>In real world scenario, the gradient is hard to estimate directly because back-propagation only captures local gradient, making it less robust to noises.</li><li>Propose a linear regression model to capture global linearity for gradient estimation.</li></ul></li></ul><h4 id="Image-manipulation-for-data-augmentation">Image manipulation for data augmentation</h4><ul><li>One application is to perform data augmentation.</li><li>By extrapolating along $\mathrm{v}$ beyond its standard deviation, we can get samples with more extreme values for the associated semantics.</li></ul><h4 id="Localized-representation-learning">Localized representation learning</h4><ul><li>Find the manipulation vectors $\hat{\mathrm{v}}$ that capture interpretable combinations of pixel value variations.</li><li>Start by defining a Jacobian matrix, which is the concatenation of all canonical pixel-level $\mathrm{v}$.</li><li>interpolation along $\hat{\mathrm{v}}$ should result in significant but localized (i.e. sparse) change across the image domain.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902133618545.png" alt="image-20210902133618545"></li></ul><h2 id="Paper-7-Unsupervised-disentanglement-of-linear-encoded-facial-semantics">Paper 7: <a href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></h2><h3 id="Why-6">Why</h3><ul><li>The current success of GNNs is attributed to an implicit assumption that the input of GNNs contains the entire attributed graph, which will collapse or the accuracy will decrease if the entire graph is too large.<ul><li>One intuitive solution for the problem is sampling: neighbor sampling or graph sampling. The graph sampling will sample subgraphs and can avoid neighbor explosion. But not like neighbor sampling, it cannot guarantee that each node is sampled.<ul><li>Neighbor sampling: GraphSAGE, VRGCN</li><li>Sampling subgraphs: Fast-GCN, ClusterGCN, DropEdge, DropConnection, GraphSAINT for edge sampling.</li></ul></li><li>Another solution is compressing the size of input graph data the the GNN model: such as pruning, shallow networks, designing compact layers and quantizing the parameters.</li></ul></li><li>The challenges of compressed GNN<ul><li>The compression of the loaded data demands more attention</li><li>The original GNN is shallow and therefore the compression will be more difficult to be achieved.</li><li>Require the compressed GNNs to possess sufficient parameters for representations.</li></ul></li></ul><h3 id="Goal-5">Goal</h3><ul><li>reduce the redundancies in the node representations while maintain the principle information.</li></ul><h3 id="How-7">How</h3><ul><li>Binarizes both the network parameters and input node features. The original matrices multiplications are revised to binary operations for accelerations. Design a new gradient approximation based back-propagation to train the proposed Bi-GCN.</li></ul><h4 id="GCN">GCN</h4><ul><li><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151200499.png" alt="image-20210902151200499"></p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151226323.png" alt="image-20210902151226323"></p></li><li><p>Use task-dependent loss function, e.g. the cross-entropy.</p></li></ul><h4 id="Bi-GCN">Bi-GCN</h4><p>Only focus on binarizing the feature extraction step, because the aggregation step possesses no learnable parameters and it only requires a few calculations. To reduce the computational complexities and accelerates the inference process, the XNOR and bit count operations are utilized.</p><h5 id="Binarization-of-the-feature-extraction-step">Binarization of the feature extraction step</h5><ul><li><p>Binarization of the parameters</p><p>Each column if the parameter matrix is splitted as a bucket, and $\alpha$ maintain the scalars for each bucket.</p></li><li><p>Binarization of the node features</p><p>Processed by the graph convolutional layers.</p><ul><li>Split the hidden state at layer $l$ into row buckets based on the constraints of the matrix multiplication.</li><li>Let $F^{(l)}$ be the binarized buckets, the binary approximation of $H^{(l)}$ can be obtained via  <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902160626221.png" alt="image-20210902160626221" style="zoom:67%;" /></li><li>$\beta$ can be considered as the node-weights for the features representations. Each element of $F^{(l)}$ and $B^{(l)}$ is either -1 or 1.</li><li>This Binarization also possesses the ability of activation, therefore the activation operations can be eliminated.</li></ul></li></ul><h5 id="Binary-gradient-approximation-based-back-propagation">Binary gradient approximation based back propagation</h5><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902164243961.png" alt="image-20210902164243961"></p><h2 id="Paper-8-An-Attractor-Guided-Neural-Networks-for-Skeleton-Based-Human-Motion-Prediction">Paper 8: <a href="https://arxiv.org/pdf/2105.09711.pdf">An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</a></h2><h3 id="Why-7">Why</h3><ul><li>Most existing methods tend to build the relations among joints, where local interactions between joint pairs are well learned. However, the global coordination of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously.</li><li>Most graphs are designed according to the kinematic structure of the human to extract motion features, but hardly do they learn the relations between spatial separated joint pairs directly.</li><li>Except for speed, other dynamic information like accelerated speed are not counted into previous work, which ignores important motion information.</li><li>Previous work<ul><li>Human motion prediction<ul><li>Many works suffer from discontinuities between the observed poses and the predicted future ones.</li><li>Consider global spatial and temporal features simultaneously, such as transform temporal space to trajectory space to take the global temporal information into account.</li></ul></li><li>Joint relation modeling<ul><li>Focus on skeletal constraints to model correlation between joints.</li><li>adaptive graph: the existed works weak the global coordination of all joints since they are learned from parts.</li></ul></li><li>Dynamic representation of skeleton sequence<ul><li>Many attempts proposed to extract enriching dynamic representation from raw data, but they only extract the dynamics from neighbor frames</li><li>Extract the dynamic features among frames through multiple timescale will extract more motion features.</li></ul></li></ul></li></ul><h3 id="Goal-6">Goal</h3><p>To characterize the global motion features and thus can learn both the local and global motion features simultaneously.</p><p>Generate predicted poses through proposed framework AGN and the historical 3D skeleton-based poses.</p><h3 id="How-8">How</h3><ul><li>Pipeline: A BA (balance attractor) is learned by calculating dynamic weighted aggregation of single joint feature. Then the difference between the BA and each joint feature is calculated. Later the resulting new joint features are used to calculate joints similarities to generate final joint relations.</li><li>Framework: Attractor-Guided Neural Network, which first learn an enriching dynamic representation from raw position information adaptively through MTDE (multi-timescale dynamics extractor). Then the AJRE (attractor-based joint relation extractor) is imported , including a LIE (local interaction extractor), a GCE (global coordination extractor) and an adaptive feature fusing module.<ul><li>AJRE: a joint relation modeling = GCE+LIE. The GCE models the global coordination of all joints, while LIE mines the local interactions between joint pairs.</li><li>MTDE: extract enriching dynamic information from raw input data for effective prediction.</li></ul></li></ul><h4 id="MTDE">MTDE</h4><ul><li><p>A combination of different time scales motion dynamics</p></li><li><p>Two stream, one path is the raw input poses, the other is the difference between adjacent frames in raw input. The dynamics of each joint separately is also modeled to avoid the interference of other joints.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903184316148.png" alt="image-20210903184316148"></p></li><li><p>The MTDE uses three 1DCNN but in different kernel size (5，3，1) to extract the local (joint-level) dynamics.</p></li></ul><h4 id="AJRE">AJRE</h4><ul><li>Consists of GCE and LIE to separately model global coordination of all joints and local interactions between joint pairs, and also AFFM which is used to fuse features according to channel-wise attention to improve the flexibility of joint relation modeling.</li><li>GCE and LIE work in parallel, and they are followed by AFFM.</li></ul><h5 id="GCE">GCE</h5><p>Global coordination of all joints, so they learn a medium to build new joint relations indirectly.</p><ul><li>BA (balance attractor unit) unit calculates all joints’ aggregation to characterize the global motion features. After transpose the input features, then BA unit applies $1\times 1$ conv to get a dynamic weighted feature aggregation of N joints features. And $X_{new}$ is the difference between the output features of BA and the original $X$.</li><li>The new relations of all joints is built by the Cosine similarity unit, which measure between $X_{new},X$. The cosine similarity between all row vector pairs to illustrate the correlation between joint pairs. The correlation matrix on each channel is calculated since each channel encodes specific spatiotemporal features and should focus on different correlations compared with other channels.</li></ul><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232718696.png" alt="image-20210903232718696"></p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232735446.png" alt="image-20210903232735446"></p><h5 id="LIE">LIE</h5><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232837194.png" alt="image-20210903232837194"></p><ul><li>It’s used to learn local interactions between joint pairs, including adjacent and distant joints.</li><li>To learn the relations between adjacent joint pairs, a pure $3\times 3$ convolution is adopted. To learn the relations between distant joint pairs, the self-attention is used.</li></ul><h5 id="AFFM">AFFM</h5><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903234025866.png" alt="image-20210903234025866"></li><li>Channel attention to fuse features adaptively and reform more reliable representation.</li><li>After the sigmoid in the AFFM unit, the importance ratio of each channel is obtained. Then the channel-wise multiplication between ratio and raw input is done to reform features.</li></ul><h2 id="Paper-9-Cascade-Graph-Neural-Networks-for-RGB-D-Salient-Object-Detection">Paper 9: <a href="https://arxiv.org/abs/2008.03087">Cascade Graph Neural Networks for RGB-D Salient Object Detection</a></h2><p>Codes: <a href="https://github.com/LA30/Cas-Gnn">https://github.com/LA30/Cas-Gnn</a></p><h3 id="Why-8">Why</h3><ul><li>How to leverage the two complementary data sources: color and depth information</li><li>Current works either simply distill prior knowledge from the corresponding depth map for handling the RGB-image or blindly fuse color and geometric information to generate the coarse depth-aware representations, hindering the performance of RGB-D saliency detectors</li><li>Identify saliency objects of varying shape and appearance, show robustness towards heavy occlusion, various illumination and background.</li><li>Network cascade is an effective scheme for a variety of high-level vision applications. It will ensemble a set of models to handle challenging tasks in a coarse-to-fine or easy-to-hard manner.</li></ul><h3 id="Goal-7">Goal</h3><p>Salient object detection for RGB-D images. To distill and reason the mutual benefits between the color and depth data sources through  a set of cascade graphs.</p><p>Predict a saliency Map given an input image and its corresponding depth image.</p><h3 id="How-9">How</h3><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210904162325031.png" alt="image-20210904162325031"></p><ul><li>CGR (cascade graph reasoning ) module to learn dense features, from which the saliency map can be easily inferred. It explicitly reasons about the 2D appearance and 3D geometry information for RGBD SOD.</li><li>Each graph consists of two types of nodes, geometry nodes storing depth features and appearance nodes storing RGB-related features.</li><li>Multiple-level graphs sequentially chained by coarsening the preceding graph into two domain-specific guidance nodes for the following cascade graph.</li></ul><h4 id="Cross-modality-reasoning-with-GNNs">Cross-modality reasoning with GNNs</h4><ul><li>Build a directed graph, where the edges connect i) the nodes from the same modality but different scales and ii) the nodes of the same scale from different modalities.</li><li>The backbone is VGG-16 plus dilated network technique, which will extract 2D appearance representations and 3D geometry representations. They also propose a graph-based reasoning (GR) module to reason about the cross-modality, high-order relations between them.</li><li>GRU module<ul><li>Input 2D features and 3D features.</li><li>gated recurrent unit for node state updating</li></ul></li></ul><h4 id="Cascade-Graph-Neural-networks">Cascade Graph Neural networks</h4><ul><li><p>To overcome the drawbacks of independent multilevel (graph-based) reasoning, propose cascade GNNs.</p></li><li><p>coarsening the preceding graph into two domain-specific guidance nodes for the following cascade graph to perform the joint reasoning</p></li><li><p>The guidance nodes only deliver the guidance information, and will stay fixed during the message passing process.</p></li><li><p>The guidance node is built by firstly concatenation and then the fusion via $3\times 3$ convolution layer.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905202700652.png" alt="image-20210905202700652"></p></li><li><p>Each guidance node propagates the guidance information to other nodes of the same domain in the graph through the attention mechanism.</p></li><li><p>Multi-level feature fusion: The merge function is either element-wise addition or channel-wise concatenation.</p></li></ul><h2 id="Paper-10-Coarse-Fine-Networks-for-Temporal-Activity-Detection-in-Videos">Paper 10: <a href="https://arxiv.org/abs/2103.01302">Coarse-Fine Networks for Temporal Activity Detection in Videos</a></h2><p>Code: <a href="https://github.com/kkahatapitiya/Coarse-Fine-Networks">https://github.com/kkahatapitiya/Coarse-Fine-Networks</a></p><h3 id="Why-9">Why</h3><ul><li>One main challenge for video representation learning  is capturing long-term motion from a continuous video.</li><li>Use of frame striding or temporal pooling has been a successful strategy to cover a larger time interval without increasing the number of parameters</li><li>Previous work<ul><li>Action localization: temporal action localization task, which tends to annotate every frame with multiple ongoing activities. Use of sequential models such as LSTMs have been popular.</li><li>Dynamic sampling: selective processing of information, like spatially, temporally or spatio-temporally sampling.</li></ul></li><li>Two challenges of the network<ul><li>how to abstract the information at a lower temporal resolution meaningfully, and</li><li>how to utilize the fine-grained context information effectively.</li></ul></li></ul><h3 id="Goal-8">Goal</h3><p>learn better video representations for long-term motion, works in multiple temporal resolutions of the input and selects frames dynamically.</p><h3 id="How-10">How</h3><ul><li>Grid Pool, a learned temporal downsampling layer to extract coarse features, which adaptively samples the most informative frame locations with a differentiable process.</li><li>Multi-stage Fusion, a spatio-temporal attention mechanism to fuse a fine-grained context with the coarse features.</li></ul><h4 id="Grid-pool">Grid pool</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905223900318.png" alt="image-20210905223900318"></p><ul><li>Samples by interpolating on a non-uniform grid with learnable grid locations. The intuition comes from that sampling frames at a higher frame rate where the confidence is high and at a lower frame rate where it is low. The stride between the interpolated frame locations should be small and vice-versa.</li><li>The confidence value is modeled as a function of the input representation.</li><li>To get a set of $\alpha T$ (an integer) grid locations based on confidence values, the CDF is considered.</li><li>When a grid location is non-integer, the corresponding sampled frame is a temporal interpolation between the adjacent frames.</li><li>A grid unpool operation is coupled with the grid locations learned by the Grid pool layer, which simply performs the inverse operation of the Grid pool. In this way, one will resamples with a low frame-rate in the regions where one used a high frame-rate in Grid pool, and vice-versa.</li></ul><h4 id="Multi-stage-fusion">Multi-stage fusion</h4><ul><li>Fuse the context from the fine stream and the coarse stream. Aims: filter out what fine-grained information should be passed down to the coarse stream, have a calibration step to align the coarse features and fine features, learn and benefit from multiple abstraction-levels of fine-grained context at each fuse-location in the coarse stream</li><li>filtering fine-grained information: self-attention mask by processing the fine feature through a lightweight head consists of point-wise convolutional layers followed by a sigmoid non-linearity.</li><li>Fine to coarse correspondence: use a set of temporal Gaussian distributions centered at each coarse frame location which abstract a location dependent weighted average of the fine feature.</li><li>Multiple abstraction-levels:  allow each fusion connection to look at the features from all abstraction levels by concatenating them channel-wise. The scale and shift features at each fusion location is calculated to finally fuse the features from any abstraction-levels.</li></ul><h4 id="Model-details">Model details</h4><ul><li>Backbone: X3D, which follows ResNet structure but designed for efficiency in video models.</li><li>The coarse stream takes in segmented clips of $T=64$ frames to follow the standard X3D architecture after the Grid pool later during training, while the fine stream always process the entire input clip.</li><li>The main difference between the coarse and the fine stream is the Grid pool layer and the corresponding grid unpool operation.</li><li>The grid pool later is placed after the 1st residual block.</li><li>The peak magnitude of each mask is normalized to 1.</li><li>The standard deviation $\sigma$ is set to be $\frac{T’}{8}$, empirically.</li></ul><h2 id="Paper-11-CoCoNets-Continuous-Contrastive-3D-Scene-Representations">Paper 11: <a href="https://arxiv.org/abs/2104.03851">CoCoNets: Continuous Contrastive 3D Scene Representations</a></h2><p><a href="https://mihirp1998.github.io/project_pages/coconets/">https://mihirp1998.github.io/project_pages/coconets/</a></p><h3 id="Why-10">Why</h3><ul><li>Combine 3D voxel grids and implicit functions and learn to predict 3D scene and object 3D occupancy from a single view with unlimited spatial resolution</li></ul><h3 id="Goal-9">Goal</h3><p>SSL learning of amodal 3D feature representations from RGB and RGBD posed images and videos, and finally generate the representations to help object detection, object tracks or visual correspondence.</p><p>Trained for contrastive view prediction by rendering 3D feature clouds in queried viewpoints and matching against the 3D feature point cloud predicted frim the query view.</p><p>Finally, the model forms plausible 3D completions of the scene given a single RGB-D image as input.</p><h3 id="How-11">How</h3><ul><li>3d feature grids as a 3D-informed neural bottleneck for contrastive view prediction, and implicit functions for handling the resolution limitations of 3D grids.</li><li>Propose CoCoNets (continuous contrastive 3D networks) that learns to map RGB-D images to infinite-resolution 3D scene representations by contrastively predicting views. Specifically, the model is trained to lift 2.5D images to 3D feature function grids of the scene by optimizing for view-contrastive prediction</li><li>There are two branches in CoCoNet, one is to encode RGB-D images into a 3D feature map, and the other is to encode the RGB-D of the target viewpoint into a 3D feature cloud.</li><li>Two branches, one adopts top-down idea which encodes the input RGB-D image and orienting the feature map to the target viewpoint, and predict the features for the target 3D points in target domain by querying, and later output a feature cloud for the target domain. The other branch is the bottom-up one, which simply encodes the target RGB-D image and predict the features for the target 3D position in target domain, obtaining the feature cloud for target domains</li><li>The positive samples are from the target domain that works in bottom-up branch.</li></ul><h4 id="Result">Result</h4><ul><li>The scene representations learnt by CoCoNets can detect objects in 3D across large frame gaps</li><li>Using the learnt 3D point features as initialization boosts the performance of the SOTA Deep Hough Voting detector.</li><li>The learnt 3D feature representations can infer 6DoF alignment between the same object in different viewpoints, and across different objects of the same category.</li><li>optimize a contrastive view prediction objective but uses a 3D girder of implicit functions as its latent bottleneck.</li></ul><h2 id="Paper-12-CutPaste-Self-Supervised-Learning-for-Anomaly-Detection-and-Localization">Paper 12: <a href="https://arxiv.org/abs/2104.04015">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</a></h2><h3 id="Why-11">Why</h3><ul><li>Difficult to obtain a large amount of anomalous data, and the difference between normal and anomalous patterns are often fine-grained.</li><li>The anomaly score defined as an aggregation of pixel-wise reconstruction error or probability densities lacks to capture a high-level semantic information.</li><li>deep one-class classifier outperforms, but most existing work focus on detecting semantic outliers, which cannot generalize well in detecting fine-grained anomalous patterns as in defet detection.</li><li>Naively applying existed methods such as rotation prediction or contrastive learning, is sub-optimal for detecting local defects.</li><li>Rotation and translation ect. lacks of irregularity.</li></ul><h3 id="Goal-10">Goal</h3><ul><li>detects unknown anomalous patterns of an image without anomalous data</li><li>Design a pretext task that can identify local irregularity.</li><li>The pretext task is also amenable to combine with existing methods, such as transfer learning from pretrained models for better performance or patch-based models for more accurate localization</li></ul><h3 id="How-12">How</h3><ul><li>Learn representations by classifying normal data from the CutPaste (data augmentation strategy that cuts an image patch at a random location of a large image). First learn SS deep representations and then build a generative one-class classifier.</li><li>transfer learning on pretrained representations on ImageNet.</li><li>designing a novel proxy classification task between normal training data and the ones augmented by the CutPaste. The CutPaste motivated to produce a spatial irregularity to serve as a coarse approximation of real defects.</li><li>A two-stage framework to build an anomaly detector, where in the first stage they learn deep representations from normal data and then construct an one-class classifier using learned representations.</li></ul><h4 id="SSL-with-CutPaste">SSL with CutPaste</h4><ul><li>CutPaste augmentation as follows:<ul><li>Cut a small rectangular area of variable sizes and aspect ratios from a normal training image.</li><li>Optionally, we rotate or jitter pixel values in the patch.</li><li>Paste a patch back to an image at a random location</li></ul></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906193605112.png" alt="image-20210906193605112"></li><li>In practice , data augmentation or color jitter, are applied before feeding x into g or CP.</li></ul><h4 id="CutPaste-variants">CutPaste variants</h4><ul><li>CutPaste scar: a long-thin rectangular box filled with an image patch</li><li>Multi-class classification: formulate a finer-grained 3-way classification task among normal, CutPaste and CutPaste-Scar by treating CutPaste variants as two separate classes.</li><li>Similarity between CutPaste and real defects: outliers exposure. CutPaste creates examples preserving more local structures of the normal examples, while is more challenging for the model to learn to find this irregularity.</li><li>CutPaste does look similar to some real defects.</li></ul><h4 id="Computing-anomaly-score">Computing anomaly score</h4><ul><li><p>A simple Gaussian density estimator whose log-density is computed as follows</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194329698.png" alt="image-20210906194329698" style="zoom:50%;" /></li><li></li></ul><h4 id="Localization-with-patch-representation">Localization with patch representation</h4><ul><li><p>CutPaste prediction is readily applicable to learn a patch representation – all we need to do at training is to crop a patch before applying CutPaste augmentation.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194519932.png" alt="image-20210906194519932" style="zoom:50%;" /></li><li></li></ul><h2 id="Paper-13-Discriminative-Latent-Semantic-Graph-for-Video-Captioning">Paper 13: <a href="https://arxiv.org/abs/2108.03662">Discriminative Latent Semantic Graph for Video Captioning</a></h2><p><a href="https://github.com/baiyang4/D-LSG-Video-Caption">https://github.com/baiyang4/D-LSG-Video-Caption</a></p><h3 id="Why-12">Why</h3><ul><li>Key challenge of the video captioning task: no explicit mapping between video frames and captions, and the output sentence should be natural</li><li>GNNs show particular advantages in modeling relationships between objects, but they don’t jointly consider the frame-based spatial-temporal contexts in the entire video sequence</li><li>discriminative modeling for caption generation suffers from stability issues and requires pre-trained generators.</li><li>Traditional GNNs for video captioning cannot take adequate information into consideration, while the work of this paper (conditional graph) jointly consider objects, contexts and motion information at both region and frame levels.</li></ul><h3 id="Goal-11">Goal</h3><ul><li>Video captioning</li><li>Encode-decoder frameworks cannot explicitly explore the object-level interactions and frame-level information from complex spatio-temporal data to generate semantic-rich captions.</li><li>Contributions on three key sub-tasks in video captioning<ul><li>Enhanced object proposal: propose a novel conditional graph that can fuse spatio-temporally information into latent object proposal.</li><li>visual knowledge: latent proposal aggregation to dynamically extract visual words</li><li>sentence validation: a novel discriminative language validator</li></ul></li><li>Propose D-LSG, where the graph model for feature fusion from multiple base models, the latent semantic refers to the higher-level semantic knowledge that can be extracted from the enhanced object proposals. The discriminative module is designed as a plug-in language validator, which uses the Multimodal Low-rank Bi-linear (MLB) pooling as metrics.</li></ul><h3 id="How-13">How</h3><ul><li>a semantic relevance discriminative graph based on Wasserstein gradient penalty.</li><li>Modeled as a sequence to sequence process.</li></ul><h4 id="Architecture-Design">Architecture Design</h4><ul><li>Multiple feature extraction: use 2D CNNs for appearance features and 3D CNNs for motion features. Then these two features are concatenated and apply LSTM on them.</li><li>Enhanced object proposal: enhanced by their visual contexts of appearance and motion respectively, which result in enhanced appearance proposals and enhanced motion proposal, together these two form the enhanced object proposals.</li><li>Visual knowledge: latent semantic proposals as $K$ dynamic visual words, after introducing the dynamic graph built by LPA to summarize the enhanced appearance and motion features.</li><li>Language decoder: language generation decoder will take the visual knowledge extracted by the LPA to generate captions. it consists of an attention LSTM for weighting dynamic visual words and a language LSTM for caption generation.</li></ul><h4 id="Latent-Semantic-graph">Latent Semantic graph</h4><ul><li><p>conditional graph operation : model the complex object-level interactions and relationships, and learn informative object-level features that are in context of frame-based background information.</p><ul><li>To build the graph, each region feature is regarded as a node. During message passing, the enhanced appearance proposal and object-level region features are handled with a kernel function to encode relations between them. The kernel is defined by linear functions followed by Tanh activation function.</li></ul><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908111357002.png" alt="image-20210908111357002"></p></li><li><p>LPA</p><ul><li>to further summarize the enhanced object proposals</li><li>propose a latent proposal aggregation method to generate visual words dynamically based on the enhanced features.</li><li>Introduce a set of object visual words, which means potential object candidates in the given video, and then they summarize the enhanced proposals into informative dynamic visual words.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908112654724.png" alt="image-20210908112654724"></li></ul></li></ul><h4 id="Discriminative-language-validation">Discriminative language validation</h4><ul><li>The module is designed to as a language validation process that encourages the generated captions to contain more informative Semantic concepts via reconstructing the visual words or knowledge based on the input sentences under the condition of corresponding true visual words encoded by LSG.</li><li>Use WGAN-GP</li><li>Extract sentence features from given captions by 1DCNN,</li><li>The output of the discriminative model is weighted since sentences have different proportions of object and motion concepts<ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120140074.png" alt="image-20210908120140074"></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120117673.png" alt="image-20210908120117673"></li></ul></li></ul><h2 id="Paper-14-Enhancing-Self-supervised-Video-Representation-Learning-via-Multi-level-Feature-Optimization">Paper 14: <a href="https://arxiv.org/abs/2108.02183?context=cs">Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</a></h2><p><a href="https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization">https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization</a></p><h3 id="Why-13">Why</h3><ul><li>most recent works have mainly focused on high-level semantics and neglected lower-level representations and their temporal relationship</li><li>The requirement of developing unsupervised video representation learning without resorting to manual labeling</li><li>Drawbacks<ul><li>previous works only explore either instance-wise or semantic-wise distribution, lacking a comprehensive perspective over both sides.</li><li>less effort has been placed on low-level features than high-level representations, while the former is proven critical for knowledge transfer</li><li>Third, directly performing temporal augmentations, e.g., shuffle and reverse, at input level instead of feature level could impair feature learning</li></ul></li><li>High-level features are more representative towards instances or semantics but less feasible towards cross-task transfer, while low-level features are transfer-friendly but lack structural information over samples.</li><li>a line of works expanded contrastive learning pipeline to video domain<ul><li>InfoNCE loss for dense future prediction</li><li>the temporal information in videos is not well leveraged</li><li>require a simple yet effective operation to apply temporal augmentations on extracted multi-level features</li></ul></li><li></li></ul><h3 id="Goal-12">Goal</h3><ul><li>proposes a multi-level feature optimization framework to improve the generalization and temporal modeling ability of learned video representations</li><li>avoids forcing the backbone model to adapt to unnatural sequences which corrupts spatiotemporal statistics.</li><li>Jointly consider the instance and semantic-wise similarity distribution to form a reliable SS signal.</li></ul><h3 id="How-14">How</h3><ul><li>high-level features obtained from naive and prototypical contrastive learning are utilized to build distribution graphs</li><li>devise a simple temporal modeling module from multi-level view features to enhance motion pattern learning.</li><li>For low-level representation, apply temporal augmentation on multi-level features to construct contrastive pairs that have different motion patterns with the objective designed to distinguish the augmented samples and original ones. And one retrieval task is proposed to match the features in short and long time spans based on their semantic consistency.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910122543720.png" alt="image-20210910122543720"></li></ul><h4 id="Beyond-instance-discrimination">Beyond instance discrimination</h4><ul><li>The one-hot labels in InfoNCE loss neglect the relationship between different samples. But there exist some negative samples that may share similar characteristics.</li><li>besides instance-wise discrimination, we explicitly develop another branch on the projected high-level feature vectors for inter-sample relationship modeling.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130142797.png" alt="image-20210910130142797"></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130231035.png" alt="image-20210910130231035"></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130246505.png" alt="image-20210910130246505"></li><li>design a queue to store the semantic-wise distributions from previous batches to ensure equal partition into K prototypes, but using only those from the current batch for gradient back-propagation</li><li>Finally, we jointly leverage $\mathcal{L}<em>{ins}$ and $\mathcal{L}</em>{sem}$ to form the self-supervisory objective for high-level representations:</li></ul><h4 id="Graph-constraint-for-multi-level-features">Graph constraint for multi-level features</h4><ul><li>It is the lower-level features that mainly transfer from the pretrained network to downstream tasks. One can infer instance- and semantic-wise distribution from high-level features.</li><li>Denote the instance-wise similarity distribution as a <em><strong>directed</strong></em> graph $\mathcal{G}<em>{ins}$, and semantic-wise distribution as an <em><strong>undirected</strong></em> graph $\mathcal{G}</em>{sem}$. Each graph contains N nodes representing N different samples within a batch, and edges indicating the relationship between each sample.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131617208.png" alt="image-20210910131617208"></li><li>$\mathcal{E}<em>{ins}$ indicates the inferred instance-wise similarity distribution, which respects inter-sample relationship and is more realistic data distribution than one-hot encoding. $\mathcal{E}</em>{sem}$ to truncate the edges between nodes of different pseudo categories.</li><li>Jointly leverage $\mathcal{G}<em>{ins}$ and $\mathcal{G}</em>{sem}$ to form the combined graph $\mathcal{G}$, whose edge weights serve as the final soft targets: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131811218.png" alt="image-20210910131811218" style="zoom:53%;" /></li><li>The cross-entropy between $\mathcal{E}$ and inferred similarity distribution to optimize lower-level features.</li></ul><h4 id="Temporal-modeling">Temporal modeling</h4><ul><li><p>Use the temporal information at diverse time scales to enhance motion pattern modeling since the features at different layers possess different temporal characteristic.</p></li><li><p>A robust temporal model requires two aspects: semantic discrimination between different motion patters; semantic consistency under different temporal views.==&gt; Two learning objectives</p></li><li><p>perform temporal augmentation on multi-level features $\mathrm{f}_r$, and then leverage a lightweight motion excitation module to extract motion enhanced feature representations</p></li><li><p>Temporal transformations that result in semantically inconsistent motion patterns can be regarded as a negative pair of the original sample</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910132656530.png" alt="image-20210910132656530"></p></li><li><p>To boost the consistency, they propose to match feature of a specific timestamp from sequences of different lengths. For one short sequence $v_s$ that is contained in a long sequence $v_l$, they retrieve the feature at each timestep of $v_s$ in the feature set of $v_l$. The feature of corresponding timestamp in $v_l$ serves as the positive key, while others serve as negatives.</p></li></ul><h2 id="Paper-15-Exploring-simple-siamese-representation-learning">Paper 15: <a href="https://arxiv.org/abs/2011.10566">Exploring simple siamese representation learning</a></h2><h3 id="Why-14">Why</h3><ul><li>Siamese networks are natural tools for comparing (including but not limited to “contrasting”) entities</li><li>Recent methods define the inputs as two augmentations of one image, and maximize the similarity subject to different conditions</li><li>An undesired trivial solution to Siamese networks is all outputs “collapsing” to a constant.<ul><li>Methods like Contrastive learning, e.g., SimCLR etc. work to fix this.</li><li>Clustering is another way of avoiding constant output. While these methods do not define negative exemplars, this cluster centers can play as negative prototypes.</li><li>BYOL relies only on positive pairs but it does not collapse in case a momentum encoder is used. The momentum encoder is important for BYOL to avoid collapsing, and it reports failure results if removing the momentum encoder</li></ul></li><li>the weight-sharing Siamese networks can model invariance w.r.t. more complicated transformations</li></ul><h3 id="Goal-13">Goal</h3><ul><li>report that simple Siamese networks can work surprisingly well with none of the above strategies (contrastive learning, clustering or BYOL) for preventing collapsing</li><li>our method ( SimSiam) can be thought of as “<em>BYOL without the momentum encoder”</em>. Directly shares the weights between the two branches, so it can also be thought of as “SimCLR without negative pairs”, and “SwAV without online clustering”.</li><li>SimSiam is related to each method by removing one of its core components.</li><li>The importance of stop-gradient suggests that <em>there should be a different underlying optimization problem that is being solved</em>.</li></ul><h3 id="How-15">How</h3><ul><li><p>The proposed architecture takes as input two randomly augmented views $x_1$ and $x_2$ from an image $x$. The two views are processed by an encoder network $f$ consisting of a backbone and a project MLP head. A predict MLP head is denoted as $h$.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160437825.png" alt="image-20210910160437825"></p></li><li><p>The symmetrized loss is denoted as $\mathcal{L}=\frac{1}{2}\mathcal{D}(p_1,p_2)+\frac{1}{2}\mathcal{D}(p_2,z_1)$.  Its minimum possible value is −1.</p></li><li><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160719249.png" alt="image-20210910160719249"></p></li><li><p>the encoder on $x_2$ receives no gradient from $z_2$, but gradients from $p_2$.</p></li><li><p>Use SGD as optimizer, with a base $lr=0.05$, the learning rate is $lr\times  \mathrm{BatchSize}/256$.</p></li><li><p>Use ResNet50 as the default backbone.</p></li><li><p>Unsupervised pretraining on the 1000-class ImageNet training set without using labels.</p></li></ul><h2 id="Paper-16-GiT-Graph-Interactive-Transformer-for-Vehicle-Re-identification">Paper 16: <a href="https://arxiv.org/abs/2107.05475">GiT: Graph Interactive Transformer for Vehicle Re-identification</a></h2><h3 id="Why-15">Why</h3><ul><li>Vehicle re-identification aiming to retrieve a target vehicle from non-overlapping cameras. But there are challenges<ul><li>vehicle images of different identifications usually have similar global appearances and subtle differences in local regions</li></ul></li><li>The technologies of vehicle re-identification<ul><li>Early methods: pure CNNs, fail to catch local information</li><li>Based on CNNs, cooperate part divisions (uniform spatial division suffer from partition misalignment, part detection requires a high cost of extra manual part annotations) to learn global features and local features.</li><li>CNNs cooperate GNNs to learn global and local features: the CNN’s downsampling and convolution operations reduce the resolution of feature maps, the CNN and GNN branches are supervised with two independent loss functions and lack interaction.</li><li>This paper: couple global and local features via transformer and local correction graph modules.</li></ul></li><li>The advantages of transformer<ul><li>The transformer can use multi-head attention module to capture global context information to establish long-distance dependence on global features of vehicles.</li><li>The multi-head attention module of transformer does not require convolution and down-sampling operations, which retain more detailed vehicle information.</li></ul></li></ul><h3 id="Goal-14">Goal</h3><p>Propose a graph interactive transformer (GiT) for vehicle-reidentification. Each GiT block employs a novel local correlation graph (LCG) module to extract discriminative local features within patches.</p><p>LCG Modules and transformer layers are in a coupled status.</p><h3 id="How-16">How</h3><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913111201445.png" alt="image-20210913111201445"></p><ul><li>The transformer later and LCG module interact each other by skip connection when these two components work in sequence.</li></ul><h4 id="LCG-module">LCG module</h4><ul><li><p>To aggregate and learn discriminative local features within every patch.</p></li><li><p>flatten $n$ local features $d$ dimensions and map to $d’$ dimensions with a trainable linear projection in every patch</p></li><li><p>The spatial graph’s edges are constructed as $E_{v_{i,j}}=\frac{\exp (F_{cos}(v_i,v_j))}{\sum\limits_{k=1}^{n}\exp {(F_{cos}(v_i,v_k))}}$, where $i,j\in[1，2,…,n]$. The score of the cosine distance is denoted as $F_{cos}=\frac{v_i,v_j}{|v_i||v_j|}$.</p></li><li><p>To aggregate and update nodes, the aggregation node $U$ of $i$-th graph is updated according as follows</p><ul><li><p>$U=(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}X_i)\cdot W$,</p></li><li><p>Then $U$ is processed non-linearly as</p><p>$O=GELU(LN(U))$, where GELU represents the gaussian error linear units and LN denotes the layer normalization.</p></li></ul></li></ul><h4 id="Transformer-layer">Transformer layer</h4><ul><li>Model the global features between the different patches.</li><li>Patches are the input for multi-head attention layer</li><li>Later, the output from the attention layer is normalized and then processed by MLP.</li></ul><h4 id="Graph-interactive-Transformer">Graph interactive Transformer</h4><ul><li>Each GiT block consists of a LCG module and a Transformer layer.</li></ul><h4 id="Loss-function-design">Loss function design</h4><ul><li><p>The proposed GiT’s total loss function is</p><p>$L_{total}=\alpha L_{CE}+\beta L_{Triplet}$, where $L_{CE}$ denote cross-entropy loss, and $L_T$ denotes triplet loss.</p></li><li><p>The $L_{CE}$ formulates the cross-entropy of each patch’s label</p></li><li><p>The triplet  loss is</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913112252883.png" alt="image-20210913112252883"></p></li></ul><h2 id="Paper-17-Graph-Time-Convolutional-Neural-Networks">Paper 17: <a href="https://arxiv.org/abs/2103.01730">Graph-Time Convolutional Neural Networks</a></h2><h3 id="Why-16">Why</h3><ul><li>The key for learning on multivariate temporal data is to embed spatiotemporal relations into into its inner-working mechanism.</li><li>Spatiotemporal graph-base models<ul><li>Hybrid: combine learning</li><li>Fused</li></ul></li></ul><h3 id="Goal-15">Goal</h3><ul><li>Represent spatiotemporal relations through product graphs and develop a first principle graph-time convolutional neural network (GTCNN).</li><li>For multivariate temporal data such as sensor or social networks</li></ul><h3 id="How-17">How</h3><ul><li>Each layer consists of a graph-time convolutional module, a graphtime pooling module, and a nonlinearity.</li><li>The product graph itself is parametric to learn the spatiotemporal coupling</li><li>The zero-pad pooling preserves the spatial graph while reducing the number of active noes and parameters</li><li></li></ul><h2 id="Paper-18-Graphzoom-A-multi-level-spectral-approach-for-accurate-and-scalable-graph-embedding">Paper 18: <a href="https://arxiv.org/abs/1910.02370">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</a></h2><h2 id="Paper-19-Group-Contrastive-Self-Supervised-Learning-on-Graphs">Paper 19: <a href="https://arxiv.org/pdf/2107.09787.pdf">Group Contrastive Self-Supervised Learning on Graphs</a></h2><h2 id="Paper-20-Homophily-outlier-detection-in-non-IID-categorical-data">Paper 20: <a href="https://link.springer.com/article/10.1007/s10618-021-00750-y">Homophily outlier detection in non-IID categorical data</a></h2><h2 id="Paper-21-Hyperparameter-free-and-Explainable-Whole-Graph-Embedding">Paper 21: <a href="https://arxiv.org/abs/2108.02113">Hyperparameter-free and Explainable Whole Graph Embedding</a></h2><h2 id="Paper-22-Infograph-Unsupervised-and-semi-supervised-graph-level-representation-learning-via-mutual-information-maximization">Paper 22: <a href="https://arxiv.org/abs/1908.01000">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</a></h2><h2 id="Paper-23-Iterative-graph-self-distillation">Paper 23: <a href="https://arxiv.org/abs/2010.12609">Iterative graph self-distillation</a></h2><h2 id="Paper-24-Learning-by-Aligning-Videos-in-Time">Paper 24: <a href="https://arxiv.org/abs/2103.17260">Learning by Aligning Videos in Time</a></h2><h2 id="Paper-25-Learning-graph-representation-by-aggregating-subgraphs-via-mutual-information-maximization">Paper 25: <a href="https://arxiv.org/abs/2103.13125">Learning graph representation by aggregating subgraphs via mutual information maximization</a></h2><h2 id="Paper-26-Mile-A-multi-level-framework-for-scalable-graph-embedding">Paper 26: <a href="https://arxiv.org/abs/1802.09612">Mile: A multi-level framework for scalable graph embedding</a></h2><h2 id="Paper-27-Missing-Data-Estimation-in-Temporal-Multilayer-Position-aware-Graph-Neural-Network-TMP-GNN">Paper 27: <a href="https://arxiv.org/pdf/2108.03400.pdf">Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</a></h2><h2 id="Paper-28-Multi-Level-Graph-Contrastive-Learning">Paper 28: <a href="https://arxiv.org/abs/2107.02639">Multi-Level Graph Contrastive Learning</a></h2><h2 id="Paper-29-Permutation-Invariant-Variational-Autoencoder-for-Graph-Level-Representation-Learning">Paper 29: <a href="https://arxiv.org/abs/2104.09856">Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</a></h2><h2 id="Paper-30-PiNet-Attention-Pooling-for-Graph-Classification">Paper 30: <a href="https://arxiv.org/abs/2008.04575">PiNet: Attention Pooling for Graph Classification</a></h2><h2 id="Paper-31-Power-Law-Graph-Transformer-for-Machine-Translation-and-Representation-Learning">Paper 31: <a href="https://arxiv.org/abs/2107.02039">Power Law Graph Transformer for Machine Translation and Representation Learning</a></h2><h2 id="Paper-32-Self-supervised-Graph-level-Representation-Learning-with-Local-and-Global-Structure">Paper 32: <a href="https://arxiv.org/abs/2106.04113">Self-supervised Graph-level Representation Learning with Local and Global Structure</a></h2><h2 id="Paper-33-Self-supervised-Heterogeneous-Graph-Neural-Network-with-Co-contrastive-Learning">Paper 33: <a href="https://arxiv.org/abs/2105.09111">Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</a></h2><h2 id="Paper-34-SM-SGE-A-Self-Supervised-Multi-Scale-Skeleton-Graph-Encoding-Framework-for-Person-Re-Identification">Paper 34: <a href="https://arxiv.org/abs/2107.01903">SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</a></h2><h2 id="Paper-35-Space-time-correspondence-as-a-contrastive-random-walk">Paper 35: <a href="https://arxiv.org/abs/2006.14613">Space-time correspondence as a contrastive random walk</a></h2><h2 id="Paper-36-Spatially-consistent-representation-learning">Paper 36: <a href="https://arxiv.org/abs/2103.06122">Spatially consistent representation learning</a></h2><h2 id="Paper-37-Spatiotemporal-contrastive-video-representation-learning">Paper 37: <a href="https://arxiv.org/abs/2008.03800">Spatiotemporal contrastive video representation learning</a></h2><h2 id="Paper-38-SportsCap-Monocular-3D-Human-Motion-Capture-and-Fine-grained-Understanding-in-Challenging-Sports-Videos">Paper 38: <a href="https://arxiv.org/abs/2104.11452">SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</a></h2><h2 id="Paper-39-SSAN-Separable-Self-Attention-Network-for-Video-Representation-Learning">Paper 39: <a href="https://arxiv.org/abs/2105.13033">SSAN: Separable Self-Attention Network for Video Representation Learning</a></h2><h2 id="Paper-40-tdgraphembed-Temporal-dynamic-graph-level-embedding">Paper 40: <a href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411953">tdgraphembed: Temporal dynamic graph-level embedding</a></h2><h2 id="Paper-41-Videomoco-Contrastive-video-representation-learning-with-temporally-adversarial-examples">Paper 41: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf">Videomoco: Contrastive video representation learning with temporally adversarial examples</a></h2><h2 id="Paper-42-Visual-Relationship-Forecasting-in-Videos">Paper 42: <a href="https://arxiv.org/abs/2107.01181">Visual Relationship Forecasting in Videos</a></h2><h2 id="Paper-43-Wasserstein-embedding-for-graph-learning">Paper 43: <a href="https://openreview.net/pdf?id=AAes_3W-2z">Wasserstein embedding for graph learning</a></h2>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Paper 1: &lt;a href=&quot;https://arxiv.org/pdf/2104.00323.pdf&quot;&gt;Jigsaw Clustering for Unsupervised Visual Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 2: &lt;a href=&quot;https://arxiv.org/pdf/2104.00240.pdf&quot;&gt;Self-supervised Motion Learning from Static Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 3: &lt;a href=&quot;https://arxiv.org/pdf/2104.00862.pdf&quot;&gt;Self-supervised Video Representation Learning by Context and Motion Decoupling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 4: &lt;a href=&quot;https://arxiv.org/pdf/2104.11487.pdf&quot;&gt;Skip-convolutions for Efficient Video Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 5: &lt;a href=&quot;https://arxiv.org/pdf/2104.09496.pdf&quot;&gt;Temporal Query Networks for Fine-grained Video Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 6: &lt;a href=&quot;https://arxiv.org/pdf/2103.16605.pdf&quot;&gt;Unsupervised disentanglement of linear-encoded facial semantics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 7: &lt;a href=&quot;https://arxiv.org/pdf/2103.16605.pdf&quot;&gt;Unsupervised disentanglement of linear-encoded facial semantics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 8: &lt;a href=&quot;https://arxiv.org/pdf/2105.09711.pdf&quot;&gt;An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 9: &lt;a href=&quot;https://arxiv.org/abs/2008.03087&quot;&gt;Cascade Graph Neural Networks for RGB-D Salient Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 10: &lt;a href=&quot;https://arxiv.org/abs/2103.01302&quot;&gt;Coarse-Fine Networks for Temporal Activity Detection in Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 11: &lt;a href=&quot;https://arxiv.org/abs/2104.03851&quot;&gt;CoCoNets: Continuous Contrastive 3D Scene Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 12: &lt;a href=&quot;https://arxiv.org/abs/2104.04015&quot;&gt;CutPaste: Self-Supervised Learning for Anomaly Detection and Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 13: &lt;a href=&quot;https://arxiv.org/abs/2108.03662&quot;&gt;Discriminative Latent Semantic Graph for Video Captioning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 14: &lt;a href=&quot;https://arxiv.org/abs/2108.02183?context=cs&quot;&gt;Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 15: &lt;a href=&quot;https://arxiv.org/abs/2011.10566&quot;&gt;Exploring simple siamese representation learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 16: &lt;a href=&quot;https://arxiv.org/abs/2107.05475&quot;&gt;GiT: Graph Interactive Transformer for Vehicle Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 17: &lt;a href=&quot;https://arxiv.org/abs/2103.01730&quot;&gt;Graph-Time Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 18: &lt;a href=&quot;https://arxiv.org/abs/1910.02370&quot;&gt;Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 19: &lt;a href=&quot;https://arxiv.org/pdf/2107.09787.pdf&quot;&gt;Group Contrastive Self-Supervised Learning on Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 20: &lt;a href=&quot;https://link.springer.com/article/10.1007/s10618-021-00750-y&quot;&gt;Homophily outlier detection in non-IID categorical data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 21: &lt;a href=&quot;https://arxiv.org/abs/2108.02113&quot;&gt;Hyperparameter-free and Explainable Whole Graph Embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 22: &lt;a href=&quot;https://arxiv.org/abs/1908.01000&quot;&gt;Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 23: &lt;a href=&quot;https://arxiv.org/abs/2010.12609&quot;&gt;Iterative graph self-distillation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 24: &lt;a href=&quot;https://arxiv.org/abs/2103.17260&quot;&gt;Learning by Aligning Videos in Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 25: &lt;a href=&quot;https://arxiv.org/abs/2103.13125&quot;&gt;Learning graph representation by aggregating subgraphs via mutual information maximization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 26: &lt;a href=&quot;https://arxiv.org/abs/1802.09612&quot;&gt;Mile: A multi-level framework for scalable graph embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 27: &lt;a href=&quot;https://arxiv.org/pdf/2108.03400.pdf&quot;&gt;Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 28: &lt;a href=&quot;https://arxiv.org/abs/2107.02639&quot;&gt;Multi-Level Graph Contrastive Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 29: &lt;a href=&quot;https://arxiv.org/abs/2104.09856&quot;&gt;Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 30: &lt;a href=&quot;https://arxiv.org/abs/2008.04575&quot;&gt;PiNet: Attention Pooling for Graph Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 31: &lt;a href=&quot;https://arxiv.org/abs/2107.02039&quot;&gt;Power Law Graph Transformer for Machine Translation and Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 32: &lt;a href=&quot;https://arxiv.org/abs/2106.04113&quot;&gt;Self-supervised Graph-level Representation Learning with Local and Global Structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 33: &lt;a href=&quot;https://arxiv.org/abs/2105.09111&quot;&gt;Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 34: &lt;a href=&quot;https://arxiv.org/abs/2107.01903&quot;&gt;SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 35: &lt;a href=&quot;https://arxiv.org/abs/2006.14613&quot;&gt;Space-time correspondence as a contrastive random walk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 36: &lt;a href=&quot;https://arxiv.org/abs/2103.06122&quot;&gt;Spatially consistent representation learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 37: &lt;a href=&quot;https://arxiv.org/abs/2008.03800&quot;&gt;Spatiotemporal contrastive video representation learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 38: &lt;a href=&quot;https://arxiv.org/abs/2104.11452&quot;&gt;SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 39: &lt;a href=&quot;https://arxiv.org/abs/2105.13033&quot;&gt;SSAN: Separable Self-Attention Network for Video Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 40: &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3340531.3411953&quot;&gt;tdgraphembed: Temporal dynamic graph-level embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 41: &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf&quot;&gt;Videomoco: Contrastive video representation learning with temporally adversarial examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 42: &lt;a href=&quot;https://arxiv.org/abs/2107.01181&quot;&gt;Visual Relationship Forecasting in Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper 43: &lt;a href=&quot;https://openreview.net/pdf?id=AAes_3W-2z&quot;&gt;Wasserstein embedding for graph learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Fall Surveys</title>
    <link href="http://yoursite.com/posts/notes/2021-08-23-notes-paper-SSL-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-08-23-notes-paper-SSL-survey.html</id>
    <published>2021-08-23T14:36:39.000Z</published>
    <updated>2021-08-24T19:48:27.279Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="https://arxiv.org/pdf/2006.08218.pdf">Self-supervised Learning: Generative or Contrastive</a></li></ul><a id="more"></a><h2 id="SSL-Generative-or-Contrastive">SSL: Generative or Contrastive</h2><h3 id="Why">Why?</h3><h4 id="Supervised-Learning"><strong>Supervised Learning</strong></h4><ul><li>relies heavily on expensive manual labeling</li><li>suffers from generalization error, spurious correlations, and adversarial attacks</li><li>The characteristics of different types of falls are not taken into consideration in most of the work on fall detection surveyed. (like age, gender etc.)</li></ul><h4 id="SSL">SSL</h4><ul><li>Training data is automatically labeled by leveraging the relations between different input sensor signals.</li><li>Features<ul><li>Obtain “labels” from the data itself by using a “semi-automatic” process.</li><li>Predict part of the data from other parts.</li></ul></li></ul><h3 id="Motivation-of-SSL">Motivation of SSL</h3><h4 id="Mainstream-of-methods">Mainstream of methods</h4><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823112440152.png" alt="image-20210823112440152" style="zoom:67%;" /><ul><li>For latent distribution $z$: explicit in generative and contrastive methods, and implicit in GAN</li><li>Discriminator: GANs and contrastive have while generative method does not.</li><li>Objectives: generative methods use a reconstruction loss, the contrastive ones use a similarity metric and the GANs leverage distributional divergence as the loss (JS-divergence, Wasserstein distance )</li></ul><h4 id="Hints">Hints</h4><ul><li>Contrastive learning is useful for almost all visual classification tasks: since the contrastive object is modeling the class-invariance between different image instances.</li><li>The art of SSL primarily lies in defining proper objectives for unlabeled data.</li></ul><h4 id="Summary-of-papers">Summary of papers</h4><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823114232256.png" alt="image-20210823114232256"></p><h4 id="Outline-of-this-paper">Outline of this paper</h4><h3 id="Generative-SSL">Generative SSL</h3><h4 id="AR-model">AR model</h4><ul><li>Viewed as “Bayes net”,  where the probability of each variable is dependent on the previous variables.</li><li>objective: in NLP, usually  maximizing the likelihood under the forward autoregressive factorization.</li><li>Examples:<ul><li>PixelRNN: lower (right) pixels are generated by conditioning on the upper (left) pixels.</li><li>For 2D images: factorize probabilities according to specific directions, and therefore masked filters</li><li>For raw audio: PixelCNN, wavenet.</li><li>GraphRNN: y decompose the graph generation process into a sequence generation of nodes and edges conditioned on the graph generated so far. The objective is the likelihood of the observed graph generation sequences.</li></ul></li><li>Pros and cons: can model the context dependency well, but the token at each position can only access its context from one direction .</li></ul><h4 id="Flow-based-model">Flow-based model</h4><ul><li>Goal: estimate complex high-dimensional densities from data. It designs the mapping between $x$ and $z$ in invertible, but also requires that $x$ and $z$  have the same dimension.</li><li>Examples<ul><li>NICE and RealNVP design affine coupling layer to parameterize $f_\theta$.</li><li>GLOW: introduces invertible 1 × 1 convolutions and simplifies RealNVP.</li></ul></li></ul><h4 id="AE-model">AE model</h4><h5 id="Basic-AE">Basic AE</h5><ul><li>RBM can be regarded as a special AE.</li><li>AE model is usually a feed-forward neural network trained to produce its input at the output layer.</li><li>the linear autoencoder corresponds to the PCA method.</li><li>some interesting structures can be discovered by imposing sparsity constraints on the hidden units</li></ul><h5 id="Context-prediction-model-CPM">Context prediction model (CPM)</h5><ul><li>Idea: predict contextual information based on inputs.</li><li>negative sampling is employed to ensure computational efficiency and scalability</li><li>Examples<ul><li>CBOW, Skip-Gram, FastText based on CBOW</li><li>DeepWalk: based on a similar context prediction objective. It treats random walks as the equivalent of sentences.</li><li>LINE: aims to generate neighbors based on current nodes. Use negative sampling to sample multiple negative edges to approximate the objective.</li></ul></li></ul><h5 id="Denoising-AE-model">Denoising AE model</h5><ul><li>Intuition: representation should be robust to the introduction of noise.</li><li>Examples<ul><li>MLM (masked language model): randomly mask some of the tokes from the input and then predicts them based on their context information. But <strong>it assumes the predicted tokens are independent if the unmasked tokens are given,</strong> which does not hold in reality.</li><li>Bert: import a unique token to mask some tokens, but also replace the unique token with original words or random words with a small probability.</li><li>SpanBert:  mask continuous random spans rather than random tokens adopted by BERT. It trains the span boundary representations to predict the masked spans.</li><li>ERINE: learn entity-level and phrase-level knowledge and further integrates knowledge in knowledge graphs into language models</li><li>GPT-GNN: asks GNN to generate masked edges and attributes.</li></ul></li></ul><h5 id="Variational-AE-model">Variational AE model</h5><ul><li><p>Assumes that data are generated from underlying latent representation. The posterior distribution over a set of unobserved variable $Z$ given some data $X$ is approximated by a variational  distribution $q(z|x)\approx p(z|x)$. In variational inference, the ELBO (evidence lower bound) on the log-likelihood of data is maximized during training.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823135504436.png" alt="image-20210823135504436"></p><p>The 1st term of ELBO is a regularizer  forcing the posterior to approximate the prior. The second term is the likelihood of reconstructing the original input data based on latent variables.</p></li><li><p>Examples</p><ul><li>VQ-VAE: aims to learn discrete latent variables motivated by the fact that many modalities are inherently discrete, such as language, speech, and images. VQ-VAE relies on vector quantization (VQ) to learn the posterior distribution of discrete latent variables.</li><li>VQ-VAE-2: enlarge the scale and enhance the autoregressive priors by a powerful PixelCNN prior</li><li>VGAE: VAE combined with GCN as the encoder, with an objective to reconstruct the adjacency matrix of the graph by measuring node proximity.</li><li>DVNE: s Gaussian node embedding to model the uncertainty of nodes. 2-Wasserstein distance is used to measure the similarity between the distributions for its effectiveness in preserving network transitivity</li><li>vGraph: node representation learning and community detection. Assumed that each node can be generated from a mixture of communities.</li></ul></li></ul><h4 id="Hybrid-generative-models">Hybrid generative models</h4><h5 id="AR-AE">AR+AE</h5><ul><li>Examples<ul><li>MADE: modify autoencoder by masking its parameters to respect AR constraints.</li><li>PLM (permutation language model): AR+ auto encoding .</li><li>XLNet: introduces PLM, enables learning bidirectional contexts by <strong>maximizing the expected likelihood over all permutations</strong> of the factorization order. It also  integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL into pre-training, which can model long-range dependency better that Transformer.</li></ul></li></ul><h5 id="AE-flow-based-models">AE + flow-based models</h5><ul><li>Examples<ul><li>GraphAF: to molecule graph generation as a sequential decision process. Follow flow-based method, it defines an invertible transformation from a base distribution to a molecular graph structure. Dequantization technique is utilized to convert discrete data into continuous data.</li></ul></li></ul><h4 id="Pros-and-cons">Pros and cons</h4><ul><li>Ability: recover the original data distribution without assumptions for downstream tasks.</li><li>Shortcomings<ul><li>far less competitive than contrastive self-supervised learning in some classification scenarios because contrastive learning’s goal naturally conforms the classification objective: MOCO, SimCLR, BYOL, SwAV.</li><li>Point-wise nature of the generative objective  has some inherent defects. The MLE is based on all the samples $x$ we hope to model and the context information $c$ is  conditionally constrained.<ul><li>Sensitive and conservative distribution: when $p(x|c)\rightarrow 0$, MLE loss becomes super large, making generative model extremely sensitive to rare samples.</li><li>Low-level abstraction objective: the representation distribution in MLE is mostly modeled at $x$'s level, while most of the classification tasks target at high-level abstraction.</li></ul></li><li>Generative - contrastive SSL abandons the point-wise objective.</li></ul></li></ul><h3 id="Contrastive-SSL">Contrastive SSL</h3><p>The contrastive models show the potential of discriminative models for representation. They aim at “learn to compare” through a NCE (noise contrastive estimation ) objective formatted as</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823152108301.png" alt="image-20210823152108301" style="zoom:80%;" /><p>With more dissimilar pairs involved, we have the InfoNCE formulated as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210823152312022.png" alt="image-20210823152312022"></p><h4 id="Context-instance-Contrast">Context-instance Contrast</h4><p>Also called as global-local contrast, focusing on modeling the belonging relationship between the local feature of a sample and its global context representation. There are two main types of context-instance contrast:</p><ul><li>PRP (predict relative position): learn relative positions between local components. The global context serves as an implicit requirement for predicting these relations.  (such as understanding what an elephant looks like is critical for predicting relative position between its head and tail)</li><li>MI (maximize mutual information): learn the direct belonging relationships between local parts and global context. Ignore relative positions between local parts.</li></ul><h5 id="Predict-relative-position">Predict relative position</h5><ul><li>The PRP is also knowns as pretext task, such as jigsaw, rotation. It may also serve as tools to create hard positive samples.</li><li>Examples<ul><li>NSP (next sentence prediction ): for a sentence, the model is asked to distinguish the following and a randomly sampled one. But in RoBERTa, the NSP loss is removed since some argue that NSP may hurt performance.</li><li>ALBERT propose SOP task (sentence order prediction ): two sentences that exchange their position are regarded as a negative sample, making the model concentrate on the semantic meaning’s coherence.</li></ul></li></ul><h5 id="MI">MI</h5><ul><li><p>Target at maximizing the association between two variables. To reduce the computation price, in practice, MI is maximized by maximizing its lower bound with an NCE objective</p></li><li><p>Examples</p><ul><li>Deep InfoMax: maximizing the MI between a local patch and its global context.</li><li>CPC (contrastive predictive coding ): inspired by deep infomax. It maximizes the association between a segment of audio and its context audio. The negative context vectors are taken at the same time.</li><li>AMDIM: enhances the positive association between a local feature and its context. Instead picking a negative image which has the same context, it picks the image which is taken from a different view of the positive image.</li><li><em>CMC</em>: extends AMDIM, which take the image in several different views as the positive samples, while the negative sample is another irrelevant sampled image. It <em>measure the instance-instance similarity rather than context-instance similarity</em>.</li><li>InfoWord: maximize the mutual information between a global representation of a sentence and n-grams in it. The context is induced from the sentence with selected n-grams being masked, and the negative contexts are randomly picked out from the corpus.</li><li>DGI (deep graph InfoMax): take a nodes representation as the local feature and the average of randomly samples 2-hop neighbors as the context. To generate negative samples on one single graph, DGI corrupt the original context by keeping the sub-graph structure and permuting the node features.</li><li>InfoGraph: follow DGI, learn graph-level representation.</li><li>Similar as CMC: <a href="https://arxiv.org/pdf/2006.05582.pdf">paper</a> learns node and graph representations by maximizing MI between node representations of one view and graph representation of another view and vice versa. They find that graph diffusion is the most effective way to yield augmented positive sample pairs in graph learning.</li><li>In <a href="https://arxiv.org/abs/1905.12265">paper</a> , they attempt to unify graph pre-training in two strategies. One is structural prediction at node-level, where they propose context prediction to maximized the MI between the k-hop neighborhood’s representations and its context graph. For node-level/graph-level strategy, they propose attribute mask to predict a node’s attribute according to its neighborhood, which is a generative objective similar to token masks in bert.</li><li>$\mathrm{S}^2$GRL: separate nodes in the context graph into k-hop context subgraphs and maximizes their MI with target node, respectively. <font color=red>(There are k negative samples?)</font></li></ul></li><li><p>Cons</p><p>Existing  graph pre-training work is only applicable for a specific domain, while graph pre-training tends to learn inductive biases across graphs.</p></li></ul><h5 id="Improvements">Improvements</h5><ul><li>Some argue that the models above is only loosely connected to MI by showing that <em><strong>an upper bound MI estimator leads to ill-conditioned and lower performance representations.</strong></em>–&gt; <font color="blue">More should be attributed to encoder architecture and a negative sampling strategy related to metric learning.</font></li><li>And therefore in metric learning: <em><strong>perform hard positive sampling while increasing the negative sampling strategy</strong></em>.</li></ul><h4 id="Instance-instance-Contrast">Instance-instance Contrast</h4><ul><li>Directly studies the relationships between different samples’ instance-level local representations as what metric learning does.</li></ul><h5 id="Cluster-discrimination">Cluster discrimination</h5><ul><li>The motivation is to pull similar images near in the embedding space.</li><li>Examples<ul><li>Deep Cluster: leverage clustering to yield pseudo labels and asks a discriminator to predict images’ labels. In details, K-means to cluster pseudo labels and then the discriminator predicts whether two samples are from the same cluster and back-propagates to the encoder.<ul><li>In DeepCluster, samples are assigned to mutual-exclusive clusters. But LA identifies neighbors separately for each examples.</li><li>DeepCluster optimizes a cross-entropy discriminative loss, while LA employs an objective function that directly optimizes a local soft-clustering metric.</li></ul></li><li>LA (local aggregation): improve the cluster-based method’s boundary.<ul><li>LA identifies neighbors separately for each examples.</li><li>LA employs an objective function that directly optimizes a local soft-clustering metric.</li></ul></li><li>VQ-VAE: similar as LA. For the feature matrix encoded from an image, VQ-VAE substitutes each 1-dimensional vector in the matrix to the nearest one in an embedding dictionary.</li><li>ClusterFit: help in the generalization of other pre-trained models. Introduce a cluster prediction fine-tuning stage similar to DeepCluster.</li><li>SwAV: to improve the time-consuming two-stage training. Use online clustering ideas and multi-view data augmentation strategies into the cluster discrimination approach. To reduce time price, they propose an online computing strategy to label the images in different views.</li><li>M3S: in graph learning. Given little labeled data and many unlabeled data, for every stage, M3S first pretrain as DeepCluster does and then compares these pseudo labels with those predicted by the model being supervised trained on labeled data. . Only top-k confident labels are added into a labeled set for the next stage of semi-supervised training.</li></ul></li></ul><h5 id="Instance-Discrimination">Instance Discrimination</h5><ul><li>The prototype is InstDisc.</li><li>Examples<ul><li>CMC: adopt multiple different views of an image as positive samples and take another one as the negative. But it’s constrained by the idea of Deep Infomax, which only samples one negative sample for each positive one.</li><li>MoCo: leverage instance discrimination via momentum contrast, which substantially increases the amount of negative samples.<ul><li>It designs the <strong>momentum contrast learning with two encoders (query and key),</strong> which prevents the fluctuation of loss convergence in the beginning period</li><li>to enlarge negative samples’ capacity, MoCo employs a queue (with K as large as 65536) to save the recently encoded batches as negative samples.</li><li>But the positive sample strategy is too simple: a pair of positive representations come from the same sample without any transformation or augmentation.</li></ul></li><li>PIRL: based on MoCo, adds jigsaw augmentation.</li><li>SimCLR: hard positive sample strategy by introducing data augmentation in 10 forms. The augmentation leverages several different views to augment the positive pairs. To handle the large-scale negative samples problem, it chooses a batch size of N as large as 8196. <strong>Techniques in SimCLR can also further improve MoCo’s performance.</strong></li><li>InfoMin: more into augmenting positive samples. They argue that we should select those views with less mutual information for better augmented views in contrastive learning. To do so, they first propose an unsupervised method to minimize mutual information between views, but this will result in a loss of information for predicting labels, say a pure blank view. Then a semi-supervised method is proposed to find views  sharing only label information.</li><li>BYOL: discards negative sampling in SSL but achieves an even better result over InfoMin. <em>They argue that negative samples may not be necessary in this process</em>.<ul><li>If we use a fixed randomly initialized network to serve as the key encoder, the representation produced by query encoder would still be improved during training.</li><li>BYOL proposes an architecture with an exponential moving average strategy to update the target encoder just as MoCo does.</li><li>The loss is mean square error, which is robust to smaller batch size.</li><li>The batch size in BYOL is not as critical as what’s in MoCo and SimCLR.</li></ul></li><li>SimSiam: study how necessary is negative sampling.<ul><li>They show that the most critical component in BYOL is the stop gradient operation, which makes the target representation stable.</li><li>It converges faster than MoCo, SimCLR, and BYOL with even smaller batch sizes.</li></ul></li><li>ReLIC: argue that contrastive pre-training teaches the encoder to causally disentangle the invariant content and style in an image.<ul><li>They propose to add an extra KL-divergence regularizer between prediction logits of an image’s different views.</li></ul></li><li>GCC (graph contrastive coding): leverage instance discrimination as the pretext task for structural information pre-training.<ul><li>For each node, they sample two subgraphs independently by random walks with restart and use top eigenvectors from their normalized graph Laplacian matrices as nodes’ initial representations.</li><li>Then they use GNN to encode them and calculate the InfoNCE loss.</li></ul></li><li>GraphCL: studies the data augmentation strategies, propose four different augmentation methods based on edge perturbation and node dropping. They show that the appropriate combination of these strategies can yield even better performance.</li></ul></li></ul><h4 id="SS-contrastive-pre-training-for-semi-supervised-self-training">SS contrastive pre-training for semi-supervised self-training</h4><ul><li>No matter how self-supervised learning models improve, they are still the only powerful feature extractor, and to transfer to the downstream task, we still need labels more or less.</li><li>In self-training, a model is trained on the small amount of labeled data and then yield labels on unlabeled data. Only those data with highly confident labels are combined with original labeled data to train a new model. We iterate this procedure to find the best model.<ul><li>Student-teacher</li><li>The improvements from pre-training and self-training are orthogonal to each other. The model with joint pre-training and self-training is the best.</li></ul></li><li>SimCLR v2 adopts the conclusion above<ul><li>Do SS pre-training as SimCLR v1， with some minor architecture modification and a deeper ResNet.</li><li>Fine tune the last few layers with only 1% or 10% of original ImageNet labels.</li><li>Use the fine-tuned network as teacher to yield labels on unlabeled data to train a smaller student ResNet-50.</li></ul></li></ul><h4 id="Pros-and-cons-2">Pros and cons</h4><ul><li>usually light-weighted and perform better in discriminative downstream applications</li><li>Problems remain to be solved<ul><li>Scale to natural language pre-training</li><li>Sampling efficiency: hints from BYOL and SimSiam. The role that negative sampling plays in contrastive learning is still not clear.</li><li>data augmentation: in theory, why data augmentation can boost contrastive learning’s performance is still not clear.</li></ul></li></ul><h3 id="Generative-Contrastive-Adversarial-SSL">Generative-Contrastive (Adversarial) SSL</h3><p>Adversarial learning learns to reconstruct the original data distribution rather than the samples by minimizing the distributional divergence.</p><h4 id="Generate-with-Complete-Input">Generate with Complete Input</h4><ul><li>Capturing the sample’s complete information.</li><li>To extract the implicit distribution out $p(z)$,<ul><li>AAE: the generator in GAN is an implicit autoencoder, which can be replaced by an explicit variational autoencoder (VAE).<ul><li>AAE substitutes the KLH divergence function for a discriminative loss.</li></ul></li></ul></li></ul><h4 id="Recover-with-Partial-Input">Recover with Partial Input</h4><ul><li>Provide models with partial input and ask them to recover the rest parts. Similar as masked bert but this works in an adversarial manner.</li><li>Examples<ul><li>Colorization:  given one color channel L in an image and predicting the value of two other channels A, B. The encoder and decoder networks can be set to any form of convolutional neural network.</li><li>Inpainting: ask the model to predict an arbitrary part of an image given the rest of it. Then a discriminator is employed to distinguish the inpainted image from the original one.</li><li>SRGAN: follows the same idea in inpainting,</li></ul></li></ul><h4 id="Pre-trained-Language-model-PTM">Pre-trained Language model (PTM)</h4><ul><li>Focus on maximum likelihood estimation based on pretext task.</li><li>Examples<ul><li>ELECTRA: outperform BERT.<ul><li>The generator is a small masked language model (MLM)</li><li>The discriminator will predict which words are replaced.</li><li>Training steps: first warming-up the generator by MLM pretext task. Then train with the discriminator.</li></ul></li><li>WKLM: perform Replaced Token Detection (RTD) at the entity-level.</li></ul></li></ul><h4 id="Graph-learning">Graph learning</h4><ul><li>Adopt adversarial training<ul><li>ANE (adversarial network embedding) designs a generator that is updated in two stages: the generator encodes sampled graph into target embedding and computes traditional NCE with a context encoder like Skip-gram; discriminator will distinguish embedding from the generator and sampled one from a prior distribution.</li><li>GraphGAN: model the link prediction task and follow the original GANs style discriminative objective to distinguish directly at node-level rather than representation-level.</li><li>GraphSGAN: use the adversarial method in semi-supervised graph learning with the motivation that marginal nodes cause most classification errors in the graph. Between clusters, there are density gaps where few samples exist. They prove that we can complete classification theoretically if we generate enough fake samples in density gaps. The generator will generate fake nodes in density gaps during the training.</li></ul></li></ul><h4 id="Domain-adaption-and-multi-modality-representation">Domain adaption and multi-modality representation</h4><ul><li>GAN can help on domain adaption: <a href="https://arxiv.org/abs/1505.07818">[1]</a>, <a href="https://arxiv.org/abs/1805.05151">[2]</a>, <a href="https://arxiv.org/abs/1505.07818">[42]</a>, <a href="https://www.researchgate.net/publication/318224334_Adversarial_Representation_Learning_for_Domain_Adaptation">[113]</a>.</li><li>Leverage adversarial sampling to improve the negative samples’ quality: <a href="https://aclanthology.org/N18-1133.pdf">[16]</a>, <a href="https://arxiv.org/abs/1809.11017">[138]</a></li></ul><h4 id="Pros-and-cons-3">Pros and cons</h4><ul><li>Challenges<ul><li>Limited applications in NLP and graph.</li><li>Easy to collapse</li><li>Not for feature extraction: Contrastive learning is more practical in extraction.</li></ul></li></ul><h3 id="Theory-behind-SSL">Theory behind SSL</h3><h4 id="GAN">GAN</h4><h5 id="Divergence-matching">Divergence matching</h5><ul><li>Different divergence functions leads to different GAN variants. <a href="https://arxiv.org/abs/1606.00709">Paper</a> discusses the effects of various choices of divergence functions.</li></ul><h5 id="Disentangled-representation">Disentangled representation</h5><ul><li>GAN shows its superior potential in learning disentangled features empirically and theoretically.</li><li>InfoGAN proposes to learn disentangled representation with DCGAN.<ul><li>Since mutual information is hard to compute, they leverage the variational inference approach to estimate its lower bound.</li></ul></li><li><a href="https://arxiv.org/abs/1811.10597">GAN dissection</a>: apply causal analysis into understanding GAN. They identify the correlations between channels in the convolutional layers and objects in the generated images, and examine whether they are causally-related with the output.</li><li><a href="https://openreview.net/pdf?id=SJxDDpEKvH">Paper</a> examines the channels’ conditional independence via rigorous counterfactual interventions over them. They show that in BigGAN, it’s possible to disentangle backgrounds and objects.</li></ul><h4 id="Maximizing-Lower-Bound">Maximizing Lower Bound</h4><h5 id="Evidence-lower-bound">Evidence lower bound</h5><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824130843214.png" alt="image-20210824130843214" style="zoom:80%;" /><p>ELBO is the lower bound of the optimization target KL divergence. VAE maximizes the ELBO to minimize the difference between $q_\phi(z|x),p_\theta(z|x)$.</p><h5 id="Mutual-information">Mutual information</h5><ul><li>Maximizes the MI of the input and its representation with joint density $p(x|y$ and marginal densities $p(x),p(y)$.</li><li>Examples<ul><li>Deep Infomax maximizes the MI of local and global features and replaces KL-divergence with JS-divergence, which is similar to GAN.</li><li>Instance Discrimination directly optimizes the proportion of gap of positive pairs and negative pairs. One of the commonly used estimators is InfoNCE. And prove that useful to use large negative samples(large values of N. But then the other testify that increasing the number of negative samples does not necessarily help.</li><li>Maximizing the lower bound (MI and ELBO) is not sufficient to learn useful representations.</li><li>MI maximization can be analyzed from the metric learning view. By rewriting the InfoNCE MI as the triplet loss, it is corresponding to the expectation of the multi-class k-pair loss.</li></ul></li></ul><h4 id="Contrastive-SS-representation-learning">Contrastive SS representation learning</h4><h5 id="Relationship-with-Supervised-learning">Relationship with Supervised learning</h5><ul><li><p>How contrastive pre-training benefits supervised learning?</p><ul><li><p><em><strong>SSL cannot learn more than supervised learning, but make it with few labels.</strong></em></p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824134109250.png" alt="image-20210824134109250"></p></li></ul></li><li><p>SSL trained neural networks are more robust t adversarial examples, label corruption and common input corruptions. It also benefits OOD detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods.</p></li></ul><h5 id="Understanding-Contrastive-Loss">Understanding Contrastive Loss</h5><ul><li><p>Split the contrastive loss into two terms</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824134816367.png" alt="image-20210824134816367"></p><p>where the first term aims at “alignment” and the second aims at “uniformity” of sample vectors on a sphere given the normalization condition. They show that these two terms have a large agreement with downstream tasks.</p><ul><li>They show that by directly optimizing the two loss, it is consistently better than contrastive loss. And both these terms are necessary for a good representation.</li></ul></li><li><p>It’s doubtful that whether alignment and uniformity are necessarily in the form of upper two losses. We may still achieve uniformity via other techniques such as exponential moving average, batch normalization, regularization and random initialization.</p></li></ul><h5 id="Generalization">Generalization</h5><p>It is unclear why the learned representations should also lead to better performance on downstream tasks.</p><ul><li><a href="https://arxiv.org/abs/1902.09229">Paper</a> propose a conceptual framework to analyze contrastive learning on average classification tasks.<ul><li>Under the context of only 1 negative sample, it is proved that optimizing unsupervised loss benefits the downstream classification tasks.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210824142746586.png" alt="image-20210824142746586"></li><li>They argue that enlarging the number of negative samples does not hold for contrastive learning and shows that it can hurt performance when the negative samples exceed a threshold.</li></ul></li><li>Noise Contrastive Estimation(NCE) [49] explains that increasing the number of negative samples can provably improve the variance of learning parameters</li></ul><h3 id="Discussion-and-future-directions">Discussion and future directions</h3><ul><li>Theoretical foundation<ul><li><a href="https://arxiv.org/abs/1902.09229">Paper</a>  proposes a conceptual framework to analyze the contrastive objective’s function in generalization ability.</li><li><a href="https://arxiv.org/abs/1907.13625">Paper</a> proves that the sampling strategies and architecture design may count more.</li></ul></li><li>Transferring to downstream tasks<ul><li>pre-training task selection problem: By ALBERT, NSP for bert may hurt its performance.</li><li>NAS to design pre-training tasks for a specific downstream task automatically.</li></ul></li><li>Transferring cross datasets (inductive learning)</li><li>Exploring potential of sampling strategies<ul><li>leverage super large amounts of negative samples and augmented positive samples, whose effects are studied in deep metric learning.</li><li>How to further release the power of sampling is still an unsolved and attractive problem.</li></ul></li><li>Early degeneration for contrastive learning<ul><li>the contrastive objectives often get trapped into embedding spaces’ early degeneration problem, which means that the model over-fits to the discriminative pretext task too early, and therefore lost the ability to generalize.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2006.08218.pdf&quot;&gt;Self-supervised Learning: Generative or Contrastive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
  </entry>
  
  <entry>
    <title>Book-Graph Representation Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-05-22-notes-book-grl.html"/>
    <id>http://yoursite.com/posts/notes/2021-05-22-notes-book-grl.html</id>
    <published>2021-05-22T16:10:39.000Z</published>
    <updated>2021-05-30T12:33:53.738Z</updated>
    
    <content type="html"><![CDATA[<p>Book <a href="https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf">Graph Representation Learning</a></p><a id="more"></a><h2 id="Chapter-1-Introduction">Chapter 1: Introduction</h2><h3 id="Graphs">Graphs</h3><ul><li>Types and represent edges<ul><li>Simple graphs: at most one edge between each pair of nodes, no loop and undirected.</li><li>To represent graphs, adjacent matrix can be use. It shows as a symmetric matrix under simple graphs and may not symmetric under directed graphs.</li><li>Heterogeneous and multiplex graphs<ul><li>Heterogeneous graphs: both nodes and graphs are heterogeneous.</li><li>Multiplex: graph can be decomposed in a set of $k$ layers. Every layers corresponds to a unique relation, denoted as intra-layer edge, and those between layers are inter-layer edges. <strong>E.g., the graph of skeleton clips.</strong></li></ul></li></ul></li><li>Feature information<ul><li>If heterogeneous, each types of nodes may have different dimensions of attributes.</li></ul></li><li>Tasks on graphs with machine learning<ul><li>node classification: like whether users are bots or not, aka predict the label of node.<ul><li>Note that the <strong>nodes in a graph</strong> are not independently and identically distributed, which <strong>doesn’t satisfy the requirements of i.i.d. in supervised learning.</strong></li><li>To inference the label, one idea is to use <em>homophily</em>, which assume that nodes tend to shared attributes with their neighbors in the graph. The other idea is <em>structural equivalence</em>, which assumes that nodes with similar local neighborhood structures will have similar labels.</li></ul></li><li>Relation prediction<ul><li>Like predicting the missing interactions.</li><li>Similar problems met in node classification (i.i.d.), and requires inductive biases that are specific to the graph domain.</li></ul></li><li>Clustering and community detection<ul><li>infer latent community structures given only the input graph.</li></ul></li><li>Graph classification, regression and clustering.<ul><li>In graph clustering, the goal is to learn an unsupervised measure of similarity between pairs of graphs.</li></ul></li></ul></li></ul><h2 id="Chapter-2-Background-Traditional-Approaches">Chapter 2: Background &amp; Traditional Approaches</h2><h3 id="Graph-statistics-and-kernel-methods">Graph statistics and kernel methods</h3><h4 id="Node-level-statistics">Node-level statistics</h4><ul><li><p>Node degree: simply counts the number of edges incident to a node, which will measure how many neighbors a node has.</p></li><li><p>Node centrality: To measure the importance of a node in a graph.</p><ul><li><p>Eigenvector centrality $e_u$</p><ul><li><p>$e_u=\frac{1}{\lambda}\sum\limits_{v\in V} \mathrm{A}[u,v]e_v,\forall u\in \mathcal{V}$., it measures that satisfies the recurrence in above equation corresponds to an eigenvector of the adjacency matrix.</p></li><li><p>The vector of centrality values is given by the eigenvector corresponding to the largest eigenvalues of $\mathrm{A}$.</p></li><li><p><strong>The eigenvector centrality ranks the likelihood that a node is visited on a random walk of infinite length on the graph.</strong></p></li><li><p>After $t$ iteration, the eigenvector centrality will contain the number of length-$t$ paths arriving at each node.</p></li></ul></li><li><p>Betweeness centrality</p><ul><li>Measures how often a node lies on the shortest path between two other nodes.</li></ul></li><li><p>Closeness centrality</p><ul><li>Measures the average shortest path length between a node and all other nodes.</li></ul></li><li><p>More: M. Newman. Networks. Oxford University Press, 2018. 1, 12, 13, 108, 109</p></li></ul></li><li><p>The clustering coefficient</p><ul><li><p>Measure the structural distinction using variations of the clustering coefficient, which Measures the proportion of closed triangles in a node’s local neighborhood.</p></li><li><p>The local variant of clustering coefficient is calculated by:</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522153036032.png" alt="image-20210522153036032" style="zoom:50%;" /></li><li><p>It Measures how tightly clustered a node’s neighborhood is.</p></li><li><p>In social and biological sciences, they tend to have far higher clustering coefficients than one would expect if edges were sampled randomly.</p></li></ul></li><li><p>Closed Triangles, Ego Graphs, and Motifs</p><ul><li>The global clustering coefficient. Similar equation but this time count in the node’s ego graph.</li><li>A node’s ego graph: the subgraph containing that node, its neighbors, and all the edges between nodes in its neighborhood.</li><li>The general version of these ideas is counting arbitrary motifs or graphlets within a node’s ego graph. E.g.: triangles, cycles of particular length etc.</li></ul></li></ul><h4 id="Graph-level-features-and-graph-kernels">Graph-level features and graph kernels</h4><ul><li><p>Bag of nodes</p><ul><li>Aggregate node-level statistics</li><li>E.g., the histograms, the statistics based on the degrees, centralities and clustering coefficients of the nodes.</li><li>May miss important global properties in graph</li></ul></li><li><p>The Weisfieler-Lehman Kernel</p><ul><li><p>Iterative neighborhood aggregation. Extract node-level features that contain more information than just their local Ego graph, and then to aggregate these features into a representation.</p></li><li><p>The most well-known one is the Weisfieler-Lehman (WL) algorithm.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522162120249.png" alt="image-20210522162120249" style="zoom:57%;" /><ul><li>In other words, the WL kernel is computed by measuring the difference between the resultant label sets for two graphs.</li><li>Useful while solving the isomorphism problem: approximate graph isomorphism is to check whether or not two graphs have the same label set after $K$ rounds of the WL algorithm.</li></ul></li></ul></li><li><p>Graphlets and path-base methods</p><ul><li>Graphlets: count the occurrence of different small subgraph structures, but it’s computationally difficult.</li><li>Path-based methods: examine the different kinds of paths that occur in the graph. E.g. <a href="https://www.aaai.org/Papers/ICML/2003/ICML03-044.pdf">random walk</a> , the <a href="https://ieeexplore.ieee.org/document/1565664">shortest path kernel</a> that is similar as random walk but uses only the shortest-paths between nodes.</li></ul></li></ul><h3 id="Neighborhood-overlap-detection">Neighborhood overlap detection</h3><ul><li>The features mentioned above don’t quantify the relationships among nodes, and thus won’t work well on relation prediction.</li><li>The simplest neighborhood overlap measure: just counts the number of neighbors that two nodes share.</li><li>Hope that node-node similarity Measures computed on the training edges will lead to accurate predictions about the existence of test edges.</li></ul><h4 id="Local-overlap-measures">Local overlap measures</h4><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522180625718.png" alt="image-20210522180625718" style="zoom:80%;" /><ul><li>Use the functions of the number of common neighbors two nodes share.<ul><li>Sorensen index: normalized by the sum of the node degrees.</li><li>Salton index that normalizes by the product of the degrees of u and v.</li><li>Jaccard overlap: normalized by the degree of union  u and v.</li></ul></li><li>Consider the importance of common neighbours, give more weight to common neighbours that have low degree.<ul><li>Resource Allocation index (RA): counts the inverse degrees of the common neighbours.</li><li>Adamic-Adar index (AA): similar as RA but use the inverse logarithm of the degrees.</li></ul></li></ul><h4 id="Global-overlap-measures">Global overlap measures</h4><ul><li><p>Katz Index</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522180701697.png" alt="image-20210522180701697" style="zoom:67%;" /><ul><li>The most basic global overlap statistic. Simply count the number of paths of all lengths between a pair of nodes.</li><li>This index is a geometric series of matrices.</li><li>It’s strongly biased by node degree which will give higher overall similarity scores when considering high-degree nodes cause they will generally be with more paths.</li></ul></li><li><p>Leicht, Holme, and Newman (LHN) similarity</p><ul><li><p>Considering the ratio between the actual number of observed paths and the number of expected paths between two nodes.</p></li><li><p>To compute expectation, the <em>configuration model</em> is relied on. Under a random configuration model, the likelihood of an edge is simply proportional to the product of the two node degrees. But this heuristic calculation is intractable.</p></li><li><p>To approximate, the number of paths between two nodes grows by the largest eigenvalue of adjacent matrix. (The fact that the largest eigenvalue can be used to approximate the growth in the number of paths).</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522182722632.png" alt="image-20210522182722632" style="zoom:67%;" />, and the solution to the matrix series can be written as<br>$$<br>\mathrm{S_{LNH}}=2\alpha m\lambda_1 \mathrm{D^{-1}(I-\frac{\beta}{\lambda_1}A)^{-1}D^{-1}}<br>$$</p></li></ul></li><li><p>Random walk methods</p><ul><li>rather than exact count of paths over the graph in previous introduced index, consider random walks.</li><li>A measure of importance specific to node u will be obtained since the random walks are continually being teleported back to that node.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210522183303798.png" alt="image-20210522183303798" style="zoom:80%;" /></li></ul></li></ul><h3 id="Graph-Laplacians-and-spectral-methods">Graph Laplacians and spectral methods</h3><p>learning to cluster the nodes in a graph.</p><h4 id="Graph-Laplacians">Graph Laplacians</h4><ul><li><p>Unnormalized Laplacian</p><ul><li><p>$\mathrm{L=D-A}$</p></li><li><p>It’s symmetric and positive semi-definite. $\mathrm{L}$ has $|V|$ non-negative eigenvalues.</p></li><li><p>$\mathrm{x^TLx}=\sum\limits_{(u,v)\in\mathcal{E}}(\mathrm{x}[u]-\mathrm{x}[v])^2$</p></li><li><p>If  the graph contains $K$ connected components, then there exists an ordering of the nodes in the graph such that the Laplacian matrix can be written as<br>$$<br>\mathrm{L}={<br>\begin{bmatrix}<br>\mathrm{L}_1 &amp;  &amp; &amp; \<br>&amp; \mathrm{L}_2 &amp; &amp; \<br>&amp;  &amp; \ddots&amp; \<br>&amp;  &amp; &amp;  \mathrm{L}_K\<br>\end{bmatrix}<br>},<br>$$<br>where each blocks is a valid graph Laplacian of a fully connected subgraph of the original graph. The spectrum of $\mathrm{L}$ is the union of the eigenvalues of the $\mathrm{L}_K$ matrices and the eigenvectors are the union of the eigenvectors of all the $\mathrm{L}_K$ matrices with 0 values filled at the positions of the other blocks.</p></li></ul></li><li><p>Normalized Laplacians</p><ul><li>$\mathrm{L_{sym}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}}$, while random walk Laplacian is defined as $\mathrm{L_{RW}=D^{-1}L}$.</li><li>For $\mathrm{L_{sym}}$, the properties of previous mentioned holds but with the eigenvectors for the 0 eigenvalue scaled by $\mathrm{D^{\frac{1}{2}}}$. For $\mathrm{L_{RW}}$, the properties hold exactly.</li></ul></li><li><p>These methods just allow to cluster nodes that are already in disconnected components. The Laplacian can be used to get an optimal clustering of nodes within a fully connected graph.</p></li></ul><h4 id="Graph-cuts-and-clustering">Graph cuts and clustering</h4><ul><li><p>Graph cuts</p><ul><li>An optimal cluster means that a partition that minimizes the cut value, which is the count of how many edges cross the boundary between the partition of nodes. Theoretically the methods tend to simply make clusters that consists of a single node.</li><li>One way to solve this is minimizing the <em>Ratio Cut</em>, which penalizes the solution of choosing small cluster sizes.</li><li>Another popular solution is minimize the Normalized Cut (NCut), which enforces that all clusters have a similar number of edges incident to their a nodes.</li></ul></li><li><p>Approximating the RatioCut with the Laplacian Spectrum</p><ul><li><p>The ratio cut minimization problem can be approximated as</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524082410207.png" alt="image-20210524082410207" style="zoom:40%;" /><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524082433260.png" alt="image-20210524082433260" style="zoom:50%;" /></p></li><li><p>The formula above is NP-hard, after simplification, it will be :</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524082658389.png" alt="image-20210524082658389" style="zoom:67%;" />, and the solution of this problem is the second smallest eigenvector of $\mathrm{L}$.</p></li></ul></li><li><p>Generalized spectral clustering</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524104844842.png" alt="image-20210524104844842"></li></ul></li></ul><h1>Part I: Node Embeddings</h1><h2 id="Chapter-3-Neighborhood-Reconstruction-Methods">Chapter 3 Neighborhood Reconstruction Methods</h2><p>Goal: encode nodes as low-dimensional vectors that summarize their graph position and the structure of their local graph neighborhood.</p><h3 id="An-encoder-decoder-perspective">An encoder-decoder perspective</h3><p>An encoder maps each node in the graph into a low-dimensional vector or embedding, a decoder takes the embeddings and uses them to reconstruct information about each node’s neighborhood.</p><h4 id="The-encoder">The encoder</h4><ul><li>Most works rely on <em>shallow embedding</em>, where this encoder function is simply an embedding lookup based on the node ID.</li><li>The generalized encoders are called GNNs.</li></ul><h4 id="The-decoder">The decoder</h4><ul><li>They might predict the neighbors of one given node or one row in the graph adjacency matrix.</li><li>The standard practice is to define pairwise decoders, which can be interpreted as predicting the relationship or similarity between pairs of nodes.</li><li>While reconstruction, the goal is to optimize the encoder and decoder to minimize the reconstruction loss.</li></ul><h4 id="Optimizing-an-encoder-decoder-model">Optimizing an encoder-decoder model</h4><ul><li>Usually use SGD, and minimize the disparity of decoded latent distance and real distance.</li></ul><h4 id="Typical-encoder-decoder-methods">Typical encoder-decoder methods</h4><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524113343676.png" alt="image-20210524113343676" style="zoom:67%;" /><h3 id="Factorization-based-approaches">Factorization-based approaches</h3><p>Methods below are classified as factorization methods case their loss functions can be minimized using factorization algorithms like SVD.</p><p>The goal of these methods is to learn embeddings for each node such that the inner product between the learned embedding vectors approximates some deterministic measure of node similarity.</p><p>The inner product methods all employ deterministic measures of node similarity.</p><h4 id="Laplacian-eigenmaps-LE">Laplacian eigenmaps (LE)</h4><ul><li><p>Builds upon the spectral clustering</p></li><li><p>Formula:<br>$$<br>\mathcal{L}=\sum\limits_{(u,v)\in\mathcal{D}}\mathrm{DEC}(\mathrm{z}_u,\mathrm{z}_v)\cdot\mathrm{S}[u,v],<br>$$<br>which penalizes the model when very similar nodes have embeddings that are far apart.</p></li><li><p>If the reconstructed $\mathrm{S}$ satisfies the properties of a Laplacian matrix, then the node embeddings that minimize the loss are identical to the solution for a spectral clustering.</p></li></ul><h4 id="Inner-product-methods">Inner-product methods</h4><ul><li>More recent work employs inner-product, which assumes that the similarity between two nodes, e.g., the overlap between their local neighborhoods is proportional to the dot product of their embeddings.</li><li>Examples:<ul><li>Graph factorization: <a href="https://dl.acm.org/doi/10.1145/2488388.2488393">Distributed large-scale natural graph factorization</a></li><li>GraRep: <a href="https://dl.acm.org/doi/10.1145/2806416.2806512">Learning graph representations with global structural information</a></li><li>HOPE: <a href="https://dl.acm.org/doi/10.1145/2939672.2939751">Asymmetric transitivity preserving graph embedding</a></li><li>All of them used the mean squared error as loss function. They differ primarily in how they define $\mathrm{S[u,v]}$. GF uses the adjacency matrix and sets $\mathrm{S=A}$, GraRep defines $\mathrm{S}$ based on powers of the adjacency matrix and HOPE supports general neighborhood overlap measures.</li></ul></li></ul><h3 id="Random-walk-embedding">Random walk embedding</h3><p>Use stochastic measures of node neighborhood overlap.</p><h4 id="Deepwalk-and-node2vec">Deepwalk and node2vec</h4><ul><li><p>The two methods differ on the inner-product decoder (the notions of node similarity and neighborhood reconstruction). They all optimize embeddings to encode the statistics of random walks. Mathematically, the goal is to learn embeddings so that<br>$$<br>\mathrm{DEC(z}_u,\mathrm{z}_v)\triangleq\frac{\mathrm{z}_u^\top\mathrm{z}<em>v}{\sum\limits</em>{v_k\in\mathcal{V}}\mathrm{z}<em>u^\top\mathrm{z}<em>k}\approx p</em>{\mathcal{G,T}}(v|u),\label{4}<br>$$<br>where $p</em>{\mathcal{G,T}}(v|u)$ is the probability of visiting $v$ on a length-$T$ random walk starting at $u$, and $T$ is the range.</p></li><li><p>To train the random walk embeddings, use the cross-entropy loss.</p></li><li><p>DeepWalk employs a hierarchical softmax to approximate equation above,  where the normalizing factor is approximated using <em>negative samples</em> in the <a href="https://arxiv.org/abs/1607.00653">following way</a><br>$$<br>\mathcal{L}=\sum\limits_{(u,v)\in\mathcal{D}}-\log(\sigma(\mathrm{z}_u^\top\mathrm{z}<em>v))-\gamma\mathbb{E}</em>{v_n\sim P_n{\mathcal{V}}}[\log(-\sigma(\mathrm{z}_u^\top\mathrm{z}_v))]<br>$$</p></li><li><p>DeepWalk employ uniformly random walks to define the visiting probability and node2vec use hyperparameters to allow the probabilities to smoothly interpolate between walks.</p></li></ul><h4 id="LINE">LINE</h4><ul><li><a href="http://dx.doi.org/10.1145/2736277.2741093">Large-scale information network embeddings</a>. Rather than explicitly leverage random walks, it shares ideas from DeepWalk and node2vec. It combines two encoder-decoder objectives.</li><li>It has two objectives, the 1starting is $\mathrm{DEC(z}_u,\mathrm{z}_v)=\frac{1}{1+e^{-\mathrm{z}_u^\top\mathrm{z}_v}}$, and the 2nd has the same equation as $$\eqref{4}$$,but takes the KL-divergence to encode two-hop adjacency information.</li><li>Instead of random walks, it explicitly reconstructs 1st and 2nd order neighborhood information.</li></ul><h4 id="Additional-variants-of-the-random-walk">Additional variants of the random-walk</h4><ul><li>biasing or modifying the random walks.<ul><li>consider random walks that skip over nodes, <a href="https://arxiv.org/pdf/1605.02115.pdf">Perozzi et al.</a></li><li>define random walks based on the structural relationships between nodes: <a href="http://dx.doi.org/10.1145/3097983.3098061">Ribeiro et al.</a></li></ul></li></ul><h3 id="Random-walk-and-matrix-factorization">Random walk and matrix factorization</h3><ul><li><a href="https://dl.acm.org/doi/10.1145/3159652.3159706">https://dl.acm.org/doi/10.1145/3159652.3159706</a></li><li>Random walk are closely related to matrix factorization.</li><li>The embeddings learned by DeepWalk are closely related to the spectral clustering embeddings, but DeepWalk embeddings control the influence of different eigenvalues through T.</li><li>The disadvantages of shallow embedding<ul><li>It doesn’t share any parameters between noes in the encoder, and therefore statistically and computationally inefficient.</li><li>It doesn’t leverage node features in the encoder.</li><li>It’s inherently trans-ductive, which means it only generate embeddings for nodes that were present during the training phase.</li></ul></li></ul><h2 id="Chapter-4-Multi-relational-data-and-knowledge-graphs">Chapter 4: Multi-relational data and knowledge graphs</h2><p>The knowledge graph completion is to predict missing edges in the graph, generally. Below only covers the node embeddings way in graph completion.</p><h3 id="Reconstruction-multi-relational-data">Reconstruction multi-relational data</h3><ul><li><p>Note the edges have multiple types, and the input for decoder will be a pair of nodes and types of the edge.</p></li><li><p>One simple way is <a href="https://icml.cc/2011/papers/438_icmlpaper.pdf">RESCAL</a>:<br>$$<br>\mathrm{DEC}(u,\tau,v)=\mathrm{z}<em>u^\top,\mathrm{R}</em>\tau\mathrm{z}_v,<br>$$<br>where the embeddings $\mathrm{z}$ and relation matrices $\mathrm{R}$ are all learnable.</p></li><li><p>While solving the Reconstruction error, it’s like tensor factorization.</p></li><li><p>Nearly all multi-relational embedding methods simply define the similarity measure directly based on the adjacency tensor, or to say they all try to reconstruct immediate neighbors from the low-dimensional embeddings.</p></li></ul><h3 id="Loss-functions">Loss functions</h3><h4 id="Cross-entropy-with-negative-sampling">Cross-entropy with negative sampling</h4><ul><li>The formula</li></ul><p>$$<br>\mathcal{L}=\sum\limits_{(u,\tau,v)\in\mathcal{E}}-\log(\sigma(\mathrm{DEC(z}<em>u,\tau,\mathrm{z}<em>v)))-\gamma\mathbb{E}</em>{v_n\sim P</em>{n,u}{\mathcal{V}}}[\log(\sigma(-\mathrm{DEC(z}<em>u,\tau,\mathrm{z}</em>{v_n}))],<br>$$</p><p>where $P_{n,u}(\mathcal{V})$ denotes a negative sampling distribution. The 1st term denotes the log-likelihood that we predict “true” for an edge that does actually exist in the graph, and the 2nd term is the expected log-likelihood that is correctly predicted “false” for an edge that does not exist in the graph.</p><p>After approximating with Monte Carlo, the loss will be<br>$$<br>\mathcal{L}=\sum\limits_{(u,\tau,v)\in\mathcal{E}}-\log(\sigma(\mathrm{DEC(z}<em>u,\tau,\mathrm{z}<em>v)))-\sum\limits</em>{v_n\in \mathcal{P}</em>{n,u}}[\log(\sigma(-\mathrm{DEC(z}<em>u,\tau,\mathrm{z}</em>{v_n}))],<br>$$<br>, where $\mathcal{P}<em>{n,u}$ is a set of nodes sampled from $P</em>{n,u}(\mathcal{V})$.</p><ul><li>How to define negative sampling distribution?<ul><li>Simply use a uniform distribution over all nodes, but this may get false negative.</li><li>Sample negative samples that satisfy a predefined type constraints.</li><li>Draw negative samples for both the head node and the tail node of the relation.</li></ul></li></ul><h4 id="Max-Margin-loss">Max-Margin loss</h4><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524174533295.png" alt="image-20210524174533295" style="zoom:67%;" /><p>Contrastive estimation to get the negative sample.</p><h3 id="Multi-relational-decoders">Multi-relational decoders</h3><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210524233843533.png" alt="image-20210524233843533" style="zoom:80%;" /><h4 id="Methods">Methods</h4><ul><li>RESCAL</li></ul><p>But it’s computationally expensive.</p><ul><li><p>Translational Decoders</p><ul><li><p><em><a href="https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf">TransE</a></em>, the likelihood of an edge is proportional to the distance between the embedding of the head node and the tail node, after translating the head node according to the relation embedding.<br>$$<br>\mathrm{DEC(z}<em>u,\tau,\mathrm{z}</em>{v})=-|\mathrm{z}<em>u+\mathrm{r}</em>\tau-\mathrm{z}_{v}|<br>$$</p></li><li><p>Limitation: simplicity.</p></li><li><p>The variants of TransE</p><ul><li><p><a href="">TransX</a></p><p>Import a trainable transformation that depend on the relation $\tau$.<br>$$<br>\mathrm{DEC(z}<em>u,\tau,\mathrm{z}</em>{v})=-|g_{1,\tau}(\mathrm{z}<em>u)+\mathrm{r}</em>\tau-g_{2,\tau}(\mathrm{z}_{v})|<br>$$</p></li><li><p><a href="https://persagen.com/files/misc/wang2014knowledge.pdf">TransH</a></p><p>Project the entity embeddings onto a learnable relation-specific hyperplane-defined by the normal vector $\mathrm{w}_r$-before performing translation.</p></li></ul></li></ul></li><li><p>Multi-Liner dot products</p><ul><li>Also known as <a href="https://arxiv.org/abs/1412.6575">DistMult</a>.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525081148389.png" alt="image-20210525081148389" style="zoom:80%;" /></li><li>Generalizing the dot-product decoder from simple graphs.</li><li>One <strong>limitation</strong> is that it can only encode symmetric relations while there are many directed graph and thus they are asymmetric.</li></ul></li><li><p>Complex decoders</p><ul><li><p><a href="http://proceedings.mlr.press/v48/trouillon16.pdf">ComplEx</a>, augmenting the DistMult by employing complex-valued embeddings. It’s defined as</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525081551755.png" alt="image-20210525081551755" style="zoom:80%;" /></li><li><p><a href="https://arxiv.org/abs/1902.10197">RotatE</a> defies the decoder as rotations in the complex plane as:</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525082511044.png" alt="image-20210525082511044" style="zoom:80%;" /><p>each dimension of the relation embedding can be represented as $\mathrm{r}<em>\tau[i]=e^{i\theta</em>{r,i}}$ and thus corresponds to a rotation in the complex plane.</p></li></ul></li></ul><h4 id="Representational-abilities">Representational abilities</h4><p>The Multi-relational decoders can represent different logical patterns on relations.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525083801306.png" alt="image-20210525083801306" style="zoom:80%;" /><ul><li>Symmetry and anti-symmetry<ul><li>Symmetric decoders: DistMult</li><li>Anti-symmetric decoders: TransE</li></ul></li><li>Inversion: implies the existence of another with opposite directionality.</li><li>Compositionality:<ul><li>RESCAL: $\mathrm{R_{\tau_3}=R_{\tau_2}R_{\tau_1}}$</li><li>TransE: $\mathrm{r_{\tau_3}=r_{\tau_2}+r_{\tau_1}}$</li></ul></li></ul><h1>Part II: GNNs</h1><h2 id="Chapter-5-The-GNN-model">Chapter 5 The GNN model</h2><p>GNNs are more complex than shallow embedding, they will generate representations of nodes that actually depend on the structure of the graph.</p><p>A key desideratum of GNNs is that they should be permutation invariant.</p><h3 id="Neural-message-passing">Neural message passing</h3><p>After GNN, the embeddings contain structure-based information and feature-based information. However, the feature based information is in their k-hop neighborhoods.</p><p>The message passing in GNN can be taken as using update and aggregate functions.</p><h4 id="The-basic-GNN">The basic GNN</h4><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210525122930060.png" alt="image-20210525122930060" style="zoom:67%;" /><ul><li>The message passing in basic GNN framework is like a standard multi-layer perception.</li></ul><h4 id="With-self-loop">With self-loop</h4><ul><li>Add self-loops to omit the explicit update step.</li><li>This can alleviate overfitting, but also limit the expressivity of the GNN, sample the information coming the nodes neighbours cannot be differentiated from the information from the node itself.</li><li>In basic GNN, adding self-loops means sharing parameters between the $\mathrm{W}<em>{self}$ and $\mathrm{W}</em>{neigh}$ matrices.</li></ul><h3 id="Generalized-neighborhood-aggregation">Generalized neighborhood aggregation</h3><h4 id="Neighborhood-normalization">Neighborhood normalization</h4><ul><li><p>Simply normalized the aggregation operation based upon the degrees of the nodes involved: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526114126952.png" alt="image-20210526114126952" style="zoom:50%;" /></p></li><li><p>Symmetric normalization: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526114205704.png" alt="image-20210526114205704" style="zoom:50%;" /></p></li><li><p>Combining the symmetric-normalized aggregation along with the basic GNNs update function results in a first-order approximation of a spectral graph convolution.</p></li><li><p>GCNs</p><ul><li><p>They use symmetric-normalized aggregation as well as the self-loop update approach.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526114617065.png" alt="image-20210526114617065" style="zoom:67%;" /></li><li><p>In GNN, the use of normalization can lead to a loss of information.</p></li><li><p>Normalization is most helpful in tasks where noes feature information is far more useful than structural information, or where there is a very wide range of node degrees that can lead to instabilities during optimization.</p></li></ul></li></ul><h4 id="Set-aggregation">Set aggregation</h4><p>The embeddings of neighbours, there is no natural ordering of a nodes’ neighbours, and  any aggregation function we define must thus be permutation invariant.</p><ul><li><p>Set pooling</p><ul><li><p>Define an aggregation function based on permutation invariant, can be implemented by just adding some MLP layers, e.g.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526122458994.png" alt="image-20210526122458994" style="zoom:67%;" /><p>this always lead to small increases in performance, though with the risk of overfitting.</p></li><li><p>Or use element-wise maximum or minimum to replace summation.</p></li></ul></li><li><p>Janossy pooling</p><ul><li><p>More powerful, apply a permutation-sensitive function and average the result over many possible permutations. In practice, the permutation-sensitive function is defined to be an LSTM.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526124251769.png" alt="image-20210526124251769" style="zoom:80%;" /></li><li><p>If the set of permutations $\prod$ is equal to all possible permutations, then the aggregator is also a universal function approximator for sets. Like simple summation based set pooling.</p></li><li><p>In practice, Janossy pooling employs one of two approaches</p><ul><li>Sample a random subsets of possible permutations during each application of the aggregator, and only sum over that random subset.</li><li>Employ a canonical ordering of the nodes in the neighborhood set, e.g., order the nodes in descending order according to their degree, with ties broken randomly.</li></ul></li><li><p>Janossy-style pooling can improve upon set pooling in a number of synthetic evaluation setups.</p></li></ul></li></ul><h4 id="Neighborhood-attention">Neighborhood attention</h4><ul><li><p>Assign an attention weight or importance to each neighbour, which is used to weigh this neighbor’s influence during the aggregation step.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526130931074.png" alt="image-20210526130931074" style="zoom:67%;" /></li><li><p>Popular variants of attention include the bilinear attention model and variations of attention layers using MLPs.</p></li><li><p>Multi heads methods are popular, which is also known as transformer.</p><ul><li>The basic idea behind transformers is to define neural network layers entirely based n the attention operation. The basic transformer layer is extractly equivalent to a GNNs layer using multi-headed attention if we assume that the GNNs receives a fully connected graph input.</li><li>The time complexity is the square of the number of nodes cause each pairs attention need to be calculated.</li><li>Attention can influence the inductive bias of GNNs.</li></ul></li></ul><h3 id="Generalized-update-methods">Generalized update methods</h3><ul><li><p>One popular way is <a href="https://arxiv.org/abs/1706.02216">GraphSAGE</a>, which introduced the idea of generalized Neighborhood aggregation.</p></li><li><p>Over-smoothing of GNN: after several iterations of GNN message passing, the representations for all the nodes in the graph can become very similar to one another.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526135929759.png" alt="image-20210526135929759" style="zoom:80%;" /></li><li><p>The measure of how much the initial embedding of node $u$ influences the final embedding of node $v$ in the GNN is proportional to the probability of visiting node $v$ on a length-$k$ random walk starting from node $u$.</p></li></ul><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526140143590.png" alt="image-20210526140143590" style="zoom:80%;" /><p>As $k\rightarrow\infin$, the influence of every node approaches the stationary distribution of random walks over the graph, meaning that local neighborhood information is lost.</p><ul><li>When using simple GNN models, and especially those with the self-loop update approach-building deeper models can actually hurt performance.</li></ul><h4 id="Concatenation-and-skip-connections">Concatenation and skip-connections</h4><p>Like using  vector concatenation or skip connection that try to directly preserve information from previous rounds of message passing during the update step.</p><ul><li>Do concatenation to preserve more node-level information during message passing<ul><li>The key intuition is that we encourage the model to disentangle information during message passing–separating the information coming from the neighbors from the current representation of each model.</li></ul></li><li>Do linear interpolation between the previous representation and the representation that was updated based on the neighborhood information.</li><li>In practice, these techniques tend to be most useful for node classification tasks with moderately deep GNNs, and excel on tasks that exhibit homophily.</li></ul><h4 id="Gated-updates">Gated updates</h4><p>One way to view the GNN meaning passing is that the aggregation function is receiving an observation from the neighbors, which is then used to update the hidden state of each node. Simply replace the hidden state argument of the RNN update function with the node’s hidden state, and replace the observation vector with the message aggregated from the local neighborhood.</p><ul><li><a href="https://arxiv.org/abs/1511.05493">GRU based</a></li><li><a href="https://arxiv.org/abs/1802.03685">LSTM based</a></li></ul><h4 id="Jumping-knowledge-connection">Jumping knowledge connection</h4><p>To improve the quality of the final node representations</p><ul><li><a href="https://arxiv.org/abs/1806.03536">Jumping knowledge connections</a>: One simple way is to simply leverage the representations at each layer of message passing, rather than only using the final layer output.</li><li>With max-pooling and LSTM attention layers the result is improved.</li></ul><h3 id="Edge-features-and-Multi-relational-GNNs">Edge features and Multi-relational GNNs</h3><h4 id="Relational-GNNs">Relational GNNs</h4><ul><li><p><a href="https://arxiv.org/abs/1703.06103">RGCN</a>: relational graph convolutional network</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526152624397.png" alt="image-20210526152624397" style="zoom:67%;" /><ul><li>Augment the aggregation function to accommodate multiple relation types by specifying a separate transformation matrix per relation type.</li><li>The Multi-relational aggregation in RGCN is thus analogous to the basic a GNN approach with normalization, but we separately aggregate information across different edge types.</li></ul></li><li><p>Parameter sharing</p><ul><li><p>For RGCN, the increase of number of parameters is caused by that each edge type requires a trainable matrix.</p></li><li><p>To fix this, <a href="https://arxiv.org/abs/1703.06103">Schlichtkrull et al.</a> proposed to share with basis matrices, aka an alternative view of the parameter sharing RGCN approach is that we are learning an embedding for each relation, as well as a tensor that is shared across all relations.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526160241907.png" alt="image-20210526160241907" style="zoom:80%;" /></li></ul></li><li><p>Extensions and variations</p><ul><li>define separate aggregation matrices per relation as relational GNNs.<ul><li><a href="http://dx.doi.org/10.1093/bioinformatics/bty294">without parameter sharing</a></li><li><a href="http://dx.doi.org/10.18653/v1/d17-1159">http://dx.doi.org/10.18653/v1/d17-1159</a></li><li><a href="https://arxiv.org/pdf/1911.06962.pdf">RGCN+attention</a></li></ul></li></ul></li></ul><h4 id="Attention-and-feature-concatenation">Attention and feature concatenation</h4><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526161216469.png" alt="image-20210526161216469" style="zoom:80%;" /><p><a href="https://www.aclweb.org/anthology/D19-1458/">https://www.aclweb.org/anthology/D19-1458/</a></p><h3 id="Graph-pooling">Graph pooling</h3><p>To get <strong>representations in graph-level</strong> cause the goal is to pool together the node embedding in order to learn an embedding of the entire graph.</p><ul><li><p>Set pooling approaches</p><ul><li>One popular way it taking a sum or mean of the node embeddings. This is sufficient for small graphs.</li><li>The other popular way is using a combination of LSTM and attention to pool the node embeddings.<ul><li>The way is like what’s done is one-head bert, the output of graph-level representation is the concatenation of output from different timestep.</li></ul></li></ul></li><li><p>Graph coarsening approaches</p><p>To exploit the structure of the graph. One popular strategy to accomplish this is to perform graph clustering or coarseing as a means to pool the node representations.</p><ul><li>Estimate assignment matrices<ul><li>Use spectral clustering (the decomposition of adjacent matrix to estimate assignment matrix (from node representations to graph-level representations.))</li><li><a href="https://arxiv.org/abs/1806.08804">Employ another GNN to predict cluster assignments</a></li></ul></li><li>Use the assignment matrices to coarsen the graph.</li></ul></li></ul><h3 id="Generalized-message-passing">Generalized message passing</h3><p>Leverage edge and graph-level information at each stage of message passing</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526164837669.png" alt="image-20210526164837669" style="zoom:67%;" /><h2 id="Chapter-6-GNNs-in-practice">Chapter 6 GNNs in practice</h2><p>optimization, regularization and application-based</p><h3 id="Applications-and-loss-functions">Applications and loss functions</h3><ul><li><p>GNNs for node classification</p><ul><li>If fully supervised, use the negative log-likelihood loss and softmax to denote the predicted probability.</li><li>transductive and inductive nodes<ul><li>transductive nodes: nodes used while training but not covered while calculating loss. Semi-supervised means that the GNN is tested on transductive nodes.</li><li>inductive: not used in either the loss computation or the GNN message passing operations during training.</li></ul></li></ul></li><li><p>GNNs for graph classification</p><ul><li>similar loss functions but use graph-level representations.</li></ul></li><li><p>GNNs for relation prediction</p><ul><li>use the pairwise node embedding loss functions.</li></ul></li><li><p>Pretraining</p><ul><li><p>Pre-training the GNN using one of the neighborhood reconstruction losses.</p><ul><li><p>E.g., pre-train a GNN to reconstruct missing edges in the graph before fine-tuning on a node classification loss. However, <strong><a href="https://arxiv.org/abs/1809.10341">Veličković et al</a> found that a randomly initialized GNN is equally strong compared to one pre-trained on a neighborhood reconstruction loss.</strong></p></li><li><p>DGI (deep graph infomax): maximize the mutual information between node embeddings and graph embeddings. The basic idea is the GNN model must learn to generate node embeddings that can distinguish between the real graph and its corrupted counterpart.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210526172943470.png" alt="image-20210526172943470" style="zoom:67%;" /></li></ul></li></ul></li></ul><h3 id="Efficiency-concerns-and-node-sampling">Efficiency concerns and node sampling</h3><ul><li><p>Graph-level implementations</p><ul><li>Use sparse matrix multiplications and add self-loops to avoid redundant computations, but it requires operating on tested entire graph and all node features simultaneously, which may not be feasible due to memory limitations.</li></ul></li><li><p>Sampling and mini-batch</p><p>Work with a subset of nodes during message passing.</p><ul><li>The challenge is we cannot simply Run message passing on a subsets of the nodes in a graph without losing information.</li><li>One way is subsampling node neighbors. First select a set of target nodes for a batch and then to recursively sample the neighbors of these nodes in order to ensure that the connectivity of the graph is maintained. Subsample the neighbors of each node, using a fixed sample size to improve the efficiency of batched tensor operations.</li></ul></li></ul><h3 id="Parameter-sharing-and-regularization">Parameter sharing and regularization</h3><p>Like L2 regularization, dropout and layer normalization that both work on GNNs and CNNs.</p><ul><li>Parameter sharing across layers<ul><li>use the same parameters in all the aggregate and update functions in the GNN. It’s most effective in GNNs with more than six layers, and is often used in conjunction with gated update functions.</li></ul></li><li>Edge dropout<ul><li>randomly remove edges in the adjacency matrix during training, with the intuition that this will make the GNN less prone to overfitting and more robust to noise in the adjacency matrix.</li><li>The neighborhood subsampling approaches lead to this kind of regularization as a side effect, making it a very common strategy in large-scale GNN applications.</li></ul></li></ul><h2 id="Chapter-7-Theoretical-motivations">Chapter 7 Theoretical motivations</h2><h3 id="GNNs-and-graph-convolutions">GNNs and graph convolutions</h3><p>Generalize the notion of convolutions to general graph structured data.</p><h4 id="Convolutions-and-the-Fourier-transform">Convolutions and the Fourier transform</h4><ul><li>The Fourier analysis<ul><li>The coefficients of Fourier series tell the amplitude if the complex sinusoidal component $e^{-\frac{i2\pi}{N}k}$.</li><li>The high-frequency components have a large $k$ and vary quickly while the low-frequency components have small $k$ and vary more slowly.</li></ul></li><li>Translation equivalent:  translating a signal and then convolving it by a filter is equivalent to convolving the signal and then translating the result.</li></ul><h4 id="From-time-signals-to-graph-signals">From time signals to graph signals</h4><ul><li>Each point in time $t$ is represented as a node and the edges in graph thus represent how the signal propagates</li><li>Represent operations such as time-shifts using the adjacency and Laplacian matrices of the graph.</li><li>Multiplying a signal by the adjacency matrix propagates signals from node to node, and multiplication by the Laplacian computes the difference between a signal at each node and its immediate neighbors.</li><li>shifts and convolutions on time-varying discrete signals can be represented based on the adjacency matrix and Laplacian matrix of a chain graph.</li><li>The convolution operation matrix satisfies translation equivalence.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210527233655918.png" alt="image-20210527233655918" style="zoom:80%;" />, which means that the convolved signal $\mathrm{Q_hx}[u]$ at each node $u\in\mathcal{V}$ will correspond to some mixture of the information in the node’s $N$-hop neighborhood, with the $\alpha_0,\alpha_1,\cdots,\alpha_N$ controlling the strength of the information coming from different hops. Defining $\mathrm{Q_h}$ in this way guarantees that our filter commutes with the adjacency matrix, satisfying a generalized notion of translation equivariance.</li><li><strong>By stacking multiple message passing layers, GNNs are able to implicitly operate on higher-order polynomials of the adjacency matrix.</strong></li><li>The symmetric normalized Laplacian or  symmetric normalized adjacency matrix are usually taken as the convolutional filters cause<ul><li>They have bounded spectrums and thus numerically stable.</li><li>They are simultaneously diagonalizable, which means that they share the same eigenvectors.</li></ul></li></ul><h4 id="Spectral-graph-convolutions">Spectral graph convolutions</h4><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210527235621190.png" alt="image-20210527235621190" style="zoom:80%;" /><p>The Laplace operator ($\Delta$) tells us the average difference between the function value at a point and function values in the neighboring regions surrounding this point. The Laplacian matrix is regarded as a discrete analog of the Laplace operator since it allows to quantity the difference between the value at a node and the values at that node’s neighbors.</p></li><li><p>The eigenfunctions of Laplace operator is corresponds to the complex exponentials, which means the eigenfunctions of $-\Delta e^{2\pi ist}$ are the same complex exponentials that make up the modes of the frequency domain in the Fourier transform, with the corresponding  eigenvalue indicating the frequency.</p></li><li><p>Graph convolution can be represented as polynomials of the Laplacian (or one of its normalized variants).</p></li><li><p>Given the graph Fourier coefficients $\mathrm{U^\top f}$ of a signal $\mathrm{f}$ as well as the graph Fourier coefficients $\mathrm{U^\top h}$  of some filter $\mathrm{h}$, we can compute a graph convolution via element-wise products as<br>$$<br>\mathrm{f\star_\mathcal{G}h=U(U^\top f \circ U^\top h)}<br>$$<br>Then represent convolutions in the spectral domain based on the graph Fourier coefficients $\theta_h=\mathrm{U^\top h \in \mathbb{R}^{|\mathcal{V}|}}$.</p></li><li><p>One way is to learn a nonparametric filter by <strong>directly optimizing $\theta_h$ (spectral filter)</strong> and defining the convolution as $\mathrm{f\star_\mathcal{G}h=U(U^\top f \circ }\theta_h)=(\mathrm{Udiag}(\theta_h)\mathrm{U}^\top)\mathrm{f}$.  But this way has no real dependency on the structure of the graph and may not satisfy many of the properties that we want from a convolution, e.g. locality.</p></li><li><p>To make sure the spectral filter $\theta_h$ is corresponds to a meaningful convolution on the graph, another way is to parameterize  $\theta_h$ based on the eigenvalues of the Laplacian, e.g., a degree $N$ polynomial of the eigenvalues of the Laplacian and thus ensure the filtered signal at each node depends on information in its $k$-hop neighborhood.</p><p>$\mathrm{f\star_\mathcal{G}h}=(\mathrm{Up_N}(\Lambda)\mathrm{U}^\top)\mathrm{f}=p_N(\mathrm{L})\mathrm{f}$.</p></li><li><p>The filter (e.g., Fourier) coefficients cannot be simply interpreted as corresponding to different frequencies.</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528101813914.png" alt="image-20210528101813914" style="zoom:80%;" /></li><li>The smallest eigenvector of the Laplacian corresponds to a signal that varies from node to node by the least amount on the graph, the second-smallest eigenvector corresponds to a signal that varies the second smallest amount and so on.</li><li>Laplacian eigenvectors can be used to assign nodes to communities so that we minimize the number of edges that go between communities. The <strong>Laplacian eigenvectors define signals that vary in a smooth way across the graph, with the smoothest signals indicating the coarse-gained community structure of the graph.</strong></li></ul></li></ul><h4 id="Convolution-inspired-GNNs">Convolution inspired GNNs</h4><ul><li><p>Purely convolutional approaches</p><ul><li>Directly optimize $\theta_h$ or parameterize it</li><li>Methods<ul><li><a href="https://arxiv.org/abs/1312.6203">Bruna et al.</a> : Nonparametric spectral filter and parametric spectral filter by a cubic spline.</li><li><a href="https://arxiv.org/abs/1606.09375">Defferrard et al.</a>, parameterize the spectral filter by Chebyshev polynomials. Chebyshev polynomials have an efficient recursive formulation and have various properties that make them suitable for polynomial approximation.</li><li><a href="https://arxiv.org/abs/1901.01484">Liao et al.</a>,: learn polynomials of the Laplacian based on the Lanczos algorithm.</li><li>Beyond real-valued polynomials of the Laplacian (or the adjacency matrix): employ more general parametric rational complex functions of the Laplacian.<ul><li><a href="http://dx.doi.org/10.1109/tsp.2018.2879624">Levie et al.</a>: Cayley polynomials of the Laplacian</li><li><a href="https://arxiv.org/abs/1901.01343">Bianchi et al</a>: ARMA filters.</li></ul></li></ul></li></ul></li><li><p><a href="https://arxiv.org/abs/1609.02907">GCNs</a> and connections to message passing</p><ul><li><p>A basic GCN layer is defined as<br>$$<br>\mathrm{H}^{(k)}=\sigma (\tilde{\mathrm{A}}\mathrm{H}^{(k-1)}\mathrm{W}^{(k)}),\<br>\tilde{\mathrm{A}}=\mathrm{(D+I)^{-\frac{1}{2}}(I+A)(D+I)^{-\frac{1}{2}}},<br>$$<br>where $\mathrm{W}^{(k)}$ is a learnable parameter matrix. The model was initially motivated as a combination of a simple graph convolution (based on the polynomial $\mathrm{I+A}$), with a learnable weight matrix and a nonlinearity.</p></li><li><p>The notion of message passing can be viewed as corresponding to a simple form of graph convolutions combined with additional trainable weights and nonlinearites.</p></li><li><p>Stacking multiple rounds of message passing in a basic GNN is analogous to applying a low-pass convolutional filter, which produces a smoothed version of the input signal on the graph.</p><ul><li>The multiplication $\mathrm{A}^K_{sym}\mathrm{X}$ of the input node features by a high power of the adjacency matrix can be interpreted as convolutional filter based on the lowest-frequency signals of the graph Laplacian. Because multiplying a signal by high powers of $\mathrm{A_{sym}}$ corresponds to a convolutional filter based on the lowest eigenvalues of $\mathrm{L_{sym}}$, i.e., it produces a low-pass filter.</li><li>The deeper, the convolution filters are simpler.</li></ul></li></ul></li><li><p>GNNs without message passing</p><ul><li><p>Simplify GNNs by removing the iterative message passing process. The models are generally defined as<br>$$<br>\mathrm{Z=MLP}<em>\theta(f(\mathrm{A)MLP</em>\phi(\mathrm{A})})<br>$$</p><ul><li><p><a href="https://arxiv.org/pdf/1902.07153.pdf">Wu et al.</a> define $f(\mathrm{A})=\mathrm{\tilde{A}}^k$, with $\tilde{\mathrm{A}}$ is the symmetric normalized adjacency matrix.</p></li><li><p><a href="https://arxiv.org/abs/1810.05997">Klicpera et al.</a> defines $f$ by analogy to the personalized PageRank algorithm as<br>$$<br>f(\mathrm{A})=\alpha(\mathrm{I-(1-\alpha)\tilde{A}})^{-1}=\alpha\sum\limits_{k=0}^{\infin}(\mathrm{I-\alpha\tilde{A}})^k,<br>$$<br>cause we often do not need to interleave trainable neural networks with graph convolution layers. We can simply use neural networks to learn feature transformations at the beginning and end of the model and apply a deterministic convolution layer to leverage the graph structure. Like GAT.</p></li></ul></li><li><p>Using the symmetric normalized adjacency matrix with self-loops leads to effective graph convolutions.</p><ul><li><a href="https://arxiv.org/pdf/1902.07153.pdf">Wu et al.</a> proves that adding self-loops shrinks the spectrum of corresponding graph Laplacian by reducing the magnitude of the dominant eigenvalue.</li><li>Intuitively, adding self-loops decrease the influence of far-away noes and makes the filtered signal more dependent on local neighborhoods on the graph.</li></ul></li></ul></li></ul><h3 id="GNNs-and-probabilistic-graphical-models">GNNs and probabilistic graphical models</h3><p>View the embeddings for each node as latent variables that are inferred.</p><h4 id="Hilbert-space-embeddings-of-distributions">Hilbert space embeddings of distributions</h4><ul><li><p>The density $p(\mathrm{x})$ based on its expected value under the feature map $\phi$ is:<br>$$<br>\mu_\mathrm{x}=\int_{\mathbb{R}^m}\phi(\mathrm{x})p(\mathrm{x})d\mathrm{x}<br>$$<br>The formula will be injective under the assumption of Hilbert space embeddings of distributions. Then $\mu_\mathrm{x}$ can serve as a sufficient statistics for $p(\mathrm{x})$. Then any computations we want to perform on $p(\mathrm{x})$ can be equivalently represented as functions of the embedding $\mu_\mathrm{x}$. One well-known feature map $\phi$ is Gaussian radial basis function.</p></li><li><p>In the context of the connection to GNNs, the takeaway is simply that we can represent distributions $p(\mathrm{x})$ as embeddings $\mu_\mathrm{x}$ in some feature space.</p></li></ul><h4 id="Graphs-as-graphical-models">Graphs as graphical models</h4><ul><li><p>The notion of dependence between nodes is viewed as a formal, probabilistic way.</p></li><li><p>Assume that a graph defines a Markov random field,</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528155046247.png" alt="image-20210528155046247" style="zoom:80%;" /><ul><li>Intuitively, $\Phi(\mathrm{x}_v,\mathrm{z}_v)$ indicates the likelihood of a node feature vector $\mathrm{x}_v$ given its latent node embedding $\mathrm{z}_v$, while $\Psi$ controls the dependency between connected nodes.</li><li>Assume that node features are determined by their latent embeddings, and the latent embeddings for connected nodes are dependent on each other.</li></ul></li><li><p>GNNs will try to seek to implicitly learn the $\Psi,\Phi$ by leveraging the Hilbert space embedding idea.</p></li></ul><h4 id="Embedding-mean-field-inference">Embedding mean-field inference</h4><p>The goal is to infer latent representations for all the nodes in the graph that can explain the dependencies between the observed node features.</p><ul><li><p>The key step is computing the posterior $p({\mathrm{z}_v}|{\mathrm{x}_v})$, i.e., computing the likelihood of a particular set of latent embeddings given the observed features. But the accurate solution is intractable, one way is to approximate it.</p></li><li><p>One way to approximate the posterior is to employ mean-field variational inference, by which the posterior is approximated as $p({\mathrm{z}_v}|{\mathrm{x}_v})\approx q({\mathrm{z}<em>v})=\prod\limits</em>{v\in\mathcal{V}}q_v(\mathrm{z}_v)$. The key intuition in mean-field inference is that we assume that the posterior distribution over the latent variables factorizes into $\mathcal{V}$ independent distributions, one per node.</p><ul><li><p>The standard approach to solve is to minimize the KL divergence between the approximate posterior and the true posterior. But directly minimize it is impossible cause evaluating the <strong>KL divergence</strong> requires knowledge of the true posterior.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528213649200.png" alt="image-20210528213649200" style="zoom:80%;" /><ul><li>To minimize the KL divergence easily, one way is to use the techniques from variational inference.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528213729244.png" alt="image-20210528213729244" style="zoom:80%;" /></li><li>The approximate posterior at timestep $t$ for each latent node embedding is a function of the node’s feature $\mathrm{z}_x$ and the marginal distributions (marginalized on the node’s neighbors.) at previous timestep. Therefore, <strong>using variational inference to infer the posterior is really like message passing.</strong></li><li><strong>The key distinction</strong> is that the mean-field message passing equations operate over distributions rather than embeddings (what used in GNN).</li></ul></li></ul></li><li><p>Another way is try to learn embeddings in an end-to-end way, or to say rather than specifying a concrete probabilistic model, one can simply learn embeddings that could correspond to some probabilistic model.</p><ul><li><a href="https://arxiv.org/abs/1603.05629">Dai et al.</a> define $f$ in an analogous manner to a basic GNN and thus at each iteration the updated Hilbert space embedding for node $v$ is a function of its neighbors’ embedding as well as its feature inputs.</li></ul></li></ul><h4 id="GNNs-and-PGMs-more-generality">GNNs and PGMs more generality</h4><ul><li>Different variants of message passing can be derived based on different approximate inference algorithms. <a href="https://arxiv.org/abs/1603.05629">Dai et al.</a></li><li>How GNNs can be integrated more generally into PGM models.<ul><li><a href="https://arxiv.org/abs/1905.06214">Qu et al.</a></li><li><a href="https://arxiv.org/abs/1906.02111">Zhang et al.</a></li></ul></li></ul><h3 id="GNNs-and-graph-isomorphism">GNNs  and graph isomorphism</h3><p>The motivation of GNNs based on connections to graph isomorphism testing.</p><h4 id="Graph-isomorphism-and-representational-capacity">Graph isomorphism and representational capacity</h4><ul><li>Graph isomorphism<ul><li>The goal of graph isomorphism is to declare whether or not the given two graphs are isomorphic. If so, the graphs are essentially identical.</li><li>Formally, given adjacency matrix $\mathrm{A}_1,\mathrm{A}_2$ and node features $\mathrm{X}_1,\mathrm{X}_2$, then two graphs are isomorphic if and only if there exists a permutation matrix $\mathrm{P}$ such that $\mathrm{PA_1P^\top=A_2,PX_1=X_2}$</li></ul></li><li>The challenges of graph isomorphism<ul><li>The simple definition</li><li>Testing for graph isomorphism<ul><li>A naive approach to test for isomorphism would involve the optimization problem<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528231925373.png" alt="image-20210528231925373" style="zoom:60%;" /> with the computation complexity $\mathcal{O}(|V|!)$.</li><li>Therefore, this is regarded as NP-indeterminate. No general polynomial time algorithms are known for this problem.</li></ul></li></ul></li><li>Graph isomorphism and representational capacity<ul><li>Graph isomorphism gives a way to quantify the representational power of different learning approaches. E.g., evaluating the power by asking how useful the representations would be for testing graph isomorphism.</li><li>In practice, no representation learning algorithm is going to be “perfect&quot;.</li></ul></li></ul><h4 id="The-Weisfieler-Lehman-algorithm">The Weisfieler-Lehman algorithm</h4><ul><li><p>The steps of 1-WL</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528233109761.png" alt="image-20210528233109761" style="zoom:80%;" /></li><li><p>The WL algorithm is known to converge in at most $|V|$ iterations and is known to known to successfully test isomorphism for a broad class of graph.</p></li><li><p>WL may fail for some graphs e.g. when the graph consists of multi disconnected subgraphs.</p></li></ul><h4 id="GNNs-and-the-WL-algorithm">GNNs and the WL algorithm</h4><ul><li><p>GNNs aggregate and update node embeddings using NNs while WL aggregates and updates discrete labels.</p></li><li><p>Formally, the relation between GNNs and WL is</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528233713630.png" alt="image-20210528233713630" style="zoom:80%;" /><p>It tells that GNNs are no more powerful than the WL algorithm when we have discrete information as node features.</p></li><li><p>If the WL algorithm assigns the same label to two nodes, then any message-passing GNN will also assign the same embedding to these two nodes.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210528234128699.png" alt="image-20210528234128699" style="zoom:80%;" /></li><li><p>If we define the message passing updates as</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529080919406.png" alt="image-20210529080919406" style="zoom:80%;" /><p>then this GNN is sufficient to match the power of the WL algorithm.</p></li><li><p>To make GNN is as powerful as WL, the aggregate and update function need to be injective, which means that they need to map every unique input to a unique output value. But this cannot be satisfied usually if the aggregate functions are using a average of the neighbor embeddings.</p></li><li><p>GIN (graph isomorphism network) has few parameters but is still as powerful as the WL algorithm.</p><ul><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529084928011.png" alt="image-20210529084928011" style="zoom:80%;" /></li></ul></li><li><p>In summary, MP-GNNs (message passing GNNs) are no more powerful than the WL algorithm.</p></li></ul><h4 id="Beyond-the-WL-algorithm">Beyond the WL algorithm</h4><ul><li><p>Relational pooling</p><p>Considering the failure cases of the WL algorithm. Message passing approaches generally fail to identify closed triangles in a graph, which is a critical limitation.</p><ul><li>To address the limitation, Murphy et al. consider MP-GNNs with unique node ID features. They simply add a unique, one-hot indicator feature (node ID) for each node.<ul><li>However, this doesn’t solve the problem and rather import a new and equally problematic issue that MP-GNNs is no longer permutation equivariant since the adding of node IDs. Specifically, assigning a unique ID to each node fixes a particular node ordering for the graph, which breaks the permutation equivariance.</li></ul></li><li>The other way is <a href="https://arxiv.org/abs/1903.02541">Relational Pooling (PR) approach</a>, which involves marginalizing over all possible node permutations. In practice, it will sum over all possible permutation matrices recovers the permutation invariance.<ul><li>The limitation are<ul><li>its computation complexity.</li><li>We have no way to characterize how much more powerful that PR-GNNs are.</li></ul></li><li>But RP can achieve strong results using various approximation to decrease the computation cost.</li></ul></li></ul></li><li><p>The $k$-WL test and $k$-GNNs</p><p>Improving GNNs by adapting generalizations of the WL algorithm.</p><ul><li>$k$-WL works on subgraphs in size $k$, which can be used to test graph isomorphism by comparing the multi-sets for two graphs.<ul><li>It introduces a hierarchy of representation capacity.  For any $k\ge2$ we have that the $(k+1)$-WL test is strictly more powerful than the $k$-WL test.</li><li>To intimate this, <a href="http://dx.doi.org/10.1609/aaai.v33i01.33014602">Morris et al</a> develop a $k$-GNN that is a differentiable and continuous analog of the $k$-WL algorithm. They learn embeddings associated with subgraphs, rather than noes, and the message passing occurs according to subgraph neighborhoods.</li></ul></li><li>Graph kernel methods based on the $k$-WL test.</li></ul></li><li><p><a href="https://arxiv.org/abs/1905.11136">Invariant and Equivariant $k$ -order GNNs</a></p><ul><li>MP-GNNs are equivariant to node permutations. And permuting the input to an MP-GNNs simply results in the matrix of output node embeddings being permuted in an analogous way.</li><li>MP-GNNs can also be permutation invariant at the graph level. The pooled graph-level embedding does not change when different node orderings are used.</li><li><a href="https://arxiv.org/abs/1905.11136">Maron et al</a> propose a general form of GNN-like models based on permutation equivariant/invariant tensor operations.<ul><li>For a given input, both equivariant and invariant linear operators on this input will correspond to tensors that satisfy the fixed point in $\mathrm{P}\star\mathcal{L}=\mathcal{L},\forall \mathrm{P}\in \mathcal{P}$, but the number of channels in the tensor will differ depending on whether it is an equivariant or invariant operator.</li><li>The fixed point can be constructed as a linear combination of a set of fixed basis elements.</li><li>The equivariant linear layers involve tensors that have up to $k$ different channels.</li><li>Constructing $k$-order invariant models for $k&gt;3$ is generally computationally intractable. The built $k$-order GNNs are equally powerful as the $k$-WL algorithm.</li></ul></li></ul></li></ul><h1>Part III Generative Graph models</h1><h2 id="Chapter-8-Traditional-graph-generation-approaches">Chapter 8 Traditional graph generation approaches</h2><p>The goal of graph generation is to build models that can generate realistic graph structures. The key challenge in graph generation is generating graphs that have certain desirable properties.</p><p>Traditional approaches to graph generation generally involve specifying some kind of generative process, which defines how the edges in a graph are created.</p><p>A more through survey and discussion is <a href="https://global.oup.com/academic/product/networks-9780198805090?cc=us&amp;lang=en&amp;">Newman’s  (1,12,13,108,109).</a></p><h3 id="Erdos-Renyi-model">Erdos-Renyi model</h3><ul><li>ER model may be the simplest and most well-known generative model of graphs. It simply assumes that the probability of an edge occurring between any pairs of nodes is equal to $r$.</li><li>To generate a random ER graph, just simply choose how many nodes we want, set the density parameter $r$ and then use equation to generate the adjacency matrix.</li><li>The downside of the ER model is that it doesn’t generate very realistic graphs. The graph properties like degree distribution, existence of community structures, node clustering coefficients and tensors occurrence of structural motifs are not captured.</li></ul><h3 id="Stochastic-block-models">Stochastic block models</h3><ul><li>SBMs seek to generate graphs with community structure.</li><li>SBMs are based on blocks, every node has a probability that it belongs to block $i$, edge probabilities are defined by a block-to-block probability matrix. To generate graph, for each node assign a class (block) by sampling and then sample edges for each pair of nodes.</li><li>By controlling the  edge probabilities within and between different blocks, one can generate graphs that exhibit community structure.</li><li>The nodes have a probability $\alpha$ of having an Eden with another node that assigned to the same community and a smaller probability of having an Eden with another node that is assigned to a different community.</li><li>The variations including approaches for bipartite graphs, graphs with node features, as well as approaches to infer SBMs parameters from data.</li><li>However, SBMs is limited in that it fails to capture the structural characteristic of individuals nodes that are present in most real-world graphs. In SBMs, the structure of individual communities is relatively homogeneous in that all the nodes have similar structural.</li></ul><h3 id="Preferential-attachment">Preferential attachment</h3><ul><li><p>Preferential attachment (PA) attempts to capture the inhomogeneous of communities (real-world degree distributions). It’s built based on the assumption that many real-world graphs exhibit power-law degree distributions.</p></li><li><p>The power law distributions are heavy tailed, which means that a probability distribution goes to zero for extreme values slower than an exponential distribution. It also means that there is a large number of nodes with small degrees but also have a small number of nodes with extremely large degrees.</p></li><li><p>The steps of PA:</p> <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529144054529.png" alt="image-20210529144054529" style="zoom:80%;" /></li><li><p>The key idea is that the PA model connects new nodes to existing nodes with a probability that is proportional to the existing nodes’ degrees.</p></li><li><p>The generation process of PA is autoregressive, the edge probabilities are defined on an iterative approach.</p></li></ul><h3 id="Traditional-applications">Traditional applications</h3><p>Historically, the methods introduced above have been used in two key applications:</p><ul><li>Generating synthetic data for benchmarking and analysis tasks</li><li>Creating Null models<ul><li>We can investigate the extent to which different graph characteristics are probable under different generative models.</li></ul></li></ul><p>The traditional approaches can generate graphs, but they lack the ability to learn a generative model from data.</p><h2 id="Chapter-9-Deep-generative-models">Chapter 9 Deep generative models</h2><p>Focus on the simple and general variants of VAEs, GANs and autoregressive models. All below will only focus on generating graph structure.</p><h3 id="Variational-autoencoder-approaches">Variational autoencoder approaches</h3><ul><li><p>Under VAEs, the key idea behind can be summarized as : the goal is to train a probabilistic decoder model from which one can sample realistic graphs by conditioning on a latent variable. Or to say, the goal is to learn a conditional distribution over adjacency matrices.</p></li><li><p>The target is the decoder that generate graph from latent variable, but encoder and decoder are trained together.</p></li><li><p>The components that required:</p><ul><li><p>A probabilistic encoder model $q_\phi$</p><p>It takes a graph as input. Generally , in VAEs the representation trick with Gaussian random variables is used to design this function.</p></li><li><p>A probabilistic decoder model $p_\theta$</p><p>The decoder takes a latent representation as input and uses this input to specify a conditional distribution over graphs. Specifically, it defines a conditional distribution over the entries of the adjacency matrix.</p></li><li><p>A prior distribution over the latent space</p><p>Usually a standard Gaussian prior is used.</p></li></ul></li><li><p>With these components, the loss is minimizing the evidence likelihood lower bound (ELBO)</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210529181330038.png" alt="image-20210529181330038" style="zoom:80%;" />, with the basic idea that seek to maximize the reconstruction ability of the decoder.</p><ul><li>The motivation behind the ELBO is rooted in the theory of variational inference.</li><li>The goals will be satisfied under this optimization method<ul><li>The sampled latent representations encode enough information to allow our decoder to reconstruct the input.</li><li>The latent distribution is as close as possible to the prior. It’s important if one wants to generate new graphs after training : they can generate new graphs by sampling from the prior and feeding these latent embeddings to the decoder.</li></ul></li></ul></li></ul><h4 id="Node-level-latent">Node-level latent</h4><p>Encoding and decoding graphs based on node embeddings. The key idea is that the encoder generates latent representations for each node in the graph.</p><p>As a generative model, the node-level method is limited.</p><ul><li><a href="https://arxiv.org/pdf/1611.07308.pdf%5D">Kipf et al.</a> proposed VGAE (variational graph autoencoder)<ul><li>Encoder model: can be based on any of the GNN architectures. In particular, two GNNs are used to generate mean (for each node in the input graph) and variance parameters separately. Once they are computed, tensors set of latent node embeddings can be sampled.</li><li>Decoder model: predict the likelihood of all the edges in the graph, given a matrix of sampled node embeddings. In VGAE, thy use a dot-product decoder.</li><li>The reconstruction error is a binary cross-entropy over the edge probabilities.</li></ul></li><li>Limitations: It’s limited especially when a dot-product decoder is used. The decoder has no parameters, so the model is not able to generate non-trivial graph structures without a training graph as input.</li><li><a href="http://proceedings.mlr.press/v97/grover19a.html">Grover et al.</a> propose to augment the decoder with an iterative GNN based decoder.</li></ul><h4 id="Graph-level-latent">Graph-level latent</h4><p>The encoder and decoder functions are modified to work with graph-level latent representations.</p><ul><li>Encoder model: It can be an arbitrary GNN model augmented with a pooling layer. Again there are two separate GNNs to parameterize the mean and variance of a posterior normal distribution over latent variables.</li><li>Decoder model: One original Graph-VAE model proposed to combine a basic MLP with a Bernoulli distributional assumption. Simply independent Bernoulli distribution for each edge, and the overall  log-likelihood objective is equivalent to set of independent binary cross-entropy loss function on each edge.<ul><li>The challenges while implementing with MLP:<ul><li>Have to assume a fixed number of nodes</li><li>We don’t know the correct ordering of the  rows and columns in $\tilde{A}$ when we are computing the reconstruction loss.</li></ul></li></ul></li><li>Limitations: using graph-level latent representations introduces the issue of specifying node orderings. And with MLP, currently limits the application of the basic graph-level VAE to small graphs with hundred of nodes or less.</li></ul><h3 id="Adversarial-approaches">Adversarial approaches</h3><p>VAE suffers from serious limitations–such as the tendency for VAEs to produce blurry outputs in the image domain.</p><ul><li>Some works<ul><li><a href="https://arxiv.org/pdf/1805.11973.pdf">De Cao and Kipf et al.</a> propose one that is similar to the graph-level VAE.<ul><li>The generator is a MLP that generates a matrix of edge probabilities given a seed vector.</li><li>The discrete adjacency matrix is generated by sampling independent Bernoulli variables for each edge.</li><li>The discriminator employ any GNN-based graph classification model.</li></ul></li></ul></li><li>Benefits: GAN-based methods remove the complication of specifying a node ordering in the loss computation as long as the discriminator model is permutation invariant.</li><li>Limitations :  GAN-based approaches to graph generation have so far received less attention and success than their variational counterparts.</li></ul><h3 id="Autoregressive-methods">Autoregressive methods</h3><p>Both VAE-based approaches and basic GANs that discussed before use simple MLPs to generate adjacency matrices. Autoregressive methods can decode graph structures from latent representations, they will combine GANs and VAEs.</p><h4 id="Modeling-edge-dependencies">Modeling edge dependencies</h4><p>Previous it’s assumed that the edges are independent for convenience, but this is not true in real world.</p><ul><li><p>In autoregressive model, it’s assumed that edges are generated sequentially and that the likelihood of each edge can be conditioned on the edges that have been previously generated.</p></li><li><p>Assume the lower-triangular portion of the adjacency matrix $\mathrm{A}$ is denoted as $\mathrm{L}$.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210530073545690.png" alt="image-20210530073545690" style="zoom:80%;" /></li></ul><h4 id="Recurrent-models-for-graph-generation">Recurrent models for graph generation</h4><ul><li><a href="https://arxiv.org/abs/1802.08773">GraphRNN</a> : use a hierarchical RNN to model the edge dependencies.<ul><li>The 1st RNN in this model is a graph-level RNN  that is used to model the state of the graph that has been generated so far. It involves a stochastic sampling process to generate the discrete edges. In this way, the graph RNN is able to generate diverse samples of graphs even when a fixed initial embedding is used.</li><li>The 2nd RNN, termed a the node-level RNN, generates the entries of $\mathrm{L}$ lower triangular adjacency matrix in an autoregressive manner. It will take the graph-level hidden state as its input and then sequentially generate the binary values of $\mathrm{L}$, assuming a conditional Bernoulli distribution for each entry.</li><li>The node level RNN is initialized at each time-step with the current hidden state of the graph-level RNN.</li><li>Both RNNs can be optimized to maximize the likelihood of the training graphs using the teaching forcing strategy, meaning that the ground truth of $\mathrm{L}$ are always used to update the RNNs during training. But computing the likelihood requires the assumption that a particular a ordering over the generated nodes.</li><li>It’s more capable of generating grid-like structures, compared to the basic graph-level VAE.</li><li>Limitations: It still generates unrealistic artifacts (e.g. long chains) when trained on samples of grids. It can be difficult to train and scale to large graphs due to the need to backpropagate through many steps of RNN recurrence.</li></ul></li><li><a href="https://arxiv.org/abs/1910.00760">GRAN</a>: generate graphs by using a GNN to condition on the adjacency matrix that has been generated so far. GRAN models dependencies between edges. It maintains the autoregressive decomposition of the generation process.<ul><li>It uses GNNs to model the autoregressive generation process.</li><li>One can model the conditional distribution of each row of the adjacency matrix by running a GNN on the graph that has been generated so far.</li><li>Since there are no node attributes associated with the generated nodes, the input feature matrix $\tilde{\mathrm{X}}$ to the GNN can simply contain randomly sampled vectors.</li><li>The key benefit of GRAN compared with GraphRNN is that it does not need to maintain a long and complex history in a graph-level RNN.</li><li>To use GRAN on large graphs, one improvement is that multiple nodes can be added simultaneously in a single block rather than adding nodes one at a time.</li></ul></li></ul><h3 id="Evaluating-graph-generation">Evaluating graph generation</h3><p>Quantitatively compare the different models introduced previouly?</p><ul><li>Currently is to analyze different statistics of the generated graphs and to compare the distribution of statistics for the generated graphs to a test set.</li><li>Compute the distance between the statistic’s distribution on the test graph and generated graph using a distributional measure, such as the total variation distance.</li><li>The statistics that are used including degree distributions, graphlets counts, and spectral features with distributional distances computed using variants of the total variation score and the 1st Wassertein distance.</li></ul><h3 id="Molecule-generation">Molecule generation</h3><ul><li>The goal of molecule generation is to generate molecular graph structures that are both valid (e.g., chemically stable) and ideally have some desirable properties (e.g., medicinal properties or solubility).</li><li>Domain-specific knowledge for both model design and evaluation.</li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Building models that can infer latent graph structures beyond the input graph that we are given is a critical direction for pushing forward graph representation learning.</li><li>Message-passing GNNs are inherently bounded by the WL isomorphism test. They suffer from over-smoothing, being limited to simple convolutional filters, and being restricted to tree-structured computation graphs.</li></ul><h3 id=""></h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Book &lt;a href=&quot;https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf&quot;&gt;Graph Representation Learning&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="book" scheme="http://yoursite.com/tags/book/"/>
    
  </entry>
  
  <entry>
    <title>Paper--3D photography on your desk</title>
    <link href="http://yoursite.com/posts/notes/2021-03-04-notes-paper-cv-3dshape.html"/>
    <id>http://yoursite.com/posts/notes/2021-03-04-notes-paper-cv-3dshape.html</id>
    <published>2021-03-05T03:35:00.000Z</published>
    <updated>2021-04-28T23:14:51.990Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.2656&amp;rep=rep1&amp;type=pdf">3D photography on your desk, 1998</a>, <a href="http://www.vision.caltech.edu/bouguetj/ICCV98/">website</a></p><p>Reference: <a href="https://ieeexplore.ieee.org/abstract/document/1087109?casa_token=RZ6-EVaSgrIAAAAA:QwrjPXnkBwNtWBQR_RsNxixV2Q7NZ4qj9DFuPG4PkI3rm8J_6NZQUq4spi3Op-xpmGSEwCNuyA">A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses</a></p><a id="more"></a><h1>Reference</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/1087109?casa_token=RZ6-EVaSgrIAAAAA:QwrjPXnkBwNtWBQR_RsNxixV2Q7NZ4qj9DFuPG4PkI3rm8J_6NZQUq4spi3Op-xpmGSEwCNuyA">A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses</a></p><h2 id="Camera-calibration">Camera calibration</h2><h3 id="Types">Types</h3><ol><li><p><strong>2D image coordinates to 3D information</strong></p><ul><li><p>The Types of 3D information to be inferred</p><ul><li><p>3D information concerning the location of the object, target, or feature.</p><p>Camera calibration will offer a way of determining a ray in 3D space that the object point must lie on.</p></li><li><p>3D information concerning the position and orientation of the moving camera relative to the target world coordinate system.</p><p>Like robot camera.</p></li></ul></li></ul></li><li><p><strong>3D information to 2D image coordinates</strong></p><ul><li>If given hypothetical 3D location of the object, the 2D image coordinates can be estimated.</li></ul></li></ol><h3 id="Requirement">Requirement</h3><p>autonomous, accurate, reasonably efficient, versatile, need only common off-the-shelf camera and lens</p><h3 id="Previous">Previous</h3><h4 id="Full-scale-nonlinear-optimization">Full-scale nonlinear optimization</h4><ul><li>Advantage: allows easy adaption of any arbitrarily accurate yet complex model for imaging.</li><li>Problems: the requirement of a good initial guess and computer-intensive full-scale nonlinear search</li><li>Approaches<ul><li>Classical approach: accurate cause the large number of unknowns and images in high resolution from rather than solid-state image array like CCD</li><li>Direct linear transformation (DLT): only using linear equations, but pure DLT only works fine without lens distortion. DLT confirms that low-resolution images from like CCD can also be used for accurate calibration.</li><li>Sobel, Gennery, Lowe<ul><li>Sobel: nonlinear equation, 18 parameters</li><li>Gennery: iteratively by minimizing the error of epipolar constraints without using 3D coordinates of calibration points. But it’s error-prone.</li></ul></li></ul></li></ul><h4 id="Computing-perspective-transformation-matrix-using-linear-equation-solving">Computing perspective transformation matrix using linear equation solving</h4><ul><li>Advantage : no requirement of nonlinear optimization</li><li>Problems: Cannot take lens distortion into consideration; the number of unknowns in linear equations is much larger than the real DoF. If lens distortion is not considered, then the perspective matrix can be solved by OLS.</li><li>If the field of view is narrow and the object distance is large, then ignoring distortion should cause more error.</li></ul><h4 id="Two-plane-method">Two-plane method</h4><ul><li>Advantage : only linear equations need be solved</li><li>Problems: the number of unknowns is much larger than DoF; the formula used between 2D and 3D is empirically.</li><li>No restrictions needed for the extrinsic camera parameters, but the relative orientation between the camera coordinate system and the object world coordinate system is required. The nonlinear leans distortion theoretically cannot be corrected.</li></ul><h4 id="Geometric-technique">Geometric technique</h4><ul><li>Advantage : no linear search is needed</li><li>Problems: no lens distortion be carried; the requirement of focal length, uncertainty of image scale factor is not allowed</li></ul><h2 id="Two-stage-calibration">Two-stage calibration</h2><h3 id="Goal">Goal</h3><ul><li>Reduce the number of parameters that need to be estimated by applying a constraint. The constraint is <em><strong>radial alignment constraint.</strong></em></li><li>Radial alignment constraint: a function of the relative rotation and translation between the camera and the calibration points.</li><li>The single-plane calibration points are used so the plane must be parallel to image plane.</li><li>If the DLT-type linear approximation is used, the distortion cannot be ignored unless a very narrow angle lens is used.</li></ul><h3 id="Camera-model">Camera model</h3><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306222342885.png" alt="image-20210306222342885" style="zoom:30%;" /><ol><li><p>Rigid body transformation from the object world coordinate system $(x_w, y_w, z_w)$ to the camera 3D coordinate system $(x, y, z) $. <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306222752281.png" alt="image-20210306222752281" style="zoom:25%;" />. $R,T$ need to be calibrated. Translation is calibrated before rotation.</p><p><strong>However, how can one know $(x_w, y_w, z_w)$?</strong></p></li><li><p>Transformation from 3D camera coordinate$(x, y, z)$ to ideal (undistorted) image coordinate $(X_u, Y_u)$. $X_u=f\frac{x}{z},Y_u=f\frac{y}{z}$.</p></li><li><p>Calibrate radial lens distortion $k_1,k_2$. Experimentally only one $k$ of radial lens distortion will work fine. The more will cause numerical instability.</p><p>$X_d(1+k_1r^2+k_2r^4+\cdots)=X_u,\Y_d(1+k_1r^2+k_2r^4+\cdots)=Y_u,\r=\sqrt{X_d^2+Y_d^2}$.</p></li><li><p>Real image coordinate $(X_d,Y_d)$ to computer image coordinate $(X_f,Y_f)$. $S_x$ is gonna be calibrated.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306224649098.png" alt="image-20210306224649098" style="zoom:40%;" /></li></ol><p>Get all four steps together and suppose $C_x=0，C_y=0$, the final formula between image coordinates $(X,Y)$ and real world 3D coordinates $(x_w,y_w,z_w)$ is</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210306231044495.png" alt="image-20210306231044495" style="zoom:40%;" />.</p><p>Now that given image coordinates $(X,Y)$ and real world 3D coordinates $(x_w,y_w,z_w)$ , the camera can be calibrated.</p><h3 id="Implementation-by-a-monoview-coplanar-set-of-points">Implementation by a monoview coplanar set of points</h3><p>Before started, make sure $(x_w,y_w,z_w)$ is out of the field view and not close to the $y$ axis so as to avoid $T_y=0$.</p><ol><li><p>Compute 3D Orientation , Position ($x$ and $y$) and sclerosis factor</p><ul><li><p>Compute the distorted image coordinates $(X_d,Y_d)$</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307113715726.png" alt="image-20210307113715726" style="zoom:50%;" />, where $C_x,C_y$ are supposed as the center of the image frame (aka they cannot be calibrated),  <strong>$s_x$ is not calibrated here but from a priori.</strong></p></li><li><p>Compute the five unknowns $T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5$.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307115018316.png" alt="image-20210307115018316" style="zoom:33%;" />, this requires at least 6 points.</p></li><li><p>Compute $(r_1,\cdots,r_9,T_x,T_y)$ from $(T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5)$</p><ul><li><p>Compute $|T_y|$ from $(T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5)$</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307115902149.png" alt="image-20210307115902149" style="zoom:40%;" /></li><li><p>Determine the sign of $T_y$.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307121434690.png" alt="image-20210307121434690" style="zoom:40%;" /><p>This sign reversal of $T_y$ causes $(x,y)$ to become $-(x,y)$. But because $X_d,x$ have the same sign, $Y_d, y$ have the same sign, then <strong>only one of the two signs for $T_y$ is valid</strong> .</p></li><li><p>Compute the 3D rotation $R$</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122232570.png" alt="image-20210307122232570" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122414379.png" alt="image-20210307122414379" style="zoom: 50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122549000.png" alt="image-20210307122549000" style="zoom:50%;" /></p></li></ul><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122644888.png" alt="image-20210307122644888" style="zoom:50%;" /></li></ul></li><li><p>Compute effective focal length, distortion coefficients and $z$ position.</p><p>Here, $k_1=0$.</p><ul><li><p>Compute an approximation of $f, T_z$ by ignoring lens distortion.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122549000.png" alt="image-20210307122549000" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122644888.png" alt="image-20210307122644888" style="zoom:50%;" /></li><li><p>Compute exact solution for $f,T_z,k_1$</p><p>Solve $(8b)$: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307124032524.png" alt="image-20210307124032524" style="zoom:50%;" />, with $f,T_z,k_1$ as unknowns using standard optimization scheme such as steepest descent. The approximation of $f,T_z$ in previous step can be used as initial guess, and the initial guess of $k_1$ can be taken as zero.</p></li></ul></li></ol><h3 id="Implementation-using-monoview-noncoplanar-points">Implementation using monoview noncoplanar points</h3><p>When $s_x$ is unknown . Now a coplanar set of calibration points is required. Now $z_w$ is no longer identical zero.</p><ol><li><p>Compute 3D Orientation , Position ($x$ and $y$) and scale factor</p><ul><li><p>Compute the distorted image coordinates $(X_d,Y_d)$<br>$$<br>X_{di}={d_x}'(X_{fi}-C_x)\<br>Y_{di}=d_y(Y_{fi}-C_y)<br>$$<br>where $C_x,C_y$ are supposed as the center of the image frame (aka they cannot be calibrated),  <strong>here, the real $s_x$ is absorbed into the unknowns for the liner equation in equations below.</strong></p></li><li><p>Compute the seven unknowns $T_y^{-1}s_xr_1,T_y^{-1}s_xr_2,T_y^{-1}s_xr_3,T_y^{-1}s_xT_x,T_y^{-1}s_xr_4,T_y^{-1}s_xr_5,T_y^{-1}s_xr_6$.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307132957951.png" alt="image-20210307132957951" style="zoom:30%;" />, this requires at least 8 points.</p></li><li><p>Compute $(r_1,\cdots,r_9,T_x,T_y)$ from $T_y^{-1}s_xr_1,T_y^{-1}s_xr_2,T_y^{-1}s_xr_3,T_y^{-1}s_xT_x,T_y^{-1}s_xr_4,T_y^{-1}s_xr_5,T_y^{-1}s_xr_6$</p><ul><li><p>Compute $|T_y|$ from $(T_y^{-1}r_1,T_y^{-1}r_2,T_y^{-1}T_x,T_y^{-1}r_4,T_y^{-1}r_5)$</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133504840.png" alt="image-20210307133504840" style="zoom:50%;" /></li><li><p>Determine the sign of $T_y$.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307121434690.png" alt="image-20210307121434690" style="zoom:40%;" /><p>This sign reversal of $T_y$ causes $(x,y)$ to become $-(x,y)$. But because $X_d,x$ have the same sign, $Y_d, y$ have the same sign, then <strong>only one of the two signs for $T_y$ is valid</strong> .</p></li><li><p><em><strong>Determine s_x</strong></em>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133615299.png" alt="image-20210307133615299" style="zoom:33%;" /></p></li><li><p>Compute the 3D rotation $R$</p><p>​        <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133750413.png" alt="image-20210307133750413" style="zoom:50%;" />, <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307133819256.png" alt="image-20210307133819256" style="zoom:50%;" /></p></li></ul></li></ul></li><li><p>Compute effective focal length, distortion coefficients and $z$ position.</p><p>Here, $k_1=0$.</p><ul><li><p>Compute an approximation of $f, T_z$ by ignoring lens distortion.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122549000.png" alt="image-20210307122549000" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307122644888.png" alt="image-20210307122644888" style="zoom:50%;" /></li><li><p>Compute exact solution for $f,T_z,k_1$</p><p>Solve $(8b)$: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307124032524.png" alt="image-20210307124032524" style="zoom:50%;" />, with $f,T_z,k_1$ as unknowns using standard optimization scheme such as steepest descent. The approximation of $f,T_z$ in previous step can be used as initial guess, and the initial guess of $k_1$ can be taken as zero.</p></li></ul></li></ol><h1>Paper</h1><p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.2656&amp;rep=rep1&amp;type=pdf">3D photography on your desk, 1998</a>, <a href="http://www.vision.caltech.edu/bouguetj/ICCV98/">website</a></p><h2 id="Why">Why?</h2><ul><li>Knowing the 3D shape helps a lot, and the progress and computers and computer graphics also encourage the recovering of 3D shape</li><li>Previous<ul><li>commercial 3D scanners: <strong>accurate</strong> but expensive and bulky. Use motorized transport of temporal object and active (laser, LCD projector) lighting of the scene</li><li>Passive cues contain information on 3D shape: stereoscopic disparity, texture, motion parallax, defocus, shadows, shading and specularities,  occluding contours and other surface discontinuities amongst them. Stereoscopic disparity is popular way but it suffers from the requirement of two cameras and failing on untextured faces</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Simple and inexpensive approach for extracting the 3D shape of objects</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><p>Illuminate the camera (facing the object) by desk-lamp, then the user moves a pencil in front of the light source casting a moving shadow on the object, the 3D shape of object will be recovered by the spatial and temporal location of the observed shadow.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210304224912139.png" alt="image-20210304224912139" style="zoom:30%;" />         <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210305085918925.png" alt="image-20210305085918925" style="zoom:40%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307200112887.png" alt="image-20210307200112887" style="zoom: 33%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307200146056.png" alt="image-20210307200146056" style="zoom:33%;" /></p><h3 id="Implementation">Implementation</h3><h4 id="Calibration"><strong>Calibration</strong></h4><ul><li><p><strong>Camera calibration</strong></p><ul><li><p>recover the intrinsic camera parameters ($f,K,s_x,(u_0,v_0)$) and the location of the desk plane with respect to camera.</p></li><li><p>The method is from <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1087109">here</a>. But because the object used is planar, the optical center cannot be calibrated exactly, so the optical center is assumed to be the center of the image.</p><ul><li><p>Specifically, this method considering <strong>lens distortion</strong>. And the steps for calibration is</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210305220451049.png" alt="image-20210305220451049" style="zoom:67%;" /><ul><li><p><em>Why there are lens distortion</em> ? On some cheap camera, the captured pixel on image is rectangle but not square. This will lead to lens distortion.  Under lens distortion, suppose the image point under ideal case is $(X_u,Y_u)$, and after lens distortion the image point is $(X_d,Y_d)$, $K$ is the lens distortion coefficient, then</p><p>$r^2={X_u}^2+{Y_u}^2,\ X_d=X_u(1+Kr^2),Y_d=Y_u(1+Kr^2)$</p></li><li><p>From the image above, $P_u$ is the image coordinates of $P$ in real 3D world under ideal case (no lens distortion ), and $P_d$ is the image coordinates considering lens coordinates. Then the  formulas are:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307144904684.png" alt="image-20210307144904684"></p></li></ul></li></ul></li></ul></li><li><p><strong>lamp calibration</strong> : to determine the 3D location of the point light source $S$</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307152240120.png" alt="image-20210307152240120" style="zoom:33%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307152308342.png" alt="image-20210307152308342" style="zoom:30%;" /></p><ul><li>Given the height of pencil: $h$, the pencil will be orthogonal to the desk.</li><li>Measure the bottom of the pencil ($\bar{b}$) and the tip of the shadow $\bar{t}_s$ in captured image, then according to the calibrated camera, the coordinates of pencil bottom in 3D world (denoted as $K$) and the tip of shadow in real 3D world ($T_s$) can be estimated. Then the tip of pencil in 3D world ($T$) is estimated by $h$.</li><li>The light source $S$ will be the intersection of two rays $TT_S$. Therefore, two position of pencils will help figure the light source out.</li></ul></li></ul><h4 id="Spatial-and-temporal-shadow-edge-localization"><strong>Spatial and temporal shadow edge localization</strong></h4><p>Notice temporal shadow will be scanned from the left to the right side of the scene and thus the right edge of the shadow corresponds to the front edge of the temporal profile</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307175819939.png" alt="image-20210307175819939" style="zoom:33%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307175915876.png" alt="image-20210307175915876" style="zoom:30%;" /></p><ul><li><strong>Two steps to be done</strong><ul><li><strong>Localize</strong> the edge of the shadow that is directly projected on the tabletop $(\bar{x}<em>{top}(t),\bar{x}</em>{bot}(t))$ at every time instant $t$ (every frame).</li><li><strong>Estimate the time</strong> $t_s(\bar{x}_c)$ (shadow time) where the edge of the shadow passes through any given pixel $\bar{x}_c=(x_c,y_c)$ in the image.</li></ul></li><li>Cause the pixels corresponding to regions in the scene are not illuminated by the lamp, so they don’t provide any relevant depth information. And that’s why only processing pixels with contrast value ($I_{max}(x,y)-I_{min}(x,y)$) larger than predefined threshold $I_{thresh}$, which is 70 in this paper.</li><li>Adaptive threshold image $I_{shadow}(x,y)=(I_{max}(x,y)+I_{min}(x,y))/2$</li><li>No spatial filtering is used cause it would generate undesired blending in the final depth estimates.</li><li>But when the light source is not close to an ideal point source, the predefine threshold $I_{shadow}$ (as a mean) is not optimum. The shadow edge profile becomes shallower as the distance between the stick and the surface increase.</li><li>To <strong>do it real-time,</strong> as the images $I(x,y,t)$ are acquired , one needs to update at each frame five different arrays $I_{max}(x,y),I_{min}(x,y),I_{contrast}(x,y),I_{shadow}(x,y),t_{s}(x,y)$.<ul><li>For one pixel $(x,y)$, the maximum brightness $I_{max}(x,y)$ is collected at the 1st frame</li><li>The $I_{min}(x,y), I_{contrast}(x,y)$ is updated as time going. Once $I_{contrast}(x,y)$ crosses $I_{thresh}$<font color='red'> (<em><strong>larger than 70??</strong></em>  which means now there is a shadow on pixel (x,y)?? )</font>, the adaptive threshold $I_{shadow}(x,y)$ starts being computed and updated at every frame. This process goes on till the pixel brightness $I(x,y,t)$ is larger than than $I_{shadow}(x,y)$ at the 1st time. <strong>This time instant is registered as the shadow time $t_s(x,y)$.</strong></li></ul></li></ul><h4 id="Triangulation"><strong>Triangulation</strong></h4><ul><li>The real 3D point $P$ is gonna be the intersection of line $O_c\bar{x}_c$ and the given pixel $\bar{x}_c$'s shadow plane.</li><li>The shadow time $t_s(\bar{x}_c)$ acts as an index to the shadow plane list $\prod(t)$. Besides, the final plane $\prod(t_s(\bar{x}_c))$ will from plane $\prod(t_0-1)$ and $\prod{t_0}$ if $t_0-1&lt;t_s{(\bar{x}_c)}&lt;t_0$ and $t_0$ integer.</li><li>After the range data are recovered, a mesh can be used to build the 3D surface.</li></ul><h3 id="Noise-sensitivity">Noise sensitivity</h3><ul><li><p>For quantifying the effect of the noise in the measurement data ${x_{top}(t), x_{bot}(t), t_{s}(\bar{x}<em>c)}$ on the final reconstructed scene depth map, he analysis of **the variance of the induced noise on the depth estimation $Z_c$, aka $\sigma</em>{Z_c}$,** will help. In a word, $\sigma_{Z_c}$ can quantify the uncertainties on the depth estimation $Z_c$ at every pixel $\bar{x}_c$, and also constitute a good indicator of the overall accuracies in reconstruction (since most of the errors are located along the $Z$ direction of the camera frame ).</p></li><li><p>The variance of the induced noise on the depth estimation $Z_c$, aka $\sigma_{Z_c}$, is derived by taking the 1st order derivatives of $Z_c$ with respect to the ‘new’ noisy input $x_{top},x_{bot},\bar{x}_c$</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307205307729.png" alt="image-20210307205307729" style="zoom:33%;" />, <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307205329890.png" alt="image-20210307205329890" style="zoom:30%;" /></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307205408384.png" alt="image-20210307205408384" style="zoom:33%;" /></li><li><p>From the equation $\sigma_{x_c}=\frac{\sigma_I}{|I_x(\bar{x}<em>c)|}$, $\sigma</em>{x_c}$ does not depend on the local shadow speed. Therefore , decreasing the scanning speed would not increase the accuracy .  However, <strong>better slow down while scanning when the shadow edge  is sharper</strong> so as to get good samples for every pixel. But with slow scanning speed, an appropriate low-pass filter before extraction of $t_{s}(\bar{x}_c)$ is required for good accuracy.</p></li><li><p>Numerically, most of the variations in the variance $\sigma_{Z_c}^2$ are due to the variation of volume $V$ within a single scan. And Therefore the <strong>reconstruction noise is systematically larger in portions of the scene further away from the lamp</strong>.</p><ul><li><p><strong>To avoid the systematic error,</strong> one may take two scans of the same scene with the lamp at two different locations (on the left and right side of the camera say)</p></li><li><p><em><strong>The final depth is estimated as</strong></em><br>$$<br>Z_c=w_LZ_c^L+w_RZ_c^R<br>$$<br>where $Z_c^L,Z_c^R$ are the two estimates (from left and right) of the same depth $Z_c$.</p><ul><li><strong>If they are gaussian distributed,</strong> and independent, then using</li></ul><p>$$<br>w_L=\frac{\sigma_{Z_R}^2}{\sigma_{Z_R}^2+\sigma_{Z_L}^2}=\frac{\alpha^2}{1+\alpha^2},\<br>w_R=\frac{\sigma_{Z_L}^2}{\sigma_{Z_R}^2+\sigma_{Z_L}^2}=\frac{1}{1+\alpha^2},\<br>\alpha=\frac{V_L}{V_R}<br>$$</p><p>for averaging.</p><p>But this suffers from degradation of the overall final reconstruction cause may the $Z_c^L,Z_c^R$ are not gaussian .</p><ul><li>To avoid the problem mentioned above, another solution is sigmoid.<br>$$<br>w_L=\frac{1}{1+\exp(-\beta\Delta V)},\<br>w_R=\frac{1}{1+\exp(\beta\Delta V)},\<br>\Delta V=\frac{V_L^2-V_R^2}{V_L^2+V_R^2}=\frac{\alpha^2-1}{\alpha^2+1}<br>$$<br>The positive coefficient $\beta$ controls the amount of diffusion between the left and the right regions. As $\beta$ tends to infinity, merging reduces to hard decision: $Z_c=Z_c^L$ if $V_L&gt;V_R$ and $Z_c=Z_c^R$ otherwise. This will help reduce tends estimation error and obtain more coverage of the scene.</li></ul></li><li><p>The global accuracy depends on the scanning . This paper scan vertically, so the average relative depth error $|\frac{\sigma_{Z_c}}{Z_c}|$ is inversely proportional proportion to $|\cos\xi|$. The best value will be got while $\xi=0, \xi=\pi$. aka <strong>lamp standing either tot he right ($\xi=0$ ) or to the left ($\xi=\pi$ ) of the camera.</strong></p></li></ul></li></ul><h2 id="Issues">Issues</h2><h3 id="Point-light-source"><strong>Point light source</strong></h3><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210414221836181.png" alt="image-20210414221836181" style="zoom:67%;" /><h3 id="real-time-implementation">**real-time implementation **</h3><p><strong>while estimating the shadow time</strong></p><p>All that one needs to do is update at each frame five different arrays $I_{max}(x; y), I_{min}(x; y), I_{contrast}(x; y),$ $I_{shadow}(x; y)$ and the shadow time $t_s(x; y)$, as the images $I(x; y; t)$ are acquired. For a given pixel $(x; y)$, the maximum brightness $I_{max}(x; y)$ is collected at the very beginning of the sequence (the first frame), and then, as time goes, the incoming images are used to update the minimum brightness $I_{min}(x; y)$ and the contrast $I_{contrast}(x; y)$. Once $I_{contrast}(x; y)$ crosses $I_{thresh}$, the adaptive threshold $I_{shadow}(x; y)$ starts being computed and updated at every frame (and activated). This process goes on until the pixel brightness $I(x; y; t)$ crosses $I_{shadow}(x; y)$ for the first time (in the upwards direction). That time instant is registered as the shadow time $t_s(x; y)$. In that form of implementation, the left edge of the shadow is tracked instead of the right one, however the principle remains the same.</p><h3 id="shadow-time"><strong>shadow time</strong></h3><ul><li><strong>Function</strong>: works as an index to the shadow plane list so as to using the intersection to locate $P$.</li><li><strong>Accuracy</strong>: Since $t_s(\bar{x}_c)$ is estimated at sub-frame accuracy, the final plane $\prod(t_s(\bar{x}_c))$ actually results from linear interpolation between the two planes $\prod(t_0-1)$ and<br>$\prod(t_0)$  if $t_0-1 &lt; t_s(\bar{x}_c) &lt; t_0$ and $t_0$ integer.</li></ul><h3 id="Accuracy-of-depth-estimation">Accuracy of depth estimation</h3><ul><li>The accuracy increases as the sharpness of image increases.</li><li>Remove the lamp reflector improve the accuracy.</li><li>Decreasing the scanning speed would not increase accuracy.</li><li>To guarantee the accuracy of sharp edges of object, temporal pixel profile must be sufficiently sampled within the transition area of the shadow edge. Therefore, <strong>the sharper the shadow edge, the slower the scanning speed will help.</strong></li><li>$\sigma_{Z_c}$ is a good indicator of the overall accuracies while reconstruction, since most of the errors are located along the $Z$ direction of the camera frame.</li><li>As the shadow moves into the opposite direction of the lamp, the absolute value of the volume $|V|$ strictly decreases and making $\sigma_{Z_c}$ larger. Therefore, the reconstruction noise is systematically larger in portions of the scene further away from the lamp.</li></ul><h2 id="Experiments">Experiments</h2><h3 id="Calibration-accuracies">Calibration accuracies</h3><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307223201885.png" alt="image-20210307223201885" style="zoom:33%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210307223230315.png" alt="image-20210307223230315" style="zoom:33%;" /></p><ul><li>For camera calibration,<ul><li>10 images of the checkboard are taken, nearly 90 corners on the checkboard (8*9).</li><li>the relative error of radial distortion is larger than others.</li></ul></li><li>In lamp calibration,<ul><li>collect 10 images of the pencil shadow</li><li>$\bar{S}_c$ is the coordinate vector of the light source in the camera frame, points $b,t_s$ were manually extracted from the images.</li><li>The calibration accuracy is about 3mm, which is sufficient for final shape recovery.</li></ul></li></ul><h3 id="Scene-reconstructions">Scene reconstructions</h3><ul><li>Planarity of the plane<ul><li>There is a decrease of approximately $6%$ in residual standard deviation after quadratic warping. The global geometric deformations are negligible compared to local surface noise. It indicates that the errors of calibration for not induce significant global deformations on the final reconstruction.</li><li><strong>Why 0.23mm/5cm?</strong></li></ul></li><li>Geometry of the corner<ul><li>The overall reconstructed structure does not have any major noticeable global deformation.</li><li>The <strong>errors</strong> (which errors? The surface noise?) are the order of 0.3mm in most experiments.</li></ul></li><li>Angle scene<ul><li>With bulb naked, there is a significant improvement in the sharpness of the projected shadow compared the shadow captured with lamp reflector.</li></ul></li></ul><h3 id="Outdoor">Outdoor</h3><ul><li>outdoors where the sun may be used as a calibrated light source (given latitude, longitude, and time of day).</li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Simple and low cost system to extracting surface shape of objects. It can be used in real time. The accuracies on the final reconstruction are reasonable (at most $1%$ or 0.5mm noise error)</li><li>it easily scales to larger scenarios indoors and outdoors.</li><li>Future work:<ul><li>like using the sun as the light source if measuring outdoors.</li><li>multiple view integration so as to move freely the object in front ode the camera and lamp between scans.</li><li>Incorporate a geometrical model of extended light source to the shadow edge detection process.</li><li></li></ul></li></ul><h2 id="QA">QA</h2><ul><li><p>Briefly explain, referring to the figure, how to obtain a 3D reconstruction at from a camera, a light source, a stick, and one or two shots.</p><ul><li>preliminaries: calibrated lamp $S$ and camera $O_c$.</li><li>Slowly scan the target object by the stick and saved as a video.<ul><li>Do the statistics to get $I_{max},I_{min}$ and then define $I_{shadow}$.</li><li>Check the video, for the image of each object’s point (manually defined, such as each pixel of this object) $\bar{x}<em>c:(x</em>{c},y_{c})$ in the whole video, use $I_{shadow}$ and image intensity, draw the temporal shadow and locate the $t_s(x_c,y_c)$ (the index of frame).</li></ul></li><li>For each point in object $\bar{x}_c$:<ul><li>Check the shadow time $t_s(x_c,y_c)$ and pick the corresponding image.</li><li>Set $x_{bot}(t), x_{top}(t)$ on the picked image, and then find its corresponding 3D points $A(t),B(t)$ respectively.</li><li>Define the shadow plane as the plane consists of $S,A(t),B(t)$.</li><li>The 3D point of $x_c$, denoted as $P$ is the intersection of shadow plane and ray $O_cx_c$.</li></ul></li><li>After the 3D coordinates of all interested points $\bar{x}_c$ are detected, 3D reconstruction can be completed.</li></ul></li><li><p>Referring to the figure, explain what the reference points A (t) and B (t) are for.</p><p>Points $A(t)$ and $B(t)$ are the 3D points, and used to define the shadow plane by light source $S$ and these two points.</p></li><li><p>To find the internal and external parameters of the camera, the article offers a method of calibration with a single image of a checkerboard placed on the desktop, but in this case it is necessary know the main point (center of the image). What could we do if we don’t know the point and we want to estimate it?</p><ul><li><p><s><em>Method 1：2D-3D calibration</em></s></p><p><s>Use the dodecahedron as the 3D object to calibrate camera. The optical center is the unknown parameter of camera intrinsic matrix $\mathrm{K}$. Because the DoF of $\mathrm{K}$ is 8, at least 4 pairs of non-coplanar points (2D-3D) are required during calibration.</s></p></li><li><p><em>Method 2：plane calibration</em></p><p>Still, use the checkboard, but this time move the checkboard so as to get at least two images of checkboard with different pose (keep at least one pose on the desk), each image and checkboard pair will generate at least 4 pairs of 2D-3D points. With at least 2 homographies (induced from the images), the IAC, denoted as $\omega$ can be detected and thus $\mathrm{K}$ is detected by Cholesky decomposition cause $\omega=\mathrm{K}^{-T}\mathrm{K}^{-1}$.  The optical center is in $\mathrm{K}$.</p></li></ul></li><li><p>The article says that if we use two planes perpendicular ($\pi_h$ and $\pi_v$) rather than a single plane, we do not need calibrate the light source. Why?</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210415102209642.png" alt="image-20210415102209642" style="zoom:30%;" /><p>The requirement of light source $S$ is for the detection of shadow plane so as to get the intersection of shadow plane and ray $O_cx_c$. Three non-coplanar points define a plane. Here, $A(t), B(t)$ are detected by calibrated camera $O_c$ and images of them. Suppose $A(t),B(t)$ is on $\pi_v$ and any one shadow point $S’$ is on $\pi_h$. Since $\pi_h \perp\pi_v$, $S’$ is easy to be detected. Armed with these three points $A(t),B(t), S$, the shadow plane can be located.</p></li><li><p>Why must the light source used be “point”? Explain. <strong>Give an example of a non-point light source that would also be adequate.</strong></p><ul><li><p>If the light source is not close to an ideal point source, the mean value between maximum and minimum brightness may not always constitute the optimal value for the threshold image $I_{shadow}$. Indeed, the shadow edge profile becomes shallower as the distance between the stick and the surface increases. In addition, it deforms asymmetrically as the surface normal changes. These effects could make the task of detecting the shadow boundary points challenging.</p></li><li><p>naked bulb: like just remove the lamp reflector in paper.</p></li></ul></li><li><p>The article says that if we use a vertical pencil, we can calibrate the light source. Explain how, based on the illustration to the right. State how many images are needed and explain why.</p><ul><li>Given the height of pencil: $h$, the pencil will be orthogonal to the desk.</li><li>Measure the bottom of the pencil ($\bar{b}$) and the tip of the shadow $\bar{t}_s$ in captured image, then according to the calibrated camera, the coordinates of pencil bottom in 3D world (denoted as $K$) and the tip of shadow in real 3D world ($T_s$) can be estimated. Then the tip of pencil in 3D world ($T$) is estimated by $h$.</li><li>The light source $S$ will be the intersection of two rays $TT_S$. Therefore, two position of pencils will help figure the light source out.</li></ul></li><li><p>Explain the concepts of temporal shadow and spatial shadow and what are their roles in the reconstruction process.</p><ul><li>spatial shadow helps to detect the column index of $A(t),B(t)$'s images. After finding this, the real position of $A(t),B(t)$ can be detected by the calibrated camera $O_c$ and the shadow plane can be defined also. Then the 3D coordinates can be detected as the intersection of shadow plane and ray $O_cx_c$.</li><li>temporal shadow works as an index to the shadow plane list so as to using the intersection to locate $P$.</li></ul></li><li><p>Explain for the following elements of the assembly how it could be modified and to improve the precision of the 3D reconstruction. In each case, justify your answers.</p><ul><li><p>choice of light source:</p><ul><li>point light source, like what mentioned in the paper just remove the lamp reflector. Or can use candle without reflector. May can also use a board with only one small hole in front of the light, but this should be adjusted to make sure all required shadows are created (difficult in practical). <em>The reason is a point light source creates more precise shadows.</em></li><li>may can move the light source further away from the object to make shadow more clear.</li></ul></li><li><p>choice of stick used to shade:</p><p>make sure the edge of stick is sharp (like a square stick), and long enough to cover the target object. Because the sharper the stick, its shadow edge will be more clear and easier to be detected and thus to locate $\bar{x}<em>{bot},\bar{x}</em>{top}$. The long enough stick will generate required shadow.</p></li><li><p>spatial resolution of the camera (image size):</p><p>the higher the spatial resolution, the more detailed the image (more pixels and smaller points), and thus the more detailed the reconstruction. To make sure the high spatial resolution, one can decrease the distance between camera and object or fix the camera but prolong the focal length.</p></li><li><p>temporal resolution of the camera (number of images per second):</p><p>make the scan speed faster will increase the temporal resolution. The sharper the shadow edge, the higher the temporal resolution will help. If the scan speed is too fast, then one can see a blurred shadow that hurts the reconstruction accuracy, but this can be fixed by reducing the exposure.</p></li></ul></li><li><p>The article suggests that the same method can be used outdoors. Based on the illustration to the right, explain how this can be done.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210415103710164.png" alt="image-20210415103710164" style="zoom:35%;" /><ul><li>Calibration of light source (sun) and camera<ul><li>The camera calibration can be done by checkboard.</li><li>The calibration of light source: use the latitude, longitude, and cause the sun is very far away from the earth, each ray can be taken as a parallel ray.</li></ul></li><li>Take the video of shadow image and take the note of time of the day. Similarly, calculate $I_{max},I_{min}$ and then calculate $I_{shadow}$. Draw the temporal profile and denote the shadow time $t_s(x_c,y_c)$ as the time of day.</li><li>For each point in object $\bar{x}_c$:<ul><li>Check the shadow time $t_s(x_c,y_c)$ and pick the corresponding image.</li><li>Set $x_{bot}(t), x_{top}(t)$ on the picked image, and then find its corresponding 3D points $A(t),B(t)$ respectively.</li><li>Define the shadow plane as the plane consists of $S,A(t),B(t)$.</li><li>The 3D point of $x_c$, denoted as $P$ is the intersection of shadow plane and ray $O_cx_c$.</li></ul></li><li>After the 3D coordinates of all interested points $\bar{x}_c$ are detected, 3D reconstruction can be completed.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.2656&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;3D photography on your desk, 1998&lt;/a&gt;, &lt;a href=&quot;http://www.vision.caltech.edu/bouguetj/ICCV98/&quot;&gt;website&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Reference: &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/1087109?casa_token=RZ6-EVaSgrIAAAAA:QwrjPXnkBwNtWBQR_RsNxixV2Q7NZ4qj9DFuPG4PkI3rm8J_6NZQUq4spi3Op-xpmGSEwCNuyA&quot;&gt;A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="cv" scheme="http://yoursite.com/tags/cv/"/>
    
      <category term="3d shape" scheme="http://yoursite.com/tags/3d-shape/"/>
    
  </entry>
  
  <entry>
    <title>Papers--The reviews of anomaly detection</title>
    <link href="http://yoursite.com/posts/notes/2021-03-03-notes-paper-anomal-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-03-03-notes-paper-anomal-survey.html</id>
    <published>2021-03-04T03:35:39.000Z</published>
    <updated>2021-06-05T23:54:15.954Z</updated>
    
    <content type="html"><![CDATA[<p>Paper I: <a href="https://arxiv.org/pdf/2007.02500.pdf">Deep Learning for Anomaly Detection: A Review</a></p><a id="more"></a><h1>Paper I: <a href="https://arxiv.org/pdf/2007.02500.pdf">Deep Learning for Anomaly Detection: A Review</a></h1><h2 id="Why">Why?</h2><ul><li>outlier detection or novelty detection</li><li>Deep anomaly detection aims at learning feature representations or anomaly scores via neural networks for the sake of anomaly detection.</li></ul><h2 id="Problem-complexities-and-challenges">Problem complexities and challenges</h2><h3 id="Major-problem-complexities">Major problem complexities</h3><ul><li>Unknownness: Anomalies are associated with many unknowns</li><li>Heterogeneous anomaly classes: one class of anomalies may demonstrate completely different abnormal characteristics from another class of anomalies.</li><li>Rarity and class imbalance: it is difficult to collect a large amount of labeled abnormal instances.</li><li>Diverse types of anomaly:<ul><li>Point anomalies:  individual instances that are anomalous w.r.t. the majority of other individual instances</li><li>Conditional anomalies: contextual anomalies, also refer to individual anomalous instances but in a specific context.</li><li>Group anomalies: a subset of data instances anomalous as a whole w.r.t. the other data instances.</li></ul></li></ul><h3 id="Main-challenges-tackled-by-deep-anomaly-detection">Main challenges tackled by deep anomaly detection</h3><ul><li>CH1: low anomaly detection recall rate: How to reduce false positives and enhance detection recall rates</li><li>CH2: anomaly detection in high-dimensional and/or not-independent data:<ul><li>High-dimensional anomaly detection<ul><li>Performing anomaly detection in a reduced lower dimensional space spanned by a small subsets of original features or newly constructed features.</li><li>But challenges on identifying intricate (e.g., high-order, nonlinear and heterogeneous) feature interactions and couplings.</li></ul></li><li>Guarantee the new feature space preserved proper information for specific detection methods.<ul><li>Due to the aforementioned unknowns and heterogeneities of anomalies.</li></ul></li><li>Detect anomalies from instances that may be dependent on each other.</li></ul></li><li>CH3: data-efficient learning of normality/abnormality<ul><li>Fully supervised anomaly detection is often impractical.</li><li>Unsupervised methods do not have any prior knowledge of true anomalies. They rely heavily on their assumption on the distribution of anomalies.</li><li>Weakly supervised anomaly detection<ul><li>How to learn expressive normality/abnormality representations with a small amount of labeled anomaly data.</li><li>How to learn detection models that are generalized to novel anomalies uncovered by the given labeled anomaly data.</li></ul></li></ul></li><li>CH4： Noise-resilient anomaly detection<ul><li>Large-scale anomaly-contaminated unlabeled data</li><li>The amount of noises can differ significantly from datasets and noisy instances may be irregularly distributed in the data space.</li></ul></li><li>CH5：Detection of complex anomalies<ul><li>The generation from point anomalies to conditional anomalies and group anomalies</li><li>How to  incorporate the concept of conditional/group anomalies into anomaly measures/models.</li><li>The detection of anomalies with multiple heterogeneous data sources.</li></ul></li><li>CH6： Anomaly explanation<ul><li>Have anomaly explanation algorithms that provide straightforward clues about why a specific data instance is identified as anomaly.</li><li>A main challenge to well balance the model’s interpretability and effectiveness.</li></ul></li></ul><h2 id="Addressing-the-challenges-with-deep-anomaly-detection">Addressing the challenges with deep anomaly detection</h2><h3 id="Preliminaries">Preliminaries</h3><ul><li>Deep anomaly detection aims at learning a feature representation mapping function $\phi (·) : X\mapsto Z$ or an anomaly score learning function $\tau (·): X\mapsto R$ in a way that anomalies can be easily differentiated from the normal data instances in the space yielded by the $\phi$ or $\tau$ function.</li></ul><h3 id="Categorization-of-deep-anomaly-detection">Categorization of deep anomaly detection</h3><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602121703067.png" alt="image-20210602121703067" style="zoom:70%;" /><h2 id="Deep-learning-for-feature-extraction">Deep learning for feature extraction</h2><ul><li>The deep learning components work purely as dimensionality reduction only.</li><li>Assumptions: The feature representations extracted by deep learning models preserve the discriminative information that helps separate anomalies from normal instances.</li><li>Research lines<ul><li>Directly use popular pre-trained deep learning models such as VGG etc.<ul><li>An unmasking process:  iteratively train a binary classifier to separate one set of video frames from its subsequent video frames in a sliding window, with the most discriminant features removed in each iteration step. The power of unmasking framework relies heavily on the quality of the features.</li><li>Using features extracted from a dynamically updated sampling pool of video frames is found to improve the performance of the framework.</li><li>Pretrained model and fine-tuning.</li></ul></li><li>Explicitly train a deep feature extraction model rather than a pre-trained model for the downstream anomaly scoring.<ul><li>Methods<ul><li>Three separate autoencoder networks are trained to learn low-dimensional features for respective appearance, motion, and appearance-motion joint representations for video anomaly detection.</li><li>Unsupervised classification approaches to enable anomaly scoring in the projected space. Use cluster methods to assign pseudo labels and then do one-vs-the-rest classification. The classification probabilities are used to define frame-wise anomaly scores.</li><li>Graph anomaly detection: learn the representations of graph vertices by minimizing autoencoder-based reconstruction loss and pairwise distances of neighbored graph vertices.</li></ul></li></ul></li></ul></li><li>Advantages<ul><li>A large number of state-of-the-art (pre-trained) deep models and off-the-shelf anomaly detectors are readily available.</li><li>Deep feature extraction offers more powerful dimensionality reduction than popular linear methods.</li><li>It is easy-to-implement given the public availability of the deep models and detection methods.</li></ul></li><li>Disadvantages<ul><li>The fully disjointed feature extraction and anomaly scoring often lead to suboptimal anomaly scores.</li><li>Pre-trained deep models are typically limited to specific types of data.</li></ul></li><li>Challenges Targeted<ul><li>The lower-dimensional space often helps reveal hidden anomalies and reduces false positives (CH2).</li><li>May not preserve sufficient information for anomaly detection as the data projection is fully decoupled with anomaly detection.</li><li>Allows to leverage multiple types of features and learn semantic-rich detection models, and then reduce CH1.</li></ul></li></ul><h2 id="Learning-feature-representation-of-normality">Learning feature representation of normality</h2><h3 id="Generic-normality-feature-learning">Generic normality feature learning</h3><ul><li>Learns the representations of data instances by optimizing a generic feature learning objective function that is not primarily designed for anomaly detection.</li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602130709282.png" alt="image-20210602130709282" style="zoom:67%;" /></li></ul><h4 id="Autoencoders">Autoencoders</h4><ul><li><p>Aims to learn some low-dimensional feature representation space on which the given data instances can be well reconstructed. The learned feature representations are enforced to learn important regularities of the data to minimize reconstruction errors, anomalies are difficult to be reconstructed from the resulting representations and thus have large reconstruction errors.</p></li><li><p>Assumptions:  Normal instances can be better restructured from compressed space than anomalies.</p></li><li><p>Formally,</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602132252366.png" alt="image-20210602132252366" style="zoom:70%;" /></li><li><p>Methods</p><ul><li>Sparse AE: encourage sparsity in the activation units of the hidden layer.</li><li>Denoising AE:  learning representations that are robust to small variations by learning to reconstruct data from some predefined corrupted data instances rather than original data.</li><li>Contractive AE: takes a step further to learn feature representations that are robust to small variations of the instances around their neighbors. By adding a penalty term based on the Forbenius norm of the Jacobian matrix of the encoder’s activations.</li><li>Variational AE: introduces regularization into the representation space by encoding data instances using a prior distribution over the latent space.</li></ul></li><li><p>Implementations</p><ul><li>Replicator NNs</li><li>RandNet: learning an ensemble of AEs</li><li><a href="https://git.io/JfYG5">RDA</a>: motivated by robust PCA, it attempts to iteratively decompose term original data into 2 subsets, normal instance set and anomaly set. This is achieved by adding a sparsity penalty $\ell_1$ or grouped penalty $\ell_{2,1}$ into its RPCA-alike objective function to regularize the coefficients of the anomaly set.</li></ul></li><li><p>For more complex data</p><ul><li>Adapting the network architecture to the type of input data, they embeds the encoder-decoder scheme into the full procedure of these methods, such as CNN-AE, LSTM-AE, Conv-LSTM-AE, GCN-AE etc.</li><li>First use AEs to learn low-dimensional representations of the complex data and then learn to predict these learned representations.<ul><li>denoising AE is combined with RNNs to learn normal patterns of multivariate sequence data,</li></ul></li></ul></li><li><p>Advantages</p><ul><li>The idea of AEs is straightforward and generic to different types of data.</li><li>Different types of powerful AE variants can be leveraged to perform anomaly detection.</li></ul></li><li><p>Disadvantages</p><ul><li>The learned feature representations can be biased by infrequent regularities and the presence of outliers or anomalies in the training data.</li><li>The objective function of the data reconstruction is designed for dimension reduction or data compression, rather than anomaly detection.</li></ul></li><li><p>Challenges</p><ul><li>CH2: attributed graph data etc.</li><li>CH1: reduce false positives</li><li>CH4: RPCA</li></ul></li></ul><h4 id="GANs">GANs</h4><p>Some form of residual between the real instance and the generated instance are then defined as anomaly score.</p><ul><li><p>Assumption: Normal data instances can be better generated than anomalies from the latent feature space of the generative network in GANs</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfGgc">AnoGAN</a>: computational inefficiency in the iterative search of latent representation $\boldsymbol{\mathrm{z}}$.</p><ul><li>To alleviate the inefficiency<ul><li><a href="https://git.io/JfGgG">EBGAN</a> : based on BiGAN, discriminate the pair of instances $(\mathrm{x}, 𝐸(\mathrm{x}))$ from the pair $(𝐺(\mathrm{z}),\mathrm{z})$.  EBGAN is extended to ALAD by adding two more discriminators with one discriminator trying to discriminate the pair $(\mathrm{x}, \mathrm{x})$ from $(\mathrm{x},𝐺(𝐸(\mathrm{x}))) $and another one trying to discriminate the pair $(\mathrm{z},\mathrm{z})$ from $(\mathrm{z}, 𝐸(𝐺(\mathrm{z})))$.</li><li><a href="https://git.io/JfZRn">fast AnoGAN</a>: share the same spirit of EBGAN.</li><li><a href="https://git.io/JfZ8v">ALAD</a>: the extension of EBGAN</li></ul></li></ul></li><li><p><a href="https://git.io/JfGgn">GANomaly</a>: further improves the generator over the previous work by changing the generator network to an encoder-decoder-encoder network and adding two more extra loss function.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602143617119.png" alt="image-20210602143617119" style="zoom:60%;" /></li><li><p>Combined with wassertein GAN or adversarially learn end-to-end one-class classification.</p></li></ul></li><li><p>Advantages</p><ul><li>Demonstrated superior capability in generating realistic instances, especially on images.</li><li>A large number of existing GAN-based models and <a href="https://arxiv.org/abs/1710.07035">theories</a> may be adapted for anomaly detection.</li></ul></li><li><p>Disadvantages</p><ul><li>The training of GANs can suffer from multiple problems, such as failure to converge and mode collapse.</li><li>The generator network can be misled and generates data instances out of the manifold of normal instances.</li><li>The GANs-based anomaly scores can be suboptimal since they are built upon the generator network with the objective designed for data synthesis rather than anomaly detection.</li></ul></li><li><p>Challenges</p><ul><li>CH2， CH1</li></ul></li></ul><h4 id="Predictability-modeling">Predictability modeling</h4><p>Learn feature representations by predicting the current data instances using the representations of the previous instances within a temporal window as the context. The prediction errors can be used to define the anomaly scores</p><ul><li><p>Assumptions: Normal instances are temporally more predictable than anomalies</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/Jf4pc">FFP</a>: Using like U-net prediction, popular in video anomaly detection.</p><p>U-Net as the frame generator, and measure the objective loss by intensity, gradient and optical flow. After training, for a given video frame $\mathrm{x}$, a normalized Peak Signal-to-Noise Ratio based on the prediction difference is used to define the anomaly score.</p><ul><li><a href="https://git.io/Jf4pW">LSA</a></li></ul></li><li><p>Another way is based on autoregressive models that assume each element in a sequence is linearly dependent on the previous elements.</p><ul><li>At the evaluation stage, the reconstruction error and the log-likelihood are combined to define the anomaly score.</li></ul></li></ul></li><li><p>Advantages</p><ul><li>A number of sequence learning techniques can be adapted and incorporated into this approach.</li><li>This approach enables the learning of different types of temporal and spatial dependencies.</li></ul></li><li><p>Disadvantages</p><ul><li>This approach is limited to anomaly detection in sequence data.</li><li>The sequential predictions can be computationally expensive.</li><li>The learned representations may suboptimal for anomaly detection as its underlying objective is for sequential predictions rather than anomaly detection.</li></ul></li><li><p>Challenges</p><ul><li>CH1&amp;CH2</li><li>CH5</li></ul></li></ul><h4 id="Self-supervised-classification">Self-supervised classification</h4><ul><li><p>Learns representations of normality by building self-supervised classification models and identifies instances that are inconsistent to the classification models as anomalies.</p></li><li><p>Evaluate the normality of data instances by their consistency to a set of predictive models, with each model learning to predict one feature based on the rest of the other feature.</p></li><li><p>Focuses on image data and builds the predictive models by using feature transformation-based augmented data.</p></li><li><p>Assumptions:  Normal instances are more consistent to self-supervised classifiers than anomalies.</p></li><li><p>Methods</p><ul><li><a href="https://git.io/JfZRW">GT</a>: Like horizontal flipping, translations, and rotations. <strong>The classification scores of each test instance w.r.t. different $𝑇_𝑗$ are then aggregated to compute the anomaly score.</strong></li><li><a href="https://git.io/Jf4pl">$E^3$outlier</a> : Training data contains normal instances only. <a href="https://proceedings.neurips.cc/paper/2019/file/6c4bb406b3e7cd5447f7a76fd7008806-Paper.pdf">UOD</a><ul><li><em>The gradient magnitude induced by normal instances is normally substantially larger than outliers during the training of such self-supervised multiclass classification models;</em></li><li><em>The network updating direction is also biased towards normal instances.</em></li></ul></li></ul><p><strong>Normal instances often have stronger agreement with the classification model than anomalies.</strong></p><p>Negative entropy-based anomaly scores perform generally better than average prediction probability and maximum prediction probability.</p></li><li><p>Advantages</p><ul><li>They work well in both the unsupervised and semi-supervised settings.</li><li><em>Anomaly scoring is grounded by some intrinsic properties of gradient magnitude and its updating.</em></li></ul></li><li><p>Disadvantages</p><ul><li>The feature transformation operations are often data-dependent. And the operations (rotation) only work on images.</li><li>The consistency-scores are derived upon the classification scores rather than an integrated module in the optimization, and thus they may be suboptimal.</li></ul></li><li><p>Challenges</p><ul><li>CH1 &amp; CH2</li><li>CH4</li></ul></li></ul><h3 id="Anomaly-measure-dependent-feature-learning">Anomaly measure-dependent feature learning</h3><p>Learning feature representations that are specifically optimized for one particular existing measure.  They  incorporates an existing anomaly measure $f$ into the feature learning objective function to optimize the feature representations specifically for $f$.</p><h4 id="Distance-based-measure">Distance-based measure</h4><ul><li><p>Aims to learn feature representations that are specifically optimized for a specific type of distance-based anomaly measures.</p></li><li><p>Like DB outliers, $k$-nearest neighbor distance, average $k$-nearest neighbor distance, relative distance and random nearest neighbor distance. But they fail to work effectively in high dimensional data.</p></li><li><p>Assumption : anomalies are distributed far from their closet neighbors while normal instances are located in dense neighbors.</p></li><li><p>Methods</p><ul><li><a href="https://git.io/JfZRg">REPEN</a>: Random neighbor distance-based: The representations are optimized so that the nearest neighbor distance of pseudo-labeled anomalies in random subsamples is substantially larger than that of pseudo-labeled normal instances. The pseudo labels are generated by some off-the-shelf anomaly detectors. The loss function is built upon the hinge loss.</li><li><a href="https://git.io/RDP">RDP</a>: The other uses  the distance between optimized representations and randomly projected representations of the same instances to guide the representation learning. Solving <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602202900471.png" alt="image-20210602202900471" style="zoom:50%;" /> is equivalent to have a knowledge distillation from a random NNs and helps learn the frequency of different underlying patterns in the data.</li><li>At the evaluation stage, the function of $f$ is used to compute the anomaly score.</li></ul></li><li><p>Advantages</p><ul><li>The distance-based anomalies are straightforward and well defined with rich theoretical supports in the literature.</li><li>They work in low-dimensional representation spaces and can effectively deal with high-dimensional data that traditional distance-based anomaly measures fail.</li><li>They are able to learn representations specifically tailored for themselves.</li></ul></li><li><p>Disadvantages</p><ul><li>The extensive computation involved in most of distance-based anomaly measures</li><li>Their capabilities may be limited by the inherent weaknesses of the distance-based anomaly measures.</li></ul></li><li><p>Challenges</p><ul><li>CH1&amp;CH2</li><li>CH3,CH4</li></ul></li></ul><h4 id="One-class-classification-measure">One-class classification measure</h4><p>Aims to learn feature representations customized to subsequent one-class classification-based anomaly detection. Learn a description of a set of data instances to detect whether new instances conform to the training data or not.</p><ul><li><p>One way is to learn representations that are specifically optimized for these traditional one-class classification models, like one-class SVM.</p></li><li><p>Assumption: all normal instances come from a single class and can be summarized by a compact model, to which anomalies do not conform.</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfGgl">AE-1SVM</a>, <a href="https://git.io/JfGgZ">OC_NN</a>: Deep one-class SVM, learn the one-class hyperplane from the neural network-enabled low-dimensional representation space rather than the original input space.</p><p>The formula is <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602205007059.png" alt="image-20210602205007059" style="zoom:33%;" />   Any instances that have $𝑟 − \mathrm{w}^\top \mathrm{z}_𝑖 $ can be reported as anomalies. It has two benefits:</p><ul><li>It can leverage deep networks to learn more expressive features for downstream anomaly detection</li><li>It can also help remove the computational expensive pairwise distance computation in the kernel functions.</li><li>Based on one-class SVM, one may use a random mapping to map latent representation $\mathrm{z}$ to Fourier features since many kernel functions can be approximated by random Fourier features.</li></ul></li><li><p><a href="https://git.io/JfZRR">Deep SVDD</a>, <a href="https://git.io/JfOkr">Deep SAD</a>: Another way is deep models for SVDD, which aims at Learning a minimum hyperplane characterized by a center $\mathrm{c}$ and a radius $r$ so that the sphere contains all training data instances.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210602210101160.png" alt="image-20210602210101160" style="zoom:67%;" /><p>It’s shown that $\mathrm{c}$ as trainable parameters can lead to trivial solutions. $\mathrm{c}$ can be fixed as the mean of the feature representations yield by performing a single initial forward pass. The key idea is to minimize the distance of labeled normal instances to the center while at the same time maximizing the distance of known anomalies to the center.</p></li></ul></li><li><p>Advantages</p><ul><li>The one-class classification-based anomalies are well studied in the literature and provides a strong foundation of deep one-class classification-based methods.</li><li>The representation learning and one-class classification models can be unified to learn tailored and more optimal representations.</li><li>They free the users from manually choosing suitable kernel functions in traditional one-class model.</li></ul></li><li><p>Disadvantages</p><ul><li>The one-class models may work ineffectively in datasets with complex distributions within the normal class.</li><li>The detection performance is dependent on the one-class classification-based anomaly measure.</li></ul></li><li><p>Challenges : CH1&amp;CH2， CH3</p></li></ul><h4 id="Clustering-based-measure">Clustering-based measure</h4><p>Aims at learning representations so that anomalies are clearly deviated from the clusters in the newly learned representation space.</p><ul><li><p>Methods use like cluster size, distance to cluster centers, distance between cluster centers, and cluster membership to define clusters.</p></li><li><p>Assumptions: Normal instances have stronger adherence to clusters than anomalies.</p></li><li><p>Many methods are explored based on the motivation that the performance of clustering methods is highly dependent on the input data.</p></li><li><p>The deep clustering methods typically consist of two modules: performing clustering in the forward pass and learning representations using the cluster assignment as pseudo class labels in the backward pass.</p><ul><li><p>The clustering loss can be initialized with a kmeans loss, a spectral clustering loss, an agglomerative loss or a GMM loss.</p></li><li><p>The auxiliary loss can be an autoencoder-based reconstruction loss to learn robust and/or local structure preserved representations.</p></li><li><p>The cluster assignments in the resulting function is used to compute anomaly scores.</p></li><li><p>The aforementioned deep clustering methods are focused on learning optimal clustering results, but  the learned representations may not be able to well capture the abnormality of anomalies. <a href="https://www.researchgate.net/publication/330625995_A_Unified_Unsupervised_Gaussian_Mixture_Variational_Autoencoder_for_High_Dimensional_Outlier_Detection">Papers</a></p><p><a href="https://git.io/JfZR0">DAGMM</a>:</p><ul><li>The cluster loss is GMM loss and the auxiliary loss is autoencoder-based reconstruction loss.</li><li>The auxiliary loss is an an autoencoder-based reconstruction loss, but to learn deviated representations of anomalies.</li></ul></li></ul></li><li><p>Advantages</p><ul><li>A number of deep clustering methods and theories can be utilized to support the effectiveness and theoretical foundation of anomaly detection.</li><li>Learn specifically optimized representations that help spot the anomalies easier than on the original data, especially when dealing with intricate data sets.</li></ul></li><li><p>Disadvantages</p><ul><li>The performance of anomaly detection is heavily dependent on the clustering results.</li><li>The clustering process may be biased by contaminated anomalies in the training data, which in turn leads to less effective representations.</li></ul></li><li><p>Challenges: CH1&amp;CH2， CH4</p></li></ul><h2 id="End-to-end-anomaly-score-learning">End-to-end anomaly score learning</h2><p>Aims at learning scalar anomaly scores in an end-to-end fashion. It has a NN that directly learns the anomaly scores. Methods here won’t be limited by the inherent disadvantages of the incorporated anomaly measures. There are two design directions: one focuses on how to synthesize existing anomaly measures and neural network models, while another focuses on devising novel loss functions for direct anomaly score learning.</p><h3 id="Ranking-models">Ranking models</h3><p>Aims to directly learn a ranking model, such that data instances can be sorted based on an observable ordinal variable associated with the absolute/relative ordering relation of the abnormality.</p><ul><li><p>Assumptions: There exists an observable ordinal variable that captures some data abnormality.</p></li><li><p>Methods</p><ul><li><p>One line is to devise ordinal regression -based loss functions to drive the anomaly scoring neural network.</p><ul><li>Two-class ordinal regression.<ul><li>The end-to-end anomaly scoring network takes $\mathcal{A}$ and $\mathcal{N}$ as inputs and learns to optimize the anomaly scores such that the data inputs of similar behaviors as those in $\mathcal{A(N)}$ receive large (small) scores as close $𝑐_1 (𝑐_2)$ as possible, resulting in larger anomaly scores assigned to anomalous frames than normal frames.</li></ul></li></ul></li><li><p>Weakly-supervision</p><ul><li><p><a href="https://git.io/JfZRz">MIL</a>: the model is optimized to learn larger anomaly scores for the pairs of two anomalies than the pairs with one anomaly or none.</p><p>MIL ranking model, directly learn the anomaly score for each video segment. Its key objective is to guarantee that the maximum anomaly score for the segments in a video that contains anomalies somewhere is greater than the counterparts in a normal video.<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605124035832.png" alt="image-20210605124035832"></p><ul><li><font color='red'>Why not $\min$ at the last term in the 1st term?</font></li><li>The first term is to guarantee the relative anomaly score order, i.e., the anomaly score of the most abnormal video segment in the positive instance bag is greater than that in the negative instance bag. The last two terms are extra optimization constraints, in which the former enforces score smoothness between consecutive video segments while the latter enforces anomaly sparsity, i.e., each video contains only a few abnormal segments.</li></ul></li></ul></li></ul></li><li><p>Advantages：</p><ul><li>The anomaly scores can be optimized directly with adapted loss functions.</li><li>They are generally free from the definitions of anomalies by imposing a weak assumption of the ordinal order between anomaly and normal instances.</li><li>This approach may build upon well-established ranking techniques and theories from areas like learning to rank.</li></ul></li><li><p>Disadvantages</p><ul><li>At least some form of labeled anomalies are required.</li><li>Methods  may not be able to generalize to unseen anomalies cause they are designed to detect labeled anomalies.</li></ul></li><li><p>Challenges:</p><ul><li>CH1&amp;CH2;</li><li>CH3</li><li>CH6</li><li>CH4</li></ul></li></ul><h3 id="Prior-driven-models">Prior-driven models</h3><p>Use a prior distribution to encode and drive the anomaly score learning. The prior may be imposed on either the internal module or the learning output of the score learning function.</p><ul><li><p>Assumptions:  The imposed prior captures the underlying (ab)normality of the dataset.</p></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfZRw">DevNet</a>: enforce a prior on the anomaly scores. It uses a Gaussian prior to encode the anomaly scores and enable the direct optimization very well.  The deviation loss is built upon contrastive loss.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605131355985.png" alt="image-20210605131355985"></p><ul><li>Driven by the deviation loss, it will push the anomaly scores of normal instances as close as possible to 𝜇 while guaranteeing at least 𝑚 standard deviations between 𝜇 and the anomaly scores of anomalies.</li><li>The loss is equivalent to enforcing a statistically significant deviation of the anomaly score of the anomalies from that of normal instances in the upper tail.</li><li>It’s also interpretable.</li></ul></li></ul></li><li><p>Advantages</p><ul><li>The anomaly scores can be directly optimized w.r.t. a given prior.</li><li>It provides a flexible framework for incorporating different prior distributions into the anomaly score learning.</li><li>The prior can also result in more interpretable anomaly scores than the other methods.</li></ul></li><li><p>Disadvantages</p><ul><li>It’s difficult to design a universally effective prior for all anomaly detection application.</li><li>The efficiency of model depends on how the picked prior fits the underlying distribution.</li></ul></li><li><p>Challenges</p><ul><li>CH1&amp;CH2</li><li>CH1&amp;CH3</li><li>CH4</li></ul></li></ul><h3 id="Softmax-Likelihood-models">Softmax Likelihood models</h3><p>Learning anomaly scores by maximizing the likelihood of events in the training data. Normal instances are presumed to be high-probability events whereas anomalies are prone to be low-probability events. Tools like NCE are used.</p><ul><li>Assumptions: Anomalies and normal instances are respectively low- and high-probability events.</li><li>Methods<ul><li>Use log negative likelihood. Learning the likelihood function 𝑝 is equivalent to directly optimizing the anomaly scoring function.</li><li>But the original likelihood is computed costly, NCE is used to alleviate. For each instance $\mathrm{x}$, $k$ noise samples $\mathrm{x}_{1, \cdots,k}\sim Q$ are generated from some synthetic known ‘noise’ distribution $Q$.</li></ul></li><li>Advantages<ul><li>Different types of interactions can be incorporated into the anomaly score learning process.</li><li>The anomaly scores are faithfully optimized w.r.t. the specific abnormal interactions we aim to capture.</li></ul></li><li>Disadvantages<ul><li>The computation of the interactions can be very costly when the number of features/elements in each data instance is large.</li><li>The anomaly score learning is heavily dependent on the quality of the generation of negative samples.</li></ul></li><li>Challenges<ul><li>CH2&amp;CH5</li><li>CH1</li></ul></li></ul><h3 id="End-to-end-one-class-classification">End-to-end one-class classification</h3><p>Train a one-class classifier that learns to discriminate whether a given instance is normal or not. It does not rely on any existing one-class classification measures. Methods like adversarially learned one-class classification are used.  It  learns a one-class discriminator of the normal instances so that it well discriminates those instances from adversarially generated pseudo anomalies. The goal is to learn a discriminator and this discriminatory will be directly used as anomaly scorer.</p><ul><li><p>Assumptions :</p><ul><li>Data instances that are approximated to anomalies can be effectively synthesized.</li><li>All normal instances can be summarized by a discriminative one-class model.</li></ul></li><li><p>Methods</p><ul><li><p><a href="https://git.io/JfZRw">ALOCC</a>:</p><ul><li>The key idea is to train two deep networks, with one network trained as the one-class model to separate normal instances from anomalies while the other network trained to enhance the normal instances and generate distorted outlier. The generator is based on a denoising AE.</li><li>The outliers are randomly sampled from some classes other than the classes where the normal instances come from.</li><li>But this method may be unavailable in many domains cause the reference outliers are beyond the given data.</li></ul></li><li><p><a href="https://git.io/JfYGb">OCAN</a>, <a href="https://git.io/Jf4pR">FenceGAN</a>: generate fringe data instances based on the given training data and use them as negative reference instances to enable the training of the one-class discriminator.</p><ul><li><p>OCAN: The generator is trained to generate data instances that are complementary to the training data.</p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605190140300.png" alt="image-20210605190140300" style="zoom:67%;" /><p>The 1st two terms are devised to generate low-density instances in the original feature space, and the last term is to help better generate data instances within the original data space.</p><p>The objective of the discriminatory is enhanced with an extrovert conditional entropy loss to enable the detection with high confidence.</p></li><li><p><a href="https://git.io/Jf4pR">FenceGAN</a>: generate data instances tightly lying at the boundary of the distribution of the training data, which is achieved by introducing two loss functions into the generator that enforce the generated instances to be evenly distributed along a sphere boundary of the training data.</p></li><li><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605191312110.png" alt="image-20210605191312110"></p><p>The first term is called encirclement loss that enforces the generated instances to have the same discrimination score, ideally resulting in instances tightly enclosing the training data. The second term is called dispersion loss that enforces the generated instances to evenly cover the whole boundary.</p></li></ul></li><li><p><a href="https://git.io/Jf4p0">OCGAN</a>: uniformly distributed instances can be generated to enforce the normal instances to be distributed uniformly across the latent space.</p></li><li><p>An ensemble of generator is used with each generator synthesizing boundary instances for one specific cluster of normal instances.</p></li></ul></li><li><p>Advantages</p><ul><li>Its anomaly classification model is adversarially optimized in an end-to-end fashion.</li><li>It can be developed and supported by the affluent techniques and theories of adversarial learning and one-class classification.</li></ul></li><li><p>Disadvantages</p><ul><li>It is difficult to guarantee that the generated reference instances well resemble the unknown anomalies.</li><li>The instability of GANs may lead to generated instances with diverse quality and consequently unstable anomaly classification performance.</li><li>Its applications are limited to semi-supervised anomaly detection scenarios.</li></ul></li><li><p>Challenges</p><ul><li>CH1 &amp; CH2</li></ul></li></ul><h2 id="Algorithms-and-datasets">Algorithms and datasets</h2><h3 id="Representative-algorithms">Representative algorithms</h3><ul><li><p>Most methods operate in an unsupervised or semi-supervised mode.</p></li><li><p>Deep learning tricks like data augmentation, dropout and pre-training are under-explored.</p></li><li><p>The network architecture used is not that deep.</p></li><li><p>(leaky) ReLU is the most popular one</p></li><li><p>diverse backbone networks can be used to handle different types of input data.</p><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605192143473.png" alt="image-20210605192143473"></p></li></ul><h3 id="Datasets-with-Real-anomalies">Datasets with Real anomalies</h3><ul><li><a href="https://github.com/GuansongPang/anomaly-detection-datasets">The datasets</a></li><li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210605192324925.png" alt="image-20210605192324925"></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li><p>Contributions</p><ul><li>Problem nature and challenges:  some unique problem complexities underlying anomaly detection and the resulting largely unsolved challenges.</li><li>Categorization and formulation:  three principled frameworks: deep learning for generic feature extraction, learning representations of normality, and end-to-end anomaly score learning</li><li>Comprehensive literature review</li><li>Future opportunities</li><li>Source codes and datasets</li></ul></li><li><p>Exploring anomaly-supervisory signals</p><ul><li>The key issue for these formulations is that their objective functions are generic.</li><li>Explore new sources of anomaly-supervisory signals that lie beyond the widely-used formulations such as data reconstruction and GANs, and have weak assumptions on the anomaly distribution.</li><li>Develop domain-driven anomaly detection by leveraging domain knowledge.</li></ul></li><li><p>Deep weakly-supervised anomaly detection</p><ul><li>Leveraging deep neural networks to learn anomaly-informed detection models with some weakly-supervised anomaly signals.<ul><li>Utilize a small number of accurate labeled anomaly examples to enhance detection models.</li></ul></li><li>Unknown anomaly detection: aim to build detection models that are generated from the limited labeled anomalies to unknown anomalies.</li><li>Data-efficient anomaly detection or few-shot anomaly detection: given only limited anomaly examples.</li></ul></li><li><p>Large scale normality leaning</p><ul><li>Since  it is difficult to obtain sufficient labeled data</li><li>The goal is to first learn transferable pre-trained representation models from large-scale unlabeled data in an unsupervised/self-supervised mode, and then fine-tune detection models in a semi-supervised mode.</li><li>May need to be domain/application-specific.</li></ul></li><li><p>Deep detection of complex anomalies</p><ul><li>conditional/group anomalies</li><li>Multimodal anomaly detection</li></ul></li><li><p>Interpretable and actionable deep anomaly detection</p><ul><li>The abnormal feature selection methods but may render the explanation less useful</li></ul></li><li><p>Novel applications and settings</p><ul><li>Out-of-distribution detection: closely related area. It is generally assumed that fine-grained normal class labels are available during training.</li><li>Curiosity learning: learning a bonus reward function in reinforcement learning with sparse rewards.  Augmenting the environment with a bonus reward in addition to the original sparse rewards from the environment.</li><li>non-IID anomaly detection: e.g., the abnormality of different instances/features is interdependent and/or heterogeneous.  May be confused with anomaly instances.</li><li>detection of adversarial examples, anti-spoofing in biometric systems, and early detection of rare catastrophic events.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper I: &lt;a href=&quot;https://arxiv.org/pdf/2007.02500.pdf&quot;&gt;Deep Learning for Anomaly Detection: A Review&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
      <category term="anomaly" scheme="http://yoursite.com/tags/anomaly/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Fall Surveys</title>
    <link href="http://yoursite.com/posts/notes/2021-02-07-notes-paper-fall-survey.html"/>
    <id>http://yoursite.com/posts/notes/2021-02-07-notes-paper-fall-survey.html</id>
    <published>2021-02-07T13:46:39.000Z</published>
    <updated>2021-03-05T03:34:36.201Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Fall detection<ul><li>Paper I: <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full">Elderly Fall Detection Systems: A Literature Survey, 2020</a></li><li>Paper II: <a href="https://dl.acm.org/doi/10.1145/2769493.2769540">A survey on vision-based fall detection, 2015</a></li><li>Paper III: <a href="https://www.sciencedirect.com/science/article/pii/S2352864815000681">3D depth image analysis for indoor fall detection of elderly people, 2015</a></li><li>Paper IV: <a href="https://ieeexplore.ieee.org/document/9186685">Deep learning based systems developed for fall detection: a review, 2020</a></li><li>Paper V: <a href="https://ieeexplore.ieee.org/abstract/document/8869737">Implementation of Fall Detection System Based on 3D Skeleton for Deep Learning Technique, 2019</a></li><li>Paper VI: <a href="https://ieeexplore.ieee.org/document/8369778">Human fall-down event detection based on 2D skeletons and deep learning approach</a></li><li></li></ul></li><li>Human activity recognition<ul><li>Paper VII: <a href="https://link.springer.com/content/pdf/10.1007/s11042-020-09004-3.pdf">Vision-based human activity recognition: a survey, 2020</a></li><li>3D and depth data<ul><li>Paper VIII: <a href="https://www.sciencedirect.com/science/article/pii/S0167865514001299">Human activity recognition from 3d data: a review, 2014</a></li><li><a href="https://arxiv.org/abs/1711.08362">RGB-D-based Human Motion Recognition with Deep Learning: A Survey, 2017</a></li></ul></li><li>3D skeleton-based human representations<ul><li><a href="https://arxiv.org/abs/1601.01006">Space-Time Representation of People Based on 3D Skeletal Data: A Review, 2016</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S0031320315004392">3D skeleton-based human action classification: A survey,2015</a></li><li><a href="https://d1wqtxts1xzle7.cloudfront.net/47833181/Crowd_analysis_A_survey20160806-21965-155miur.pdf?1470471444=&amp;response-content-disposition=inline%3B+filename%3DCrowd_analysis_a_survey.pdf&amp;Expires=1614620280&amp;Signature=gO5XOCbzmA4O~6zc1hli7UqnkZmethCye13xIqVW58A~NTeZYwbbxSs3vZsO4E9~73WX7gYBapzo3quA7UV5jFDRfaDQ6v0ds8dA3BDhB5ys2PlxRFWxEmPsfGAPSp7G6inWLRrfw89L2xXRnX-KM1caNEnqcsg18OD9zf8LU3aovB4hXyB0kvMtc2T2FXdg1HdlQbjqVAlZmrcSl2Y98j1Gr4it23BLSbmUmwZpYAtVA4WqUwFihyqQco5XHX3dhJn7eUdKTOc6QdqQ2KumIhXBwnHSR8TOF9StECcxoUlOf9fcrEgRH4tDauMCsVqgCWJkANhI4~lp0nJEPP21fQ__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">Crowd analysis: a survey, 2008</a></li><li><a href="https://ieeexplore.ieee.org/document/6909476">Human action recognition by representing 3d skeletons points in a lie group, CVPR 2014</a></li></ul></li><li>knowledge-based HAR activity recognition<ul><li><a href="https://www.sciencedirect.com/science/article/pii/S0957417416302913">A survey on using domain and contextual knowledge for human activity recognition in video streams.,2016</a></li></ul></li><li>Abnormal HAR<ul><li><a href="https://www.sciencedirect.com/science/article/pii/S0952197618301775">A review of state-of-the-art techniques for abnormal human activity recognition, 2018</a></li><li></li></ul></li></ul></li></ul><a id="more"></a><h1 id="fall-detection">Fall detection</h1><h2 id="paper-i-2020">Paper I: 2020</h2><p><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full">Elderly Fall Detection Systems: A Literature Survey, 2020</a></p><h3 id="why">Why?</h3><h4 id="types-of-falls"><strong>Types of falls</strong></h4><ul><li>Types<ul><li>forward, lateral and backward in <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B35">El-Bendary et al.</a></li><li>forward, backward, left-side, right-side, blinded-forward and blinded-backward in <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B95">Putra et al.</a></li><li>fall lateral left lie on the floor, fall lateral left and sit up from floor, fall lateral right and lie on the floor, fall lateral and left sit up from the floor, fall forward and lie on the floor, and fall backward and lie on the floor in <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B23">Chen et al</a></li></ul></li><li>Elderly people may suffer from longer duration of falls, because of motion with low speed in the activity of daily living</li><li>The characteristics of different types of falls are not taken into consideration in most of the work on fall detection surveyed. (like age, gender etc.)</li></ul><h4 id="previous-work-and-falls">Previous work and falls</h4><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>Paper</th><th>contents</th><th>summary</th></tr></thead><tbody><tr class="odd"><td><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B18">Chaudhuri et al. (2014)</a></td><td>fall detection devices for people of different ages (excluding children) from several perspectives, including background, objectives, data sources, eligibility criteria, and intervention methods.</td><td>most of the studies were based on synthetic data</td></tr><tr class="even"><td><strong><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B131">Zhang et al. (2015)</a></strong></td><td>vision-based fall detection systems and their related benchmark data sets. Methods are divided into four categories, namely <strong>individual single RGB cameras, infrared cameras, depth cameras, and 3D-based methods using camera arrays</strong>. Methods are also divided to rely on the activity/inactivity of the subjects, shape (width-to-height ratio), and motion.</td><td>Non-vision sensors are not included</td></tr><tr class="odd"><td><strong><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B16">Cai et al. (2017)</a></strong></td><td><strong>depth cameras</strong>, reviewed the benchmark data sets acquired by Microsoft Kinect and similar cameras.</td><td>helpful for looking for benchmark data sets</td></tr><tr class="even"><td><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B21">Chen et al. (2017a)</a></td><td>vision- and non-vision-based systems</td><td>fusion of depth cameras and inertial sensor resulted in a system that is more robust</td></tr><tr class="odd"><td><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B50">Igual et al. (2013)</a></td><td>low-cost cameras and accelerometers embedded in smartphones may offer the most sensible technological choice for the investigation of fall detection. They also reported three main challenges: (i) real-world deployment performance, (ii) usability, and (iii) acceptance.</td><td>another option of sensors</td></tr></tbody></table><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210207202446658.png" alt="image-20210207202446658" style="zoom:40%;" /></p><h3 id="goals">Goals</h3><p>provide a literature survey of work conducted on elderly fall detection using sensor networks and IoT in terms of data acquisition, data analysis, data transport and storage, sensor networks and Internet of Things (IoT) platforms, as well as security and privacy.</p><h3 id="how">How?</h3><h4 id="the-components-of-system">The components of system</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210207204832364.png" alt="image-20210207204832364" style="zoom:60%;" /></p><h4 id="sensors">Sensors</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210207230722406.png" alt="image-20210207230722406" style="zoom:50%;" /></p><ul><li>Individual wearable sensors<ul><li>possible choices: accelerometers, gyroscopes, glucometers, pressure sensors, ECG (Electrocardiography), EEG (Electroencephalography), or EOG (Electromyography)</li><li><a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B14">Bourke et al. (2007)</a> found that accelerometers are regarded as the most popular sensors for fall detection. Smart phones are more practical compared with wearable sensors.</li></ul></li><li>Individual vision sensors<ul><li>Possible choices: infrared, RGB, RGB-D etc.</li><li>main challenge of vision-based detection is the potential violation of privacy</li><li>Almost use synthetic falling dataset. <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B13">Boulard et al. (2014)</a> has actual fall data and the other by <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B108">Stone and Skubic (2015)</a> has mixed data.</li></ul></li><li>Individual ambient sensors<ul><li>Possible choices: active infrared RFID,pressure, smart tiles, magnetic switches, doppler radar, ultrasonic and microphone.</li></ul></li><li>Subjects<ul><li>simulated data from OpenSim contributed to an increase in performance to the resulting models</li><li>transfer learning which adapt to subjects who were not represented in the training data</li><li>reinforcement learning for different subjects</li></ul></li></ul><h4 id="sensor-fusion">Sensor fusion</h4><ul><li><p>Types</p><p><img src="https://www.frontiersin.org/files/Articles/520978/frobt-07-00071-HTML/image_m/frobt-07-00071-g007.jpg" style="zoom:50%;" /></p></li><li><p>feature fusion is the most popular approach, followed by decision fusion.</p></li></ul><h3 id="trends-challenges">Trends &amp; challenges</h3><h4 id="trends">Trends</h4><ul><li>Sensor fusion</li><li>ML,DL and RL</li><li>With 5G wireless networks</li><li>Data augmentation<ul><li>Personalized data: the historical medical and behavioral data of Individuals (<a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B35">El-Bendary et al. (2013)</a> and <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B84">Namba and Yamada (2018b)</a>)</li><li>Use skeletal models for simulation: <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B79">Mastorakis et al. (2007</a>, <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B78">2018)</a>, applied the skeletal model simulated in Opensim,</li></ul></li><li>Fog computing : Intel RealSense includes a 28 nanometer processor</li></ul><h4 id="challenges">Challenges</h4><ul><li>The rarity of data of real falls : only <a href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full#B70">Liu et al. (2014)</a> used a data set with nine real falls along with 445 simulated ones.</li><li>Detection in real-time</li><li>Security and privacy</li><li>Platform of sensor fusion</li><li>Limitation of locations: indoor and outdoor environments</li><li>Scalability and flexibility</li></ul><h2 id="paper-ii-vision">Paper II: vision</h2><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1077.2575&amp;rep=rep1&amp;type=pdf">A survey on vision-based fall detection, 2015</a></p><h3 id="goal">Goal</h3><ol type="1"><li><p>focus on recent vision-based fall detection techniques, including depth-based methods;</p></li><li><p>discuss the publicly available fall datasets.</p></li></ol><h3 id="fall-datasets">Fall datasets</h3><p>[1] Definition and performance evaluation of a robust svm based fall detection solution.</p><p>[2] Multiple cameras fall dataset</p><p>The EDF dataset mentioned in this paper, I cannot find the original paper except for <a href="http://vlm1.uta.edu/~athitsos/publications/zhang_isvc2014.pdf">this one</a> , in which EDF and OCCU are both created. But if it's the paper I list, the description of EDF in this paper is not totally correct.</p><table><colgroup><col style="width: 11%" /><col style="width: 15%" /><col style="width: 10%" /><col style="width: 9%" /><col style="width: 22%" /><col style="width: 30%" /></colgroup><thead><tr class="header"><th>-</th><th>SDUFall</th><th>EDF?</th><th>OCCU</th><th>[1]</th><th>[2]</th></tr></thead><tbody><tr class="odd"><td>camera type</td><td>1 kinetic</td><td>2 kinetics</td><td>2 kinetics</td><td>1 RGB camera</td><td>8 calibrated RGB camera</td></tr><tr class="even"><td>camera viewpoints</td><td>1</td><td>2</td><td>2</td><td>NaN</td><td>8</td></tr><tr class="odd"><td>fall type</td><td>falls with different directions</td><td>eight fall directions</td><td><strong>occluded falls</strong></td><td>falls with different directions</td><td>forward, backward falls, falls from sitting down and loss of balance</td></tr><tr class="even"><td>#falls</td><td>200</td><td>320</td><td>60</td><td>192</td><td>200</td></tr><tr class="odd"><td>actions in daily life?</td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td><td><span class="math inline">\(\checkmark\)</span></td></tr><tr class="even"><td>#scenarios</td><td>1</td><td>1</td><td>1</td><td>4 (home, coffee room, office, lecture room)</td><td>24</td></tr><tr class="odd"><td>#subjects</td><td>20</td><td>10</td><td>5</td><td></td><td>1</td></tr><tr class="even"><td>resolution</td><td><span class="math inline">\(320\times240\)</span></td><td><span class="math inline">\(320\times240\)</span></td><td></td><td><span class="math inline">\(320\times240\)</span></td><td></td></tr><tr class="odd"><td>frame rate</td><td>30 fps</td><td>25 fps</td><td>30 fps</td><td>25 fps</td><td></td></tr><tr class="even"><td>year</td><td>2014</td><td>2008</td><td>2014</td><td>2012</td><td>2010</td></tr></tbody></table><h3 id="vision-based-fall-detectors">Vision-based fall detectors</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Comment: <span class="keyword">the</span> classification rules <span class="keyword">for</span> <span class="keyword">each</span> section are <span class="keyword">not</span> <span class="keyword">the</span> same.</span><br></pre></td></tr></table></figure><h4 id="using-single-rgb-camera">Using single RGB camera</h4><p>Firstly classify by features, then the tasks of it is mentioned</p><ul><li>Shape-related features : based on width to height aspect ratio of the person.<ul><li>background separation to get silhouette and then use SVM for classification:</li><li>shape deformation from silhouettes and use shape deformation based GMM for classification</li><li>ellipse shape for body modeling and GMM for extracting moving object.</li><li>2 HMMs for classify falls and normal activities</li><li>extracting projection histograms of the segmented body silhouette and then use it as feature vector, complete posture classification by KNN.</li></ul></li><li>Motion pattern<ul><li>human motion analysis (analyzing the energy of the motion active area ) + human silhouette shape variations to detect slip-only and fall events</li><li>applying Integrated Time Motion Image (ITMI) to fall detection. ITMI is the calculated the PCs of typical video clip for representing a motion pattern.</li><li>threshold-based by the last few frames (falling, the magnitude of the fall, the maximum velocity of the fall ) etc.</li><li>extract the 3D head trajectory using a single calibrated camera.</li><li>extract foreground human silhouette via background modeling. Ellipse fitting for human body and analyze silhouette motion by an integrated normalized motion energy image.</li></ul></li><li>Inactivity detection<ul><li>use ceiling-mounted, wide-angle cameras with vertically oriented optical axes to reduce the influences of occlusion.</li><li>use learned models of spatial context in conjunction with a tracker</li><li>STHF descriptor by <a href="https://ieeexplore.ieee.org/document/6916794">Charfi et al</a>.</li></ul></li></ul><h4 id="using-multiple-cameras">Using multiple cameras</h4><p>Classified by tasks</p><ul><li>3D reconstruction<ul><li><strong>volume distribution</strong> along the vertical axis, check <a href="https://scholar.google.ca/scholar?q=Fall+detection+using+body+volume+reconstruction+and+vertical+repartition+analysis&amp;hl=zh-CN&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart">here</a>.</li><li>multiple cameras + a hierarchy of fuzzy logic to detect falls. Use voxel person (linguistic summarization of temporal fuzzy inference curves)</li></ul></li><li>Multiple viewpoint<ul><li>Use LHMM (layered hidden Markov model)</li><li>combining decisions from different camera</li><li>Using the measures of humans heights and occupied areas.</li></ul></li></ul><h4 id="using-depth-cameras">Using depth cameras</h4><ul><li>Using the distance from the top of the person to the floor<ul><li>the distance of human centroid to floor is smaller than threshold and the person does not move for a certain seconds</li><li>Other features like head to floor distance, person area and shape's major length to width</li></ul></li><li>analyzing how a human has moved during the last frames<ul><li>characterize the vertical state of a segmented 3D object for each frame. Then compute a confidence by the features extracted on ground event which are fed into an ensemble of decision trees.</li><li>Bayes network on fall detection, including duration, total head drop, maximum speed, smallest head height and fraction of frames for which the head drops</li><li>calculating the velocity based on the contraction or expansion of the width, height and depth of the 3D bounding box.</li></ul></li><li>human body key joints<ul><li>3D body joints at each frame are extracted by randomized decision tree, and the 3D trajectory of the head joint is used to determine whether the fall action has occurred.</li><li>3D depth for main detection (structure similarity and vertical height of the person), RGB for out-of-the-rage of the depth camera (the width-height ratio of the detected human bounding box is for recognizing different activities).</li><li>an action is represented by a bag of curvature scale space features (BoCSS) of human silhouettes. Or represent an action as Fisher Vector (on CSS). Or check whether the width-height ratio of temporal bounding box is greater than the predefined threshold, if it is, then it's fall. Otherwise check 2D velocity and the 3D centroid information</li></ul></li></ul><h3 id="conclusions">Conclusions</h3><ul><li><p>Methods with cameras</p><table><colgroup><col style="width: 20%" /><col style="width: 38%" /><col style="width: 40%" /></colgroup><thead><tr class="header"><th>method</th><th>merits</th><th>demerits</th></tr></thead><tbody><tr class="odd"><td>single RGB camera</td><td>* no requirement of camera calibration<br />* inexpensive</td><td>* case specific and viewpoint-dependent<br />* occlusion problem</td></tr><tr class="even"><td>calibrated multi-camera systems</td><td>* viewpoint independent <br />* No occlusion</td><td>* careful and time-consuming calibration<br />* repeatedly calibration if any of them moved</td></tr><tr class="odd"><td>depth camera</td><td>* viewpoint independent <br />* No occlusion</td><td>* price</td></tr></tbody></table></li><li><p>Tips for benchmark</p><ul><li>both falls and activities of daily life (ADL) are requried</li><li>include various falls</li><li>include different camera viewpoints: to verify whether the proposed algorithms are viewpoint-independent</li><li>real falls: consider the distribution of volunteers like gender, ages etc.</li></ul></li><li><p>Tips for fall detection</p><ul><li>combine with Other type of data like sound,</li></ul></li></ul><h2 id="paper-iii-3d-depth">Paper III: 3D depth</h2><p><a href="https://www.sciencedirect.com/science/article/pii/S2352864815000681">3D depth image analysis for indoor fall detection of elderly people, 2015</a></p><h3 id="previous">Previous</h3><ul><li><p>Existing fall detection methods</p><ul><li><p>wearable sensor based</p><p>small , cheap, wearable, but high drift</p></li><li><p>ambient sensor-based</p><p>low detection precision of these sensors, and the precision is distance-relied and thus more sensors are required if with a bigger room</p></li><li><p>computer-vision based</p><p>more robust and less intrusive.</p><ul><li>methods with a single RGB camera</li><li>3d-based methods Using multiple cameras</li><li>depth camera</li></ul></li></ul></li><li><p>Usually <strong>shape relative features of human motion analysis and inactivity detection are used as clues for detecting falls.</strong></p></li><li><p>moment functions are powerful while describing the human shape</p><ul><li>On grey or color images an ellipse is more accurate than a bounding box</li><li>depth camera can offer 3D data so as to work on shape analysis</li></ul></li><li><p>Threshold based fall detection</p><ul><li>Features that can be used: head-ground distance gap and head-shoulder distance gap, the orientation of the ellipse of the human object and the motion of the human object</li><li>methods: DT, NNs, SVM, Bayesian Belief Network</li></ul></li></ul><h3 id="goal-1">Goal</h3><p>deals with the fall detection of the single elderly person in the home environments</p><h3 id="how-1">How?</h3><h4 id="idea">Idea</h4><ul><li>extracting silhouette of moving Individual by extracting background frame</li><li>with the horizontal and vertical projection histogram statistics the depth images are converted to disparity map</li><li>coefficients of the human body are calculated to determine the direction of individual</li><li>threshold based: centroids of the human body to the floor plane and the angle between the human body and the floor plane</li></ul><h4 id="method">Method</h4><p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S2352864815000681-gr1.jpg" style="zoom:67%;" /></p><ul><li><p>Data preprocessing</p><ul><li><p>depth data: only adopt kinetic data in trusted range (1.2-3.8m)</p></li><li><p>extracting silhouette of the moving individual: subtracting the median-filtered background depth frame</p></li><li><p>floor plane: disparity map. <strong>Floor plane will be a noticeable slant and thick straight line in the disparity map</strong></p><p>$= $, for kinetic, <span class="math inline">\(f=580\)</span> pixels, <span class="math inline">\(b=7.5\)</span> cm, and <span class="math inline">\(d\)</span> is the distance between one point in the space TPM the center of the kinetic.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210215180234037.png" alt="image-20210215180234037" style="zoom:80%;" /></p><p>The floor equation then is <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210215202222673.png" alt="image-20210215202222673" style="zoom:70%;" /></p></li><li><p>The orientation of human body: after estimating the ellipse of human body on the image plane, then estimate orientation</p></li><li><p>calculate the distance from the centroid of the human body to the floor plane and the angles between the body and the floor plane.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210215215635773.png" alt="image-20210215215635773" style="zoom:70%;" /></p></li></ul></li></ul><h3 id="experiments">Experiments</h3><ul><li>The dataset is based on their self-captured dataset.-- <strong>Cannot find the sharing link of dataset</strong></li><li>Only two clips' trajectories are listed, no accuracy data shown</li></ul><h3 id="conclusions-1">Conclusions</h3><ul><li>the method may be taken as a candidate</li><li>the result of experiments is not convince.</li></ul><h2 id="paper-iv-dl">Paper IV: DL</h2><p><a href="https://ieeexplore.ieee.org/document/9186685">Deep learning based systems developed for fall detection: a review, 2020</a></p><h3 id="previous-1">Previous</h3><ul><li>the reviewed fall detection systems have some general steps, combined with sensing, data processing, fall event recognition and emergency alert system to rescue the victim.</li><li></li></ul><h3 id="goal-2">Goal</h3><p>presenting a summary and comparison of existing state-of-the-art deep learning based fall detection systems to facilitate future development in this field.</p><p>The categorization focuses on how the different principal methods (CNN, LSTM, and AE) handle the event data captured by sensors.</p><h3 id="how-2">How?</h3><p>Methods used for fall detection</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210219211345800.png" alt="image-20210219211345800" style="zoom:67%;" /></p><h4 id="cnn-based-fall-detection-systems">CNN based fall detection systems</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210221105345137.png" alt="image-20210221105345137" style="zoom:67%;" /></p><h5 id="cnn">CNN</h5><ul><li><a href="http://eprints.bournemouth.ac.uk/29421/1/activity-recognition-indoor.pdf">Adhikari et al</a> : based on videos images from RGB-D camera .<ul><li>used their own dataset</li><li>poor sensitivity when the user was in crawling, bending and sitting positions. The system also works in a selected environment. Developed on a single-person scenario</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8302004">Li et al</a>: extract human shape deformation by CNNs<ul><li>used dataset URFD</li><li>not tested on real dataset</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8732857">Yhdego et al.</a>:<ul><li>convert the data from accelerometer to image by continuous wavelet transform</li><li>use transfer learning for these images</li><li>use URFD dataset</li></ul></li><li><a href="https://dl.acm.org/doi/pdf/10.1145/3136755.3136802">Yu et al</a><ul><li>extract human body silhouette by background subtraction. CNN is for preprocessing the silhouette</li><li>codebook background subtraction</li></ul></li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-93659-8_53">Shen et al.</a><ul><li>cloud</li><li>deep-cut NNs to detect the key points of human body, feed key points into NNs</li></ul></li><li><a href="https://pubmed.ncbi.nlm.nih.gov/30959877/">Santos et al</a><ul><li>IoT, fog computing</li><li>used three open datasets</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8551564">Zhou et al</a> : multi-sensor fusion<ul><li>STFT is for extracting the time frequency (TF) micro-motion features</li><li>Two Alexnet and one SSD (single shot multi-box detector) net are for classifying the TF features</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8662651">He et al.</a>: FD-CNN net<ul><li>collected data is mapped into 3-channel RGB bitmap image</li><li>the image plus SisFall and MobiFall datasets for training FD-CNN</li></ul></li><li><a href="https://ieeexplore.ieee.org/document/8664624">Sadreazami et al</a> : CNN for time-series data (radar)</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-030-20257-6_22">Sortis et al</a> : raw accelerometer data, CUSUM (cumulative sum) algorithm</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8295206">Lu et al</a>: use optical flow images, pretrained 3D CNN on different dataset, transfer learning. Accurate for single-person detection but may not work well on multi-person detection</li><li><a href="https://ieeexplore.ieee.org/document/8813332">Wang et al</a>: tri-axial accelerometer, gyroscope sensor in the smart insole. CNN for improving the accuracy level (it's used directly on the raw sensor data)</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8796672">Zhang and Zhu</a>: CNN that works on raw 3-axis accelerometer data streams.</li><li><a href="https://ieeexplore.ieee.org/document/8787213">Camerio et al</a>: multi-stream model, takes high-level handcrafted feature generators. CNN for optical flow， RGB and human estimated pose. Use URFD and FDD datasets for training</li><li><a href="https://pubmed.ncbi.nlm.nih.gov/32155936/">Casilari et al</a> :CNN for recognizing the pattern from the tri-axial transportable accelerometer. Using multiple dataset for training. For real-life performance, LSTM is used</li><li><a href="https://www.sciencedirect.com/science/article/pii/S0010482519303816">Espinosa et al</a> : Using UP-fall detection multi-modal dataset. <strong>Only vision-based</strong> .</li></ul><h5 id="d-cnn">1D CNN</h5><ul><li><a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/el.2018.6117">Cho and Yoon</a> : SVD on accelerometer data for 1Data CNN. Works for triaxial acceleration data.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8869737">Tsai and Hsu et al.</a>: feature extraction algorithm for converting the depth image to skeleton information . Only seven highlighted feature points are picked from the skeleton joints. Performs well on NTU RGB+D</li></ul><h5 id="d-cnn-1">3D CNN</h5><ul><li><a href="https://ieeexplore.ieee.org/document/8622342">Ranhnemoonfar et al.</a>: Kinect depth camera, Adam optimizer. SDUFall dataset for training.</li><li><a href="https://ieeexplore.ieee.org/document/8779504">Li et al.</a>: pre-impact fall detection. Pretrained model + new samples for fine tuning.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/7946918">Hwang et al.</a>: 3D-CNN for continuous motion data from depth cameras. Using data augmentation. Using TST fall detection dataset</li><li><a href="https://www.researchgate.net/publication/333852390_Human_Fall_Recognition_using_the_Spatiotemporal_3D_CNN">Kasturi et al.</a>: kinetic camera. the data fed into 3DCNN is a staked cube. Tested on UR fall detection dataset</li></ul><h5 id="fof-cnn">FOF CNN</h5><p>FOF: feedback optical flow convolution</p><ul><li><a href="https://ieeexplore.ieee.org/document/8101471">Hsieh and Jeng</a> : IoT, used Feature Feedback Mechanism Scheme (FFMS) and 3D-CNN. KTH dataset.</li></ul><h4 id="lstm-based-fall-detection-systems">LSTM based fall detection systems</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210221123633848.png" alt="image-20210221123631712" style="zoom:67%;" /></p><h5 id="lstm-with-3d-cnn">LSTM with 3D CNN</h5><ul><li><a href="https://ieeexplore.ieee.org/abstract/document/8256202">Lu et al.</a>: trained an extractor only on kinetic data, LSTM+ spatial visual attention. Use Sports-1M dataset, and fall dataset of Multiple Cameras.</li></ul><h5 id="lstm-with-rnn-and-cnn">LSTM with RNN and CNN</h5><ul><li><a href="https://ieeexplore.ieee.org/document/8369778">Lie et al.</a>: CNN for extracting 2D skeletons from RGB camera, LSTM for classifying actions. Has an online version</li><li><a href="https://ieeexplore.ieee.org/document/8615759">Abobakr et al.</a>: Kinect RGB-D camera. Convolutional LSTM + ResNet for visual feature extraction, LSTM for sequence modeling and logistics regression for fall detection. Use URFD public dataset, can work in real-time</li><li><a href="https://iopscience.iop.org/article/10.1088/1742-6596/1267/1/012044/meta">Xu et al.</a>: IoT, acceleration data from a tri-axial accelerometer as input . Outperfoms SVM+CNN model,</li><li><a href="https://journals.sagepub.com/doi/full/10.1177/1550147717703257">Tao and Yun</a>: body posture and human biomechanics equilibrium, fed depth camera data to RNN+LSTM and extracting 3D skeletons. Then by computing center of mass (COM) positions and the region of base support, falls are detected.</li><li><a href="https://www.hindawi.com/journals/wcmc/2019/9507938/">Ajerla <em>et al.</em></a> : used edge devices like a laptop for computing, MetaMotionR sensor (tri-axial accelerometer), subset of the MobiAct public dataset for training,</li><li><a href="https://www.researchgate.net/publication/332780940_Edge-AI_in_LoRa-based_Health_Monitoring_Fall_Detection_System_with_Fog_Computing_and_LSTM_Recurrent_Neural_Networks">Queralta <em>et al.</em></a>: edge computing and fog computing. LSTM+RNN</li><li><a href="https://core.ac.uk/download/pdf/286564597.pdf">Luna-Perejon <em>et al.</em></a>: LSTM+GRU, accelerometer data as input, SisFall dataset for training.</li><li><a href="https://www.researchgate.net/publication/328310209_Embedded_Real-Time_Fall_Detection_with_Deep_Learning_on_Wearable_Devices">Torti et al.</a>: Micro-Controller Unit (MCU) with Tri-axial accelerometers, LSTM, tensorflow, SisFall dataset</li><li><a href="https://link.springer.com/chapter/10.1007/978-981-10-7419-6_25">Theodoridis <em>et al.</em></a> : acceleration measurements, RNN, URFD dataset + random 3D rotation augmentation for training. Comparing 4 type of model: LSTM-Acc, LSTM-Acc Rot, Acc + SVM-Depth, UFT</li><li><a href="https://www.researchgate.net/publication/326622687_Convolutional_recurrent_neural_networks_for_posture_analysis_in_fall_detection">Hsiu-Yu <em>et al.</em></a> : kinetics, posture Types, LSTM compared with CNN, fusion images as input from RGB images after extracting body shape by GMM and optical flow.</li></ul><h5 id="lstm-with-rcn-and-rnn">LSTM with RCN and RNN</h5><ul><li><a href="https://www.semanticscholar.org/paper/Co-Saliency-Enhanced-Deep-Recurrent-Convolutional-Ge-Gu/37b625459bd85caba627a7516741c50c05117344">Ge <em>et al.</em></a>: kinect, co-saliency-enhanced RCN, input video clips, RCN + (RNN+LSTM) to label the output.</li></ul><h4 id="ae-based-fall-detection-systems">AE based fall detection systems</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210222085226504.png" alt="image-20210222085226504" style="zoom:80%;" /></p><ul><li><a href="https://ieeexplore.ieee.org/document/7485147">Jokanovic <em>et al.</em></a>: time-frequency (TF) analysis. two stacked AE and a softmax layer.</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-95095-2_18">Droghini et al.</a>: acoustic fall detection, DT (downstream threshold) classifier.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8803671">Zhou and Komuro et al.</a>: VAE+3D-CNN residual block, reconstruction error for detecting fall actions. Dataset High-quality simulated fall dataset (HQFD) and Le2i dataset.</li><li><a href="https://ieeexplore.ieee.org/abstract/document/8283539">Seyfioglu et al.</a>: 3 layer-CAE. unsupervised pretraining for the convolutional layers, radar</li></ul><h3 id="conclusion">Conclusion</h3><ul><li>Sensors:<ul><li>RGB camera: not privacy and thus cannot be used in bathroom which with highly risk. Hacker</li><li>Depth camera: only depth, but can be used in like bathroom</li><li>Accelerator: cheap and the data captured are easy to be used,</li></ul></li><li>Vision based methods really depend on the background</li></ul><h2 id="paper-v-3d-skeletons">Paper V: 3D skeletons</h2><p><a href="https://ieeexplore.ieee.org/abstract/document/8869737">Implementation of Fall Detection System Based on 3D Skeleton for Deep Learning Technique</a></p><h3 id="previous-2">Previous</h3><ul><li>Kinect: depth sensor + RGB camera + microphone array</li><li>silhouette normalization : sensitive to the background</li><li>high computation price</li></ul><h3 id="goal-3">Goal</h3><p>real-time fall detection</p><h3 id="idea-1">Idea</h3><ul><li><p>Methods: seven highlight feature points+ pruned CNNs</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210226225948892.png" alt="image-20210226225948892" style="zoom:30%;" /></p><ul><li>Foreground segmentation: GMM, depth information</li><li>Labeling: use the area size to determine whether the object is human or not.</li><li>Thinning: Use Zhang Suen's rapid thinning method. Dilation + erosion</li><li>Searching: just Using the joints on arms and head, they <strong>argue that when a falling event will occur, the joints above the waist will have enormous change.</strong></li></ul></li><li><p>Input: 7 3D joints in 30 frames, (30，21).Use conv1d (so as to prune the number of parameters)</p></li></ul><h3 id="experiments-1">Experiments</h3><ul><li><p>Dataset: NTU RGB+D</p></li><li><p>Performance</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210226232235263.png" alt="image-20210226232235263" style="zoom:40%;" /></p></li><li><p>system: 15 frames per second in real-time</p></li></ul><h3 id="conclusion-1">Conclusion</h3><ul><li>depth image for skeleton extraction, conv1d for parameter pruning</li><li>15 frames per second for real-time implementation.</li></ul><h2 id="paper-vi-3d-skeletons">Paper VI: 3D skeletons</h2><p><a href="https://ieeexplore.ieee.org/document/8369778">Human fall-down event detection based on 2D skeletons and deep learning approach</a></p><h3 id="previous-3">Previous</h3><ul><li>Most of the existing skeleton-based action recognition approaches model actions based on well-designed hand-crafted local features.<ul><li>depth value+HMM etc.</li><li>single RGB image + NNs (CNNs, RNNs)</li></ul></li></ul><h3 id="idea-2">Idea</h3><ul><li>CNN for extracting skeletons and LSTM for final detection</li><li>Skelton extractor:<ul><li>input: 2D RGB image (1920*1080)</li><li>DeeperCut skeleton extraction: resnet backbone, output 14 joints (“forehead”, “chin”, and left and right “shoulder”, “elbow”, “wrist”, “hip”, “knee”, “ankle”, the mean of hip, the mean of chin and central-hip. The last two joints are for robustness with respect to background, so as to build a translation-invariant skeleton model).</li></ul></li><li>LSTM:<ul><li>input: 8 frames, try to classify 5 actions. Supervised training. (8*28), where (28=2*14)</li></ul></li></ul><h3 id="experiments-2">Experiments</h3><ul><li>800 training, 255 validation and 250 test. Manually remove the wrong skeletons (incomplete skeletons) by like a threshold (sum of the distances between each joint and the centroid is calculated)</li><li>trigger rule: during the last 30 outputs,<ul><li>current-time output is “lying” and the previous 19 outputs contain more than 14 “danger” statuses, an alarm will be triggered.</li><li>current output belongs to “safe” status and the previous 8 outputs contain over 5 “safe” statuses, then the alarm signal will be reset.</li></ul></li><li><strong>8 frames per second</strong></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210228193517428.png" alt="image-20210228193517428" style="zoom:40%;" /></li></ul><h1 id="human-activity-recognition">Human Activity Recognition</h1><h2 id="paper-vii">Paper VII</h2><p>Paper VII: <a href="https://link.springer.com/content/pdf/10.1007/s11042-020-09004-3.pdf">Vision-based human activity recognition: a survey</a></p><h3 id="goal-4">Goal</h3><p>review and summarize the progress of HAR systems from the computer vision perspective.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210228233502592.png" alt="image-20210228233502592" style="zoom:50%;" /></p><h3 id="har">HAR</h3><ul><li>Related: determining and naming activities using sensory observations<br /></li><li>Goal: labeling the same activity with the same label even when performed by different persons under different conditions or styles</li><li>Methods: usually by an activity detection task, which includes the temporal identification and localization. Formally, the activity recognition task is divided into classification and detection</li><li>HAR systems are influenced by two technologies: contact-based and remote methods.<ul><li>contact-based: require the physical interaction of the user with the command acquisition machine or device. Sensors like accelerometers, multi-touch screens etc. But it is not that popular now, cause the complicated sensors and price to make it easily to be accepted and wore</li><li>remote based: vision, societal trust, no requirement of ordinary users and thus non-intrusive</li></ul></li><li>Contributions:<ul><li>New update on this rarely focused theme</li><li>thoroughly analysis</li><li>multiple classification methods so as to analyze: detection, tracking and classification stages; feature extraction progress; input data modalities; supervision level; evaluation methods;</li></ul></li></ul><h3 id="benchmarks">Benchmarks</h3><ul><li>CONVERSE: complex conversational interactions</li><li>ALSAN:</li></ul><h3 id="previous-4">Previous</h3><figure><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210301221944071.png" alt="image-20210301221944071" /><figcaption aria-hidden="true">image-20210301221944071</figcaption></figure><h3 id="har-approaches">HAR approaches</h3><h4 id="har-approaches-according-to-feature-extraction-process">HAR approaches according to feature extraction process</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210301232609121.png" alt="image-20210301232608959" style="zoom:50%;" /></p><ul><li><p>The main steps of handcrafted-based features:</p><ul><li>foreground detection that corresponds to action segmentation</li><li>feature selection and extraction by an expert</li><li>classification of action represented by the extracted features</li></ul></li><li><p>The spatial and temporal representations of actions</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210301233924987.png" alt="image-20210301233924987" style="zoom:50%;" /></p><ul><li><p>Based on spatial cues (spatial representation )</p><ul><li>body models: use kinematic joint model, can be 3D model or directly recognize on 2D model</li><li>image models: holistic representation of actions that use a regular grid bounded by ROI centered around the person. E.g., silhouettes, contours, motion history images, motion energy images and optical flow.</li><li>spatial statistics: use a set of statistics of local features from the surrounding regions. E.g., calculated spatial-temporal interest points (STIP) of the image and assign each region to a set of features.<ul><li>volume-based: rely on features like texture, color, posture etc. Recognizing actions by the similarity between two action volumes. <strong>Only works fine on simple action or gesture.</strong> Like SIFT.</li><li>trajectory-based: represent joint positions with 2D or 3D points and then track them. The tracked changes in the posture is used for classification. Noise, view and /or illumination changes robust. Like HOG, HOF etc.</li></ul></li></ul></li><li><p>Based on temporal cues (temporal representation )</p><ul><li>action grammars: represent the action as a sequence of moments, and each moments is described by its own appearance and dynamics. Features are grouped into similar configurations called states and temporal transition between these states are learned. Like HMM, CRF, regression models and context-free grammars</li><li>action templates: representing the appearance of temporal blocks of features and dynamics called templates. Take representations of dynamics from several frames. Methods like Fourier Transform, Wavelet representations and trajectories of body parts are templates that cane be used</li><li>temporal statistics: use statistical models to describe the distributions of unstructured features over time.</li></ul></li><li><p>Appearance based approaches: <em>can be classified according to either shape or motion based characteristics.</em></p><p>Use 2D or 3D depth images and are based on shape features, motion features or any combination of both features. One advance is the skeleton-based recognition. Methods of it can be classified as (1) joint locations: consider the skeleton as a set of points; (2) joint angles: assume the human body as a system of rigid connected segments and the movement as an evolution of their spatial configuration.</p><ul><li>shape based methods: local shape features such as contour points, local region, silhouette and geometric features from the Human image or video after foreground segmentation.</li><li>motion based methods: optical flow and motion history volume from extracting action representation. methods like vector quantization of motion descriptors is used for action recognition. It uses histograms of optical flow and classifiers of bag-of-words.</li><li>hybrid methods: shape + motion features</li></ul></li></ul></li><li><p>Feature learning based methods for extracting representations</p><ul><li>traditional approaches<ul><li>dictionary learning: provides a sparse representation of the input data. It's proved that the use of over-complete dictionaries can produce even more compact representations</li><li>genetic programming: search a space of possible solutions without having any prior knowledge and can discover functional relationships between features in data enabling its classification. GP is used to construct holistic descriptors that allow to maximize the performance of action recognition tasks.</li><li>bayesian networks: PGMs, some use PGMs to represent and capture the semantic relationships among action units, and the correlations of the action unit intensities</li></ul></li><li>deep-learning-based methods<ul><li>generative methods: the main goal of these models is to understand the data distribution including the features that belong to each class. Methods like AE, VAE, GANs</li><li>discriminative methods: DNN,RNN, CNN. E.g., propose long-term temporal convolutions + high-quality optical flow</li><li>hybrid models</li></ul></li></ul></li></ul><h4 id="har-approaches-according-to-the-recognition-stages">HAR approaches according to the recognition stages</h4><ul><li>First stage methods (detection)<ul><li>skin color: used to detect the desire body part. May face problems since the chosen color space or when the objects of the scenario whose color is close to that of the skin. <strong>Seems not that robust</strong></li><li><strong>????</strong>Shape: the contours of the body part shape. This kind of methods are independent of the camera view, skin color and conditions of lighting.</li><li>pixel values: appearance is represented as pixel values change between images of a sequence according to the activity</li><li>3d models: build matches between characteristics of the model based on various features of the images. These methods are independent ode the viewpoint</li><li>motion: Using like the difference in brightness of pixels of two successive images</li><li><strong>????</strong>anisotropic-diffusion: based on the extension of the successful anisotropic diffusion based segmentation to the whole video sequence to improve the detection</li></ul></li><li>Second stage methods (tracking)<ul><li><p>temporal based methods: models are used to follow body parts</p><ul><li>features tracking based on the correlation: try to track the regions that contains body parts. These models require the part being tracked remains in the same neighborhood in the successive images. E.g., use 3D information of depth maps; optical flow</li><li>contours based tracking: snakes . initially place a contour close to the ROI (region of interest ), then it's warped in an interactive way using active shape models in each frame to make to snake converge. Sensitive to color intensity variations and smoothing and softening of contours</li></ul></li><li><p>optimal estimation</p><ul><li>evaluate the state of moving systems from series of measures. In HAR, they are used to estimate the movements of the human body. In real-time systems. Limitations against cluttered backgrounds.</li><li>similar to KF</li></ul></li><li><p>particle filters</p><p>following the body parts and their configurations in complex environments. The location of one body part is modelled by a set of particles</p></li><li><p>cam shift</p><p>based on mean shift algorithm (it uses the models of appearance based on density to represent the targets).</p></li></ul></li><li>Third stage methods (classification)<ul><li>SVM: used with kernel like Gaussian, RBF or linear kernel</li><li>Naive Bayesian classifier:</li><li>KNN: sensitive to local fatal structure</li><li>Kmeans: sensitive to data structures.</li><li>mean shift clustering: no requirement of prior knowledge about the number of clusters and no limit of the forms</li><li>machines finite state: states (the static gestures and postures) and transitions (temporal and /or probabilistic constraints). However, the models need to be changed everytime a new gesture appears. Computational expensive.</li><li>HMMs:</li><li>dynamic time wrapping: calculates the distances between each pair of possible points from two signals. Used for estimating and detecting the movement in a video sequence</li><li>NNs</li></ul></li></ul><h4 id="har-approaches-according-to-the-source-of-the-input-data">HAR approaches according to the source of the input data</h4><ul><li>Uni-modal methods<ul><li>space-time methods: time and the 3D representation f the body to locate activities in space. But sensitive to noise and occlusion. Cannot works on complex actions</li><li>stochastic methods: models like Markov Model. The training is difficult cause the amount of parameters.</li><li>rule-based methods: characterize the activity using a set of rules or attributes</li><li>shape-based methods: shape features to represent and recognize activities. Dependent on the viewpoint, occlusion, people clothing and sensitive to lighting variations</li></ul></li><li>Multi-modal methods<ul><li>emotional methods: associate visual and textual features</li><li>behavior methods: recognize the behavior methods.</li><li>social networks-based methods: allow recognition of social events and interactions</li></ul></li></ul><h4 id="har-approaches-according-to-the-ml-supervision-level">HAR approaches according to the ML supervision level</h4><ul><li>supervised: mostly used to classify and recognize short term actions</li><li>unsupervised: outperform on finding spatio-temporal patterns of motion. computationally complex, less accurate and trustworthy.</li><li>semi-supervised: hybrid.</li></ul><h3 id="activities-type">Activities type</h3><p>Divided by the complexity</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210303222430563.png" alt="image-20210303222430563" style="zoom:40%;" /></p><ul><li>elementary human actions: simple atomic activities</li><li>gestures: a language or part of the non-verbal communication which can be employed to express significant ideas or orders</li><li>behaviors: a set of physical actions and reaction of individuals in specific situations</li><li>interactions: reciprocal actions or exchanges between two entities or more</li><li>group actions:</li><li>events: social actions between individuals</li></ul><h4 id="body-parts">Body parts</h4><ul><li>hand gestures: be tracked to <strong>detect the communication between individuals</strong></li><li>foot: detect shifting and movements of people or Other actions</li><li><strong>facial expressions: interpret specific Types of human activities, especially for handicapped or disabled people</strong></li><li>full body: posturers and human actions by the whole body</li></ul><h3 id="the-types-of-input-data">The Types of Input Data</h3><h4 id="image-or-videos">image or videos</h4><ul><li>HAR on static images: when activity is distinguishable compared to others by its characteristic.</li><li>HAR by videos: offer extra information related to prior and post event, then the relation between two successive frames can be established</li></ul><h4 id="single-viewpoint-or-multi-view-acquisition">single viewpoint or multi-view acquisition</h4><ul><li>single view acquisition</li><li>multiple view acquisition</li></ul><h3 id="evaluations">Evaluations</h3><h4 id="validation-means">Validation means</h4><ul><li>One platform, validated on different Types of datasets (differ on actions) acquired by this platform.</li><li>Different platforms, different datasets but all be evaluated so as to test the generalization across datasets</li><li>Compared with results in literature</li></ul><h4 id="datasets-benchmarks">Datasets (Benchmarks)</h4><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210304202438344.png" alt="image-20210304202438344" style="zoom:67%;" /></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210304202924893.png" alt="image-20210304202924893" style="zoom:50%;" /></p><h4 id="evaluation-metrics">Evaluation metrics</h4><ul><li><p>sensitivity: also known as TP, recall or probability of detection. It determines the failure of the system to detect actions</p><p><span class="math inline">\(Sensitivity =\frac{TP}{TP+FN}\)</span></p></li><li><p>precision: also known as PPV (positive prediction value). <span class="math inline">\((1-precision)\)</span> determines the probability of the recognizer incorrectly identifying a detected activity</p><p><span class="math inline">\(Precision=\frac{TP}{TP+FP}\)</span></p></li><li><p>Specificity: also known as false positive rate (FPR).</p><p><span class="math inline">\(Specificity=\frac{TN}{TN+FP}\)</span></p></li><li><p>Negative predictive value (NPV): also known as negative precision. It measures the system sensitivity to negative class.</p><p><span class="math inline">\(NPV=\frac{TN}{TN+FN}\)</span></p></li><li><p>F_measure: the harmonic mean of precision and recall. It gives information of tests accuracy. Best with 1 and worst with 0.</p><p>$F_measure=2 $</p></li><li><p>Accuracy: the percentage of correct predictions relative to the total number of samples</p><p><span class="math inline">\(Accuracy =\frac{\#CorrectPredictions}{\#Predictions}\\Accuracy =\frac{TP+TN}{\#samples}\)</span></p></li><li><p>Likelihood ratio: the likelihood of an activity predicted when it matches the ground truth compared to the likelihood when it's predicted wrongly.</p><p><span class="math inline">\(LR+=\frac{Sensitivity}{1-Specificity},\\ LR-=\frac{1-Sensitivity}{Specificity}\)</span></p></li><li><p>AUC: it's 1 if predicted perfectly.</p></li><li><p>Confusion matrix</p></li><li><p>IoU: intersection over Union, also known as Jaccard index or Jaccard similarity coefficient.</p><p><span class="math inline">\(IoU=\frac{AreaOfOverlap}{AreaOfUnion}\)</span></p></li></ul><h3 id="limitations-and-challenges">Limitations and Challenges</h3><h4 id="limitations">Limitations</h4><p>Show various issues that may affect the effectiveness of HAR system.</p><ul><li>specific to the methods used during the various phase of the recognition process.<ul><li>methods based on the form or the appearance<ul><li>like colorimetric segmentation: confuse the objects of the scene and body parts</li><li>variation in appearance or clothing of people</li></ul></li></ul></li><li>related to the acquisition devices, experimentation environment or various applications of the system.<ul><li>light variations: affect the image quality and then features</li><li>perspective change: if data is acquired by single view it would be a big problem<ul><li>self-occlusion: body parts occlude each other</li><li>occlusion of another object</li><li>partial occlusion of human body parts</li></ul></li><li>variety of gestures linked to the complex structure of human activities and the similarity between classes of different actions.<ul><li>due to data association</li></ul></li><li>noise, complex or moving backgrounds and unstructured scenes, and scale variation</li></ul></li><li>rely on their own recorded dataset to test performance<ul><li>call of a benchmark</li></ul></li></ul><h4 id="challenges-1">Challenges</h4><ul><li>the requirement of continuous monitoring and generate reliable answers at the right time</li><li>modeling and analyzing interactions between people and objects with an appropriate level of accuracy</li><li>societal challenges: acceptance by the society, privacy, side effects of installation<ul><li>privacy: HAR systems on smartphones may be a way out</li></ul></li><li>HAR system should be independent on users' age, color, size or capacity to use</li><li>gestures independence and gestures spotting from continuous data streams<ul><li>detect and recognize various gestures under different background conditions</li><li>tolerant with the scalability and growth of gestures</li></ul></li><li>context-aware: improve applications in its domain</li><li>daily life activities: complex videos and hard to be modeled; overlapping of starting and ending time of each particular activity; discrimination between intentional and involuntary actions</li><li>HAR through missed parts of video, recognition more than one activities performed by one person at the same time, early recognition and prediction of actions</li><li>The implementation of DL HAR system: memory constraint, high number of parameters update, collection an fusion of large multi-modal variant data for the training process, deployment of different architectures of DL based methods in smartphones or wearable devices</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Fall detection
&lt;ul&gt;
&lt;li&gt;Paper I: &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full&quot;&gt;Elderly Fall Detection Systems: A Literature Survey, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper II: &lt;a href=&quot;https://dl.acm.org/doi/10.1145/2769493.2769540&quot;&gt;A survey on vision-based fall detection, 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper III: &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2352864815000681&quot;&gt;3D depth image analysis for indoor fall detection of elderly people, 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper IV: &lt;a href=&quot;https://ieeexplore.ieee.org/document/9186685&quot;&gt;Deep learning based systems developed for fall detection: a review, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper V: &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8869737&quot;&gt;Implementation of Fall Detection System Based on 3D Skeleton for Deep Learning Technique, 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper VI: &lt;a href=&quot;https://ieeexplore.ieee.org/document/8369778&quot;&gt;Human fall-down event detection based on 2D skeletons and deep learning approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Human activity recognition
&lt;ul&gt;
&lt;li&gt;Paper VII: &lt;a href=&quot;https://link.springer.com/content/pdf/10.1007/s11042-020-09004-3.pdf&quot;&gt;Vision-based human activity recognition: a survey, 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D and depth data
&lt;ul&gt;
&lt;li&gt;Paper VIII: &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0167865514001299&quot;&gt;Human activity recognition from 3d data: a review, 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.08362&quot;&gt;RGB-D-based Human Motion Recognition with Deep Learning: A Survey, 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;3D skeleton-based human representations
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1601.01006&quot;&gt;Space-Time Representation of People Based on 3D Skeletal Data: A Review, 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0031320315004392&quot;&gt;3D skeleton-based human action classification: A survey,2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://d1wqtxts1xzle7.cloudfront.net/47833181/Crowd_analysis_A_survey20160806-21965-155miur.pdf?1470471444=&amp;amp;response-content-disposition=inline%3B+filename%3DCrowd_analysis_a_survey.pdf&amp;amp;Expires=1614620280&amp;amp;Signature=gO5XOCbzmA4O~6zc1hli7UqnkZmethCye13xIqVW58A~NTeZYwbbxSs3vZsO4E9~73WX7gYBapzo3quA7UV5jFDRfaDQ6v0ds8dA3BDhB5ys2PlxRFWxEmPsfGAPSp7G6inWLRrfw89L2xXRnX-KM1caNEnqcsg18OD9zf8LU3aovB4hXyB0kvMtc2T2FXdg1HdlQbjqVAlZmrcSl2Y98j1Gr4it23BLSbmUmwZpYAtVA4WqUwFihyqQco5XHX3dhJn7eUdKTOc6QdqQ2KumIhXBwnHSR8TOF9StECcxoUlOf9fcrEgRH4tDauMCsVqgCWJkANhI4~lp0nJEPP21fQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA&quot;&gt;Crowd analysis: a survey, 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/6909476&quot;&gt;Human action recognition by representing 3d skeletons points in a lie group, CVPR 2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;knowledge-based HAR activity recognition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0957417416302913&quot;&gt;A survey on using domain and contextual knowledge for human activity recognition in video streams.,2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Abnormal HAR
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0952197618301775&quot;&gt;A review of state-of-the-art techniques for abnormal human activity recognition, 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="survey" scheme="http://yoursite.com/tags/survey/"/>
    
      <category term="fall" scheme="http://yoursite.com/tags/fall/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Graph Level Anomaly Detection</title>
    <link href="http://yoursite.com/posts/notes/2021-01-28-notes-paper-anomal-graphlevel-SSL.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-28-notes-paper-anomal-graphlevel-SSL.html</id>
    <published>2021-01-28T18:33:39.000Z</published>
    <updated>2021-07-13T02:31:58.919Z</updated>
    
    <content type="html"><![CDATA[<p>Project <a href="http://snap.stanford.edu/class/cs224w-2019/project/26424135.pdf">Graph Level Anomaly Detection</a></p><p>Paper <a href="https://www.osti.gov/servlets/purl/1214009">Multi-Level Anomaly Detection on Time-Varying Graph Data</a></p><a id="more"></a><h2 id="Why">Why?</h2><p>Anomaly detection at a graph level rather than node level or links level.</p><h2 id="Goals">Goals</h2><p>modeling a comprehensive representation of a graph’s local and high level structural features, as well as a challenging problem because of the unique properties of graph based data, such as long dependencies and size variability.</p><h2 id="Previous">Previous</h2><ul><li>You et al. propose an autoregressive approach to graph generation that is trained sequentially on existing graphs and then generates them at inference time by breaking the process into a sequence of node and edge formations</li><li>You et al.’s work in an RL approach to goal-directed molecular graph generation. Using partially observed subgraphs (discussed later) and an action-based generation framework v</li></ul><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li><p>Formulate two unsupervised learning objectives for graph level anomaly detection. Namely, we compare 1) generative modeling for graph likelihood estimation and 2) a novel method based on masked graph representation learning.</p></li><li><p>look to learn meaningful representations over a family of graphs by modeling <strong>one step edge completion problems.</strong></p></li></ul><h3 id="Implementation">Implementation</h3><h2 id="Experiments">Experiments</h2><h2 id="Conclusion">Conclusion</h2><h2 id="Multi-Level-Anomaly-Detection-on-Time-Varying-Graph-Data">Multi-Level Anomaly Detection on Time-Varying Graph Data</h2><p><a href="https://www.osti.gov/servlets/purl/1214009">Here</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Project &lt;a href=&quot;http://snap.stanford.edu/class/cs224w-2019/project/26424135.pdf&quot;&gt;Graph Level Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paper &lt;a href=&quot;https://www.osti.gov/servlets/purl/1214009&quot;&gt;Multi-Level Anomaly Detection on Time-Varying Graph Data&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Graph Embedded Pose Clustering for Anomaly Detection</title>
    <link href="http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-15-notes-paper-anomaly-gepc.html</id>
    <published>2021-01-15T20:17:13.000Z</published>
    <updated>2021-01-21T22:51:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1912.11850.pdf">Graph Embedded Pose Clustering for Anomaly Detection</a></p><p>Code <a href="https://github.com/amirmk89/gepc">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li><p>Anomaly detection task</p><ul><li>Fine-grained anomaly detection: Detecting abnormal variations of an action: e.g. an abnormal type of walking</li><li>Coarse-grained anomaly detection: Defining normal actions and regard other action as abnormal. Aka there are multiple poses regarded as normal actions, rather than a single normal action.</li></ul></li><li><p>Video anomaly detection</p><ul><li>Reconstructive models:  learn a feature representation for each sample and attempt to reconstruct a sample based on that embedding, often using <strong>Autoencoder</strong>. Samples poorly reconstructed are considered anomalous.</li><li>Predictive models: model the current frame based on a set of previous frames, often relying on recurrent neural networks or 3D convolutions. Samples poorly predicted are considered anomalous.</li><li>Reconstructive + predictive models</li><li>Generative models: used to reconstruct, predict or model the distribution of the data, often using Variational Autoencoders (VAEs) or GANs. E.g. the differences in gradient-based features and optical flow.</li></ul></li><li><p>GNNs</p><p>The point is the weighted adjacency matrix.</p><ul><li>Temporal and multiple adjacency extensions. (ST-GCN)</li><li>Graph attention networks. (2s-AGCN)</li></ul></li><li><p>Deep clustering models</p><p>Provide useful cluster assignments by optimizing a deep model under a cluster inducing objective.</p></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Skeleton-based methods make the analysis independent of nuisance parameters such as viewpoint or illumination.</li></ul></li><li>Limitations<ul><li>Traditional RGB-based anomaly detection methods have to consider many trivial information (viewing direction, illumination, background clutter etc.), and those data are sparse in human pose.</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Generating action words from skeleton-based graphs and then classify actions into normal and abnormal (anomaly detection). With an aim at it can work both on fine-grained and coarse-grained task.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><p>Map graphs into representation space and then cluster them so as to get action words. At last, Dirichlet process based mixture is used for classifying normal and abnormal.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115162120.png" alt="image-20210115162120660" style="zoom:50%;" /><h3 id="Data-Preparation">Data Preparation</h3><ul><li>Similar skeleton graph as what used in ST-GCN.</li></ul><h3 id="Implementation">Implementation</h3><ul><li><p>Backbone: ST-GCN</p></li><li><p>ST-GCAE network</p><ul><li><p>GCN block</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115185527.png" alt="image-20210115185526918" style="zoom:50%;" /><p>The block will be used in SAGC</p></li><li><p><strong>SAGC</strong> block</p><p>Each adjacency type is applied with its own GCN, using separate weights.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115190041.png" alt="image-20210115190041541" style="zoom:50%;" /><ul><li><p>Adjacency matrices</p><table><thead><tr><th>matrix</th><th>sharing</th><th>level</th><th>Dimension</th></tr></thead><tbody><tr><td>$\mathrm{A}$</td><td>fixed and shared by all layers</td><td>body-part connectivity over node relations</td><td>$[V,V]$, $V$ is the number of nodes</td></tr><tr><td>$\mathrm{B}$</td><td>individual at each layer, applied equally to all samples</td><td>dataset level keypoint relations</td><td>$[V,V]$</td></tr><tr><td>$\mathrm{C}$</td><td>is different for different sample</td><td>sample specific relations</td><td>$[N,V,V]$, $N$ is the batch size</td></tr></tbody></table></li></ul></li><li><p>ST-GCAE</p><p>The encoder uses large temporal strides with an increasing channel number to compress an input sequence to a latent vector. The decoder uses temporal up-sampling layers and additional graph convolutional blocks.</p></li></ul></li><li><p><strong>Deep embedded cluster</strong></p><ul><li><p>The input is the embedding from ST-GCAE, denoted as $\mathrm{z}_i$ for sample $i$</p></li><li><p>Soft-assignment --clustering layer</p><p>The probability $p_{ik}$ for the $i$-th sample to be assigned to the $k$-th cluster is:</p><p>$p_{ik}=Pr(y_i=k|\mathrm{z}<em>i,\Theta)=\frac{exp(\theta^T_k\mathrm{z}<em>i)}{\sum\limits</em>{k’=1}^{K}exp(\theta^T</em>{k’}\mathrm{z}_i)}$, where $\Theta$ is the clustering layer’s parameters. (Simple softmax)</p></li><li><p><strong>Optimize clustering layer</strong></p><ul><li>Objective: Minimize the KL-divergence between the current model probability clustering prediction $P$ and a target distribution $Q$. The target distribution aims to strengthen current cluster assignments by normalizing and pushing each value closer to a value of either 0 or 1.</li><li><strong>EM</strong> style. In expectation step, the entire model is fixed and the target distribution $Q$ is updated. In maximization stage, the model is optimized to minimize the clustering loss $L_{cluster}$.</li></ul></li></ul></li><li><p>Anomaly classifier–<strong>normality scoring??</strong></p><ul><li>Two types of multimodal distributions. One is at the cluster assignment level; the other is at the soft-assignment vector level.</li><li>DPMM based. Classifier is fitted by soft-assignment vector (e.g., for class $i$ the softmax result) and then it can do inference.</li></ul></li><li><p>Model</p><p>Feeding the embedding from ST-GCAE to clustering layer, then fixing decoder, fine-tune the encoder in ST-GCAE and clustering layer by combined loss. After fine tuning, using DPMM-based classifier for final inference.</p><ul><li><p>Loss function</p><ul><li><p><strong>Reconstruction loss</strong> $L_{rec}$: $\ell_2$ loss between the original temporal pose graphs and those reconstructed by ST-GCAE, <strong>used in pre-training stage</strong>, for training the whole ST-GCAE.</p></li><li><p><strong>Clustering loss</strong> $L_{cluster}$, combined with reconstruction loss and used for fine-tuning encoder of ST-GCAE+clustering layer</p><p>$L_{cluster}=KL(Q||P)=\sum\limits_i\sum\limits_kq_{ik}\log\frac{q_{ik}}{p_{ik}},\ q_{ik}=\frac{p_{ik}/(\sum_{i’}p_{i’k})^{\frac{1}{2}}}{\sum_{k’}p_{ik’}/(\sum_{i’}p_{i’k’})^{\frac{1}{2}}}$</p></li><li><p>Combined loss</p><p>$L_{combined}=L_{rec}+\lambda\cdot L_{cluster}$, this loss is for training encoder and clustering layer, which means the decoder is fixed while using it.</p></li></ul></li><li><p>Optimization</p><ul><li>encoder: reconstruction loss + cluster loss</li><li>decoder: reconstruction loss</li><li>clustering layer: cluster loss</li></ul></li></ul></li></ul><h2 id="Experiments">Experiments</h2><ul><li><p>Dataset</p><ul><li>ShanghaiTech: 130 abnormal events captured in 13 different scenes with complex lighting conditions and camera angles.<ul><li>training set contains only normal examples</li><li>test set contains both normal and abnormal examples</li><li>2D pose</li></ul></li><li>Kinetics-based: Kinetics-250 and NTU-RGBD. Actions in each set are sampled randomly or meaningfully. In Kinetics dataset, remove actions that focus only on slightly part joints’ movements, like hair braiding.<ul><li><em>Few vs. Many</em>: few normal actions ($3\sim5$) in the training set and many abnormal  ($10\sim 11$ hundreds) actions in the test set</li><li><em>Many vs. Few</em>: switch the training set and test set in experiment above.</li></ul></li></ul></li><li><p>Preprocessing</p><ul><li>Pre-extracting 2D pose from ShanghaiTech Campus</li></ul></li><li><p>Input features</p><ul><li>The coordinates of joints</li><li>For ShanghaiTech: The embeddings of the patch around each joint (from one of the pose estimation model’s hidden layers)</li></ul></li><li><p>Test Algorithms on coarse-grained (Kinetics and NTU-RGBD)</p><ul><li><p>Autoencoder reconstruction loss: ST-GCAE reached convergence prior to the deep clustering fine-tuning stage.</p></li><li><p>Autoencoder based one-class SVM: fit a one-class SVM using the encoded pose sequence representation</p></li><li><p>Video anomaly detection methods: Train Future frame prediction model and the skeleton trajectory model. Anomaly scores for each video are obtained by averaging the per-frame scores.</p></li><li><p>Classifier softmax scores: supervised baseline. Anomaly score is by either using the softmax vector’s max value or by using the Dirichlet normality score</p></li><li><p>Test video in fixed size but with sliding-window if the test video with unknown frames</p></li></ul></li><li><p>Evaluation metrics</p><ul><li>Frame-level score: the maximal score over all the people in the frame</li><li>AUC as the combined score over all frames of one test</li></ul></li><li><p>Summary</p><ul><li><p>On ShanghaiTech (fine-grained): Patches ST-GCAE outstands.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121170448.png" alt="image-20210121170448502" style="zoom:40%;" /></li><li><p>On coarse grained dataset, ST-GCAE outperforms, but better on meaningful actions. <em>A good skeleton help ST-GCAE</em> (NTU-RGBD has better detection on skeletons cause the depth data is known.)</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121171357.png" alt="image-20210121171357539" style="zoom:30%;" /></li><li><p><em>Failed cases</em>: occlusions, high-speed action like cycling, non-person related abnormal like bursting into a vehicle.</p></li><li><p><strong>Ablation study:</strong> adding some abnormal actions into normal actions</p><ul><li>ST-GACE on NTU-RGBD (<em><strong>only dropping, touching and Rand8 dataset are tested</strong></em>): ST-GCAE loses on average less than $10%$ of performance when trained with $5%$ abnormal actions added as noises.</li></ul></li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>The use of embedded pose graphs and a Dirichlet process mixture for video anomaly detection;</li><li>A new coarse-grained setting for exploring broader aspects of video anomaly detection;</li><li>State-of-the-art AUC of 0.761 for the ShanghaiTech Campus anomaly detection benchmark.</li></ul><h2 id="font-color-blue-Remarks-font"><font color='blue'>Remarks</font></h2><ul><li>The reconstruction (learning representations of graph) is mixed with clustering in the final loss, will this be good? Won’t the trivial information of clustering influence the reconstruction?</li><li>The training set for clustering layer is initialized by the K-Means centroids, won’t the initialization methods matter?</li><li><strong>The embeddings of patches around each joint outperforms the simple joint coordinates</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1912.11850.pdf&quot;&gt;Graph Embedded Pose Clustering for Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/amirmk89/gepc&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-14-notes-paper-anomaly-2sagcn.html</id>
    <published>2021-01-15T01:51:32.000Z</published>
    <updated>2021-01-15T20:20:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/lshiwjx/2s-AGCN">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li>Convolutional DL based methods  manually structure the skeleton as a sequence of joint-coordinate vectors or as a pseudo-image, which is fed into RNNs or CNNs to generate the prediction.</li><li>Skeleton-based action recognition<ul><li>Design handcrafted features to model human body, but they are barely satisfactory.</li><li>DL-based: CNN-based methods are generally more popular than RNN-based methods. But both fail to fully represent the structure of the skeleton data.</li><li>GCN-based: ST-GCN, eliminates the meed for designing handcrafted part assignment or traversal rules.</li></ul></li><li>GNNs<ul><li><strong>Spatial perspective</strong>: directly perform the convolution filters on the graph vertexes and their neighbors, which are extracted and normalized based on manually designed rules.</li><li>Spectral perspective: use the eigenvalues and eigenvectors of the graph Laplace matrices. They perform the graph convolution in the frequency domain with the help of graph Fourier transform.</li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>The second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods.</li></ul></li><li>Limitations:<ul><li>Representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependency between correlated joints</li><li>In GCN-based skeleton action recognition, the topology of the graph is set manually and thus may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks.</li><li>ST-GCN: 1)  The skeleton graph is heuristically predefined and represents only the physical structure of the human body. 2) The fixed topology of graph limiting the flexibility and capacity to model the multilevel semantic information. 3)  One fixed graph structure may not be optimal for all the samples of different action classes. Like hands-related actions and legs-related actions</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Propose a improved ST-GCN (graph convolutional based model), so as to use 2nd order information and improve the accuracy of action recognition based on skeletons.</p><p>Make the graph is unique for different layers and samples.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><p>Modification on ST-GCN</p><ul><li>Two types of graphs:<ul><li>Global graph: represents the common pattern for all the data</li><li>Individual graph: represents the unique pattern for each data</li></ul></li><li>Second-order information: the length and directions of bones are formulated as a vector pointing from its source joint to its target joint.</li></ul><h3 id="Data-Preparation">Data Preparation</h3><ul><li>The structure of the graph follows the work of ST-GCN.</li></ul><h3 id="A-look-at-ST-GCN">A look at ST-GCN</h3><p>Graph convolution in ST-GCN: $f_{out}(v_{ti})=\sum\limits_{v_{tj}\in \mathcal{B}<em>i}\frac{1}{Z</em>{ti}(v_{tj})}f_{in}(v_{j})\cdot \mathrm{w}(l_{ti}(v_{tj}))$, follows spatial configuration partitioning.</p><ul><li>Graph convolution in spatial dimension</li></ul><p>$f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}<em>k(f</em>{in}\mathrm{A}_k)\odot\mathrm{M}_k$, where $\mathrm{M}_k$ is an $N\times N$ attention map that indicates the importance of each vertex. $\mathrm{A}_k$ <strong>determines whether there are connections between two vertexes and $\mathrm{M}_k$ determines the strength of the connections.</strong></p><ul><li>Graph convolution in temporal dimension: $K_t\times 1$convolution on the output feature map</li></ul><p><em>The model is calculated based on a predefined graph, which may not be a good choice.</em></p><h3 id="Implementation">Implementation</h3><ul><li><p>Adaptive graph convolutional network (<strong>AGCN</strong>)</p><p><strong>BN+9 of adaptive graph convolutional blocks + global average pooling + softmax classifier</strong></p><ul><li><p>Adaptive graph convolutional layer:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142649.png" alt="image-20210115142649202" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115133332.png" alt="image-20210115133332722" style="zoom:50%;" /></p><p>$f_{out}=\sum\limits_{k}^{K_v}\mathrm{W}<em>kf</em>{in}(\mathrm{A}_k+\mathrm{B}_k+\mathrm{C}_k)$. The adjacency matrix is now divided into three parts</p><ul><li><p>$\mathrm{A}_k$: same as $N\times N$ adjacency matrix $\mathrm{A}_k$ in ST-GCN, it <em>represents the physical structure of the human body</em>.</p></li><li><p>$\mathrm{B}_k$: An $N\times N$ adjacency matrix. It’s trainable. It acts as $\mathrm{M}_k$ (attention mechanism) in ST-GCN, influenced by the connections between two joints and also the strength of the connections.</p></li><li><p>$\mathrm{C}_k$: a similarity matrix calculated by the normalized embedded Gaussian function with vectors embedded by $1\times 1$ convolutional layer.</p><p>$\mathrm{C}<em>k=softmax(\mathrm{f}</em>{in}^T\mathrm{W}<em>{\theta k}^{T}\mathrm{W}</em>{\phi k}\mathrm{f}<em>{in})$, where $\mathrm{W}</em>\theta,\mathrm{W}_\phi$ are the parameters of the embedding functions $\theta,\phi$, respectively.</p></li></ul></li><li><p>Adaptive graph convolutional block</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115132928.png" alt="image-20210115132928144" style="zoom:33%;" /></li></ul></li><li><p>Model: two stream networks</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115142748.png" alt="image-20210115142748418" style="zoom:50%;" /><ul><li>J-stream: the input data are joints, as what’s depicted in AGCN.</li><li>B-stream: the input data are bones.<ul><li>A bone is the vector pointing from its source joint to its target joint. This vector will contain both length and direction of a bone. An empty bone is added so as to make sure the B-stream has similar quantity of  input as J-stream.</li></ul></li></ul><p>Finally, the <em>softmax</em> scores of the two streams are added to obtain the fused score and do prediction.</p></li><li><p>Loss function</p><p>Cross-entropy</p></li><li><p>Why does it work?</p><ul><li>Considering bones (2nd information)</li><li>Offers trainable attention matrix $\mathrm{B}_k$ and the similarity evaluation of $\mathrm{C}_k$ to estimate the strength of connection. Both of them offer more possible connections and provide more flexibility.</li></ul></li></ul><h2 id="Experiments">Experiments</h2><ul><li>Dataset: Kinetics and NTU-RGBD<ul><li>NTU-RGBD: If the number of bodies in the sample is less than 2, the second body is padded with 0.</li><li>Kinetics: Same data augmentation as done in ST-GCN.</li></ul></li><li>Training: SGD with Nesterov momentum (0.9), batch size is 64. The weight decay is set to 0.0001.<ul><li>NTU-RGBD: learning rate is set as 0.1 and is divided by 10 at the 30th epoch and 40th epoch. The training process is ended at the 50th epoch.</li><li>Kinetics: The learning rate is set as 0.1 and is divided by 10 at the 45th epoch and 55th epoch. Training ends at the 65th epoch.</li></ul></li><li>Evaluation metrics</li><li>NTU-RGBD: top-1 accuracy<ul><li>Kinetics : top-1 and top-5 accuracy</li></ul></li></ul><h3 id="Ablation-study">Ablation study</h3><ul><li><p>Adaptive graph convolutional block</p><p>Manually  delete one of the graphs and estimate.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115150909.png" alt="image-20210115150909820" style="zoom:33%;" /><ul><li>Given each connection, a weight parameter is important, which also proves the importance of the adaptive graph structure</li></ul></li><li><p>Visualization of the learned graphs</p><p>Denote the strength of joints by dot size, the bigger the stronger connection.</p><ul><li><p>A higher layer in AGCN contains higher-level information, comparing the dot size in different layers</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115151421.png" alt="image-20210115151421543" style="zoom:50%;" /></li><li><p>The diversity for different sample in the same layer is proved.</p></li></ul></li><li><p>Two-stream framework</p><p>The two-stream method outperforms the one-stream-based methods either the J-stream or the B-stream.</p></li></ul><h3 id="Compared-with-SOTA">Compared with SOTA</h3><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210115144443.png" alt="image-20210115144443887" style="zoom:50%;" /><ul><li>Question 1: <font color='red'> ResNet helps?</font></li><li>Question 2: <font color='red'>How about compared with methods based on RGB or optical flow ?</font> In paper ST-GCN their model fails to those models.</li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>An adaptive graph convolutional network is proposed.</li><li>The second-order information of the skeleton data is explicitly formulated and combined with the first-order information using a two-stream framework, which brings notable improvement for the recognition performance.</li><li>On two large-scale datasets for skeleton-based action recognition, the proposed 2s-AGCN exceeds the SOTA by a significant margin.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf&quot;&gt;Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/lshiwjx/2s-AGCN&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-12-notes-paper-anomaly-stgcn.html</id>
    <published>2021-01-13T02:58:39.000Z</published>
    <updated>2021-01-15T18:26:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>Code <a href="https://github.com/yysijie/st-gcn">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li>Human action recognition: can be solved by appearance, depth, optical flows, and body skeletons.</li><li>Models for graph<ul><li>Recurrent neural networks</li><li>GNNs:<ul><li>Spectral perspective: the locality of the graph convolution is considered in the form of spectral analysis.</li><li><strong>Spatial perspective</strong>: the convolution filters are applied directly on the graph nodes and their neighbors.</li></ul></li></ul></li><li>Skeleton Based Action Recognition<ul><li>Handcrafted feature based methods: design several handcrafted features to capture the dynamics of joint motion. E.g., covariance matrices of joint trajectories, relative positions of joints, rotations and translations between body parts.</li><li>Deep learning methods: recurrent neural networks and temporal CNNs. Many emphasize <strong>the importance of modeling the joints within parts of human bodies.</strong></li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Earlier methods of using skeletons for action recognition simply employ the joint coordinates at individual time steps to form feature vectors, and apply temporal analysis thereon.  <strong>They do not explicitly exploit the spatial relationships among the joint</strong>.</li><li>Most existing methods which explore spatial relationship rely on hand-crafted parts or rules to analyze the spatial patterns.</li><li>Traditional CNNs are not suitable for 2D or 3D skeletons (graphs rather than data in grids).</li></ul></li><li>Limitations:<ul><li>Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization.</li><li>Models using hand-crafted parts are difficult to be generalized to others</li><li>These parts used in DL based methods are usually explicitly assigned using domain knowledge, which is not automatic and practical.</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Build a better model for dynamics of human body skeletons. Specifically, a new method that can automatically capture the patterns embedded in the <strong>spatial configuration of the joints as well as the  temporal dynamics</strong> is required.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><p>Skeletons in frames are connected as natural spatial-temporal graph, then using GCN.</p><h3 id="Data-Preparation">Data Preparation</h3><ul><li>The feature vector on a node $F(v_{ti})$ consists of coordinate vectors, as well as estimation confidence, of the i-th joint on frame t.</li><li>Construct the spatial temporal graph on the skeleton sequences in two steps. First, the joints within one frame are connected with edges according to the connectivity of human body structure. Then each joint will be connected to the same joint in the consecutive frame.</li><li>Both 18 joints skeleton model or 25 joints skeleton model work fine.</li></ul><h3 id="Implementation">Implementation</h3><ul><li><p>Model</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113115442.png" alt="image-20210113115429152" style="zoom:50%;" /><p>ResNet mechanism is applied on each ST-GCN unit.</p><table><thead><tr><th>Layer name</th><th>configuration</th></tr></thead><tbody><tr><td>$1\sim3$</td><td>64 channels</td></tr><tr><td>$4\sim6$</td><td>128 channels</td></tr><tr><td>$7\sim9$</td><td>256 channels, 9 temporal kernel size</td></tr><tr><td>Global pooling+softmax</td><td></td></tr></tbody></table></li><li><p>Loss function</p><ul><li><p><strong>Sampling function</strong> $\mathbb{p}$ enumerates the neighbors of location $x$.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113155452.png" alt="image-20210113155452481" style="zoom:40%;" /><p>In this paper, <strong>the 1-neighbor set</strong> of joint nodes are used.</p></li><li><p><strong>The filter weights</strong> $\mathrm{w} (v_{ti},v_{tj})$ are shared everywhere on the input image.</p><p>Build a mapping $l_{ti}:B(v_{ti})\rightarrow{0,\cdots,K−1}$ which maps a node in the neighborhood to its subset label. The weight function $\mathrm{w}(v_{ti}, v_{tj}):B(v_{ti})\rightarrow R^c$ can be implemented by indexing a tensor of $(c,K)$ dimension or</p><p>$\mathrm{w}(v_{ti},v_{tj})=\mathrm{w}'(l_{ti}(v_{tj})$</p><ul><li><p>Labeling strategies (the definition of $l_{ti}$)</p><img src="C:%5CUsers%5C10457%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210113195834069.png" alt="image-20210113195834069" style="zoom:50%;" /><ul><li><p>Uni-labeling</p><p>Make the whole neighbor set itself as subset. Then feature vectors on every neighboring node will have a inner product with the same weight vector. Formally, $K=1,l_{ti}(v_{tj})=0,\forall i,j\in V$</p></li><li><p>Distance partitioning</p><p>$d=0$ refers to the root node itself and remaining neighbor nodes are in the $d =1$ subset. Formally , $K=2,l_{ti}(v_{tj})=d(v_{tj},v_{ti})$</p></li><li><p>Spatial configuration partitioning</p><p>Three subsets: 1) the root node itself; 2)centripetal group and 3)  centrifugal group.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113200600.png" alt="image-20210113200600480" style="zoom:33%;" />, where $r_i$ is the average distance from gravity center to joint $i$ over all frames in the training set.</p></li></ul></li><li><p>Learnable edge importance weighting</p><p>One joint appears in multiple body parts should have different importance in modeling the dynamics of these parts. A  learnable mask M is added on every layer of spatial temporal graph convolution.</p></li></ul></li><li><p>The <strong>spatial graph convolution</strong></p><p>$f_{out}(v_{ti})=\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(\mathbb{p}(v_{ti},v_{tj}))\cdot \mathrm{w}(v_{ti},v_{tj})=\\sum\limits_{v_{tj}\in B(v_{ti})}\frac{1}{Z_{ti}(v_{tj})}f_{in}(v_{tj})\cdot \mathrm{w}(l_{ti}(v_{tj}))$, $Z_{ti}(v_{tj})=|{v_{tk}|l_{ti}(v_{tk})=l_{ti}(v_{tj})}|$ is the cardinality of the corresponding subset. It’s for balancing the contributions of different subsets to the output.</p></li><li><p><strong>Spatial temporal modeling</strong></p><p>Extend neighbors so as to include temporally connected joints</p><p>$B(v_{ti})={v_{qj}|d(v_{tj},v_{ti})\le K,|q-t|\le \lfloor\Gamma/2\rfloor}$, where $\Gamma$ is temporal kernel size (controls the temporal range to be included in the neighbor graph). Then , the sampling function is $l_{ST}(v_{qj})=l_{ti}(v_{tj})+(q-t+\lfloor\Sigma/2\rfloor)\times K$.</p></li><li><p>The final convolution formula:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210113222927.png" alt="image-20210113222926988" style="zoom:40%;" />, where $\Lambda_j^{ii}=\sum_k(A_j^{ik})+\alpha$, $\alpha=0.001$ is to avoid empty rows in $A_j$. To add learnable  mask $M$, displace $A_j$ as $A_j\otimes M$.</p></li></ul></li><li><p>Why does it work?</p><ul><li>Parts restrict the modeling of joints trajectories within “local regions” compared with the whole skeleton, thus forming a hierarchical representation of the skeleton sequences.</li></ul></li></ul><h2 id="Experiments">Experiments</h2><ul><li><p>Dataset:</p><ul><li><p>Kinetics (unconstrained action recognition dataset), provides only raw video clips without skeleton data.</p><ul><li><p>Augmentation: To avoid overfitting,two kinds of augmentation are used to replace dropout layers when training on the Kinetics dataset. 1) affine transformations, 2) sampling part of the frames from the whole frame and testing by a whole frame. <font color=Blue>May that’s a way for avoiding the two consecutive frame are too similar?</font></p></li><li><p>Videos to skeletons</p><p>To work on skeletons, openpose is used for extracting. Concretely, resize all videos to the resolution of 340 × 256 and convert the frame rate to 30 FPS. Then OpenPose toolbox is used to estimate the location of 18 joints on every frame of the clips.</p></li><li><p>Final features:</p><p>Finally <strong>the clips are represented by a tensor in shape</strong> $(3,T,18,2)$, where 18 is the number of joints, 2 is the number of people and 3 is the number of features (X,Y,C), C is the confidence.</p></li></ul></li><li><p>NTU-RGBD ( in-house captured action recognition dataset)</p><ul><li>Already annotated with 25 3D joints</li><li>Each clip is guaranteed to have at most 2 subjects</li></ul></li></ul></li><li><p>Training: SGD with a learning rate of 0.01. $lr$ decay by 0.1 after every 10 epochs.</p></li><li><p>Evaluation metrics</p><ul><li><p>Kinetics</p><p>Test on validation set. Using <strong>top-1 and top-5 classification accuracy</strong></p></li><li><p>NTU-RGBD</p><p>Report top-1 recognition accuracy.</p><ul><li><strong>Cross-subject</strong>: Train on one subset of actors and test on the remaining actors.</li><li><strong>Cross-view</strong>: Train on skeletons from camera views 2 and 3, and test on those from camera view 1.</li></ul></li></ul></li></ul><h3 id="Ablation-study">Ablation study</h3><p>Applied on Kinetics dataset.</p><ul><li><p>Spatial temporal graph convolution</p><table><thead><tr><th>Model</th><th>configuration</th></tr></thead><tbody><tr><td><em>baseline-TCN</em></td><td>squeeze Spatial dimension, concatenate all input joint locations to form the input features at each frame $t$.</td></tr><tr><td><em>local convolution</em></td><td>The input data are the same format, but with unshared convolution filters.</td></tr></tbody></table></li><li><p>Partition strategies: same as what described before. <em>Distance partitioning*</em> is as intermediate between the distance partitioning and uni-labeling. The filters in this setting only differs with a scaling factor -1, or to say $\mathrm{w}_0=-\mathrm{w}_1$.</p></li><li><p>Learnable edge importance weighting</p><p>This setting is named as <em>ST-GCN+Imp</em>.</p></li><li><p>Results</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114194501.png" alt="image-20210114194452831" style="zoom:50%;" /><ul><li>Better performance of ST-GCN based models could justify the power of the spatial temporal graph convolution in skeleton based action recognition</li><li>Distance partitioning* achieves better performance than uni-labeling, which again demonstrate <strong>the importance of the partitioning with multiple subsets</strong>.</li><li>ST-GCN model with learnable edge importance weights can learn to express the joint importance.</li></ul></li></ul><h3 id="Compared-with-SOTA">Compared with SOTA</h3><p><strong>Model setting: ST-GCN+Learnable weights+Spatial configuration partitioning</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204800.png" alt="image-20210114204800283" style="zoom:30%;" />     <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210114204722.png" alt="image-20210114204722008" style="zoom:30%;" /></p><ul><li>Kinetics: ST-GCN is able to outperform previous representative approaches, but under-perform methods in RGB or optical flow.</li><li>NTU-RGBD: No data augmentation before training. It outperforms all other selected candidates.</li><li>The skeleton based model ST-GCN can provide complementary information to RGB and optical flow models.</li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Propose ST-GCN, a generic graph-based formulation for modeling dynamic skeletons, which is the first that applies graph-based neural networks for action recognition.</li><li>Propose several principles in designing convolution kernels in ST-GCN to meet the specific demands in skeleton modeling.</li><li>On two large scale datasets for skeleton-based action recognition, the proposed model achieves superior performance as compared to previous methods using hand-crafted parts or traversal rules, with considerably less effort in manual design.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1801.07455.pdf&quot;&gt;Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/yysijie/st-gcn&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
      <category term="skeleton" scheme="http://yoursite.com/tags/skeleton/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Self-supervised Learning on Graphs, Deep Insights and New Directions</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-GNN.html</id>
    <published>2021-01-12T03:00:00.000Z</published>
    <updated>2021-01-24T16:54:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2006.10141.pdf">Self-supervised Learning on Graphs: Deep Insights and New Directions</a></p><p><a href="https://github.com/ChandlerBang/SelfTask-GNN">Codes</a></p><a id="more"></a><h2 id="Why">Why?</h2><ul><li>Nodes in graphs present unique structure information and they are inherently linked indicating not independent and identically distributed (or i.i.d.).</li><li>(SSL) has been introduced in both the image and text domains to alleviate the need of large labeled data by deriving labels for the significantly more unlabeled data.</li><li>To fully exploit the unlabeled nodes for GNNs, SSL can be naturally harnessed for providing additional supervision.</li><li>The challenges of graph to use SSL:<ul><li>graphs are not restricted to these rigid structures.</li><li>each node in a graph is an individual instance and has its own associated attributes and topological structures</li><li>instances (or nodes) are inherently linked and dependent of each other.</li></ul></li></ul><h2 id="Goals">Goals</h2><ul><li><p>Focus on advancing GNNs for node classification where GNNs leverage both labeled and unlabeled nodes on a graph to jointly learn node representations and a classifier that can predict the labels of unlabeled nodes on the graph. Aims at gain insights on when and why SSL works for GNNs and which strategy can better integrate SSL for GNNs.</p></li><li><p><em><strong>Focus on semi-supervised node classification task</strong></em></p><p>$\min\limits_{\theta}\mathcal{L}<em>{task}(\theta,\mathrm{A,X},\mathcal{D}<em>L)=\sum\limits</em>{(v_i,y_i)\in\mathcal{D}<em>L}\ell(f</em>{\theta}(\mathcal{G})</em>{v_i},y_i)$</p></li></ul><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121192249.png" alt="image-20210121192249653" style="zoom:40%;" /><h2 id="Previous-work">Previous work</h2><h3 id="Examples">Examples</h3><ul><li><a href="https://arxiv.org/abs/1902.11038">Multi-stage self-supervised learning for graph convolutional networks on graphs with few labels</a> utilize the clustering assignments of node embeddings as guidance to update the graph neural networks.</li><li><a href="https://arxiv.org/abs/2003.01604">Self-supervised graph representation learning via global context prediction</a> proposed to use the global context of nodes as the supervisory signals to learn node embeddings.</li></ul><h3 id="Basic-pretext-task-on-graphs">Basic pretext task on graphs</h3><h4 id="Structure-information-Adjacency-matrix-mathrm-A">Structure information  (Adjacency matrix $\mathrm{A}$)</h4><p>Construct self-supervision information for the unlabeled nodes based on their local structure information, or how they relate to the rest of the graph</p><h5 id="Local-structure-information"><strong>Local structure information</strong></h5><ul><li><p>Node property</p><ul><li><p>use node degree as a representative local node property for self-supervision while leaving other node properties (or the combination) as one future work</p></li><li><p>Formally, let $d_i=\sum\limits_{j=1}^{N}\mathrm{A}_{ij}$ denote the degree of $v_i$ and construct the associated loss of the SSL pretext task as</p><p>$\mathcal{L}_{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}(f</em>{\theta’}(\mathcal{G})</em>{v_i}-d_i)^2$, where $\mathcal{D}_U$ denote the set of unlabeled nodes and associated pretext task labels in the graph.</p></li><li><p>Assumption: The node property information is related to the specific task of interest.</p></li></ul></li><li><p><strong>EdgeMask</strong></p><ul><li><p>Build pretext task based on the connections between two nodes in the graph. Specifically, <strong>one can first randomly mask some edges and then the model is asked to reconstruct the masked edges</strong>.</p></li><li><p>Formally, first mask $m_e$ edges denotes as the set $\mathcal{M}_e\subset\varepsilon$ and also sample the set $\bar{\mathcal{M}_e}={(v_i,v_j)|v_i,v_j\in\mathcal{V},(v_i,v_j)\notin\varepsilon}$, $|\bar{\mathcal{M}_e}|=|\mathcal{M}_e|=m_e$ . Then the SSL pretext task is to predict whether there exist a link between a given node pair.</p><p>$\mathcal{L}<em>{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\\frac{1}{|\mathcal{M}<em>e|}\sum\limits</em>{(v_i,v_j)\in\mathcal{M}<em>e}\ell(f_w(|f</em>{\theta’}(\mathcal{G})</em>{v_i}-f</em>{\theta’}(\mathcal{G})<em>{v_j}|),1)+\frac{1}{|\bar{\mathcal{M}<em>e}|}\sum\limits</em>{(v_i,v_j)\in\bar{\mathcal{M}<em>e}}\ell(f_w(|f</em>{\theta’}(\mathcal{G})</em>{v_i}-f_{\theta’}(\mathcal{G})_{v_j}|),0)$, where $\ell(\cdot,\cdot)$ is the cross entropy loss, $f_w$ linearly maps to 1-dimension.</p></li><li><p>Expecting to help GNN learn information about local connectivity.</p></li></ul></li></ul><h5 id="Global-structure-information"><strong>Global structure information</strong></h5><p>Not only based on the node itself or limited to its immediate local neighborhood, but also considering the position of the node in the graph.</p><ul><li>PairwiseDistance<ul><li>Maintain global topology information through a pairwise comparison. Or to say, pretext task will be able to distinguish/predict the distance between different node pairs.</li><li>The measurements of distance vary.</li><li>If use the shortest path length $p_{ij}$ as a measure of the distance, then for all node pairs ${(v_i,v_j)|v_i,v_j\in\mathcal{V}}$, they are grouped into four categories: $p_{ij}={1,2,3,\ge4}$, $4$ is because of the computing price and accuracy (the more neighbors , the more unrelated noises are included.) Practically, randomly sample a certain amount of node pairs $S$ used for SSL during epoch. Then the SSL loss is $\mathcal{L}<em>{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|S|}\sum\limits</em>{(v_i,v_j)\in S}\ell(f_w(|f</em>{\theta’}(\mathcal{G})<em>{v_i}-f</em>{\theta’}(\mathcal{G})<em>{v_j}|),C</em>{p_{ij}})$, where $C_{p_{ij}}$ is the corresponding distance category of $p_{ij}$.</li></ul></li><li><em><strong>Distance2Clusters</strong></em><ul><li>Predicting the distance from the unlabeled nodes to predefined graph clusters. Thus enforce the representations to learn a global positioning vector of each of the nodes.</li><li>First partitioning the graph to get $k$ clusters ${C_1,C_2,\cdots,C_k}$ by METIS graph partitioning methods. Denote the node with highest degree as center node $c_j$ in each cluster.</li><li>Formally, the SSL will optimize $\mathcal{L}_{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-d_i|^2$, where $\mathrm{d}_i$ is the distance vector between node $v_i$ and each center.</li></ul></li></ul><h4 id="Attribute-information-Nodes-matrix-mathrm-X">Attribute information (Nodes matrix $\mathrm{X}$)</h4><p>Guide the GNN to ensure certain aspects of node/neighborhood attribute information is encoded in the node embeddings after a SSL attribute-based pretext.</p><ul><li><p>AttributeMask</p><ul><li>Let GNN learn attribute information via pretext</li><li>Randomly mask (e.g. set to zero ) the features of $m_a$ nodes $\mathcal{M}_a\subset\mathcal{V}, |\mathcal{M}<em>a|=m_a$, then SSL will try to construct these features. Formally , $\mathcal{L}</em>{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{M}<em>a|}\sum\limits</em>{v_i\in\mathcal{M}<em>a}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-\mathrm{x}_i|^2$, where $\mathrm{x}_i$ is the dense features after PCA.</li></ul></li><li><p>PairewiseAttrSim</p><ul><li><p>The similarity two nodes have in the input feature space is not guaranteed in the learned representations due to the GNN aggregating features from the two nodes local neighborhoods.</p></li><li><p>Specifically,</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210122233255.png" alt="image-20210122233252928" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210122233404.png" alt="image-20210122233403875" style="zoom:50%;" /></li><li><p>Only constrain the intra-class distance</p></li></ul></li></ul><h3 id="Merge-pretext-task-on-Graphs">Merge pretext task on Graphs</h3><ul><li><p>Joint Training</p><ul><li><p>Optimize the SSL loss (i.e., $\mathcal{L}<em>{self}$) and supervised loss (i.e., $\mathcal{L}</em>{task}$)   simultaneously.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123174253.png" alt="image-20210123174253225" style="zoom:50%;" /></li><li><p>The overall objective is $\min\limits_{\theta,\theta’}\mathcal{L}_{task}(\theta,\mathrm{A,X},\mathcal{D}_L)+\lambda(\theta’,\mathrm{A,X},\mathcal{D}_U)$, where $\lambda$ is the hyperparameter to control the distribution of self-supervision.</p></li></ul></li><li><p>Two-stage training</p><p>Fine tuning the model which is pretrained on pretext task on downstream dataset.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123181120.png" alt="image-20210123181120517" style="zoom:50%;" /></li></ul><h2 id="Analysis">Analysis</h2><ul><li><p>Targets: Understand what SSL information works for GNNs, which strategies can better integrate SSL for GNNs, and further analyze why SSL is able to improve GNNs</p></li><li><p>Datasets: Cora, Citeseer, Pubmed</p></li><li><p>Training: Adam, learning rate $0.01$, $L_2$ regularization $5e-4$, dropout rate $0.5$, $128$ hidden units across all self-supervised information and GCN, top-K=bottom-K=$3$. $\lambda$ in range ${0,0.001,0.01,0.1,1,5,10,50,100,500,1000}$, $m_e,m_a$ in ${10%,20%}$ the size of $|V|$.</p></li><li><p><strong>Two-stage training</strong></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123200058.png" alt="image-20210123200058736" style="zoom:50%;" /><ul><li>the configuration of one graph convolutional layer for feature extraction, one graph convolutional layer for the adaptation of node classification and one linear layer for the adaptation of pretext task works very well for all three strategies</li><li>In most cases, the strategy of “Tune all&quot; achieves the best performance–&gt; fine tune for downstream task is necessary.</li></ul></li><li><p><strong>SSL for GNNs</strong></p><ul><li><p><em>Joint training vs. Two-stage Training</em></p><p><em><strong>Joint training outperforms the Two-stage training in most settings.</strong></em></p></li><li><p><em>What SSL works for GNNs</em></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123202214.png" alt="image-20210123202214101" style="zoom:28%;" /> <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210123202231.png" alt="image-20210123202231858" style="zoom:30%;" /></p><ul><li>SSL for GNNs will improve the accuracy for downstream task</li><li>Across all datasets , the best performing method is a pretext task developed from <em>global structure information.</em></li><li>self-supervised information from both the structure and attributes have potentials</li><li>For the structure information, the global pretext tasks are likely to provide much more significant improvements compared to the local ones.</li></ul></li><li><p><em>Why SSL Works for GNNs</em></p><ul><li>GCN for node classification is naturally semi-supervised that has explored the unlabeled nodes, those (SSL pretext) failed to improve GCNs is argued resulted in GCN has already learned that information.</li><li>GCN is unable to naturally learn the global structure information and employing pairwise node distance prediction as the SSL task can help boost its performance for the downstream task.</li></ul></li><li><p>The capability of pretext representations maintaining similarity</p><p>The most popular similarity for graph is structural equivalence and regular equivalence (规则的等效节点是那些不一定具有相同邻居但具有自身相似的邻居的节点).</p><ul><li>Authors argue pretext task can maintain these two similarities by changing the definition of task (like nodes attribute task or distance between a pair of nodes can maintain structure similarity and regular equivalence.</li></ul></li></ul></li></ul><h2 id="Advanced-pretext-task-on-graphs">Advanced pretext task on graphs</h2><p>Pretext tasks are built with the intuition of adapting the notion of regular equivalence to having neighbors with similar node labels (or regular task equivalence). Specifically, <strong>if every node constructs a pretext vector based on information in regards to the labels from their neighborhood, then two nodes having similar (or dissimilar) vectors will be encouraged to be similar (or dissimilar) in the embedding space.</strong></p><h3 id="Proposed-Tasks">Proposed Tasks</h3><h4 id="Distance2Labeled">Distance2Labeled</h4><ul><li>Modify Distance2Cluster. Propose to predict the distance vector from each node to the labeled nodes (i.e., $\mathcal{V}_L$) as the pretext task. <em>For class $c_j\in{1，\cdots,K}$ and unlabeled node $v_i\in\mathcal{V}_U$, the distance vector $\mathrm{d}_i$ for node  $v_i$ is defined as  three shortest path length (average, minimum, maximum) from $v_i$ to all labeled nodes in class $c_i$.</em></li><li>Formally, the objective is $\mathcal{L}_{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-d_i|^2$.</li></ul><h4 id="ContextLabel">ContextLabel</h4><ul><li>Considering the sparsity of labels, use similarity based function which utilize structure , attributes , and the current labeled nodes to <strong>construct a neighbor label distribution context vector $\bar{\mathrm{y}}_i$</strong> for each nodes as follows: $f_s({\mathrm{A,X},\mathcal{D}_L,\mathcal{V}_U})\rightarrow{\bar{\mathrm{y}}<em>i|v_i\in\mathcal{V}<em>U}$. Specifically , the $c$-th item of $\bar{\mathrm{y}}$ is: $\bar{\mathrm{y}}</em>{ic}=\frac{|\mathcal{N}</em>{\mathcal{V}<em>L}(v_i,c)|+|\mathcal{N}</em>{\mathcal{V}<em>U}(v_i,c)|}{|\mathcal{N}</em>{\mathcal{V}<em>L}(v_i)|+|\mathcal{N}</em>{\mathcal{V}_U}(v_i)|},c=1,\cdots,K$. (For the neighbors of node $v_i$ (including unlabeled and labeled neighbors), the ratio of neighbors in class $c$)</li><li>Formally, the objective is $\mathcal{L}_{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U)=\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-\mathrm{y}_i|^2$</li><li>The labels of nodes (aka $f_s$) can be generated by LP (Label propagation ) or ICA (Iterative Classification Algorithm). But these  will import weak  labels that are too noisy.</li></ul><h4 id="EnsembleLabel">EnsembleLabel</h4><ul><li>Ensemble various functions $f_s$. $\bar{y}<em>i=\arg\max_c\sigma</em>{LP}(v_i)+\sigma_{ICA}(v_i),c=1,\cdots,K$</li><li>The objective is the same as ContextLabel method.</li></ul><h4 id="CorrectedLabel">CorrectedLabel</h4><ul><li>Enhance ContextLabel by iteratively improving the context vectors. GNN $f_{\theta}$ is trained on both the original (e.g., $\bar{\mathrm{y}}_i$) and corrected (e.g., $\hat{\mathrm{y}}_i$) context distributions.</li><li>Formally, the loss is $\mathcal{L}<em>{self}(\theta’,\mathrm{A,X},\mathcal{D}<em>U,\hat{\mathcal{D}}<em>U)=\\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-\bar{\mathrm{y}}<em>i|^2+\alpha(\frac{1}{|\mathcal{D}<em>U|}\sum\limits</em>{v_i\in\mathcal{D}<em>U}|f</em>{\theta’}(\mathcal{G})</em>{v_i}-\hat{\mathrm{y}}<em>i|^2)$, where the 1st and second terms are to fit the original and corrected context distributions respectively, and $\alpha$ controls the contribution from the corrected context distribution. $\hat{y}<em>i=\arg\max_c\frac{1}{p}\sum\limits</em>{l=1}^p\cos(f</em>{\theta’}(\mathcal{G})</em>{v_i},\mathrm{z}</em>{cl}),c=1,\cdots,K$. Where $p$ indicates the prototype nodes in top-$p$ largest $\rho$ values, indicating <strong>the nodes if the measurements ($\rho$) of their neighbors’ label similarity is in top-$p$.</strong>. Concretely, the similarity of labels’ similarity is defined as $\rho_i=\sum\limits_{j=1}^m\mathrm{sign}(\mathrm{S}<em>{ij}-S_c)$, where $\mathrm{S}</em>{ij}$ is the  cosine similarity between two nodes based on their embeddings, $S_c$ indicating a constant value (which is selected as the value rank in top $40%$ in $\mathrm{S}$).</li><li>In other words, the average similarity between $v_i$ and $p$ prototypes is used to represent the similarity between $v_i$ and the corresponding class, and then assign the class $c$ having the largest similarity to $v_i$.</li></ul><h3 id="Experiments-for-evaluating">Experiments for evaluating</h3><ul><li><p>Experiment settings</p><ul><li>Datasets: Cora, Citeseer, Pubmed and Reddit</li><li>Model:  2-layer GCN as the backbone, with hidden units of 128, $L_2$ regularization $5e−4$, dropout rate $0.5$ and learning rate $0.01$. For the SSL loss, the hidden representations from the first layer of GCN are fed through a linear layer to solve SSL pretext task. Jointly train SSL and GCNs. $\lambda$ ranges in ${1, 5, 10, 50, 100, 500}$. $\alpha$ ranges in ${0.5, 0.8, 1, 1.2, 1.5}$.</li><li>Measurements: the average accuracy with standard deviation.</li></ul></li><li><p>Analysis</p><ul><li><p>Performance comparison: Though they argue the performance exist, but seems not that significant. They summarize: <strong>label correction can better</strong> extend label information to unlabeled nodes than ensemble, but it’s much less inefficient. A tradeoff must be taken in.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124113911.png" alt="image-20210124113911642" style="zoom:50%;" /></li><li><p>Fewer Labeled Samples</p><ul><li><p>By randomly sampling 5 or 10 nodes per class for training and the same number of nodes for validation.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124114147.png" alt="image-20210124114147761" style="zoom:50%;" /></li><li><p><strong>SelfTask achieves even greater improvement when the labeled samples are fewer and consistently outperforms the state-of-the-art baselines.</strong></p></li><li><p><font color='red'>Why with fewer samples per class SelfTask can be even better?</font></p></li></ul></li><li><p>Parameter Analysis</p><ul><li><p>Only the sensitivity of the best model <em>SelfTaskCorrectedLabel-ICA</em> is evaluated. Vary $\lambda$ in the range of ${0, 0.1, 0.5, 1, 5, 10, 50, 100}$ and $\alpha$ from 0 to 2.5 with an interval of 0.25.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124114646.png" alt="image-20210124114646842" style="zoom:50%;" /></li><li><p>The performance of this model first increases with the increase of $\lambda$, which controls the contribution of SSL pretext task.</p></li><li><p>The using of correction is confirmed.</p></li><li><p>They don’t report sensitivity on other datasets, which should have been in supplementary.</p></li></ul></li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Present detailed empirical study to understand when and why SSL works for GNNs and which strategy can better work with GNNs.</li><li>Propose a new direction SelfTask to build advanced pretext tasks which further exploit task-specific self-supervised information, and demonstrate that our advanced method achieves state-of-the-art performance.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2006.10141.pdf&quot;&gt;Self-supervised Learning on Graphs: Deep Insights and New Directions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ChandlerBang/SelfTask-GNN&quot;&gt;Codes&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Predicting What You Already Know Helps, Provable Self-Supervised Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-11-notes-paper-SSL-alreadyknow.html</id>
    <published>2021-01-11T20:00:00.000Z</published>
    <updated>2021-01-12T19:38:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/2008.01064.pdf">Predicting What You Already Know Helps: Provable Self-Supervised Learning</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li><p>Pretext tasks</p><ul><li><strong>Reconstruct images from corrupted versions or just part it</strong>: including denoising auto-encoders, image inpainting, and split-brain autoencoder</li><li><strong>Using visual common sense</strong>, including predicting rotation angle, relative patch position, recovering color channels, solving jigsaw puzzle games, and discriminating images created from distortion.</li><li><strong>Contrastive learning</strong>:  learn representations that <strong>bring similar data points closer</strong> while pushing randomly selected points further away or <strong>maximize a contrastive-based mutual information lower bound</strong> between different views</li><li><strong>Create auxiliary tasks</strong>: The natural ordering or topology of data is also exploited in video-based, graph-based or map-based self-supervised learning. For instance, the pretext task is to determine the correct temporal order for video frames.</li></ul></li><li><p>Theory for self-supervised learning: contrastive learning</p><ul><li>Contrastive learning may not work when conditional independence holds only with additional latent variables</li></ul><table><thead><tr><th style="text-align:left">Theory</th><th style="text-align:left">Limitations</th></tr></thead><tbody><tr><td style="text-align:left">Shows shows guarantees for contrastive learning representations on linear classification tasks using a class conditional independence assumption</td><td style="text-align:left">Not handle approximate conditional independence</td></tr><tr><td style="text-align:left">Contrastive learning representations can linearly recover any continuous functions of the underlying topic posterior under a topic modeling assumption for text</td><td style="text-align:left">The <strong>assumption of independent sampling of words</strong> that they exploit is <strong>strong</strong> and <strong>not generalizable to other domains</strong> like images</td></tr><tr><td style="text-align:left">Studies contrastive learning on the hypersphere through intuitive properties like alignment and uniformity of representations</td><td style="text-align:left">No connection made to downstream tasks</td></tr><tr><td style="text-align:left">A mutual information maximization view of contrastive learning</td><td style="text-align:left">Some issues point by paper [45]</td></tr><tr><td style="text-align:left">Explain negative sampling based methods use the theory of noise contrastive estimation</td><td style="text-align:left"><strong>guarantees are only asymptotic and not for downstream tasks.</strong></td></tr><tr><td style="text-align:left">Conditional independence assumptions and redundancy assumptions on multiple views are used to analyze co-training</td><td style="text-align:left">not for downstream task</td></tr></tbody></table></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Forming the pretext tasks:<ul><li>Colorization: can be interpreted as $p(X_1,X_2|Y)=p(X_1|Y)\times p(X_2|Y)$, aka $X_1,X_2$ are independently conditioned on $Y$</li><li>Inpainting: $p(X_1,X_2|Y,Z)=p(X_1|Y,Z)\times p(X_2|Y,Z)$,aka the inpainted $X_2$ is conditionally independent of $X_2$ (the remainder) given $Y,Z$.</li></ul></li><li>The only way to solve the pretext task is to first implicitly predict $Y$ and then predict $X_2$ from $Y$</li></ul></li><li>Limitations:<ul><li>The underlying principles of self-supervised learning are still mysterious since it is a-priori unclear why predicting what we already know should help.</li></ul></li></ul><h2 id="Goals">Goals</h2><p><em><strong>What conceptual connection between pretext and downstream tasks ensures good representations?</strong></em></p><p><em><strong>What is a good way to quantify this?</strong></em></p><h2 id="How">How?</h2><h3 id="Notations">Notations</h3><table><thead><tr><th style="text-align:left">Symbol</th><th style="text-align:left">Meaning</th></tr></thead><tbody><tr><td style="text-align:left">$\mathbb{E}^L[Y</td><td style="text-align:left">X]$</td></tr><tr><td style="text-align:left">$\Sigma_{XY</td><td style="text-align:left">Z}$</td></tr><tr><td style="text-align:left">$X_1,X_2$</td><td style="text-align:left">the input variable and the target random variable for the pretext tasks</td></tr><tr><td style="text-align:left">$Y$</td><td style="text-align:left">label for the downstream task</td></tr><tr><td style="text-align:left">$P_{X_1X_2Y}$</td><td style="text-align:left">the joint distribution over $\mathcal{X}_1 \times \mathcal{X}_2 \times \mathcal{Y}$</td></tr></tbody></table><h3 id="Idea">Idea</h3><ul><li>Under approximate condition independence (CI) (quantified by the norm of a certain partial covariance matrix), show similar sample complexity improvements.</li><li>Testify pretext task helps when CI is approximately satisfied in text domain.</li><li>Demonstrate on a real-world image dataset that a pretext task-based linear model outperforms or is comparable to many baselines.</li></ul><h3 id="Formalize-SSL-with-pretext-task">Formalize SSL with pretext task</h3><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111171426.png" alt="image-20210111171415185" style="zoom:50%;" /><p>It will be estimated by:</p><ul><li><strong>approximation erro</strong>r:<img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173318.png" alt="image-20210111173318840" style="zoom:33%;" />, where $f^*=\mathbb{E}[Y|X_1]$ is the optimal predictor for the task</li><li><strong>estimation error</strong>: <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210111173520.png" alt="image-20210111173520930" style="zoom:25%;" />, it’s the difference between Predicting $Y$ directly by $X_1$ and Predicting by the representations from pretext task</li></ul><h2 id="Experiments">Experiments</h2><ul><li></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks</li><li>Quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) <strong>allows us to learn representations that can solve the downstream task with drastically reduced sample complexity</strong> by just training a linear layer on top of the learned representation.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/2008.01064.pdf&quot;&gt;Predicting What You Already Know Helps: Provable Self-Supervised Learning&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Colorful Image Colorization</title>
    <link href="http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-10-notes-paper-SSL-colorimage.html</id>
    <published>2021-01-10T22:15:39.000Z</published>
    <updated>2021-01-12T21:19:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1603.08511.pdf">Colorful Image Colorization</a></p><p>Code <a href="http://richzhang.github.io/colorization/">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><p>Predicting colors in free way: taking the image’s $L$ channel as input and its $ab$ channels as the supervisory signal–&gt; but  tend to look desaturated, one explanation is using loss functions that encourage conservative predictions</p><ul><li><p>Non-parametric methods:  given an input grayscale image, first define one or more color reference images. Then, transfer colors onto the input image from analogous regions of the reference image(s).</p></li><li><p>Parametric methods: learn prediction functions from large datasets of color images at training time, posing the problem as either regression onto continuous color space or classification of quantized color values. --&gt; Work in this paper is also classification task.</p></li><li><p>Concurrent work on colorization</p><table><thead><tr><th>Paper</th><th style="text-align:center">loss</th><th>CNNs</th><th>Dataset</th></tr></thead><tbody><tr><td>Larsson et al.</td><td style="text-align:center">un-rebalanced classification loss</td><td>hypercolumns on a VGG</td><td>ImageNet</td></tr><tr><td>Iizuka et al.</td><td style="text-align:center">regression loss</td><td>two-stream architecture in which fuse global and local features</td><td>Places</td></tr><tr><td>This paper</td><td style="text-align:center">classification loss, with rebalanced rare classes,</td><td>a single-stream, VGG-styled network with added depth and dilated convolutions</td><td>ImageNet</td></tr></tbody></table></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Even in gray images, the semantics of the scene and its surface texture provide ample cues for many regions in each image</li><li>Color prediction is inherently multimodal --&gt; sparks for a loss tailored to their work</li></ul></li><li>Limitations:<ul><li>Loss only cares Euclidean distance: If an object can take on a set of distinct ab values, the optimal solution to the Euclidean loss will be the mean of the set. In color prediction, this averaging effect favors grayish, desaturated results. Additionally, if the set of plausible colorizations is non-convex, the solution will in fact be out of the set, giving implausible results.</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Design colorization based pretext task to get a good image semantic representations: <strong>produce a plausible colorization that could potentially fool a human observer</strong></p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li><strong>Produce vibrant colorization</strong>: Predict a distribution of possible colors for each pixel. Then, re-weight the loss at training time to emphasize rare colors. This encourages the model to exploit the full diversity of the large-scale data on which it is trained. Lastly, produce a final colorization by taking the annealed mean of the distribution.</li><li>Evaluate synthesized images:  set up a “colorization Turing test”.</li></ul><h3 id="Data-Preparation">Data Preparation</h3><ul><li>Quantize the $ab$ output space into bins with grid size $10$ and keep the $Q = 313$ values which are in-gamut. Then this is the label $Z$ of each pixel. Formally, denote the raw label as $Y$, then $Z = H^{−1}_{gt} (Y)$, which converts ground truth color $Y$ to vector $Z$.</li></ul><h3 id="Implementation">Implementation</h3><ul><li><p>Model</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110175555.png" alt="image-20210110175554683" style="zoom:30%;" /></li><li><p>Loss function</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210112160157.png" alt="image-20210110180438146" style="zoom:33%;" /><p>where $v(·)$ is a weighting term that can be used to re-balance the loss based on color-class rarity.</p><ul><li><p>Re-balancing</p><p>The distribution of $ab$ values in natural images is strongly biased towards values with low $ab$ values.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110233506.png" alt="image-20210110233506301" style="zoom:33%;" />, where $\tilde{p}$ the  empirical probability of colors in the quantized ab space $p\in \Delta Q$ from the full ImageNet training set and smooth the distribution with a Gaussian kernel $G_{\sigma}$. $\lambda=\frac{1}{2}, \sigma=5$ worked well.</p></li></ul></li><li><p>Inferring point estimates</p><p>map probability distribution $\hat{Z}$ to color values $\hat{Y}$ with function $\hat{Y} = H(\hat{Z})$</p></li></ul><p>They interpolate by re-adjusting the temperature $T$ of the softmax distribution, and taking the mean of the result. Lowering the temperature $T$ produces a more strongly peaked distribution, and setting $T\rightarrow 0$ results in a 1-hot encoding at the distribution mode. They find that $T=0.38$ captures the vibrancy of the mode while maintaining the spatial coherence of the mean.</p><h2 id="Experiments">Experiments</h2><ul><li>Dataset: ImageNet</li><li>Base models</li></ul><table><thead><tr><th>Model Name</th><th style="text-align:center">Loss</th><th>Train</th></tr></thead><tbody><tr><td>Ours(full)</td><td style="text-align:center">classification loss</td><td>from scratch with kmeans initialization, ADAM solver for about 450K iterations. $\beta_1 = .9, \beta_2 = .99$, and weight decay = $10^{−3}$ . Initial learning rate was $3 × 10^{−5}$ and dropped to $10^{−5}$ and $3 × 10^{−6}$ when loss plateaued, at 200k and 375k iterations, respectively.</td></tr><tr><td>Ours(class)</td><td style="text-align:center">classification loss withou rebalancing ($\lambda=1$)</td><td>similar training protocol as Ours(full)</td></tr><tr><td>Ours(L2)</td><td style="text-align:center">L2 regression loss</td><td>same training protocol</td></tr><tr><td>Ours(L2,ft)</td><td style="text-align:center">L2 regression loss</td><td>fine tuned from our full classification with rebalancing network</td></tr><tr><td>Larsson et al.</td><td style="text-align:center"></td><td>CNN method</td></tr><tr><td>Dahl</td><td style="text-align:center">L2 regression loss</td><td>a Laplacian pyramid on VGG features</td></tr><tr><td>Gray</td><td style="text-align:center">–</td><td>every pixel is gray, with $(a, b) = 0$</td></tr><tr><td>Random</td><td style="text-align:center">–</td><td>Copies the colors from a random image from the training set</td></tr></tbody></table><h3 id="Colorization-quality">Colorization quality</h3><ul><li>AMT: participants confirm their results. They argue that their work produce a more prototypical appearance for those are poorly white balanced</li><li>Semantic interpretability (VGG classification): Are the results  realistic enough colorizations to be interpretable to an off-the-shelf object classifier? They check it by  by feeding their fake colorized images to a VGG network  that was trained to predict ImageNet classes from real color photos.<ul><li>The result is $3.4%$ lower than Larsson’s.</li><li>Without any additional training or fine-tuning, one can improve performance on grayscale image classification, simply by colorizing images with our algorithm and passing them to an off-the-shelf classifier.</li></ul></li><li>Raw accuracy (AuC):<ul><li>L2 metric can achieve accurate colorizations, but has difficulty in optimization from scratch</li><li>class-rebalancing in the training objective achieved its desired effect</li></ul></li><li>Compared with others<ul><li>LEARCH: On SUN dataset, authors have $17.2%$ on AMT task while LEARCH has $9.8%$</li></ul></li></ul><h3 id="Cross-channel-encoding-as-SSL-Feature-learning">Cross-channel encoding as SSL Feature learning</h3><ul><li><p>Datasets: ImageNet, PASCAL (fine tuned after training on ImageNet)</p></li><li><p>Backbone: AlexNet</p></li><li><p>Settings</p><ul><li>ImageNet: fixing the extractor and retrain the classifier (softmax layer) by labels</li><li>PASCAL: : (1) keeping the input grayscale by disregarding color information (Ours (gray)) and (2) modifying conv1 to receive a full 3-channel $Lab$ input, initializing the weights on the $ab$ channels to be zero (Ours (color)).</li></ul></li><li><p>Summary</p><ul><li>For ImageNet, there is a $6%$ performance gap between color and grayscale inputs. Except for the 1st layer, representations from other deeper layers catch and outperform most methods, indicating that <strong>solving the colorization task encourages representations that linearly separate semantic classes in the trained data distribution</strong></li><li>On PASCAL, when conv1 is frozen, the network is effectively only able to interpret grayscale images.</li></ul></li></ul><h3 id="The-properties-of-network">The properties of network</h3><ul><li><p>Is it exploiting low-level cues?</p><p>Given a grayscale Macbeth color chart as input, it was unable to recover its colors. On the other hand, given two recognizable vegetables that are roughly <strong>isoluminant</strong>, <strong>the system is able to recover their color</strong>.</p></li><li><p>Does it learn multimodal color distributions ?</p><p>Take effective dilation ( the spacing at which consecutive elements of the convolutional kernel are evaluated, relative to the input pixels, and is computed by the product of the accumulated stride and the layer dilation) as the measurement.  Through each convolutional block from conv1 to conv5, the effective dilation of the convolutional kernel is increased. From conv6 to conv8, the effective dilation is decreased</p></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Designing an appropriate objective function that handles the multimodal uncertainty of the colorization problem and captures a wide diversity of colors</li><li>Introducing a novel framework for testing colorization algorithms, potentially applicable to other image synthesis tasks</li><li>Setting a new high-water mark on the task by training on a million color photos.</li><li>Introduce the colorization task as a competitive and straightforward method for self-supervised representation learning, achieving state-of-the-art results on several benchmarks.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1603.08511.pdf&quot;&gt;Colorful Image Colorization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;http://richzhang.github.io/colorization/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Revisiting Self-Supervised Visual Representation Learning</title>
    <link href="http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-09-notes-paper-SSL-revisit-cv.html</id>
    <published>2021-01-09T21:21:00.000Z</published>
    <updated>2021-01-12T19:33:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf">Revisiting Self-Supervised Visual Representation Learning</a></p><p>Code <a href="https://github.com/google/revisiting-self-supervised">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li>robotics: the result of interacting with the world, and the fact that multiple perception modalities simultaneously get sensory inputs are strong signals for pretext</li><li>videos: the synchronized cross-modality stream of audio, video, and potentially subtitles, or of the consistency in the temporal dimension</li><li>image  datasets:<ul><li>Patch-based methods: E.g.: predicting the relative location of image patches; “jigsaw puzzle”</li><li>Image-level classification tasks:<ul><li>RotNet, create class labels by clustering images, image inpaiting, image colorization, split-brain and motion segmentation prediction;</li><li>Enforce structural constraints on the representation space: an equivariance relation to match the sum of multiple tiled representations to a single scaled representation;  predict future patches in via autoregressive predictive coding</li><li>Combining multiple pretext task: E.g. extend the “jigsaw puzzle” task by combining it with colorization and inpainting; Combining the jigsaw puzzle task with clustering-based pseudo labels ( Jigsaw++) ;  make one single neural network learn all of four different SSL methdos in a multi-task setting; combined the selfsupervised loss GANs objective</li></ul></li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>Expensive labeled data for supervised task</li></ul></li><li>Limitations:<ul><li>Previous works mostly concentrate on pretext task, but didn’t pay much attention to the choice of backbones etc.</li></ul></li></ul><h2 id="Goals">Goals</h2><p><strong>An optimal CNN architecture for pretext task</strong>,  investigating the influence of architecture design on the representation quality.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li>a comparison of different self-supervision methods using a unified neural network architecture, but with the goal of combining all these tasks into a single self-supervision task</li></ul><h3 id="Implementation">Implementation</h3><h4 id="Family-of-CNNs">Family of CNNs</h4><ul><li><p>variants of ResNet:</p></li><li><p><strong>ResNet50</strong>, the output before task-specific logits layer is named as $pre-logits$. explore $k \in {4, 8, 12, 16}$, resulting in pre-logits of size $2048, 4096, 6144$ and $8192$ respectively. $k$ is the widening factor.</p></li><li><p><strong>ResNet v1</strong>: ???batch normalization (BN) right after each convolution and before activation???</p></li><li><p><strong>ResNet v2</strong>: ?</p></li><li><p><strong>ResNet (-)</strong>: without ReLU preceding the global average pooling</p></li><li><p>a batch-normalized <strong>VGG</strong> architecture since VGG is structurally close to AlexNet. BN between CNN and activation, VGG19.</p></li><li><p><strong>RevNets</strong>: stronger invertibility guarantees so as to compare with ResNets. The residual unit used here is equivalent to double application of the residual unit.</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110133429.png" alt="image-20210110133429332" style="zoom:50%;" />, check <a href="https://papers.nips.cc/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf">here</a> for details. Apart from this slightly complex residual unit, others are the same as ResNet.</p></li></ul><h4 id="Family-of-pretext-tasks">Family of pretext tasks</h4><ul><li><strong>Rotation</strong>: same as RotNet, ${0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}}$</li><li><strong>Exemplar</strong>: triplet loss</li><li><strong>Jigsaw</strong>: recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches was performed. Patches are sampled with a random gap between them. Each patch is then independently converted to grayscale with probability $\frac{2}{3}$ and normalized to zero mean and unit standard deviation. Extract final image representations by averaging representations of 9 cropped patches.</li><li><strong>Relative patch location</strong>: predicting the relative location of two given patches of an image. Extract final image representations by averaging representations of 9 cropped patches.</li></ul><h4 id="Evaluation-of-the-quality-of-learned-representations">Evaluation of the quality of learned representations</h4><ul><li>Idea: <strong>Using learned representations for training a linear logistic regression model to solve multiclass image classification tasks</strong> (downstream tasks). All representations come from pre-logits level.</li><li>Details: the linear logistic regression model is trained by L-BFGS. But for comparison, using SGD with momentum and use data augmentation during training.</li></ul><h2 id="Experiments">Experiments</h2><ul><li>Datasets</li></ul><table><thead><tr><th>Datasets</th><th style="text-align:center">Train</th><th>Test</th></tr></thead><tbody><tr><td>ImageNet</td><td style="text-align:center">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr><tr><td>Places 205</td><td style="text-align:center">training set</td><td>Most on validation set, only Table 2 on official test set</td></tr></tbody></table><h3 id="Pretext-CNNs-Downstream">Pretext? CNNs? Downstream?</h3><ul><li><p>Pretext and its preferred CNN architecture: <strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110151545.png" alt="image-20210110151545316" style="zoom:30%;" /></li><li><p>The generalization of representations from pretext tasks: each pretext task can be generalized to other dataset. Check the trendings in figure 2.</p></li><li><p>Optimal CNNs for Pretext and downstream tasks: not consistent. But after selecting the right architecture for each self-supervision and increasing the widening factor, models significantly outperform previously reported results.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155158.png" alt="image-20210110155157874" style="zoom:45%;" /></li><li><p><strong>Better performance on the pretext task does not always translate to better representations</strong>: Performance on pretext cannot be used to reliably select the model architecture.</p></li></ul><h3 id="CNNs-architecture">CNNs architecture</h3><ul><li>Skip-connection: For VGG, representations deteriorate towards the end of the network cause models specialize to the pretext task in the later layers. ResNet prevent this deterioration. They argue that this is because <strong>ResNet’s residual units being invertible under some conditions</strong> and confirm this by RevNet.</li><li>Depth of CNNs: For residual architectures, the pre-logits are always best.</li><li>Model-width and representation size:<ul><li>whether the increase in performance is due to increased network capacity or to the use of higher-dimensional representations, or to the interplay of both? To answer it, authors disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer.</li><li><strong>Model-width and representation size both matter independently, and larger is always better.</strong></li><li>SSL techniques are likely to <strong>benefit from using CNNs with increased number of channels</strong> across wide range of scenarios, even under low-data regime.</li></ul></li></ul><h3 id="Evaluate-the-quality-of-representations">Evaluate the quality of representations</h3><ul><li><strong>A linear model is adequate</strong>:  MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged</li></ul>  <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210110155830.png" alt="image-20210110155829820" style="zoom:50%;" /><ul><li>To train <strong>linear model, SGD optimization hyperparameters:</strong>  very long training (≈ 500 epochs) results in higher accuracy</li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Most affect performance in the fully labeled setting, may significantly affect performance in the selfsupervised setting.</li><li>the quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</li><li>Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations</li><li>The evaluation procedure, where a linear model is trained on a fixed visual representation using stochastic gradient descent, is sensitive to the learning rate schedule and may take many epochs to converge</li><li><strong>neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures.</strong>–&gt;<strong>pretext tasks for self-supervised learning</strong> should not <strong>be considered</strong> in isolation, but <strong>in conjunction with underlying architectures</strong>.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&quot;&gt;Revisiting Self-Supervised Visual Representation Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code &lt;a href=&quot;https://github.com/google/revisiting-self-supervised&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Representation Learning by Predicting Image Rotations</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-rotation.html</id>
    <published>2021-01-09T04:11:12.000Z</published>
    <updated>2021-01-12T20:56:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1803.07728.pdf">Unsupervised Representation Learning by Predicting Image Rotations</a></p><p>Codes <a href="https://github.com/gidariss/FeatureLearningRotNet">here</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><p>How to get <strong>a high-level image semantic representation using unlabeled data</strong></p><ul><li>SSL:  defines an annotation free pretext task, has been proved as good alternatives for transferring on other vision tasks. E.g.: colorize gray scale images, predict the relative position of image patches, predict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames.</li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li><strong>the attention maps are equivariant w.r.t. the image rotations, check appendix A.</strong></li></ul></li><li>Limitations<ul><li>supervised feature learning has the main limitation of requiring intensive manual labeling effort</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Provide a “self-supervised” formulation for image data, a <strong>self defined supervised task involving predicting the transformations used for image.</strong> The model won’t have access to the initial image.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li>Concretely, define the geometric transformations as the image rotations by 0, 90, 180, and 270 degrees. Thus, the ConvNet model is trained on the 4-way image classification task of recognizing one of the four image rotations.</li></ul><h3 id="Implementation">Implementation</h3><h4 id="Data-preparation">Data preparation</h4><ul><li><p>2D image Rotation</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109124726.png" alt="image-20210109124726420" style="zoom:40%;" /><table><thead><tr><th>Operations</th><th style="text-align:center">Implementation</th></tr></thead><tbody><tr><td>+90</td><td style="text-align:center">transpose then flip vertically</td></tr><tr><td>+180</td><td style="text-align:center">flip  vertically then flip horizontally</td></tr><tr><td>+270</td><td style="text-align:center">flip vertically then transpose</td></tr></tbody></table></li></ul><h4 id="Learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function:</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109123815.png" alt="image-20210109123815880" style="zoom:45%;" />, where $F(\cdot)$ is the predicted probability of the geometric transformation with label $y$ and $\theta$ are the learnable parameters of model $F(\cdot)$, and $g(X_i|y)$ is the transformed image with transformation $y$.</p></li><li><p><strong>Why does it works?</strong></p><ul><li>To work for this pretext task, extractor has to  <strong>understand the concept of the objects depicted in the image</strong>. Models must learn to localize salient objects in the image, recognize their orientation and object type, and then relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images.</li><li>Easy to be implemented by flipping and transpose, no chance for importing low-level visual artifacts so as to avoid trivial features (which have no practical value)</li><li>Operations are easy to be recognized manually.</li></ul></li></ul><h2 id="Experiments">Experiments</h2><h3 id="CIFAR-object-recognition">CIFAR: object recognition</h3><ul><li>Dataset:</li></ul><table><thead><tr><th>Datasets</th><th>Preprocess</th></tr></thead><tbody><tr><td>CIFAR-10</td><td>Rotations</td></tr></tbody></table><ul><li>Training:  SGD with batch size 128, momentum 0.9, weight decay $5e−4$ and $lr$ of 0.1. We drop the learning rates by a factor of 5 after epochs 30, 60, and 80. 100 epochs. Each time feeding with all 4 images.</li><li>Summary<ul><li><em>The learned feature hierarchies</em>: convnet with different number of layers. <strong>Representations from the 2nd block</strong> are good, and <strong>increasing the total depth</strong> of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers.</li><li><em>The quality of the learned features w.r.t. the number of recognized rotations</em>: 4 discrete rotations outperform.</li><li><em>Compared with previous work</em> : almost the same as the NIN  supervised model. Fine-tuned the unsupervised learned features further improves the classification performance.</li><li><em>Correlation between object classification task and rotation prediction task</em>: The representations from pretext make classifier converge faster compared with the classifier trained from scratch.</li><li><em>Semi-supervised setting</em>: pretrained on the whole dataset without labels, then fine-tuned on a small labeled subset. It exceeds the supervised model when the number of examples per category drops below 1000.</li></ul></li></ul><h3 id="Others-classification-object-detection-segmentation">Others: classification, object detection , segmentation</h3><ul><li><p>Dataset: ImageNet, Places, and PASCAL VOC.</p><table><thead><tr><th>Task</th><th>Datasets</th></tr></thead><tbody><tr><td>Classification</td><td>Pretrained on ImageNet, then test on ImageNet, Places, and PASCAL VOC.</td></tr><tr><td>Object detection</td><td>PASCAL VOC</td></tr><tr><td>Object segmentation</td><td>PASCAL VOC</td></tr></tbody></table></li><li><p>Backbones: AlexNet without local response normalization units, dropout units, or groups in the  colvolutional layers while it includes batch normalization units after each linear layer</p></li><li><p>Pretrained: on ImageNet, SGD with batch size 192, momentum 0.9, weight decay $5e − 4$ and $lr$ of 0.01. Learning rates are dropped by a factor of 10 after epochs 10, and 20 epochs. Trained in total for 30 epochs.</p></li><li><p>Summary:</p><ul><li><p>ImageNet classification task: surpasses all the other unsupervised methods by a significant margin, narrows the performance gap between unsupervised features and supervised features.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154626.png" alt="image-20210109154626599" style="zoom:50%;" /></li><li><p>Transfer learning evaluation on PASCAL VOC: fine tuning, used weight rescaling proposed by Krahenbuhl et al.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154831.png" alt="image-20210109154831162" style="zoom:60%;" /></li><li><p>Places classification task: the learnt features are evaluated w.r.t. their generalization on classes that were “unseen” during the unsupervised training phase</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210109154953.png" alt="image-20210109154952935" style="zoom:50%;" /></li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Propose a new self-supervised task that is very simple and at the same time.</li><li>Rotationsod under various settings (e.g. semi-supervised or transfer learning settings) and in various vision tasks (i.e., CIFAR-10, ImageNet, Places, and PASCAL classification, detection, or segmentation tasks).</li><li>They argue this self-supervised formulation demonstrates state-of-the-art results with dramatic improvements w.r.t. prior unsupervised approaches, and narrows the gap between unsupervised and supervised feature learning.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1803.07728.pdf&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Codes &lt;a href=&quot;https://github.com/gidariss/FeatureLearningRotNet&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Unsupervised Visual Representation Learning by Context Prediction</title>
    <link href="http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-08-notes-paper-SSL-cv-context.html</id>
    <published>2021-01-08T21:22:12.000Z</published>
    <updated>2021-01-12T19:33:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf">Unsupervised Visual Representation Learning by Context Prediction</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><p>How to get <strong>a good image representation</strong></p><ul><li>The latent variables of an appropriate generative model.  --&gt; generative models<ul><li>But given an image, inferring the latent structure is <strong>intractable</strong> for even relatively simple models --&gt; to fix, use <strong>sampling</strong> to perform approximate inference.</li></ul></li><li>An embedding that can discriminate the semantics in images by distances of them. – create a supervised “pretext” task. But hard to tell whether the predictions themselves are correct.<ul><li>Reconstruction-based:  E.g., denoising autoencoders (reconstruction ), sparse autoencoders (reconstruction + sparsity penalty )</li><li>Context prediction: “skip-gram” to “filling the blank” task, and convert the prediction task to discriminate task like discriminating between real images vs. images where one patch has been replaced by a random patch from elsewhere in the dataset. But not hard enough for high-level representations</li><li>Discover object categories using hand-crafted features and various forms of clustering. But they will lose shape information. To keep more shape information, some take contour extraction or defining similarity metrics.</li><li>Video-based: since the identity of objects remains unchanged -</li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>the difficulties for generalizing CNNs models on Internet -scale datasets</li><li>context has proven to be a powerful source of automatic supervisory signal for learning representations --&gt; context can be regarded as a ‘pretext’ task to force the model to learn a good word embedding</li><li>current reconstruction-based algorithms struggle with low-level phenomena, like stochastic textures, making it hard to even measure whether a model is generating well.</li></ul></li><li>Limitations:<ul><li>generative models are rather efficiently on smaller datasets but burden on high-resolution  natural images</li><li>Some are too simple for extracting high-level representations</li><li>Hard to tell whether the model has obtained good representations.</li></ul></li></ul><h2 id="Goals">Goals</h2><p>Provide a “self-supervised” formulation for image data, a supervised task involving predicting the context for a patch.</p><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li>Hypothesis: Doing well on predicting patches’ positions requires understanding scenes and objects–&gt; a good visual representation</li><li>Concretely, sample random pairs of patches in one of eight spatial configurations, and present each pair to a machine learner. The algorithm must then guess the position of one patch relative to the other.</li></ul><h3 id="Implementation">Implementation</h3><h4 id="Data-preparation">Data preparation</h4><ul><li><p>Two patches are fed into network</p></li><li><p>Given an image, one patch will be sampled uniformly, then according to the position of this sampled patch, then 2nd patch will be sampled randomly from the eight possible neighboring locations.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108181759.png" alt="image-20210108181757911" style="zoom:33%;" /></li><li><p>including a gap between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</p></li><li><p>For some images ( chromatic aberration), after solving the relative location task (like by detecting the separation between green and magenta (red + blue). ), this problem will be relaxed.</p><ul><li>Shift green and magenta toward gray</li><li>Color dropping : randomly drop 2 of the 3 color channels from each patch and replace them by gaussian noise.</li></ul></li></ul><h4 id="Learning-algorithm">Learning algorithm</h4><ul><li><p>Siamese network based on AlexNet. But not all layers share weights, LRN (local response normalization ) layers won’t.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210108183110.png" alt="image-20210108183110662" style="zoom:33%;" /></li><li><p><strong>Why does it works?</strong></p><ul><li>Avoid ‘trivial’ shortcuts (like boundary patterns or textures continuing between patches)–&gt; including a gap (up to 48 pixels) between patches (patches are not aligned side by side ), also randomly jitter each patch location by up to 7 pixels</li><li>Enhancing performance on images with  chromatic aberration.</li></ul></li></ul><h2 id="Experiments">Experiments</h2><p>Pre-Training: SGD+BN+high momentum, 4 weeks on K40 GPU.</p><table><thead><tr><th>Datasets</th><th style="text-align:center">Resizing</th><th>Preprocess</th></tr></thead><tbody><tr><td>ImageNet</td><td style="text-align:center">$150K\sim450K$ total pixels</td><td>1. sample patches at resolution $96\times 96$<br />2. mean subtraction, projecting or dropping colors, and randomly downsampling some patches to as little as 100 total pixels, and then upsampling it.</td></tr></tbody></table><h3 id="Ability-on-semantic">Ability on semantic</h3><p>Does it get similar representations for patches with similar semantics?</p><ul><li><p>check nearest neighbors by normalized correlation of $fc6$'s output. Compared with results from  random initialized model and ImageNet AlexNet.</p></li><li><p>Summary</p><ul><li>in a few cases, random (untrained) ConvNet also does reasonably well</li><li>the representations from proposed model often capture the semantic information</li></ul></li></ul><h3 id="Learnability-of-Chromatic-Aberration">Learnability of Chromatic Aberration</h3><ul><li>Patches displayed similar aberration tend to be predicted at the same location.</li><li>The effect of  color projection operation is canceled for this kind of images.</li></ul><h3 id="Object-detection">Object detection</h3><ul><li>Dataset : VOC 2007</li><li>Train: fine-tune the pretrained model (model is slight different with the previous one considering the image size in VOC) on VOC 2007.</li><li>Test: output from $fc7$ is taken.</li><li>Summary<ul><li>Pre-trained model outperforms the one trained from scratch</li><li>Obtained the best result on VOC 2007 without using labels</li><li>Robustness of the representations for one object in different datasets: acceptable</li></ul></li></ul><h3 id="Visual-data-mining">Visual data mining</h3><ul><li>Task : aims to use a large image collection to discover image fragments which happen to depict the same semantic objects</li><li>Specification for this task: sample a constellation of four adjacent patches from an image, after finding the top 100 images which have the strongest matches for all four patches, then use a type of geometric verification to filter away the images where the four matches are not geometrically consistent. Finally, rank the different constellations by counting the number of times the top 100 matches geometrically verify.</li><li>To define the geometric verification: first compute the best-fitting square $S$ to the patch centers (via least-squares), while constraining that side of $S$ be between 2/3 and 4/3 of the average side of the patches. Then compute the squared error of the patch centers relative to $S$ (normalized by dividing the sum-of-squared-errors by the square of the side of $S$). The patch is geometrically verified if this normalized squared error is less than 1.</li><li>Test: VOC 2011, Street View images from Paris</li><li>Summary<ul><li>The discovery of birds and torsos is good</li><li>The gains in terms of coverage, suggesting increased invariance for learned features</li><li>The pretext task is difficult:  for a large fraction of patches within each image, the task is almost impossible</li><li>Limitations: some loss of purity, and cannot currently determine an object mask automatically (although one could imagine dynamically adding more sub-patches to each proposed object).</li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>instance-level supervision appears to improve performance on category-level tasks</li><li>The proposed model is sensitive to objects and the layout of the rest of the image</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf&quot;&gt;Unsupervised Visual Representation Learning by Context Prediction&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Paper--Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
    <link href="http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html"/>
    <id>http://yoursite.com/posts/notes/2021-01-06-notes-paper-SSL-examplarcnn.html</id>
    <published>2021-01-06T16:17:12.000Z</published>
    <updated>2021-01-12T20:58:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1406.6909.pdf">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</a></p><a id="more"></a><h2 id="Why">Why?</h2><h3 id="Previous-work">Previous work</h3><ul><li>Supervised : labeled data with a specific CNN<ul><li>directly penalizing the derivative of the output with respect to the magnitude of the transformations, but will be sensitive to the magnitude of the applied transformation.</li></ul></li><li>Unsupervised: learning invariant representations<ul><li>Directly modeling the input distribution and are hard for jointly training multiple layers of a CNN<ul><li><strong>autoencoders</strong>:  denoising auto encoders, say reconstruct data from randomly perturbed input samples; or learn representations from videos by  enforcing a temporal slowness constraint on the feature representation learned by a linear autoencoder.</li><li>invariant to local transformations</li></ul></li><li>most aims at regularization of the latent representation</li></ul></li><li>Semi-supervised<ul><li>Regularization supervised algorithms by unlabeled data: self-training, entropy regularization</li></ul></li></ul><h3 id="Summary">Summary</h3><ul><li>Observations<ul><li>the features learned by one network often generalize to new datasets</li><li>a network can be adapted to a new task by replacing the loss function and possibly the last few layers of the network and fine-tuning it to the new problem</li></ul></li><li>Limitations:<ul><li>the need for huge labeled datasets to be used for the initial supervised training</li><li>the transfer becomes less efficient the more the new task differs from the original training task</li></ul></li></ul><h2 id="Goals">Goals</h2><p>a more general extractor using unlabeled data. The extractor should satisfy two requirements:</p><ul><li>there must be at least one feature that is similar for images of the same category $y$ (invariance);</li><li>there must be at least one feature that is sufficiently different for images of different categories (ability to discriminate)</li></ul><h2 id="How">How?</h2><h3 id="Idea">Idea</h3><ul><li>creating an auxiliary task + invariant features to transformations</li></ul><h3 id="Implementation">Implementation</h3><h4 id="Data-preparation">Data preparation</h4><ul><li>Do random selected transformation (from  a predefined  family of transformations) for sampled patches (regions containing considerable gradients so that sample a patch with probability proportional to mean squared gradient magnitude within the patch )</li><li>The family of transformations<ul><li>translation</li><li>scaling</li><li>rotation</li><li>contrast: PCA and HSV</li><li>color: works on HSV space</li><li>blur etc.</li></ul></li><li>Before feeding into model, do normalization (subtract the mean of each pixel over the whole resulting dataset)</li><li>Labeling: all transformed patches from the same seed patch are labeled by the same index</li></ul><h4 id="Learning-algorithm">Learning algorithm</h4><ul><li><p>Loss function :</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155621.png" alt="image-20210107155618997" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107155911.png" alt="image-20210107155854389" style="zoom:30%;" /><ul><li><p>After transformations, the loss for a whole class (augmented by the same seed patch ) can be taken as</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210107162118.png" alt="image-20210107162118193" style="zoom:50%;" />, notice the 2nd and the 4th can be canceled.</p><ul><li>The 1st term: enforces correct classification of the average representation $\mathbb{E}<em>\alpha[g(T</em>\alpha x_i)]$ for a given input sample</li><li>The 2nd term: a regularizer enforcing all $ h(T_\alpha x_i)$ to be close to their average value, i.e., the feature representation is sought to be approximately invariant to the transformations $ T_\alpha$, note the convergence to global minimum is listed at appendix.</li></ul></li></ul></li><li><p><strong>Why does it works?</strong></p><ul><li>Previous works mostly focus on modeling the input distribution $p(x)$, based on the assumption that a good model of  $p(x)$ contains information about the category distribution  $p(y|x)$.  Therefore, to get the invariance, one will do regularization of the latent representation and obtain representation by reconstruction .</li><li>Their work  does not directly model the input distribution  $p(x)$  but learns a representation that discriminates between input samples. They argue that this <strong>allows more DOF to model the desired variability of a sample and avoid task-unnecessary reconstruction.</strong></li><li>However, their work will <strong>fail on color-relied task</strong></li></ul></li></ul><h2 id="Experiments">Experiments</h2><h3 id="Classification">Classification</h3><ul><li><p>Datasets: <strong>STL-10</strong>, CIFAR-10, Caltech-101 and Caltech-256. report mean and standard deviation</p><table><thead><tr><th>Datasets</th><th style="text-align:center">Resizing</th><th>Train</th><th>Test</th></tr></thead><tbody><tr><td>STL-10</td><td style="text-align:center">64c5-64c5-128f</td><td>10 pre-defined folds of the training data</td><td>fixed test set</td></tr><tr><td>CIFAR-10</td><td style="text-align:center">resize from $32\times 32$ to $64\times 64$</td><td>1. whole training set<br />2. 10 random selections of 400 training samples per class</td><td>1. results on CIFAR-10<br />2. average results on 10 sets.</td></tr><tr><td>Caltech-101</td><td style="text-align:center">to $150\times 150$</td><td>30 random samples per class</td><td>not more than 50 samples per class</td></tr><tr><td>Caltech-256</td><td style="text-align:center">$256\times 256$</td><td>randomly selected 30 samples per class</td><td>those except for training</td></tr></tbody></table></li><li><p>backbones of network</p><table><thead><tr><th>Network</th><th style="text-align:center">Structure</th><th>Training</th></tr></thead><tbody><tr><td>small</td><td style="text-align:center">64c5-64c5-128f</td><td>1.5 days, SGD with fixed momentum of 0.9</td></tr><tr><td>medium</td><td style="text-align:center">64c5-128c5-256c5-512f</td><td>4 days, SGD with fixed momentum of 0.9</td></tr><tr><td>large</td><td style="text-align:center">92c5-256c5-512c5-1024f</td><td>9 days, SGD with fixed momentum of 0.9</td></tr></tbody></table></li><li><p>Training: learning rate starts at 0.01， then when there was no improvement in validation error, decreased the learning rate by a factor of 3. All networks are trained on one Titan</p></li><li><p>Test features: one-vs-all linear SVM.</p></li><li><p>Summary</p><ul><li>with increasing feature vector dimensionality and number of labeled samples, training an SVM becomes less dependent on the quality of the features</li><li>Relation of <strong>the number of surrogate classes</strong> :  <strong>sampling too many, too similar images for training can even decrease the performance of the learned features</strong>. ( the discriminative loss is no longer reasonable with too many similar surrogate classes.) --&gt; fix: e.g. clustering the output features then do augmentation for clusters and feed these augmented classes as surrogate data</li><li>Relation of <strong>the number of samples per surrogate class</strong> :  around 100 samples is sufficient</li><li>Relation of **types of transformations ** : each time remove a group of transformations and check how the performance is decreased , e.g. scaling, rotation etc.  Translations, color variations and contrast variations are significantly more important. For the matching task, using blur as an additional transformation improves the performance.</li><li>Relation of **Influence of the dataset  **:  the learned features generalize well to other datasets</li><li>Relation of <strong>Influence of the Network Architecture on Classification Performance</strong>: Classification accuracy generally improves with the network size</li></ul></li></ul><h3 id="Descriptor-matching">Descriptor matching</h3><ul><li>Task: Matching of interest points</li><li>Datasets: by Mikolajczyk et al., augmented by applying  6 different types of transformations with varying strengths to 16 base images from Flickr. In addition to the transformations used before, also change the lighting and blur .</li><li>Backbones: 64c7s2-128c5-256c5-512f, named as Exemplar-CNN-blur</li><li>Training:  use unlabeled images from Flickr for training</li><li>Test and measurements: prediction is $TP$ if $IOU\ge 0.5$. Compared with SIFT and Alexnet</li><li>Summary<ul><li>Optimum patch size (or layer in CNNs): SIFT is based on normalized finite differences, and thus very robust to blurred edges caused by interpolation. In contrast, for the networks, especially for their lower layers, there is an optimal patch size. They argue that features from higher layers have access to larger receptive fields and, thus, can again benefit from larger patch sizes.</li><li>A loss function that focuses on the invariance properties (rather than class-specific features) required for descriptor matching yields better results.</li><li>Features obtained with the unsupervised training procedure outperform the features from AlexNet on both datasets</li></ul></li></ul><h2 id="Conclusion">Conclusion</h2><ul><li>Pretty good on tasks: object classification , descriptor matching</li><li>emphasizes the value of data augmentation in general and suggests the use of more diverse transformations.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1406.6909.pdf&quot;&gt;Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>notes-DAPPER</title>
    <link href="http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html"/>
    <id>http://yoursite.com/posts/notes/2019-10-05-notes-DAPPER.html</id>
    <published>2019-10-05T16:52:21.000Z</published>
    <updated>2021-01-12T19:34:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see <a href="https://github.com/nansencenter/DAPPER">here</a>.</p><p>This notes keep records of the problems encountered while using DAPPER.</p><a id="more"></a><ol><li>Pre-setting</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.</span><br></pre></td></tr></table></figure><p>Copy mkl_*.dll, libiomp5md.dll and <em>libiomp5md.pdb</em> from directory “./Library/bin” to the root directory of python exe.</p><p>Reference from <a href="https://blog.csdn.net/supertangcugu/article/details/89790617">here</a>.</p><ol start="2"><li>Manual</li></ol><p>Manual <a href="https://dapper.readthedocs.io/en/latest/implementation.html">online</a></p><p>For EnKF: Nx-by-N (维度数<em>样本数)<br>For ndarrays: N-by-Nx (样本数</em>维度数)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. For more details, see &lt;a href=&quot;https://github.com/nansencenter/DAPPER&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This notes keep records of the problems encountered while using DAPPER.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="da" scheme="http://yoursite.com/tags/da/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks and Remote Sensing, July 26th, 2019.</title>
    <link href="http://yoursite.com/posts/talks/2019-07-26-talk-gan-rs.html"/>
    <id>http://yoursite.com/posts/talks/2019-07-26-talk-gan-rs.html</id>
    <published>2019-07-26T17:33:39.000Z</published>
    <updated>2021-02-16T18:17:05.386Z</updated>
    
    <content type="html"><![CDATA[<p>Some works using GANs handle the problems in remote sensing.</p><p>Check <a href="/assets/slides/GAN/GANRS.pdf">slide</a> for more details.``</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Some works using GANs handle the problems in remote sensing.&lt;/p&gt;
&lt;p&gt;Check &lt;a href=&quot;/assets/slides/GAN/GANRS.pdf&quot;&gt;slide&lt;/a&gt; for more detai
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="remote sensing images" scheme="http://yoursite.com/tags/remote-sensing-images/"/>
    
  </entry>
  
  <entry>
    <title>Notes of Mathematics</title>
    <link href="http://yoursite.com/posts/notes/2019-06-19-notes-math.html"/>
    <id>http://yoursite.com/posts/notes/2019-06-19-notes-math.html</id>
    <published>2019-06-19T13:40:09.000Z</published>
    <updated>2021-04-28T23:26:01.555Z</updated>
    
    <content type="html"><![CDATA[<h2 id="泛函">泛函</h2><center>    <img src="/assets/img/Rules/lines.png" width=500"></center><h3 id="不动点定理">不动点定理</h3><ol><li><p>不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。</p><p><a href="https://zhuanlan.zhihu.com/p/33885648">链接</a></p></li></ol><h3 id="紧性的利用">紧性的利用</h3><ul><li>证明存在性</li><li>在无限维空间中“模仿”有限维的欧式空间</li></ul><p>紧集是为了模仿描述欧式空间中的有界闭集合么？<br>紧=相对紧+闭</p><h3 id="流形-Manifolds">流形(Manifolds)</h3><ol><li>概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。</li></ol><h3 id="支撑集-Support">支撑集(Support)</h3><ol><li>概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合</li></ol><a id="more"></a><h2 id="数学分析">数学分析</h2><h3 id="Lipschitz连续">Lipschitz连续</h3><ol><li>若存在一个常数K，使得定义域内的任意两点x1,x2满足：<br>$ \left|f\left(x_1\right)-f\left(x_2\right)\right|=K\left|x_1-x_2\right|$<br>则称函数为Lipschitz连续函数。此性质限定了f的导函数的绝对值不超过K，规定了函数的最大局部变动幅度。</li></ol><h2 id="Statistical-learning-theory">Statistical learning theory</h2><h3 id="Introduction">Introduction</h3><p>Determine how well a model performs on unseen data</p><h3 id="Preliminary">Preliminary</h3><ul><li><p>Markov Inequality</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210155914107.png" alt="image-20210210155914107" style="zoom:67%;" />, in the order of $\mathcal{O}(\frac{1}{deviation})$</p></li><li><p>Chebyshev’s Inequality</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210155723575.png" alt="image-20210210155723575" style="zoom:67%;" />, in the order of $\mathcal{O}(\frac{1}{deviation^2})$</p></li><li><p>Generic Chernoff’s Bound</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210160010328.png" alt="image-20210210160010328" style="zoom:67%;" /></li><li><p>Hoeffding’s Inequality</p><ul><li><p>Hoeffding’s lemma</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210160051227.png" alt="image-20210210160051227" style="zoom:67%;" /></li><li><p>Hoeffding’s Inequality</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210160116965.png" alt="image-20210210160116965"></p><p>Hoeffding’s inequality is useful to bound the probability of the gap between an empirical value and the true expectation of an average of <strong>bounded</strong> random variables.</p></li></ul></li><li><p>McDiarmid’s Inequality</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210181355241.png" alt="image-20210210181355241" style="zoom:50%;" /><ul><li>A concentration inequality.</li><li>This bound is useful because if we prove that an algorithm is β stable then we will have this property on a specific function.</li></ul></li></ul><h3 id="PAC">PAC</h3><p>Check <a href="http://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-7-notes.pdf">here</a> for PAC, VC, uniform bound and others.</p><ul><li><p><strong>PAC Learning (agnostic PAC learnable)</strong>–finite hypothesis class</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210155242200.png" alt="image-20210210155242200"></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210090621996.png" alt="image-20210210090621996" style="zoom:70%;" /><ul><li>Consider a simple linear classifier with 2 weights $\vec{w} = (w_1;w_2)$, which are stored using a 32 bit floats. This implies that the hypothesis class is finite with $|\mathcal{H}| = 2^{32\times2}  $.</li><li>This theorem <strong>works on finite hypothesis class</strong>, and answers that for a hypothesis class $\mathcal{H}$, to make the generation gap is smaller than $\epsilon$ with at least $1-\delta$, the required samples complexity.</li><li><em>Once a hypothesis class is PAC learnable, with high probability the training set is $\epsilon$-representative.</em></li><li>Note Suppose $\mathcal{H}$ is PAC learnable, there <strong>is not a unique function</strong> $m_{\mathcal{H}}$ that satisfies the requirements given in the definition of PAC learnability.</li><li>Finite classes are PAC Learnable, also agnostic PAC Learnable.</li></ul></li><li><p><strong>Uniform convergence</strong></p><ul><li>Formalize that over all hypothesis in $\mathcal{H}$, the empirical risk is close to the true risk. This will <strong>make sure the ERM to work.</strong></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210161725993.png" alt="image-20210210161725993" style="zoom:80%;" /></li></ul></li><li><p>VC dimension</p><ul><li><p>measure the complexity of hypothesis class other than cardinality.</p></li><li><p><strong>Shattering</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210163136908.png" alt="image-20210210163136908"></p></li><li><p><strong>VC Dimension</strong></p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210163157194.png" alt="image-20210210163157194"></p><p>Informally VC dimension is the maximum number of distinct points that a hypothesis in $\mathcal{H}$ can correctly classify every possible labeling with zero error.</p><ul><li>With infinite VC dimension, the hypothesis class won’t be PAC learnable.</li><li>There exist hypothesis classes with uncountable cardinality but finite VC dimension.</li><li>For every two hypothesis classes if $\mathcal{H}_0 \subset \mathcal{H}$ then $VCdim(\mathcal{H}_0) \leq VCdim(\mathcal{H})$.</li></ul></li></ul></li></ul><h3 id="Occam’s-bound">Occam’s bound</h3><ul><li><p>The PAC bound can be treated as Occam’s bound with a uniform prior .</p></li><li><p>Occam’s bound will put a distribution over the countably infinite hypothesis class $\mathcal{H}$ that is independent of dataset $S$ we will receive. In doing so we will <strong>get bounds on the generalization gap that no longer depend on the size of the hypothesis class</strong>, $|\mathcal{H}|$. These bounds now become variable depending on how we weigh each individual hypothesis $h$, i.e. $P(h)$.</p></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210090847116.png" alt="image-20210210090847116" style="zoom:80%;" /><ul><li><p><strong>Regularizers offers higher probability assigned to $\vec{w}$ near the origin and thus a tighter bound,</strong> it won’t influence the algorithm (loss). Specifically , when  an $\ell_2$ regularization term is added to the learning algorithm, it adds concavity to the loss function.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210092846655.png" alt="image-20210210092846655" style="zoom:80%;" /></li></ul></li><li><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210091000849.png" alt="image-20210210091000849" style="zoom:80%;" />, where $D(Q|P)$ is the KL divergence which serves as a complexity measure.</p><ul><li><p>It tells that with a good posterior that is close to the prior, then the KL-divergence will become smaller and out bound will be tighter. <strong>Even though it’s tight, the bound is tight for hypothesis that we may not care about, e.g. tight on the bound with respect to $P$  prior on hypothesis.</strong></p></li><li><p><em>Note that the posterior is after applying the prior $P$ on hypothesis, and seeing the data, then one can get this posterior $Q$.</em></p></li><li><p><strong>Why posterior?</strong></p><p>different choices of prior and posterior hypotheses can be made, each resulting in a new bound without us touching the algorithm.</p></li><li><p>The dropout PAC-Bayes is a lower bound on the PAC-Bayes bound that becomes tight when the dropout factor is 0.</p></li></ul></li></ul><h3 id="Stability-Generalization">Stability, Generalization</h3><ul><li><p>PAC learning and Occam’s bound work as algorithm-agnostic bounds.</p></li><li><p>Stability</p><ul><li><p>Hint: a change in data distribution does not change the predictions.</p></li><li><p>Definition: <strong>uniform stability</strong></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210183949427.png" alt="image-20210210183949427" style="zoom:67%;" /><p>An algorithm with this property can be understood as one that produces a hypothesis such that the loss function $\ell$ is not drastically affected by perturbing the dataset in this<br>manner.</p></li><li><p><font color='blue'>EMR with regularization is $\beta$-stable.</font></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210211826259.png" alt="image-20210210211826259" style="zoom:50%;" /><ul><li><em>If we perturb the data by a single element, we learn $\mathcal{A}$ that can become arbitrarily close for large $n$.</em></li></ul></li><li><p><font color='blue'>SGD is stable</font></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210214207908.png" alt="image-20210210214207908" style="zoom:57%;" /><ul><li><strong>It holds for a finite number of steps $T$</strong></li><li></li></ul></li></ul></li><li><p><strong>Defect</strong>:</p><p>$D[h_S]=R[h_S]-\hat{R}_S[h_S]$</p><ul><li><p>Defect $D[h_S]$ for a hypothesis $h_S$ derived from an algorithm after seeing the dataset $S$ is defined as the difference between the population risk and the empirical risk.</p></li><li><p><strong>It’s expectation is not zero.</strong></p></li><li><p><strong>The expectation value of defect can be bounded under certain conditions.</strong></p><p><font color='blue'>If $ \mathcal{A}$ is a $\beta$-uniformly stable algorithm, then $-\beta\le \mathbb{E}[D[h_S]]\le\beta$. </font></p></li><li><p><font color='blue'>Let $\mathcal{A}$ be a $\beta$-uniformly stable learning algorithm with respect to a loss function $\ell:\mathcal{Y\times Y}\rightarrow [0，M]$. <strong>The absolute difference of the defect calculated on a dataset $S$ and on a perturbed version of the dataset</strong> $S^{i,z}$ is bounded by $|D[h_S]-D[h_{S^{i,z}}]|\le 2\beta +\frac{M}{n}$. </font></p></li></ul></li><li><p><font color='blue'>For a $\beta$-uniformly stable algorithm, the relationship between the empirical and the population risk is $\mathbb{E}[R[h_S]]\le\mathbb{E}_S[\hat{R}_S[h_S]]+\beta$. </font></p><ul><li>It’s a bound on the expectation value of the population risk. But this bound does not hold for all possible $h_S$.</li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/image-20210210205331186.png" alt="image-20210210205331186" style="zoom:40%;" /></li></ul></li><li><p><font color='blue'>For a $\beta$-uniformly stable algorithm $\mathcal{A}$ with respect to a loss function $\ell:\mathcal{Y\times Y}\rightarrow [0，M]$ and a hypothesis $h_S$ with $|S|=n$. <strong>The relationship between the empirical and the population risk holds with probability $1-\delta$</strong>: $R[h_S]\le \hat{R}_S[h_S]+\beta +(n\beta+\frac{M}{2})\sqrt{\frac{2\log\frac{2}{\delta}}{n}}$. </font></p><ul><li>The last term is a concentration Inequality (McDiramid’s)</li><li>For this bound, as $n$ goes up, it becomes less tight.</li></ul></li></ul><h2 id="Convex-Optimization">Convex Optimization</h2><p>Book <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>, <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></p><table><thead><tr><th>Terminology</th><th>Definition</th></tr></thead><tbody><tr><td>Convex function: origin</td><td>$f(\theta x+(1-\theta)y)\le \theta f(x)+(1-\theta)f(y)$</td></tr><tr><td>Convex function: 1st order (once differential )</td><td>$f(y)\ge f(x)+\nabla f(x)^T (y-x) $</td></tr><tr><td>Convex function: 2nd order (twice differential )</td><td>$\nabla^2f(x)\succeq0$, the eigenvalues won’t be negative</td></tr><tr><td>$L$-Lipschitz</td><td>$|f(x)-f(y)|\le L|x-y|$</td></tr><tr><td>$\beta$-smooth: $\beta$-Lipschitz on gradient</td><td>$|\nabla f(x)-\nabla f(y)|\le \beta|x-y|\Rightarrow \nabla^2f(x)\preceq\beta\mathrm{I}$</td></tr><tr><td>$\alpha$-strong convex, limited the domain mostly.</td><td>$f(y)-f(x)\le\nabla f(x)^T (y-x)-\frac{\alpha}{2}|y-x|^2\Rightarrow \nabla^2f(x)\succeq\alpha\mathrm{I}$</td></tr></tbody></table><center><strong>Table: Optimizer in different conditions</strong></center></br> For each optimizer, from the top line downwarding, the rate of convergence is increasing. The optimal step size is gotten by minimizing the bound.</center><table><thead><tr><th>Optimizer</th><th>Condition</th><th>Converge rate</th><th>Optimal step size</th><th>Sub-optimal gap</th><th>Bounds of the gap</th></tr></thead><tbody><tr><td>GD after $T$ steps</td><td>L-Lipschitz convex</td><td>$\mathcal{O}(\frac{1}{\sqrt{T}})$</td><td>$\gamma=\frac{|x_1-x^*|_2}{L\sqrt{T}}$</td><td>$f(\frac{1}{T}\sum\limits_{k=1}^{T}x_k)-f(x^*)$</td><td>$\le\frac{|x_1-x^*|L}{\sqrt{T}}$, <br />the initial point matters</td></tr><tr><td>GD</td><td>$\beta$-smooth<br />+convex</td><td>$\mathcal{O}(\frac{1}{T})$</td><td>$\gamma=\frac{1}{\beta}$, <br />constant and independent of $T$</td><td>$f(x_k)-f(x^*)$<br />Notice the average on all samples can be erased</td><td>$\le\frac{2\beta|x_1-x^*|^2}{k-1}$, <br />$k$ means the $k$ step. <br />Bound depends on initial point</td></tr><tr><td>Projected subGD after $T$ steps</td><td>$\alpha$-strong<br /> + $L$-Lipschitz</td><td>$\mathcal{O}(\frac{1}{T})$</td><td>$\gamma_k=\frac{2}{\alpha (k+1)}$,<br /> diminish at every step</td><td>$f(\sum\limits_{k=1}^T\frac{2k}{T(T+1)}x_k)-f(x^*)$</td><td>$\le\frac{2L^2}{\alpha (T+1)}$</td></tr><tr><td>GD</td><td>$\lambda$-strong <br />+ $\beta$-smooth</td><td>$\mathcal{O}(\exp{(-T)})$</td><td>$\gamma=\frac{2}{\lambda+\beta}$</td><td>$f(x_{t+1})-f(x^*)$<br />Notice the average on all samples can be erased</td><td>$\le\frac{\beta}{2}\exp{(-\frac{4t}{\kappa+1})}|x_1-x^*|^2$,<br />$ \kappa=\frac{\beta}{\lambda}$. <br />$k$ is the same meaning of $t$,<br /> rather than the $\kappa$ in denominator.</td></tr><tr><td>Polyak (heavy ball)</td><td>Quadratic loss</td><td>$\mathcal{O} ((\frac{\sqrt\kappa-1}{\sqrt\kappa+1})^t)\\approx\exp(-\frac{C}{\sqrt\kappa})$,<br />$\kappa=\frac{h_{max}}{h_{min}}$</td><td>$\gamma^*=\frac{(1+\sqrt\mu)^2}{h_{max}}\=\frac{(1-\sqrt\mu)^2}{h_{min}}$</td><td>$\left|\left[ \begin{matrix}   x_{t+1}-x*\ x_t-x^*    \end{matrix}  \right]\right|_2$</td><td>$\le\mathcal{O} (\rho(A)^T)=\mathcal{O} (\sqrt{\mu}^T)$.<br /> $\mu$ is the curvature</td></tr><tr><td>Nesterov NAG</td><td>$\beta$-smooth</td><td>$\mathcal{O}(\frac{1}{T^2})$</td><td></td><td>$f(y_t)-f(x^*)$</td><td></td></tr><tr><td>Nesterov NAG</td><td>$\alpha$-strong <br />+$\beta$-smooth</td><td>$\mathcal{O}(\exp(-\frac{T}{\sqrt{\kappa}}))$</td><td></td><td>$f(y_t)-f(x^*)$</td><td>$\le\frac{\alpha+\beta}{2}|x_k -x^*|^2\exp(-\frac{t-1}{\sqrt\kappa})$</td></tr><tr><td>SGD</td><td>$L$-Lipschitz by<br /> $|\tilde{g}(x)|\le L$ with prob. 1</td><td>$\mathcal{O}(\frac{1}{\sqrt T})$<br />If wants a $\epsilon$-tolerance, $T\ge\frac{B^2L^2}{\epsilon^2}$</td><td>$\gamma=\frac{B}{L\sqrt{T}}$</td><td>$\mathbb{E}[f(\bar{x})]-f(x^<em>)$, where <br />$x^</em>\in\arg\min_{x:|x|&lt;B}f(x)$</td><td>$\le\frac{BL}{\sqrt T}$</td></tr><tr><td>SGD</td><td>$\alpha$-strong+ $\mathbb{E}|\tilde{g}(x)|_*^2\le B^2$ (kind of $B$-smooth)</td><td>$\mathcal{O}(\frac{1}{T})$</td><td>$\gamma=\frac{2}{\alpha(s+1)}$</td><td>$f(\sum\limits_{s=1}^t\frac{2s}{t(t+1)}x_s)-f(x^*)$</td><td>$\le\frac{2B^2}{\alpha(t+1)}$</td></tr></tbody></table><h3 id="Estimate-sequence"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&amp;rep=rep1&amp;type=pdf">Estimate sequence</a></h3><ul><li><p>Definition</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210129143844.png" alt="image-20210129143844647" style="zoom:67%;" /></li><li><p>Properties</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131115340.png" alt="image-20210131115338221" style="zoom:50%;" /><p>for any sequence ${\lambda_k}$, satisfying $(2.2.2)$ we can derive <strong>the rate of convergence of the minimization process directly from the rate of convergence of the sequence ${\lambda_k}$.</strong></p><p>Note: below in estimate sequence, all $L$ means $L$-smooth function</p></li><li><p>How to form an estimate sequence?</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131150937.png" alt="image-20210131150935371" style="zoom:40%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210131151028.png" alt="image-20210131151028446" style="zoom:40%;" /></li><li><p>How to ensure $(2.2.2)$?</p><ul><li><p>Method 1: Do scheme (2.2.6), it will generate a sequence ${x_k}_{k=0}^\infin$ such that $f(x_k)-f^<em>\le \lambda_k[f(x_0)-f^</em>+\frac{\gamma_0}{2}|x_0-x^*|^2]$. It will make sequence satisfy $(2.2.2)$</p></li><li><p>Method 2: Using gradient step</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210201094020.png" alt="image-20210201094020408" style="zoom:50%;" /></li></ul></li><li><p>Some cases</p><ul><li><font color='orange'>If in the scheme (2.2.6) we choose $\gamma_0\ge\mu$, then  $\lambda_k\le\min{(1-\sqrt{\frac{\mu}{L}})^k,\frac{4L}{(2\sqrt{L}+k\sqrt{\gamma})^2}}$.</font></li><li><font color='blue'>If in the scheme (2.2.6) we choose $\gamma_0=L$, then this scheme generates a sequence ${x_k}<em>{k=0}^{\infin}$ such that $f(x_k)-f^<em>\le L\min {(1-\sqrt{\frac{\mu}{L}})^k,\frac{4}{(k+2)^2}}|x_0-x^</em>|^2$. This  means that it’s optimal for the class $\mathcal{S}</em>{\mu,L}^{1，1}(R^n)$ with $\mu\ge0$. </font></li><li><font color='blue'>If in the scheme $(2.2.8)$ we choose $\alpha_0\ge \sqrt{\frac{\mu}{L}}$, then this scheme generates a sequence ${x_k}_{k=0}^{\infin}$ such that $f(x_k)-f^<em>\le \min {(1-\sqrt{\frac{\mu}{L}})^k,\frac{4L}{(2\sqrt{L}+k\sqrt{\gamma_0})^2}}[f(x_0)-f^</em>+\frac{\gamma_0}{2}|x_0-x^*|^2]$, where $\gamma_0=\frac{\alpha_0(\alpha_0L-\mu)}{1-\alpha_0}$. Here $\alpha_0\ge \sqrt{\frac{\mu}{L}}$ is equivalent to $\gamma_0\ge\mu$. </font></li><li></li></ul></li></ul><h3 id="Convex-sets">Convex sets</h3><h4 id="Affine-sets">Affine sets</h4><ul><li>Affine sets:<ul><li>Definition : A set $C\subseteq R^n$ is affine if the line through any two distinct points in $C$ lies in $C$, aka for any $x_1 ,x_2\in C$ and $\theta\in R$, one has $\theta x_1+(1-\theta)x_2\in C$. It indicates that the $C$ contains the linear combination of any two points in $C$.</li><li>Induction: If $C$ is an affine set, $x_1，\cdots,x_k\in C$ and $\theta_1+\cdots+\theta_k=1$, then the point $\theta_1 x_1+\cdots+\theta_kx_k$ also belongs to $C$.</li></ul></li><li>Affine hull<ul><li>Definition: the set of all affine combinations of points in some set $C\subseteq R^n$, denoted as $\mathrm{aff}C$</li><li>It’s the smallest affine set that contains $C$</li></ul></li><li>Affine dimension<ul><li>Definition: as the dimension of its affine hull.</li><li>E.g.: ${x\in R^2|x_1 ^2+x_2^2=1}$, the affine dimension is 2.</li></ul></li></ul><h4 id="Convex-sets-2">Convex sets</h4><ul><li>Definition: If every point in the set can be seen by every other point, along an unobstructed straight path between them, where unobstructed means lying in the set.</li><li>Every affine set is also convex.</li><li>Convex hull, denotes as $\mathrm{conv} C$, is the set of all convex combinations of points in $C$. It is always convex, and it’s the smallest convex set that contains $C$</li><li>More generally, suppose $p: R^n \rightarrow R$ satisfies $p(x)\ge0$ for all $x\in C$ and $\int_Cp(x)dx=1$, where $C\subseteq R^n$ is convex, then $\int_Cp(x)x dx \in C$, if the integral exists.</li></ul><h3 id="Convex-functions">Convex functions</h3><h4 id="Convex-functions-2">Convex functions</h4><ul><li><p>Definition</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117165609.png" alt="image-20210117165609361" style="zoom:50%;" /></li><li><p>All affine function are both convex and concave.</p></li><li><p>$f$ is convex if and only if for all $x\in \mathrm{dom}f$ and all $v$, the function $g(t)=f(x+tv)$ is convex.</p></li></ul><h4 id="Extended-value-extensions">Extended-value extensions</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117170246.png" alt="image-20210117170246166" style="zoom:50%;" /><h4 id="First-order-conditions">First-order conditions</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117171410.png" alt="image-20210117171410378" style="zoom:50%;" /><p>The inequality (3.2) states that for a convex function, the first-order Taylor approximation is in fact a global <em>underestimator</em> of the function. Conversely, if the first-order Taylor approximation of a function is always a global <em>underestimator</em> of the function, then the function is convex.</p><ul><li>The inequality (3.2) shows that if $\nabla f(x) = 0$, then for all $y \in \mathrm{dom} f, f(y) ≥ f(x)$, i.e., $x$ is a global minimizer of the function $f$.</li></ul><h4 id="Second-order-conditions">Second-order conditions</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117173919.png" alt="image-20210117173919136" style="zoom:67%;" /><h4 id="More-constraints-on-convex-function">More constraints on convex function</h4><h5 id="beta-smooth">$\beta$-smooth</h5><ul><li>Definition: a continuously function $f$ is $\beta$-smooth if the gradient $\nabla f$ is $\beta$-Lipschitz, that is $|\nabla f(x)-\nabla f(y)|\le\beta|x-y|$.</li><li><em><strong>If $f$ is twice differentiable then this is equivalent to the eigenvalues of the Hessians being smaller than $\beta$ at any point.</strong></em> , $\nabla^2f(x)\preceq\beta \mathrm{I}_n,\forall x$.</li><li><font color='green'>smoothness removes dependency from the averaging scheme</font></li><li><font color='cyan'>If extend the $\beta$-smooth to multi power, it’s called <a href="https://en.wikipedia.org/wiki/H">Holder condition</a></font></li><li>The bigger your function changes in gradients, the upper you have to explore.</li></ul><h5 id="alpha-strong-convexity">$\alpha$-strong convexity</h5><p>Strong convexity can significantly speed-up the convergence of first order methods.</p><ul><li><p>Definition</p><p>We say that $f:\mathcal{X}\rightarrow\mathbb{R}$ is a $\alpha$<em>-strongly convex</em> if it satisfies the following improved subgradient inequality:</p><p>$f(x)-f(y)\le\nabla f(x)^T(x-y)-\frac{\alpha}{2}|x-y|^2$. A large value of $\alpha$ will lead to a faster rate.  A $\alpha$<strong>-strong convexity for twice differential function $f$ can also be interpreted as $\nabla^2f(x)\succeq\alpha \mathrm{I}_n,\forall x$</strong>.</p></li><li><p>Strong convexity plus $\beta$-smoothness will lead to the gradient descent with a constant step-size achieves a linear rate of convergence, precisely the oracle complexity will be $O(\frac{\beta}{\alpha}\log(1/\varepsilon)), \beta\ge\alpha$. In some sense strong convexity is a dual assumption to smoothness, and in fact this can be made precise within the framework of Fenchel duality.</p></li><li><p>$\alpha$ can often be reviewed as large as the sample size. Thus reducing the number of step from <strong>sample size</strong> to $\sqrt{\mathrm{sample \quad size}}$ (cause $\kappa=\frac{\beta}{\alpha}$ for $\beta$-smooth and $\alpha$-strong function, and in basic gradient descent algorithm, to reach $\epsilon$-accuracy, it requires $\mathcal{O}(\kappa\log(\frac{1}{\epsilon}))$, and for Nesterov’s Accelerated Gradient Descent attains the improved oracle complexity of $\mathcal{O}(\sqrt{\kappa}\log(\frac{1}{\epsilon}))$) can be a huge deal, especially in large scale applications.</p></li></ul><h4 id="Examples-of-Convex-functions">Examples of Convex functions</h4><ul><li><em>Norms</em></li><li><em>Max function</em></li><li><em>Quadratic-over-linear function</em> $\frac{x^2}{y}$</li><li><em>Log-sum-exp</em>: $\log(e^{x_1}+\cdots+e^{x_n})$, which is regarded as a differentiable approximation of the max function</li><li><em>Geometric mean</em>: $(\prod\limits_{i=1}^{n}x_i)^{1/n}$, concave</li><li><em>Log-determinant</em>: $\log\det X$, concave</li></ul><p>For proofs, check Chapter $3.1.5$ of <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a>.</p><h4 id="Sublevel-sets">Sublevel sets</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181258.png" alt="image-20210117181258728" style="zoom:50%;" /><h4 id="Epigraph">Epigraph</h4><p>A function is convex if and only if its epigraph is a convex set.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117181415.png" alt="image-20210117181415039" style="zoom:50%;" /><h4 id="Jensen’s-inequality-and-extensions">Jensen’s inequality and extensions</h4><p>Once a function is convex, then you can get</p><p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117185801.png" alt="image-20210117185801678" style="zoom:50%;" /> the simplest version of it is $f(\frac{x+y}{2})\le\frac{f(x)+f(y)}{2}$.</p><h4 id="Holder’s-inequality">Holder’s inequality</h4><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190336.png" alt="image-20210117190336823" style="zoom:50%;" /><h4 id="Operations-that-preserve-convexity">Operations that preserve convexity</h4><ul><li><p>Nonnegative weighted sums: $f=w_1f_1+\cdots+w_mf_m$</p></li><li><p>Composition with an affine mapping: $g(x)=f(Ax+b)$. If $f$ is convex, so is $g$; if $f$ is concave, so is $g$.</p></li><li><p>Pointwise maximum and suprenum: $f(x)=\max{f_1(x),f_2(x)}$ and $f(x)=\max{f_1(x),\cdots,f_m(x)}$</p></li><li><p>Composition: $f(x)=h(g(x))$</p><ul><li><p>Scalar composition</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117190943.png" alt="image-20210117190943043" style="zoom:50%;" /></li><li><p>Vector composition $f(x)=h(g(x))=h(g_1(x),\cdots,g_k(x))$</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117191112.png" alt="image-20210117191112754" style="zoom:50%;" /></li><li><p>Minimization</p></li></ul></li></ul><h3 id="Typical-Numerical-optimization">Typical Numerical optimization</h3><h4 id="Gradient-descent">Gradient descent</h4><p>The basic principle behind <strong>gradient descent</strong> is to make a small step in the direction that minimizes the local first order Taylor approximation of $f$ (also known as the steepest descent direction). This kind of methods will <strong>obtain an oracle complexity <em>independent of the dimension</em>.</strong></p><p>$x_{t+1}=x_t-\eta\nabla f(x_t)$</p><p>Taking $f(w)=\frac{1}{2}w^TAw-b^Tw,w\in\mathbb{R}^n$ into consideration, suppose $A$ is symmetric and invertible, then $A=Q\Lambda Q^T,\Lambda=(\lambda_1,\cdots,\lambda_n),\lambda_1\le\lambda_2\le\cdots\le\lambda_{n-1}\le\lambda_n$.</p><ul><li><p>All errors are not made equal. Indeed, there are different kinds of errors, $n$ to be exact, one for each of the eigenvectors of $A$.</p><p>$f(w^k)-f(w^*)=\sum(1-\alpha\lambda_i)^{2k}\lambda_i[x_i^0]^2$</p></li><li><p><strong>Denote the condition number $\kappa=\frac{\lambda_n}{\lambda_1}$, then the bigger the $\kappa$ is, the slower gradient descent will be</strong>, cause the condition number is a direct description of pathological curvature.</p></li><li><p><strong>The optimal step-size causes the first and last eigenvectors to converge at the same rate.</strong></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124212843.png" alt="image-20210124212843050" style="zoom:50%;" /></li></ul><h4 id="Projected-gradient-descent">Projected gradient descent</h4><ul><li><p>Subgradient</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222307.png" alt="image-20210117222306873" style="zoom:40%;" /></li><li><p>Projected subgradient descent</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117222725.png" alt="image-20210117222725673" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117223015.png" alt="image-20210117222937543" style="zoom:40%;" /></li></ul><h4 id="Gradient-descent-with-momentum-Polyak’s-Momentum"><a href="https://distill.pub/2017/momentum/">Gradient descent with momentum</a> : Polyak’s Momentum</h4><ul><li><p>Sometimes SGD fail with a reason of pathological curvature of objective. (like valley, trench)</p></li><li><p>Momentum modify gradient descent by adding a short-term memory</p><p>$y_{t+1}=\beta y_t+\nabla f(x_t)\x_{t+1}=x_{t}-\alpha y_{t+1}$.</p><p>When $\beta=0$, it’s gradient descent.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124224747.png" alt="image-20210124224747333" style="zoom:50%;" /></li><li><p>Momentum allows us to crank up the step-size up by a factor of 2 before diverging.</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124225628.png" alt="image-20210124225628085" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124225657.png" alt="image-20210124225656929" style="zoom:50%;" /></li><li><p>Optimize over $\beta$: The critical value of $\beta = (1 - \sqrt{\alpha \lambda_i})^2$ gives us a convergence rate (in eigenspace $i$) of $1 - \sqrt{\alpha\lambda_i}$. A square root improvement over gradient descent, $1-\alpha\lambda_i$! Alas, this only applies to the error in the $i^{th}$ eigenspace, with $\alpha$ fixed.</p></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124231402.png" alt="image-20210124231402777" style="zoom:50%;" /></li><li><p><strong>Failing</strong>: there exist strongly-convex and smooth functions for which, by choosing carefully the hyperparameters $\alpha$ and $\beta$ and the initial condition $x_0$, the heavy-ball method fails to converge.</p></li></ul><h4 id="Nesterov’s-Accelerated-Gradient-Descent">Nesterov’s Accelerated Gradient Descent</h4><ul><li><p><a href="https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/">Some notes of it</a></p></li><li><p>Iterations: starting at an arbitrary initial point $x_1=y_1$</p><p>$$y_{s+1}=x_s-\frac{1}{\beta}\nabla f(x_s),\x_{s+1}=(1+\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})y_{s+1}-\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}y_s$$</p></li><li><p><font color='blue'>Let $f$ be $\beta$-smooth and $\alpha$-strongly convex, then Nesterov’s gradient descent satisfies for $t\ge0$, $f(y_t)-f(x^<em>)\le\frac{\alpha+\beta}{2}|x_1-x^</em>|^2\exp{(-\frac{t-1}{\sqrt{\kappa}})}$ .</font></p></li><li><p><strong>Converge in $\mathcal{O}(\frac{1}{T^2})$ for smooth case, and $\mathcal{O}(\exp(-\frac{T}{\sqrt{\kappa}}))$, it guaranteed convergence for quadratic functions (and not piece-wise quadractic)</strong></p></li></ul><h4 id="Stochastic-gradient-descent">Stochastic gradient descent</h4><ul><li><p>Cases: one is $\mathbb{E}<em>{\xi}\nabla_x\ell(x,\xi)\in\part f(x)$, where $\xi$ is sampled. This method cannot be reproduced; the other is directly minimize $f(x)=\frac{1}{m}\sum\limits</em>{i=1}^{m}f_i(x)$, here gradient is reported as $\nabla f_I(x)$, where $I\in[m]$, this method can be reproduced.</p></li><li><p>Non-smooth stochastic optimization</p><ul><li>Definition: there exists $B&gt;0$ such that $\mathbb{E}|\tilde{g}(x)|_*^2\le B^2$ for all $x\in\mathcal{X}$</li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202153046.png" alt="image-20210202153043826" style="zoom:67%;" /></li><li><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202153117.png" alt="image-20210202153116696" style="zoom:67%;" /></li></ul></li><li><p>Smooth stochastic optimization and mini-batch SGD</p><ul><li><p>Definition: there exists $\sigma&gt;0$ such that $\mathbb{E}|\tilde{g}(x)-\nabla f(x)|_*^2\le \sigma^2$ for all $x\in\mathcal{X}$.</p></li><li><p><font color='orange'>smoothness does not bring any acceleration for a general stochastic<br>oracle , while in exact orale case it does.</font></p></li><li><p><font color='blue'>Stochastic smooth optimization converge in $\frac{1}{\sqrt{t}}$. Deterministic smooth optimization converge in $\frac{1}{t}$</font></p></li><li><p><font color='blue'>Mini-batch SGD converges between $\frac{1}{\sqrt{t}}$ and  $\frac{1}{t}$.</font></p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210202154653.png" alt="image-20210202154653243" style="zoom:67%;" /></li><li></li></ul></li></ul><h4 id="Relations">Relations</h4><h5 id="Generalization">Generalization</h5><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210124232634.png" alt="image-20210124232634072" style="zoom:50%;" /><h5 id="The-momentum-SGD-and-Adaptive-optimizers">The momentum SGD and Adaptive optimizers</h5><p><a href="https://arxiv.org/pdf/1706.03471.pdf">YellowFin and the Art of Momentum Tuning</a></p><ul><li>Summary:  hand-tuning a single learning rate and momentum makes it competitive with Adam.  The proposed YellowFin (an automatic fine tuner for momentum and learning rate in SGD), can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing.</li><li>Adaptive optimizers: like Adam, AdaGrad, RmsProp</li><li>For details of paper check <a href="">here</a></li></ul><h3 id="Dimension-free-convex-optimization">Dimension-free convex optimization</h3><h4 id="Projected-subgradient-descent-for-Lipschitz-functions">Projected subgradient descent for Lipschitz functions</h4><ul><li><p><strong>Theorem</strong></p><p>Assume that $\mathcal{X}$ is contained in an Euclidean ball centered at $x_1\in \mathcal{X}$ and of radius $R$. Assume that $f$ is such that for any $x\in \mathcal{X}$ and of any $g\in\part f(x)$ (assume $\part f(x)\ne \emptyset$) one has $|g|\le L$. (This implies that $f$ is L-Lipschitz on $\mathcal{X}$, that is $|f(x)-f(y)|\le L|x-y|$)</p><p><font color='blue'><strong>The projected subgradient descent method with $\eta=\frac{R}{L\sqrt{t}}$ satisfies</strong> $f(\frac{1}{t}\sum\limits_{s=1}^{t}x_s)-f(x^*)\le\frac{RL}{\sqrt{t}}$</font></p><ul><li><p>Proof</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117225400.png" alt="image-20210117225359857" style="zoom:40%;" /></li><li><p><font color='blue'><strong>The rate is unimprovable from a black-box perspective.</strong></font></p></li></ul></li></ul><h4 id="Gradient-descent-for-smooth-functions">Gradient descent for smooth functions</h4><h5 id="Theorems-under-unconstrained-cases">Theorems under unconstrained cases</h5><p>In this section all $f$ is a convex and $\beta$-smooth function on $\mathbb{R}^n$.</p><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let $f$ be convex and $\beta$-smooth function on $\mathbb{R}^n$. Then gradient descent with $\eta=\frac{1}{\beta}$ satisfies $f(x_t)-f(x^<em>)\le\frac{2\beta|x_1-x^</em>|^2}{t-1}$ .</font></p><p>For the proof check $3.2$ in  <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a>.</p></li><li><p>Gradient descent attains a much faster rate in $\beta$-smooth situation than in the non-smooth case of the previous section.</p></li><li><p>The Definition of smooth convex functions</p><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210117233245.png" alt="image-20210117233244773" style="zoom:50%;" /></li></ul><h5 id="The-constrained-cases">The constrained cases</h5><p>This time <em>consider the projected gradient descent algorithm $x_{t+1}=\prod_{\mathcal{X}}(x_t-\eta\nabla f(x_t))$</em></p><ul><li><strong>Lemma</strong></li></ul><p><font color='orange'>Let $x,y\in \mathcal{X},x^+=\prod_{\mathcal{X}}(x-\frac{1}{\beta}\nabla f(x))$ and $g_{\mathcal{X}}(x)=\beta (x-x^+)$ , then the following holds true: $f(x^+)-f(y)\le g_{\mathcal{X}}(x)^T(x-y)-\frac{1}{2\beta}|g_{\mathcal{X}}(x)|^2$.</font></p><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let $f$ be convex and $\beta$-smooth function on $\mathcal{X}$. Then projected gradient descent with $\eta=\frac{1}{\beta}$ satisfies $f(x_t)-f(x^<em>)\le\frac{3\beta|x_1-x^</em>|^2+f(x_1)-f(x^*)}{t}$ .</font></p><h4 id="Strong-convexity">Strong convexity</h4><h5 id="Strongly-convex-and-Lipschitz-functions"><strong>Strongly convex and Lipschitz functions</strong></h5><img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210118152328.png" alt="image-20210118152328268" style="zoom:50%;" /><ul><li><strong>Theorem</strong></li></ul><p><font color='blue'>Let $f$ be $L$-Lipschitz and $\alpha$-strongly convex  on $\mathcal{X}$. Then projected gradient descent with $\eta_s=\frac{2}{\alpha(s+1)}$ satisfies $f(\sum\limits_{s=1}^{t}\frac{2s}{t(t+1)}x_s)-f(x^*)\le\frac{2L^2}{\alpha(t+1)}$ .</font></p><p><em>The combination of $\alpha$-strongly convex and $L$-Lipschitz means that function has to be constrained in  a bounded domain.</em></p><h5 id="Strongly-convex-and-smooth-functions"><strong>Strongly convex and smooth functions</strong></h5><ul><li><p><strong>Theorem</strong></p><p><font color='blue'>Let $f$ be $\beta$-smooth and $\alpha$-strongly convex  on $\mathcal{X}$, then projected gradient descent with $\eta=\frac{1}{\beta}$ satisfies for $t\ge0$, $|x_{t+1}-x^<em>|^2\le\exp(-\frac{t}{\kappa})|x_a -x^</em>|^2$ .</font></p><p>The intuition of changing $\alpha$ and $\beta$: If increasing $\beta$, the upper bound will be decreased, and if increasing $\alpha$, the lower bound will be increased.  <img src="https://raw.githubusercontent.com/skaudrey/picBed/master/img/20210121112619.png" alt="image-20210121112605020" style="zoom:33%;" /></p></li><li><p><strong>Lemma</strong></p><p><font color='orange'>Let $f$ be $\beta$-smooth and $\alpha$-strongly convex  on $\mathbb{R}^n$, then  for all $x,y\in \mathbb{R}^n$, one has $(\nabla f(x)-\nabla f(y))^T(x-y)\ge\frac{\alpha\beta}{\alpha+\beta}|x-y|^2+\frac{1}{\beta+\alpha}|\nabla f(x)-\nabla f(y)|^2$ .</font></p></li><li><p><strong>Theorem</strong></p><p><font color='blue'> Let $f$ be $\beta$-smooth and $\alpha$-strongly convex  on $\mathbb{R}^n$, $\kappa=\frac{\beta}{\alpha}$ as the condition number. Then gradient descent with $\eta=\frac{2}{\beta+\alpha}$ satisfies $f(x_{t+1})-f(x^<em>)\le\frac{\beta}{2}\exp(-\frac{4t}{\kappa+1}|x_1-x^</em>|^2)$   </font></p></li></ul><h4 id="Lower-bound-–-black-box">Lower bound – black box</h4><ul><li><p>A black-box procedure is a mapping from “history” to the next query point, that is it maps ($x_a ,g_1,\cdots,x_t,g_t$) (with $g_s\in\part f(x_s)$) to $x_{t+1}$. To simplify, make the following assumption on the black-box procedure: $x_1=0$  and for any $t\ge0，x_{t+1}$ is in the linear span of $g_1,\cdots,g_t$, that is $$x_{t+1}\in\mathrm{Span}(g_t ,\cdots,g_t)\tag{3.15}\label{eq315}$$</p></li><li><p><strong>Theorem</strong></p><ul><li><font color='blue'> Let $t\le n,L,R&gt;0$. There exists a convex and $L$-Lipschitz function $f$ such that for any black-box procedure satisfying  $\eqref{eq315}$, $\min\limits_{1\le s\le t}f(x_s)-\min\limits_{x\in B_2®}f(x_s)\ge\frac{RL}{2(1+\sqrt{t})}$, where $B_2®={x\in\mathbb{R}^n:|x|\le R}$ . There also exists an $\alpha$-strongly convex and  $L$-Lipschitz function $f$ such that for any black-box procedure satisfying  $\eqref{eq315}$, $\min\limits_{1\le s\le t}f(x_s)-\min\limits_{x\in B_2(\frac{L}{2\alpha})}f(x_s)\ge\frac{L^2}{8\alpha t}$</font></li><li><font color='blue'> Let $t\le (n-1)/2,\beta&gt;0$. There exists a $\beta$-smooth convex function $f$ such that for any black-box procedure satisfying  $\eqref{eq315}$, $\min\limits_{1\le s\le t}f(x_s)-f(x^<em>)\ge\frac{3\beta}{32}\frac{|x_1-x^</em>|^2}{(t+1)^2}$.</font></li><li><font color='blue'> Let $\kappa\ge 1$. There exists a $\beta$-smooth and $\alpha$-strongly convex function $f:\ell_2\rightarrow \mathbb{R}$ with $\kappa=\frac{\beta}{\alpha}$ such that for any $t\ge1$ and black-box procedure satisfying  $\eqref{eq315}$ one has $f(x_t)-f(x^<em>)\ge\frac{\alpha}{2}(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})^{2(t-1)}|x_1-x^</em>|^2$. Note that for large values of the condition number $\kappa$ one has $(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})^{2(t-1)}\approx\exp(-\frac{4(t-1)}{\sqrt{\kappa}})$</font></li></ul></li></ul><h2 id="References">References</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/103359560">【凸优化笔记5】-次梯度方法（Subgradient method）</a></li><li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></li><li><a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity</a></li><li>‘Understanding Analysis’ by Stephen Abbott. It’s a nice and light intro to analysis</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;泛函&quot;&gt;泛函&lt;/h2&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/img/Rules/lines.png&quot; width=500&quot;&gt;
&lt;/center&gt;
&lt;h3 id=&quot;不动点定理&quot;&gt;不动点定理&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;不动点定理的基本逻辑：对于一个存在性问题，构造一个度量空间和一个映射，使得存在性问题等价于这个映射的不动点。只要证明这个映射存在不动点，那么原来的存在性问题即得证。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33885648&quot;&gt;链接&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;紧性的利用&quot;&gt;紧性的利用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;证明存在性&lt;/li&gt;
&lt;li&gt;在无限维空间中“模仿”有限维的欧式空间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;紧集是为了模仿描述欧式空间中的有界闭集合么？&lt;br&gt;
紧=相对紧+闭&lt;/p&gt;
&lt;h3 id=&quot;流形-Manifolds&quot;&gt;流形(Manifolds)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;概念：高维空间中曲线、曲面概念的推广，如三维空间中的曲面为一二维流形。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;支撑集-Support&quot;&gt;支撑集(Support)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;概念：函数的非零部分子集；一个概率分布的支撑集为所有概率密度非零部分的集合&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>About</title>
    <link href="http://yoursite.com/posts/uncategorized/2019-06-13-about.html"/>
    <id>http://yoursite.com/posts/uncategorized/2019-06-13-about.html</id>
    <published>2019-06-13T17:02:18.000Z</published>
    <updated>2021-04-28T23:27:52.930Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to Mia Feng’s Blog!</p><p>I’m doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in Montreal, Canada. I’m interested in data analysis, GCNs, transfer learning, and anomaly detection.</p><p>I got the bachelor degree from Wuhan University, China, and then I did my master at the National University of Defense Technology, China. During my studies, I’ve processed spatial-temporal data, and financial data. I also worked as an internship of fintech in Meituan-Dianping. The main areas I have learned including machine learning, geographical information system, software engineering, data assimilation, and finance. Currently I am learning something about computational neuroscience. I want to learn more about transfer learning and visualization and explanation of neural networks.</p><p>Have a look at my <a href="https://github.com/skaudrey/cv/blob/master/cv.pdf">resume</a> for more information.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to Mia Feng’s Blog!&lt;/p&gt;
&lt;p&gt;I’m doing my PhD since 2020 in Machine Learning and video surveillance at University of Montreal in Mo
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>rl-intro</title>
    <link href="http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-26-notes-rl-intro.html</id>
    <published>2019-05-26T17:02:18.000Z</published>
    <updated>2021-01-12T20:56:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver.</p><a id="more"></a><h2 id="Why-RL">Why RL?</h2><h3 id="The-difference-between-supervised-learning-and-RL">The difference between supervised learning and RL</h3><ul><li><p>task directed from fixed data sets vs goal directed learning from interaction</p></li><li><p>characteristics:</p><ul><li>RL<ul><li>trial-and-error search</li><li>delayed reward</li></ul></li><li>ML<ul><li>generalization methods: regularization, data augmentation</li><li>real-time loss</li></ul></li></ul></li><li><p>methods:</p><ul><li>RL<ul><li>exploit: get reward</li><li>exploration: make better action selection in the future.</li></ul></li><li>ML<ul><li>discriminative and generative: parameterized and semi-parameterized</li><li>supervised and unsupervised: depends on whether have labeled data</li></ul></li></ul></li></ul><center>    <img src="/assets/img/RLIntro/RLML.png" width=400"></center><h3 id="The-capacity-of-RL">The capacity of RL</h3><ul><li>sequential decision maker facing unknown or known environment</li><li>works for non i.i.d. data</li></ul><center>    <img src="/assets/img/RLIntro/RLCapacity.png" width=300"></center><h2 id="What’s-RL">What’s RL?</h2><h3 id="The-agent-environment-interaction">The agent-environment interaction</h3><center>    <img src="/assets/img/RLIntro/RLEA.png" width=300"></center><ul><li>At each step t the agent:<ul><li>Executes action at</li><li>Transform to state St</li><li>Receives scalar reward rt</li></ul></li><li>The environment:<ul><li>Receives action at</li><li>Transform to state St</li><li>Emits scalar reward rt+1</li></ul></li><li>t increments at env. step</li></ul><h3 id="The-elements-of-RL">The elements of RL</h3><ul><li><p>Policy: agent’s behaviour function, mostly is a PDF mapping state to action</p><ul><li><p>Deterministic policy</p><p>$$<br>a = \pi\left(S\right)<br>$$</p></li><li><p>Stochastic policy</p><p>$$<br>\pi\left(a|S\right)=\mathbb{P}\left(A_t=a|S_t=s\right)<br>$$</p></li></ul></li><li><p>Value function: how good is each state and/or action, the scalar value is also named reward.</p><p>$$<br>v_{\pi}\left(s\right)=\mathbb{E}\left(R_{t+1}+\gamma R_{t+1}+{\gamma}^2 R_{t+2}+\cdots|S_t=s\right)<br>$$</p><p>Mostly, to make algorithm converge, a final state will be rewarded 0, and other non-final states are rewarded as a minus value.</p></li><li><p>Model: agent’s representation of the environment, they can be modeled by TKinter, gym etc.</p><ul><li><p>e.g.: models in assimilation, maze</p><p>$$<br>\mathcal{P}<em>{ss’}^{a}=\mathbb{P}\left(S</em>{t+1}=s’|S_t=s,A_t=a\right)\<br>\mathcal{R}<em>{s}^{a}=\mathbb{E}\left(R</em>{t+1}=s’|S_t=s,A_t=a\right)<br>$$</p></li><li><p>unknown environment can be stimulated by sampling</p></li></ul></li></ul><h3 id="Classification-of-RL">Classification of RL</h3><h4 id="What-you-want">What you want</h4><pre><code>* Value Based: No Policy (Implicit), Value Function* Policy Based: Policy, No Value Function* Actor Critic: Policy, Value Function</code></pre><h4 id="What-you-knew">What you knew</h4><pre><code>* Model-free: Policy and/or Value Function, No Model* Model-based: Policy and/or Value Function, Model</code></pre><h2 id="How-to-RL">How to RL?</h2><h3 id="Markov-process-–-to-simplify">Markov process – to simplify</h3><h4 id="Markov-Process">Markov Process</h4><p>$$<br>\mathbb{P}\left(S_{t+1}\right)=\mathbb{P}\left(S_{t+1}|S_1,\cdots,S_t\right)\<br>\mathcal{P}<em>{ss’}=\mathbb{P}\left(S</em>{t+1}=s’|S_t=s\right)<br>$$</p><h4 id="Markov-Rewarded-Process">Markov Rewarded Process</h4><p>$$<br>\langle S,\mathcal{P},\mathcal{R},\gamma\rangle<br>$$<br>solve the reward from state at time t to the final state, which can be also solved by adding immediate reward and discounted value of successor state.</p><p>$$<br>v\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)=\mathbb{E}\left(R_{t+1}+\gamma v\left(S_{t+1}|S_t=s\right)\right)<br>$$</p><h4 id="Markov-Decision-Process">Markov Decision Process</h4><p>$$<br>\langle S,\mathcal{A},\mathcal{P},\mathcal{R},\gamma\rangle<br>$$</p><p>Sequential decision making.</p><ul><li><p>state-value function<br>$$<br>v_{\pi}\left(s\right)=\mathbb{E}\left(G_t|S_t=s\right)<br>$$</p></li><li><p>action-value function<br>$$<br>q_{\pi}\left(s,a\right)=\mathbb{E}_{\pi}\left(G_t|S_t=s,A_t=a\right)<br>$$</p></li></ul><h3 id="Value？Policy？">Value？Policy？</h3><h4 id="Policy-Iteration">Policy Iteration</h4><center>    <img src="/assets/img/RLIntro/policyitr.png" width=300"></center><h4 id="Value-Iteration">Value Iteration</h4><center>    <img src="/assets/img/RLIntro/valueitr.png" width=300"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list an introduction of reinforce learning, mainly based on the slides given by David Silver.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>gnn</title>
    <link href="http://yoursite.com/posts/notes/2019-05-24-gnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-24-gnn.html</id>
    <published>2019-05-24T22:17:30.000Z</published>
    <updated>2019-09-23T09:27:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is built to list the generation and improvement of graph neural networks.</p><h1>Why Use</h1><ul><li>Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,</li></ul><h1>Basic lines</h1><p>Contents in this block mainly comes from paper <a href="https://arxiv.org/pdf/1812.08434.pdf">Graph Neural Networks: A Review of Methods and Applications</a> and</p><a id="more"></a><h2 id="History">History</h2><h3 id="The-proposal-of-GNNs">The proposal of GNNs</h3><p>learn a target node’s representation by propagating neighbor information via recurrent neural architectures in an iterative<br>manner until a stable fixed point is reached<br>Computation expensive</p><ul><li><p><a href="https://www.researchgate.net/publication/4202380_A_new_model_for_earning_in_raph_domains">A new model for learning in graph domains</a></p><ul><li>Big Question: processing the graph without losing topological information<ul><li>reason<br>Traditional preprocessing methods for graphs dropped topological information, and thus leads to poor performance and generalization.</li></ul></li><li>background<br>RNN can only handle graph-level problems; Traditional methods dropped topological information.</li></ul></li><li><p>the approximation capability of GNN, <a href="https://www.researchgate.net/publication/23763868_Computational_Capabilities_of_Graph_Neural_Networks">Computational Capabilities of Graph Neural Networks</a><br>under mild generic conditions, most of the practically useful functions on graphs can be approximated in probability by GNNs up to any prescribed degree of accuracy.</p></li><li><p><a href="https://ieeexplore.ieee.org/document/4773279">Neural network for graphs: A contextual constructive approach</a></p></li><li><p><a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a></p></li></ul><h3 id="Go-to-GNNs">Go to GNNs</h3><h4 id="Spectral-based-Graph">Spectral-based Graph</h4><p>difficult to parallel or scale to large graphs,cause they need to load the whole graph into the memory.<br>relies on eigen-decomposition of the Laplacian matrix.</p><ul><li>Spectral GNN, <a href="https://arxiv.org/pdf/1312.6203v3.pdf">Spectral networks and locally connected networks on graphs</a></li><li>ChebNet, <a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a>，codes [here](<a href="https://github.com/mdeff/cnn">https://github.com/mdeff/cnn</a> graph).</li><li>1st ChebNet, <a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a>, code <a href="https://github.com/tkipf/gcn">here</a>.<br>localized in space, but the computation requirement grow exponentially, to reduce it, sampling methods are proposed. See <a href="https://arxiv.org/pdf/1801.10247.pdf">FASTGCN</a>, <a href="https://arxiv.org/pdf/1710.10568.pdf">reduce variance</a> and <a href="https://arxiv.org/pdf/1809.05343v1.pdf">adaptive sampling</a> for details.</li><li>AGCN, calculate a pairwise distance of nodes to construct a residual graph, see <a href="https://arxiv.org/pdf/1801.03226.pdf">Adaptive Graph Convolutional Neural Networks</a> for details.</li></ul><h4 id="Spatial-based-Graph-Convolution">Spatial-based Graph Convolution</h4><p>has gained more attention</p><ul><li><a href="https://arxiv.org/pdf/1706.02216.pdf">Inductive representation learning on large graphs</a></li><li><a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model cnns</a></li><li><a href="https://arxiv.org/pdf/1605.05273.pdf">Learning convolutional neural networks for graphs</a></li><li><a href="https://arxiv.org/abs/1808.03965">Large-scale learnable graph convolutional networks</a><br>[1],[4] used sampling strategy<br>the common way is to stack multiple graph convolution layer together.</li></ul><h5 id="Recurrent-based-Spatial-GCNs">Recurrent-based Spatial GCNs</h5><p>update a node’s representation recursively until a stable fixed point is reached</p><ul><li>GNNs, <a href="https://persagen.com/files/misc/scarselli2009graph.pdf">The graph neural network model</a></li><li>GGNNs, used GRU, <a href="https://www.aclweb.org/anthology/D14-1179">Learning phrase representations using rnn encoder-decoder for statistical machine translation</a>, codes <a href="https://github.com/yujiali/ggnn">here</a>.</li><li>Stochastic Steady-state Embedding (SSE), updates the node latent representations stochastically in an asynchronous fashion, <a href="http://proceedings.mlr.press/v80/dai18a/dai18a.pdf">Learning steady-states of iterative algorithms over graphs</a>, codes [here](<a href="https://github.com/Hanjun-Dai/steady">https://github.com/Hanjun-Dai/steady</a> state embedding).</li></ul><h5 id="Composition-based-Spatial-GCNs">Composition-based Spatial GCNs</h5><ul><li>Message Passing Neural Networks (MPNNs), <a href="https://arxiv.org/pdf/1704.01212.pdf">Neural Message Passing for Quantum Chemistry</a></li><li>GraphSage, <a href="https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf">Inductive representation learning on large graphs</a>, codes <a href="https://github.com/williamleif/GraphSAGE">here</a>.</li></ul><h6 id="Miscellaneous-Variants-of-Spatial-GCNs">Miscellaneous Variants of Spatial GCNs</h6><ul><li>Diffusion Convolution Neural Networks (DCNN), the hidden node representation is get by independently convolving inputs with power series or transition probability matrix, <a href="https://arxiv.org/pdf/1511.02136.pdf">Diffusion-convolutional neural networks</a></li><li>Build GCN into a standard grid to do CNN, <a href="http://proceedings.mlr.press/v48/niepert16.pdf">Learning convolutional neural networks for graphs</a>, but it ignored the node information.</li><li>Large-scale Graph Convolution Networks (LGCN), <a href="https://arxiv.org/pdf/1808.03965.pdf">Large-scale learnable graph convolutional networks</a>, still using standard grid, but it also collects nodes’ information and draw subgraph for mini-batch training. Codes <a href="https://github.com/divelab/lgcn/">here</a>.</li><li>Mixture Model Network (MoNet), <a href="https://arxiv.org/pdf/1611.08402.pdf">Geometric deep learning on graphs and manifolds using mixture model CNNs</a>, introduce pseudo-coordinates and weight functions to let the weight of a node’s neighbor be determined by the relative position (pseudo-coordinates) between the node and its neighbor.</li><li><a href="https://arxiv.org/abs/1802.00910">Geniepath: Graph neural networks with adaptive receptive paths</a>, everages gating mechanisms to control the depth and breadth of a node’s neighborhood.</li><li><a href="https://persagen.com/files/misc/zhuang2018dual.pdf">Dual graph convolutional networks for graph-based semi-supervised classification</a>, one for global representation the other for local representation.</li><li><a href="https://arxiv.org/pdf/1811.10435.pdf">On filter size in graph convolutional networks</a>, introduce a hyperparameter to influence the receptive field size of a node.</li></ul><h4 id="Pooling-module">Pooling module</h4><ul><li><a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a></li><li>pooling by rearranging vertices into meaningful order, <a href="https://arxiv.org/pdf/1506.05163.pdf">Deep convolutional networks on graph-structured data</a></li><li><a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf">An end-to-end deep learning architecture for graph classification</a></li><li>DIFFPOOL pools nodes hierarchically by learning a cluster assignment matrix in each layer to get a cluster embedding, which can be combined with any standard GCN module, <a href="https://arxiv.org/pdf/1806.08804.pdf">Hierarchical graph representation learning with differentiable pooling</a></li></ul><h3 id="Graph-attention-networks">Graph attention networks</h3><p>For sequence-based tasks, in total, assigning attention weights to different neighbors when aggregating feature information, ensembling multiple models according to attention weights, and using attention weights to guide random walks.</p><ul><li><a href="https://arxiv.org/pdf/1710.10903.pdf">Graph attention networks</a>, (GAT), multi-head weights. Codes <a href="https://github.com/PetarV-/GAT">here</a>.</li><li><a href="https://arxiv.org/pdf/1803.07294.pdf">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</a>， also use multi-head, but it use a self attention mechanism to compute a different head for each head.</li><li><a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">Graph classification using structural attention</a>, Graph Attention Model (GAM), adaptively visiting a sequence of important nodes.</li><li><a href="https://arxiv.org/pdf/1710.09599.pdf">Watch your step: Learning node embeddings via graph attention</a>, factorize the co-occurrence matrix with differentiable attention weights.</li></ul><h3 id="Graph-Autoencoders">Graph Autoencoders</h3><p>had to handle the problem caused by the sparsity of adjacency matrix.</p><ul><li><a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf">Deep neural networks for learning graph representations</a>, uses the stacked denoising auto-encoders to reconstruct PPMI matrix. Codes <a href="https://github.com/ShelsonCao/DNGR">here</a></li><li><a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">Structural deep network embedding</a>, preserve nodes first-order proximity (drive representations of adjacent nodes close to each other) and second-order proximity (a node’s neighbourhood information) jointly. Codes <a href="https://github.com/suanrong/SDNE">here</a></li><li><a href="https://arxiv.org/pdf/1611.07308.pdf">Variational graph auto-encoders</a>, Graph Auto-Encoder (GAE), combined with GCN firstly. Codes <a href="https://github.com/limaosen0/Variational-Graph-Auto-Encoders">here</a></li><li><a href="https://shiruipan.github.io/pdf/CIKM-17-Wang.pdf">Mgae: Marginalized graph autoencoder for graph clustering</a>, reconstruct node’s hidden state.</li><li><a href="https://www.ijcai.org/proceedings/2018/0362.pdf">Adversarially regularized graph autoencoder for graph embedding</a>, using GANs to regularize the graph auto-encoders, recover adjacency matrix. Codes <a href="https://github.com/Ruiqi-Hu/ARGA">here</a></li><li><a href="https://www.kdd.org/kdd2018/accepted-papers/view/learning-deep-network-representations-with-adversarially-regularized-autoen">Learning deep network representations with adversarially regularized autoencoders</a>, recover node sequences rather than adjacency matrix.</li><li><a href="http://pengcui.thumedialab.com/papers/NE-RegularEquivalence.pdf">Deep recursive network embedding with regular equivalence</a>, codes <a href="https://github.com/tadpole/DRNE">here</a></li></ul><h3 id="Graph-Generative-Networks">Graph Generative Networks</h3><p>Not scalable to large graphs.</p><ul><li><a href="https://arxiv.org/pdf/1802.08773.pdf">Graphrnn: A deep generative model for graphs</a>, graph-level RNN + node-level RNN, use breadth-first-search (BFS) to sequence the nodes and Bernoulli assumption for edge generation. Codes <a href="https://github.com/snap-stanford/GraphRNN">here</a>.</li><li><a href="https://arxiv.org/pdf/1803.03324.pdf">Learning deep generative models of graphs</a>, utilize spatial-based GCNs to obtain a hidden representation of an existing graph.</li><li><a href="https://arxiv.org/pdf/1805.11973.pdf">Molgan: An implicit generative model for small molecular graphs</a>, RL+GAN+GCN</li><li><a href="https://arxiv.org/pdf/1803.00816.pdf">Net-gan: Generating graphs via random walks</a>, combines LSTM with Wasserstein GAN to generate graphs from a random-walk-based approach. As for random walk, see <a href="http://leogrady.net/wp-content/uploads/2017/01/grady2004multilabel.pdf">here</a>.</li><li><a href="https://arxiv.org/pdf/1809.02630.pdf">Constrained generation of semantically valid graphs via regularizing variational autoencoders</a></li></ul><h3 id="Graph-Spatial-Temporal-Networks">Graph Spatial-Temporal Networks</h3><ul><li><a href="https://arxiv.org/pdf/1612.07659.pdf">Structured sequence modeling with graph convolutional recurrent networks</a></li><li><a href="https://arxiv.org/pdf/1707.01926.pdf">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</a>, can work forwardly or backwardly. Codes <a href="https://github.com/liyaguang/DCRNN">here</a>.</li><li><a href="https://arxiv.org/pdf/1709.04875.pdf">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</a>, codes <a href="https://github.com/VeritasYin/STGCN_IJCAI-18">here</a>.</li><li><a href="https://arxiv.org/pdf/1801.07455.pdf">Spatial temporal graph convolutional networks for skeleton-based action recognition</a>, extend the temporal flow as graph edges, and then assign each a label to each edge. Codes <a href="https://github.com/yysijie/st-gcn">here</a>.</li><li><a href="https://arxiv.org/pdf/1511.05298.pdf">Structural-rnn: Deep learning on spatio-temporal graphs</a>, aims at predicting nodes’ labels at each time, has nodeRNN and edgeRNN, and split nodes and edges into semantic groups. Codes <a href="https://github.com/asheshjain399/RNNexp">here</a>.</li></ul><h2 id="Main-Methodologies-–-Graph-Embedding">Main Methodologies – Graph Embedding</h2><h3 id="Matrix-Factorization">Matrix Factorization</h3><ul><li><a href="https://www.ijcai.org/proceedings/2018/0493.pdf">Discrete network embedding</a></li><li><a href="https://shiruipan.github.io/pdf/ICDM-18-Yang.pdf">Binarized attributed network embedding</a></li></ul><h3 id="Random-Walks">Random Walks</h3><ul><li><a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf">Deepwalk: Online learning of social representations</a></li></ul><h2 id="Problems">Problems</h2><ul><li>Does going deeper always work in GNNs?</li><li>How to select representative receptive field for a node?</li><li>How to work on large graphs?</li><li>How to handle dynamic and heterogeneous graph structures?</li></ul><h1>Papers</h1><h2 id="Introduction">Introduction</h2><ul><li>M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, &quot;Geometric deep learning: going beyond euclidean data,&quot;IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 18–42, 2017</li></ul><h2 id="others">others</h2><h3 id="Are-Graph-Neural-Networks-Miscalibrated"><a href="https://arxiv.org/pdf/1905.02296v1.pdf">Are Graph Neural Networks Miscalibrated?</a></h3><h3 id="Estimating-Node-Importance-in-Knowledge-Graphs-Using-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1905.08865.pdf">Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks</a></h3><p>GENI, a GNN-based method designed to deal with distinctive challenges involved with predicting node importance in KGs.</p><h3 id="Understanding-attention-in-graph-neural-networks"><a href="https://arxiv.org/pdf/1905.02850.pdf">Understanding attention in graph neural networks</a></h3><p>We aim to better understand attention over nodes in graph neural networks and identify factors influencing its effectiveness. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 40% in some of our classification tasks</p><h3 id="Are-Graph-Neural-Networks-Miscalibrated-2"><a href="https://arxiv.org/pdf/1905.02296.pdf">Are Graph Neural Networks Miscalibrated?</a></h3><p>Graph Neural Networks (GNNs) have proven to be successful in many classification tasks, outperforming previous state-of-the-art methods in terms of accuracy</p><h3 id="Graph-Convolutional-Networks-with-EigenPooling"><a href="https://arxiv.org/pdf/1904.13107.pdf">Graph Convolutional Networks with EigenPooling</a></h3><p>To apply graph neural networks for the graph classification task, approaches to generate the \textit{graph representation} from node representations are demanded. Experimental results of the graph classification task on $6$ commonly used benchmarks demonstrate the effectiveness of the proposed framework.</p><h3 id="PAN-Path-Integral-Based-Convolution-for-Deep-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1904.10996.pdf">PAN: Path Integral Based Convolution for Deep Graph Neural Networks</a></h3><p>Experimental results show that the path integral based graph neural networks have great learnability and fast convergence rate, and achieve state-of-the-art performance on benchmark tasks.</p><h3 id="Attacking-Graph-based-Classification-via-Manipulating-the-Graph-Structure"><a href="https://arxiv.org/pdf/1903.00553.pdf">Attacking Graph-based Classification via Manipulating the Graph Structure</a></h3><p>We evaluate our attacks and compare them with a recent attack designed for graph neural networks. Results show that our attacks 1) can effectively evade graph-based classification methods; 2) do not require access to the true parameters, true training dataset, and/or complete graph; and 3) outperform the existing attack for evading collective classification methods and some graph neural network methods</p><h3 id="Deep-learning-in-bioinformatics-introduction-application-and-perspective-in-big-data-era"><a href="https://arxiv.org/abs/1903.00342">Deep learning in bioinformatics: introduction, application, and perspective in big data era</a></h3><p>After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures</p><h3 id="Constant-Time-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1901.07868.pdf">Constant Time Graph Neural Networks</a></h3><p>Recent advancements in graph neural networks (GNN) have led to state-of-the-art performance in various applications including chemo-informatics, question answering systems, and recommendation systems, to name a few</p><h3 id="A-Comprehensive-Survey-on-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1901.00596.pdf">A Comprehensive Survey on Graph Neural Networks</a></h3><p>We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories</p><h3 id="Graph-Transformation-Policy-Network-for-Chemical-Reaction-Prediction"><a href="https://openreview.net/pdf?id=r1f78iAcFm">Graph Transformation Policy Network for Chemical Reaction Prediction</a></h3><p>To this end, we propose Graph Transformation Policy Network (GTPN) – a novel generic method that combines the strengths of graph neural networks and reinforcement learning to learn the reactions directly from data with minimal chemical knowledge. Evaluation results show that GTPN improves the top-1 accuracy over the current state-of-the-art method by about 3% on the large USPTO dataset</p><h3 id="Contextualized-Non-local-Neural-Networks-for-Sequence-Learning"><a href="https://arxiv.org/pdf/1811.08600.pdf">Contextualized Non-local Neural Networks for Sequence Learning</a></h3><p>Recently, a large number of neural mechanisms and models have been proposed for sequence learning, of which self-attention, as exemplified by the Transformer model, and graph neural networks (GNNs) have attracted much attention. Specifically, we propose contextualized non-local neural networks (CN$^{\textbf{3}}$), which can both dynamically construct a task-specific structure of a sentence and leverage rich local dependencies within a particular neighborhood.</p><h3 id="Automated-Theorem-Proving-in-Intuitionistic-Propositional-Logic-by-Deep-Reinforcement-Learning"><a href="https://arxiv.org/pdf/1811.00796.pdf">Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning</a></h3><p>Using the large volume of augmented data, we train highly accurate graph neural networks that approximate the value function for the set of the syntactic structures of formulas. Within the specified time limit, our prover solved 84% of the theorems in a benchmark library, while $\texttt{tauto}$ was able to solve only 52%.</p><h3 id="Pileup-mitigation-at-the-Large-Hadron-Collider-with-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1810.07988.pdf">Pileup mitigation at the Large Hadron Collider with Graph Neural Networks</a></h3><p>We present a classifier based on Graph Neural Networks, trained to retain particles coming from high-transverse-momentum collisions, while rejecting those coming from pileup collisions. This model is designed as a refinement of the PUPPI algorithm, employed in many LHC data analyses since 2015</p><h3 id="Weisfeiler-and-Leman-Go-Neural-Higher-order-Graph-Neural-Networks"><a href="https://arxiv.org/pdf/1810.02244.pdf">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</a></h3><p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. The following work investigates GNNs from a theoretical point of view and relates them to the $1$-dimensional Weisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have the same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic (sub-)graphs</p><h3 id="Multitask-Learning-on-Graph-Neural-Networks-Learning-Multiple-Graph-Centrality-Measures-with-a-Unified-Network"><a href="https://arxiv.org/pdf/1809.07695.pdf">Multitask Learning on Graph Neural Networks - Learning Multiple Graph Centrality Measures with a Unified Network</a></h3><p>Graph neural networks (GNN), consisting of trained neural modules which can be arranged in different topologies at run time, are sound alternatives to tackle relational problems which lend themselves to graph representations. The proposed model achieves $89%$ accuracy on a test dataset of random instances with up to 128 vertices and is shown to generalise to larger problem sizes</p><h3 id="Meta-GNN-On-Few-shot-Node-Classification-in-Graph-Meta-learning"><a href="https://arxiv.org/pdf/1905.09718.pdf">Meta-GNN: On Few-shot Node Classification in Graph Meta-learning</a></h3><p>However, there are very few works applying meta-learning to non-Euclidean domains, and the recently proposed graph neural networks (GNNs) models do not perform effectively on graph few-shot learning problems. Additionally, Meta-GNN is a general model that can be straightforwardly incorporated into any existing state-of-the-art GNN</p><h3 id="MR-GNN-Multi-Resolution-and-Dual-Graph-Neural-Network-for-Predicting-Structured-Entity-Interactions"><a href="https://arxiv.org/pdf/1905.09558.pdf">MR-GNN: Multi-Resolution and Dual Graph Neural Network for Predicting Structured Entity Interactions</a></h3><p>In recent years, graph neural networks have become attractive. Experiments conducted on real-world datasets show that MR-GNN improves the prediction of state-of-the-art methods.</p><h3 id="Revisiting-Graph-Neural-Networks-All-We-Have-is-Low-Pass-Filters"><a href="https://arxiv.org/pdf/1905.09550.pdf">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</a></h3><p>In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property</p><h3 id="Multi-hop-Reading-Comprehension-across-Multiple-Documents-by-Reasoning-over-Heterogeneous-Graphs"><a href="https://arxiv.org/pdf/1905.07374.pdf">Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs</a></h3><p>We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based model (single model) achieves state-of-the-art result.</p><h3 id="IPC-A-Benchmark-Data-Set-for-Learning-with-Graph-Structured-Data"><a href="https://arxiv.org/pdf/1905.06393.pdf">IPC: A Benchmark Data Set for Learning with Graph-Structured Data</a></h3><p>The data set, named IPC, consists of two self-contained versions, grounded and lifted, both including graphs of large and skewedly distributed sizes, posing substantial challenges for the computation of graph models such as graph kernels and graph neural networks</p><h1>Datasets</h1><ul><li><a href="https://www.aminer.cn/citation">Citation Networks</a>: <a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/cora_raw.zip">Cora</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/citeseer.zip">Citeseer</a>,<a href="https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/pubmed.zip">Pubmed</a>,<a href="https://www.aminer.cn/citation">DBLP</a></li><li><a href="http://networkrepository.com/soc_BlogCatalog.php">Social Networks</a>: <a href="http://socialcomputing.asu.edu/datasets/BlogCatalog">BlogCatalog</a>,<a href="https://github.com/linanqiu/reddit-dataset">Reddit</a>,<a href="http://www.trustlet.org/downloaded_epinions.html">Epinions</a></li><li>Chemical/Biological Graphs: <a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI1.zip">NCI-1</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/NCI109.zip">NCI-9</a>,<a href="https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/MUTAG.zip">MUTAG</a>, D&amp;D,<a href="https://github.com/bigdata-ustc/QM9nano4USTC">QM9</a>,<a href="https://tripod.nih.gov/tox21/challenge/data.jsp">Tox21</a>,<a href="http://snap.stanford.edu/graphsage/ppi.zip">PPI</a>.</li><li>Unstructured Graphs: convert <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, <a href="http://www.mattmahoney.net/dc/textdata.html">Wikipedia</a> or News Groups into graphs.</li><li>Others: <a href="https://pan.baidu.com/s/14Yy9isAIZYdU__OYEQGa_g#list/path=%2F">METR-LA</a>, <a href="https://grouplens.org/datasets/movielens/1m/">Movies-Lens1M</a>, NELL.</li></ul><p>Thanks for the links given <a href="https://www.jianshu.com/p/67137451b67f">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is built to list the generation and improvement of graph neural networks.&lt;/p&gt;
&lt;h1&gt;Why Use&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Non-euclidean data: Irregular. Each graph has a variable size of unordered nodes and each node in a graph has a different number of neighbors,&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Basic lines&lt;/h1&gt;
&lt;p&gt;Contents in this block mainly comes from paper &lt;a href=&quot;https://arxiv.org/pdf/1812.08434.pdf&quot;&gt;Graph Neural Networks: A Review of Methods and Applications&lt;/a&gt; and&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>UNet proposed by Olaf Ronneberger etc.</title>
    <link href="http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-23-notes-paper-UNet.html</id>
    <published>2019-05-23T21:18:21.000Z</published>
    <updated>2021-01-12T19:33:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>The notes of paper <a href="https://arxiv.org/abs/1505.04597v1">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>.</p><a id="more"></a><h2 id="Big-Question-classification-in-pixel-level-and-thus-image-segmentation">Big Question: classification in pixel level and thus image segmentation</h2><h3 id="reason">reason</h3><ul><li>single label for a image is not enough to support segmentation.</li></ul><h3 id="background">background</h3><ul><li>why focus on it<ul><li>biomedical images: like cells segmentation.<br>Add: also appropriate for other entity segmentation</li></ul></li><li>how have been done:<ul><li>the development of deeper CNNs</li><li>using CNNs segmenting electron microscopy images</li></ul></li><li>what have been missed:<ul><li>computation efficiency and redundancy: slow cause every patch require a running of network; patch overlapping</li><li>difficult trade-off for localization and the usage of context.</li></ul></li></ul><h2 id="Methods">Methods</h2><h3 id="For-what">For what?</h3><p>Cells segmentation.</p><h3 id="Framework-of-Methods">Framework of Methods</h3><p>Convolution2D + deconvolution (upsampling 2D). The output of one downsampling layer is contracted as part of the input of the corresponding symmetric upsampling layer.</p><h3 id="Novelty">Novelty</h3><ul><li>data augmentation randomly elastic deformations: shift, rotation, gray value, random elastic deformations are the most important</li><li>replace pooling by upsampling.</li><li>No fully connection layers.</li><li>weighted the loss of touching objects (cells).</li></ul><h2 id="Details">Details</h2><h3 id="weighted-map-to-segment-overlapped-cells">weighted map to segment overlapped cells</h3><p>According to the paper, they pre-compute the weight map for each ground truth segmentation to compensate the different frequency of pixels.</p><h3 id="data-augmentation">data augmentation</h3><p>Smooth deformations using random displacements vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation.</p><h2 id="Abstract">Abstract</h2><p>The main idea in abstract are contracted NNs and data augmentation so that the new NNs can get reasonable results by fewer images.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The notes of paper &lt;a href=&quot;https://arxiv.org/abs/1505.04597v1&quot;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>paper--Faster RCNN</title>
    <link href="http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster%20rcnn.html"/>
    <id>http://yoursite.com/posts/notes/2019-05-19-notes-paper-faster rcnn.html</id>
    <published>2019-05-19T15:17:12.000Z</published>
    <updated>2021-01-12T20:48:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Some understanding about the details in Faster RCNN, based on the codes in tensorflow.</p><a id="more"></a><h2 id="Big-Question">Big Question</h2><h3 id="reason">reason</h3><h3 id="background">background</h3><h2 id="Region-Proposal-Network">Region Proposal Network</h2><h3 id="Some-numbers">Some numbers</h3><ul><li>The number of anchor boxes for one anchor target $k = scale \times ratios$,</li><li>The number of anchor boxes for one feature layer (which has $W \times H$ grids), will get $ W \times H \times k $ anchor boxes. Every grid in the feature map (the output of a popular CNN without FC layers) will have $k$ anchor boxes.</li><li>Not like the ROI method, the size of features are fixed, but anchor boxes are rescaled by $k$ regressors.</li></ul><h2 id="Experiments">Experiments</h2><h3 id="prove">prove</h3><ul><li>The top-ranked RPN proposals are accurate.</li><li>NMS does not harm the detection mAP and may reduce false alarms.</li></ul><h2 id="Construct">Construct</h2><h3 id="Add-loss">Add loss</h3><h3 id="Problems-using-it-processing-typhoon-data">Problems using it processing typhoon data</h3><ul><li>Does NMS lead to loss of typhoon? Not really, the texture of typhoon is obvious in image.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Some understanding about the details in Faster RCNN, based on the codes in tensorflow.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Positioning Data of FY4 AGRI.</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-FY4-AGRI-Calibration.html</id>
    <published>2019-04-26T22:53:05.000Z</published>
    <updated>2019-04-26T15:31:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.</p><p>There are two ways for positioning, one is querying the lookup table given by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">NSMC</a>, the other is calculating by <a href="http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3">formulas</a>.</p><p>However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days.</p><a id="more"></a><h2 id="Querying-the-lookup-table">Querying the lookup table.</h2><p>There are two errors in the files.</p><ul><li>The first 8 bytes denote latitude, and the next 8 bytes are reserved for longitude.</li><li>The data are stored as little-endian data.</li></ul><center>    <img src="/assets/img/FY4-AGRI/lookup.png" width="400"></center><h2 id="Calculating-by-formulas">Calculating by formulas</h2><p>The formulas are OK, but the constant variable $\lambda_D$ should be measured in rad before being used.</p><center>    <img src="/assets/img/FY4-AGRI/formulas.png" width="400"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently I processed some data detected by AGRI, a sensor loaded on FY-4 Satellite, which was launched by China. Fourteen channels designed for AGRI observe almost half of the earth in minutes. However, because AGRI is an imager, data generated by it need positioning.&lt;/p&gt;
&lt;p&gt;There are two ways for positioning, one is querying the lookup table given by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;NSMC&lt;/a&gt;, the other is calculating by &lt;a href=&quot;http://satellite.nsmc.org.cn/PortalSite/StaticContent/DocumentDownload.aspx?TypeID=3&quot;&gt;formulas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, there are some errors in the files given by NSMC, I wrote this note in case others will meet the same trouble I got these days.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="Sensor" scheme="http://yoursite.com/tags/Sensor/"/>
    
  </entry>
  
  <entry>
    <title>A Tex Template of Cornell Notes</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-Cornell-notes-tex-templates.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2019-04-26T15:34:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these.</p><a id="more"></a><h2 id="Preliminaries">Preliminaries</h2><p>Before starting, you need install</p><ul><li>CTex</li></ul><h2 id="Install-missed-packages-of-CTex">Install missed packages of CTex</h2><h3 id="Download-the-required-packages">Download the required packages</h3><p>Search <a href="https://www.ctan.org/pkg">here</a>.</p><h3 id="Unzip-and-compile-manually-if-needed">Unzip and compile manually if needed.</h3><p>Unzip the downloaded file and jump to the directory after unzip. If need compile, run</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  pdflatex ***.ins</span><br></pre></td></tr></table></figure><h3 id="Install">Install</h3><p>Copy the compiled file folder to the CTex path: ~/CTex/CTex/tex/latex</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ texhash --admin</span><br></pre></td></tr></table></figure><p>The notes will be generated like this:</p><center>    <img src="/assets/img/CornellNotes.png" width="400"></center><h1>Acknowledgement</h1><p>Thank <a href="https://blog.csdn.net/Myriad_Dreamin/article/details/83384110">this blog</a>.</p><h1>Resources</h1><p>Tex file can be found <a href="https://github.com/skaudrey/skaudrey.github.io/tree/master/assets/notes/Cornell">here</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I just found a new method known as Cornell method for keeping notes. To keep notes efficiently, I deploy a tex template on my laptop. There are some packages of tex missed, like tcolorbox, and I fixed these.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>Understand and debug the codes of GCN proposed by Thomas N. Kipf</title>
    <link href="http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html"/>
    <id>http://yoursite.com/posts/notes/2019-04-26-notes-paper-GCN-SemiClassification.html</id>
    <published>2019-04-26T22:21:35.000Z</published>
    <updated>2021-01-27T14:45:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1609.02907.pdf">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>.</p><a id="more"></a><h2 id="Big-Question-semi-supervised-classification-of-graph-data">Big Question: semi-supervised classification of graph data</h2><ul><li>reason<ul><li>computation effective: semi-supervision</li><li>the complex of graphs, the information of nodes and edges are not structural information.</li></ul></li><li>background<ul><li>the improvement of GCNs: spectral GCNs</li></ul></li></ul><h2 id="Key-points">Key points</h2><h3 id="The-approximation-of-spectral-graph-convolution">The approximation of spectral graph convolution</h3><p>The lines in paper had confused me at first before I ran the codes.</p><p>The difference of graph convolution and valina convolution is the input, as the input is a graph rather than data in same dimension, the key point is how to convert data represented by node and graph to a tensor in fixed dimension.</p><p>To solve it, Thomas maps the graph into a spectral space and also, to be computational efficient, approximate the infinite coefficients by second-order Chebyshev polynomial formulas.</p><p>After those approximation, it is input into the whole network with features.</p><h3 id="Build-model">Build model</h3><p>Actually, except the complicated preprocess to represent graph G into a sparse tensor, the other step are not that complex, just the similar as what a convolution layer do.<br>$$<br>Z = f\left(\mathbf{X},A\right)=softmax\left(\hat{A}ReLU\left({\hat{A}XW^{\left(0\right)}}\right)W^{\left(1\right)}\right)<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Paper &lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="gcn" scheme="http://yoursite.com/tags/gcn/"/>
    
  </entry>
  
  <entry>
    <title>Build up personal blog</title>
    <link href="http://yoursite.com/posts/notes/2019-01-28-hexo+killy+github%20pages=blog.html"/>
    <id>http://yoursite.com/posts/notes/2019-01-28-hexo+killy+github pages=blog.html</id>
    <published>2019-01-28T18:33:39.000Z</published>
    <updated>2021-02-07T15:48:06.557Z</updated>
    
    <content type="html"><![CDATA[<p>This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github.</p><a id="more"></a><h2 id="Preliminaries">Preliminaries</h2><p>Before starting, you need:</p><ul><li><p>node.js+npm</p><p>Get node.js from <a href="https://pan.baidu.com/s/1kU5OCOB#list/path=%2Fpub%2Fnodejs">here</a>. Check <a href="https://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000/00143450141843488beddae2a1044cab5acb5125baf0882000">here</a> for more info of node.</p></li><li><p>hexo<br>Install by npm:</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li><li><p>git,</p></li><li><p>an account of Github,</p></li></ul><p>and configure the ssh-key on your device.</p><h2 id="Build-blog">Build blog</h2><h3 id="Initialize-hexo-with-hexo">Initialize hexo with hexo</h3><p>Create a local folder as your root directory, such as “blog”, and go to the directory in your terminal and initialize it by hexo.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br><span class="line">$ <span class="built_in">cd</span> blog</span><br></pre></td></tr></table></figure><p>Then initialize this directory with npm.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="Link-hexo-with-Github">Link hexo with Github</h3><p>Set deployment tool,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>and initialize the remote repository for your blog on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br><span class="line">$ git add *</span><br><span class="line">$ git commit -m <span class="string">&quot;init commit&quot;</span></span><br></pre></td></tr></table></figure><p>Change the deployment in file “_config.yml” like:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line">deploy: </span><br><span class="line">    <span class="built_in">type</span>: git </span><br><span class="line">    repo: git@github.com:jack/jack.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure><p>Tips: The name of your hosting repository should be “[githubname].github.io”, such as “<a href="http://jack.github.io">jack.github.io</a>”. And mind the blankspaces while rewriting file “_config.yml”.</p><h3 id="Generate-static-files">Generate static files</h3><p>Do it before you push it on Github.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><h1>see your pages</h1><p>Click https://[githubname].github.io/, such as <a href="https://jack.github.io/">https://jack.github.io/</a>.</p><h2 id="Customization">Customization</h2><h3 id="change-theme">change theme</h3><p>I picked theme yilia. Configuration should be done as bellow:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ git <span class="built_in">clone</span> https://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br></pre></td></tr></table></figure><p>Change the default theme defined in “_config.yml” under root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: yilia</span><br></pre></td></tr></table></figure><h3 id="upload-your-avatar">upload your avatar</h3><p>New a folder under the “source” directory, I named it assets. I also new the “img” folder for pictures. Put you avatar picture here. Then reconfigure the _config.yml file beneath theme “yilia”'s folder, which is:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avatar: /assets/img/avatar.jpg</span><br></pre></td></tr></table></figure><h3 id="classify-your-posts-by-categories-rather-than-tags-in-default">classify your posts by categories rather than tags in default</h3><p>Now take your eye away from file “_config.yml” under theme yilia, open the file “_config.yml” under the root directory of your blog. You need to configure category_map, for instance,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  about: about</span><br><span class="line">  notes: notes</span><br><span class="line">  projects: projects</span><br><span class="line">  papers: papers</span><br><span class="line">  talks: talks</span><br><span class="line">  meetings: meetings</span><br></pre></td></tr></table></figure><p>Each pair of it can be different, it is just a mapping, such as:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">category_map:</span><br><span class="line">  parole: talks</span><br><span class="line">  关于我: about</span><br></pre></td></tr></table></figure><h3 id="change-the-naming-rule-of-a-new-post">change the naming rule of a new post</h3><p>The default naming rule of hexo is YYYY/MM/DD/[post name], which leads to a hyper-link without html suffix. I change it as html. It can be accomplished by configure the _config.yml in root directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permalink: posts/:category/:year-:month-:day-:title.html</span><br></pre></td></tr></table></figure><h3 id="Truncate-the-post-in-home-list-when-it-is-too-long">Truncate the post in home list when it is too long.</h3><p>You need to add</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure><p>after where you want to truncate in a post. And configure the _<em>config.yml</em> which is under themes’ folder.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The truncate signal while post is too long.</span></span><br><span class="line">excerpt_link: <span class="string">&quot;more&quot;</span></span><br></pre></td></tr></table></figure><h3 id="Support-Latex">Support Latex</h3><p>Check <a href="https://www.jianshu.com/p/5623c5e35c93">here</a> for details.</p><p>Tips: if things don’t work try restart, regenerate and redeploy.</p><h1>Tips</h1><p>You can debug pages locally by</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>, which is convenient before deployment.</p><h1>Acknowledgement</h1><p>Thank <a href="https://www.cnblogs.com/wumz/p/8030244.html">Mauger</a>, and <a href="https://github.com/litten/hexo-theme-yilia">litten</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post will show you how to build up a personal blog by node and hexo. Killy is responsible for building static pages. Laterly the blog will be hosted on Github.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>Clouds detection of infrared hyperspectral data based on logistic.</title>
    <link href="http://yoursite.com/posts/projects/2019-01-28-lr.html"/>
    <id>http://yoursite.com/posts/projects/2019-01-28-lr.html</id>
    <published>2019-01-28T18:33:39.000Z</published>
    <updated>2021-02-16T18:18:55.095Z</updated>
    
    <content type="html"><![CDATA[<p>This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed with what clouds release, and they exist in more than 90% IFOVs, cloudy IFOVs have to be kicked off in order to get clean data.</p><p>Therefore, a new feature construction method is proposed for infrared hyperspectral data, such as what IASI releases. Concretely, four channels of IASI are picked, namely channel 921, channel 386, channel 306 and channel 241. They are picked because of physical characteristics. And then, cloudy IFOVs are detected by logistic regression.</p><p>The recall, auc and accuracy of this new method carried on IASI data was more than 0.95 when detecting IFOVs of sea, while the result of land’s IFOVs was less than it. After adding surface emissivity features, the auc of it increased by aroud 5%, and recall of it grew by 10% approximately.</p><p>Codes are available <a href="https://github.com/skaudrey/cloud">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project distinguishes cloudy fields of view (IFOVs) from clear IFOVs. As the brightness values released by target objects are mixed 
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="logistic" scheme="http://yoursite.com/tags/logistic/"/>
    
  </entry>
  
  <entry>
    <title>HCR--Compress and Resonstruct Hyperspectral Data.</title>
    <link href="http://yoursite.com/posts/projects/2018-12-28-hcr.html"/>
    <id>http://yoursite.com/posts/projects/2018-12-28-hcr.html</id>
    <published>2018-12-28T18:33:39.000Z</published>
    <updated>2021-02-16T18:19:17.158Z</updated>
    
    <content type="html"><![CDATA[<p>This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression and reconstruction. The numerous infrared hyperspectral data are overloaden for computing resources currently. Taking IASI, an atmosphere detector on satellite Metop launched by European Organization for the Exploitation of Meteorological Satellites (EUMETSAT), as an example, it has 8461 channels, which can detect atmosphere vertically in details. To process these data more efficiently, compressing them and then reconstructing is required.</p><p>Considering their high correlation in spectral and spatial dimension, a new compressing and reconstructing network HCR is proposed. Concretely, the radiation brightness values are gridded so that one value at specific location is recongnized as a color value at this pixel. After normalizing by batch normalization, HCR compresses by convolution and reconstructs by deconvlution.</p><p>Carrying on IASI data, the RMSE of this new method was decreased by 5% at least compared with the result of principle component analysis (PCA) in the same compression ratio. The compression kernels encode tempetature information and reconstruct it. In reconstruction, the kernels’ weights for likewise data are similar.</p><p>Codes are available <a href="https://github.com/skaudrey/hyp">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project compresses and reconstructs infrared hyperspectral data. The network proposed is named HCR, aka hyperspectral compression an
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="cnn" scheme="http://yoursite.com/tags/cnn/"/>
    
      <category term="compress" scheme="http://yoursite.com/tags/compress/"/>
    
      <category term="infrared" scheme="http://yoursite.com/tags/infrared/"/>
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
  </entry>
  
  <entry>
    <title>What Can Artificial Intelligence Do in Data Assimilation? Dec. 9th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2018-12-09-talk-mlutility.html"/>
    <id>http://yoursite.com/posts/talks/2018-12-09-talk-mlutility.html</id>
    <published>2018-12-09T18:33:39.000Z</published>
    <updated>2021-02-16T18:16:19.620Z</updated>
    
    <content type="html"><![CDATA[<p>This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my lab, and most of them haven’t learn much about AI. The talk would like to show them what can AI do these days, and help them figure out what else can AI do in data assimilation.</p><p>Slides are avaliable <a href="/assets/slides/mlDo/mlUtility.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk explained what is AI, and the relationship between AI, ML, Data Mining, Knowledge Graph etc. The audiences are students in my l
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="Data assimilation" scheme="http://yoursite.com/tags/Data-assimilation/"/>
    
  </entry>
  
  <entry>
    <title>Computing in the 21th Century &amp; Asia Faculty Summit held by Microsoft, Nov., 2018.</title>
    <link href="http://yoursite.com/posts/meetings/2018-11-23-microsoft.html"/>
    <id>http://yoursite.com/posts/meetings/2018-11-23-microsoft.html</id>
    <published>2018-11-23T18:33:39.000Z</published>
    <updated>2021-02-16T18:18:38.701Z</updated>
    
    <content type="html"><![CDATA[<p>This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is <a href="https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda">here</a>.</p><p>Babysitting AI and computational neuroscience impressed me a lot.</p><a id="more"></a><h1>computing neuroscience: robots with feelings.</h1><p>Prof. Lenore Blue’s keynote is about computational neuroscience. They try to let robots feel pain, and to simulate the long and short term encoding happening in our brains.</p><p>What she talked reminds me of something I had read before.</p><p>According to the book <em>Psychology</em> written by Daniel Schacter, our brains do encode information into long and short codes, and the short form is possibly trasformed into a long one. Even though we don’t mean to encode or memorize something sometimes, encoding still occurs unconsciously.</p><p>Reviewing is one useful way for recalling these information. Moreover, if you are in the similar environment in which you encoded the codes before, you will have higher possibility to recall it. However, the <strong>encoding error</strong> happend during reviewing is more, and that’s why a detective should try to get full information while inquiring evidences from witnesses at the 1st time.</p><p>Also, our brains are more sensible to pictorial information compared with text information. So, if you try to make each thing you want to keep code as a photo, you can boost the capability of memorizing.</p><h1>Yoshu Bengio: Beyond i.i.d. and babysitting AI</h1><p>One of the basic assumption that makes generation possible is independent identically distributed assumption. However, influenced by observing equipments, imbalanced samples and others, the distributions of training data and test data are not always the same. Hence, Prof. Bengio’s team proposed that all data are sampled from the same system rather than same distribution. As for weather of two different seasons, data desciping them are sampled from the same atmospheric circulation system, but they don’t distribute identically. Bengio said they tend to initialize this system with diversed initial conditions, and the result of this distribution will be taken as what the data set follows. It makes sense.</p><p><em>One thing that troubles me is, how can I model the system and figure out the initial conditions? For things with obvious physical rules, it is easy, and even the model’s codes are open-acssessed online. What if the one I don’t know? How to make this idea works in common situations?</em></p><p>CNN is renowned as its power in representation learning, which encodes a variety of information into vectors. Our brains also work like this. Nontheless, what they learn are supervised, and the utility of binary network, the simplification of network structure all demostrate that the captured information are redundant. The model learned is fragile, too. After adding some noises into an image, even though the image dosen’t change visually for us humans, model can not tell what it is as before. It all comes from the uncontrolling unsupervision. Babysitting AI aims at modeling with environmental information and other information, so that leading AI models.</p><h1>Andrew C. Yao: The Advent of Quantum Computing</h1><p>The quantum computing will offer exponential speedup for crypto-code breaking, simulation of quantum physical systems, simulation of materials, chemistry, and biology, nonlinear optimization, ML and AI. It will break through the bottleneck of computing.</p><p>Its implementation is like crystallography. In terms of crystallography, you take an X-ray photo for a crystal and then compute its structure. For quantum computing, instead of taking a real photo, you just need to collect a polynomial number of sample points. By wave-particle duality, this single photo can recreate the raw image probabilistically.</p><p>According to Andrew, dimond qubits are in the highest possibility to be used in our laptops.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This summit invited many professors, including Yoshua Bengio, Bishop, Lenore etc. Ageda is &lt;a href=&quot;https://www.microsoft.com/en-us/research/event/computing-in-the-21st-century-conference-asia-faculty-summit-on-msras-20th-anniversary/#!agenda&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Babysitting AI and computational neuroscience impressed me a lot.&lt;/p&gt;
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="applications" scheme="http://yoursite.com/tags/applications/"/>
    
      <category term="tendency" scheme="http://yoursite.com/tags/tendency/"/>
    
  </entry>
  
  <entry>
    <title>The Introduction of Infrared Hyperspectral Data and Kernel PCA, June 5th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2018-06-05-talk-hyp.html"/>
    <id>http://yoursite.com/posts/talks/2018-06-05-talk-hyp.html</id>
    <published>2018-06-05T17:33:39.000Z</published>
    <updated>2021-02-16T18:16:35.981Z</updated>
    
    <content type="html"><![CDATA[<p>Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:</p><ul><li>high spectral correlation,</li><li>high spatial correlation,</li><li>and sparsity,</li></ul><p>and they casue a trouble during processing.</p><a id="more"></a><p>This talk explained why they are highly correlated but also sparse. Kernel PCA for compressing was also tested.</p><p>Check <a href="/assets/slides/hyp/hypCompression.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Infrared hyperspectral data are typical meteorological observations, which can detect the atmosphere vertically in many spectrums. Distinguishing obsorption peaks of different materials appearing in specific spectrums can help classify those materials. However, there are three characteristics of these data, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;high spectral correlation,&lt;/li&gt;
&lt;li&gt;high spatial correlation,&lt;/li&gt;
&lt;li&gt;and sparsity,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and they casue a trouble during processing.&lt;/p&gt;
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="hyperspectral" scheme="http://yoursite.com/tags/hyperspectral/"/>
    
      <category term="compression" scheme="http://yoursite.com/tags/compression/"/>
    
      <category term="reconstruction" scheme="http://yoursite.com/tags/reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Weather processes interpolation based on GPR</title>
    <link href="http://yoursite.com/posts/projects/2018-05-17-gpr.html"/>
    <id>http://yoursite.com/posts/projects/2018-05-17-gpr.html</id>
    <published>2018-05-17T17:33:39.000Z</published>
    <updated>2021-02-16T18:19:54.655Z</updated>
    
    <content type="html"><![CDATA[<p>This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale dependencies of weather processes. Weather processes with and without cyclones are discussed, and two interpolation methods are proposed. Check <a href="http://www.mdpi.com/2073-4433/9/5/194/pdf">paper</a> for more information. Codes are available <a href="https://github.com/skaudrey/gpml">here</a>.</p><h1>Reference</h1><pre><code>Carl Edward Rasmussen. Gaussian process for Machine Learning.</code></pre><h1>Acknowledgement</h1><pre><code>Thanks for the opening source toolbox GAUSSIAN PROCESS REGRESSION AND CLASSIFICATION Toolbox version 4.0, programmed by Carl et al.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project aims at interpolating wind fields. The main idea of it is multi-scale anisotropy kernel, which can extract multi-scale depen
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
  </entry>
  
  <entry>
    <title>Multivariate Interpolation of Wind Fields Based on Gaussian Process Regression, Jan. 24th, 2018.</title>
    <link href="http://yoursite.com/posts/talks/2018-01-24-talk-gpr.html"/>
    <id>http://yoursite.com/posts/talks/2018-01-24-talk-gpr.html</id>
    <published>2018-01-24T18:33:39.000Z</published>
    <updated>2021-02-16T18:16:51.349Z</updated>
    
    <content type="html"><![CDATA[<p>This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check the <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html">projects’ introduction </a> and <a href="https://github.com/skaudrey/gpml/">github</a> for more details.</p><p>Slides are avaliable <a href="/assets/slides/gpr/windInterpolation.pdf">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This talk showed the multivariate interpolation models for wind fields, which are designed based on Gaussian Process Regression. Check th
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
      <category term="interpolation" scheme="http://yoursite.com/tags/interpolation/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2017-09-11-notes-fourier-GCN.html"/>
    <id>http://yoursite.com/posts/talks/2017-09-11-notes-fourier-GCN.html</id>
    <published>2017-09-11T17:33:39.000Z</published>
    <updated>2021-02-16T18:18:02.480Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.</p><p>Check <a href="/assets/slides/notes/GCN/fourier-GCN.pdf">slides</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Notes about GCN in spectral space. It deduces from traditional fourier transformation to spectral graph convolution.&lt;/p&gt;
&lt;p&gt;Check &lt;a href
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Discussion about Data Assimilation and Machine Learning, Sep. 11th, 2017.</title>
    <link href="http://yoursite.com/posts/talks/2017-09-11-talk-da.html"/>
    <id>http://yoursite.com/posts/talks/2017-09-11-talk-da.html</id>
    <published>2017-09-11T17:33:39.000Z</published>
    <updated>2021-02-16T18:17:17.766Z</updated>
    
    <content type="html"><![CDATA[<p>Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it from other forms of machine learning, image analysis, and statistical methods. This talk discussed the basic ideas of machine leaning, and compared it with machine learning. It is given after I came back from Harbin’s summer school in Agust, 2017. After this talk, I began to throw myself into studying machine learning.</p><p>Check <a href="/assets/slides/D.A/pres.pdf">slide</a> for more details.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Data assimilation is popular in numerical weather forecasting, hydrological forecasting etc. Utilizing a dynamical model distinguishes it
      
    
    </summary>
    
      <category term="talks" scheme="http://yoursite.com/categories/talks/"/>
    
    
      <category term="data assimilation" scheme="http://yoursite.com/tags/data-assimilation/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>The summer school held in Harbin, Aug. 2017.</title>
    <link href="http://yoursite.com/posts/meetings/2017-08-22-harbin.html"/>
    <id>http://yoursite.com/posts/meetings/2017-08-22-harbin.html</id>
    <published>2017-08-22T17:33:39.000Z</published>
    <updated>2021-02-16T18:19:33.454Z</updated>
    
    <content type="html"><![CDATA[<p>I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check <a href="http://mss2017.hit.edu.cn/showSubjectDominWebSite.do">here</a> for more info. I gave a talk to the students in my lab after returning, <a href="https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html">here</a> are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation.</p><a id="more"></a><p>The themes given by those professors are listed below.</p><ul><li><p>Prof. Francois</p><p>Research Area：Variational data assimilation (VAR), especially 4DVAR.</p><p>Keynotes：The direvation of adjoint models, sensitivity analysis and the introduction of image assimilation. See the [minutes file](/assets/notes/harbin/François meeting minutes.pdf) for details.</p></li><li><p>Prof. Jordan</p><p>Research Area：Statistical Learning</p><p>Keynotes: Summarize popular machine learning algorithms, and prove the convergence etc. See the [minutes file](/assets/notes/harbin/Jordan meeting minutes.pdf) for details.</p></li><li><p>Prof. Jurgen</p><p>Research Area：AI, DL</p><p>Keynotes: The introduction of utilizing AI. Check more from his <a href="http://people.idsia.ch/~juergen/">home page</a>.</p></li><li><p>Prof. Ma</p><p>Research Area：Compression sensing.</p><p>Keynotes: The introduction of compression sensing and its applications.</p></li><li><p>Prof. Cai</p><p>Research Area：Statistical inference.</p><p>Keynotes: Statistical inference in high-dimensions. No slides, the minutes file is [here](/assets/notes/harbin/Tony meeting minutes.pdf).</p></li></ul><p>Slides and records are available in the <a href="https://pan.baidu.com/s/1jGj07koiMIV-MOf17N_jeg">baidu network disk</a> with password 6o2x. Enjoy yourself.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I went to classes given by the summer school held in Harbin Industrial University from July to August, 2017. They invited some professors. Check &lt;a href=&quot;http://mss2017.hit.edu.cn/showSubjectDominWebSite.do&quot;&gt;here&lt;/a&gt; for more info. I gave a talk to the students in my lab after returning, &lt;a href=&quot;https://skaudrey.github.io/posts/talks/2018-11-12-da+talk.html&quot;&gt;here&lt;/a&gt; are the slides. The main goal of this talk is to show the difference of machine learning and data assimilation.&lt;/p&gt;
    
    </summary>
    
      <category term="meetings" scheme="http://yoursite.com/categories/meetings/"/>
    
    
      <category term="summer school" scheme="http://yoursite.com/tags/summer-school/"/>
    
      <category term="mathematics" scheme="http://yoursite.com/tags/mathematics/"/>
    
  </entry>
  
  <entry>
    <title>The naive implementation of some popular machine learning algorithms.</title>
    <link href="http://yoursite.com/posts/projects/2017-06-28-ml-implement.html"/>
    <id>http://yoursite.com/posts/projects/2017-06-28-ml-implement.html</id>
    <published>2017-06-28T17:33:39.000Z</published>
    <updated>2021-02-16T18:18:21.140Z</updated>
    
    <content type="html"><![CDATA[<p>Naive implementations of some M.L. algorithms, which are updated continuously.</p><p>The algorithms that have been implemented are listed as follows:</p><ul><li>Logistic Regression,</li><li>SVM solved by SMO,</li><li>K-Means，</li><li>GMM solved by EM,</li><li>Perceptron，</li><li>Naive Bayes,</li><li>LeNet-Keras，</li><li>MLP-Numpy solved with BP,</li><li>MCMC sampling.</li></ul><p>Codes are available <a href="https://github.com/skaudrey/ml_algorithm">here</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Naive implementations of some M.L. algorithms, which are updated continuously.&lt;/p&gt;
&lt;p&gt;The algorithms that have been implemented are liste
      
    
    </summary>
    
      <category term="projects" scheme="http://yoursite.com/categories/projects/"/>
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="naive" scheme="http://yoursite.com/tags/naive/"/>
    
  </entry>
  
</feed>
