@book{陈雨时2014,
  title={高光谱数据降维及压缩技术},
  author={陈雨时},
  publisher={哈尔滨工程大学出版社},
  year={2014},
 keywords={光谱分辨率},
}

@article{Pellet2016Dimension,
  title={Dimension reduction of satellite observations for remote sensing. Part 2: Illustration using hyperspectral microwave observations},
  author={Pellet, V and Aires, F},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={142},
  number={700},
  pages={2670-2678},
  year={2016},
 keywords={Sampling;channel compression;channel selection;microwave;remote sensing},
 abstract={Abstract In the first article of this series, the two classical strategies to reduce satellite data dimension (i.e. compression and channel selection) were presented, together with the introduction of a new method, the so-called 鈥榖ottleneck channels鈥 (BC). BC are a compromise between the two classical approaches and can benefit from the advantages of both. In this article, the three methodologies are tested using experiments on a synthetic dataset corresponding to a hyperspectral conceptual instrument in the microwave, for frequencies up to 500 GHz. As expected, principal component analysis (PCA) based methods are best to compress data, but their components lack the physical interpretability of real channels. Channel selection methods preserve this physical meaning but require a much larger number of channels in order to use the redundancy of information to reduce instrumental noise. The new BC method appears to be a good compromise. It can be seen as a PCA compression method where the components are constrained to be instrument channels, facilitating their understanding, inversion or assimilation. BC allows for an easy calibration of data based on radiative transfer simulations and also alleviates the mixing problem of the PCA technique, where various physical variabilities (e.g. temperature, humidity, clouds) can be mixed in the same extracted components. Furthermore, the BC compression rate is equivalent to that of PCA-based methods even with a limited number of BC.},
}

@article{Aires2016Dimension,
  title={Dimension reduction of satellite observations for remote sensing. Part 1: A comparison of compression, channel selection and bottleneck channel approaches},
  author={Aires, F and Pellet, V and Prigent, C and Moncet, J. ‐L},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={142},
  number={700},
  pages={2658-2669},
  year={2016},
 keywords={sampling;channel compression;channel selection;remote sensing},
 abstract={Abstract With the increasing volume of satellite observations, dimension reduction techniques are more and more important for storage or transmission. Furthermore, they are essential for inversion schemes that, in practice, cannot handle the huge amount of information provided by modern instruments for near-real-time inversion. In this article, we compare the theoretical advantages and limitations of the two general strategies: compression (i.e. feature extraction) and channel selection (i.e. feature selection). The statistical ‘input variable selection’ framework is adopted to revisit the basis of these remote-sensing techniques. The flexibility of these methods to specific applications is demonstrated. Special emphasis is put on the optimization of observation dimension reduction for the simultaneous retrieval of several variables (e.g. temperature and humidity). In addition to considering the signal-to-noise ratio for the variable to retrieve, we propose to account for contamination by other unknown variables. We also introduce a new approach named ‘bottleneck channels’ (BC), which combines compression and channel selection techniques and can therefore benefit from the advantages of both strategies. Various configurations of the BC approach can be considered: strict, linearly or nonlinearly projected, each with advantages and drawbacks. In the companion article, experiments will be conducted on microwave data to illustrate the practical advantages of each approach. BC are able to compress and suppress noise in a similar way to principal component analysis (PCA) and they can be interpreted as channels, as in channel selection.},
}

@book{Goodfellow2016Deep,
  title={Deep Learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher={The MIT Press},
  year={2016},
}

@article{杜华栋2010,
  title={星基大气探测资料信息容量研究},
  author={杜华栋 and 黄思训 and 方涵先 and 龙智勇 and 王永琪},
  journal={物理学报},
  volume={59},
  number={1},
  pages={683-691},
  year={2010},
 keywords={大气探测资料;信息容量;权函数矩阵},
}

@book{2014数学之美,
  title={数学之美},
  publisher={人民邮电出版社},
  year={2014},
 keywords={电子计算机},
}

@article{Wang2012Kernel,
  title={Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models},
  author={Wang, Quan},
  journal={Computer Science},
  year={2012},
 keywords={Computer Science - Computer Vision and Pattern Recognition},
 abstract={Abstract:  Principal component analysis (PCA) is a popular tool for linear dimensionality reduction and feature extraction. Kernel PCA is the nonlinear form of PCA, which better exploits the complicated spatial structure of high-dimensional features. In this paper, we first review the basic ideas of PCA and kernel PCA. Then we focus on the reconstruction of pre-images for kernel PCA. We also give an introduction on how PCA is used in active shape models (ASMs), and discuss how kernel PCA can be applied to improve traditional ASMs. Then we show some experimental results to compare the performance of kernel PCA and standard PCA for classification problems. We also implement the kernel PCA-based ASMs, and use it to construct human face models.},
}

@inproceedings{Mika1999Kernel,
  title={Kernel PCA and de-noising in feature spaces},
  author={Mika, Sebastian and Smola, Alex and Scholz, Matthias},
  booktitle={Conference on Advances in Neural Information Processing Systems II},
  pages={536-542},
  year={1999},
 keywords={CiteSeerX;citations;Kernel PCA and de-noising in feature spaces;S Mika;B Schölkopf;A Smola;K Müller;M Scholz;G Rätsch},
 abstract={Kernel PCA as a nonlinear feature extractor has proven powerful as a preprocessing step for classification algorithms. But it can also be considered as a natural generalization of linear principal component analysis. This gives rise to the question how to use nonlinear features for data compression, reconstruction, and de-noising, applications common in linear PCA\@. This is a nontrivial task, as the results provided by kernel PCA live in some high dimensional feature space and need not have pre-images in input space. This work presents ideas for finding approximate pre-images, focusing on Gaussian kernels, and shows experimental results using these pre-images in data reconstruction and de-noising on toy examples as well as on real world data.},
}