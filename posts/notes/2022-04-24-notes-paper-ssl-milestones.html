<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <link rel="dns-prefetch" href="//cdn.bootcss.com" />
  <link rel="dns-prefetch" href="//cdn.mathjax.org" />
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>Important  Papers of SSL. | Mia&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper 1: Vincent et al (2008) Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. JMLR 2008 Analysis of SSL methods  Paper 2: Koles">
<meta property="og:type" content="article">
<meta property="og:title" content="Important  Papers of SSL.">
<meta property="og:url" content="http://yoursite.com/posts/notes/2022-04-24-notes-paper-ssl-milestones.html">
<meta property="og:site_name" content="Mia&#39;s Blog">
<meta property="og:description" content="Paper 1: Vincent et al (2008) Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. JMLR 2008 Analysis of SSL methods  Paper 2: Koles">
<meta property="og:locale">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104529719.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104552474.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105305843.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105332503.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105801573.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111611299.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111628453.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111826403.png">
<meta property="og:image" content="c:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220427142848736.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220427142828464.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220429135409169.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504131635374.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504133549850.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506132933037.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125714185.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125757738.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220505143215455.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506133343254.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506134524556.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506140132378.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143138612.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143236840.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507144936733.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507145012695.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220508144638659.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509105849555.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509121402548.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510105556229.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510110242447.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510104751609.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510114836113.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220511122629288.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220511130512584.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220511130823288.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512112020319.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512140025628.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512122436330.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512135955654.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512123010624.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512123038832.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512123621658.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512124151867.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512140312637.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512130430962.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220513193229770.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220513193545502.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516113317398.png">
<meta property="og:image" content="c:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220516115120389.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516114526058.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516121917957.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516121932767.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516122506492.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517122848579.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517124109412.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517131012759.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517132323518.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517132346424.png">
<meta property="og:image" content="c:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220517133547079.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517151001418.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517161725969.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517153843819.png">
<meta property="og:image" content="c:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220517162126472.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517162948011.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517163454672.png">
<meta property="og:image" content="c:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220517164044255.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220518155552625.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220518161528661.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220519130518491.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220519134302563.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220519134915317.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220519145227280.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220520122900354.png">
<meta property="og:image" content="c:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220520124431616.png">
<meta property="article:published_time" content="2022-04-24T16:02:00.000Z">
<meta property="article:modified_time" content="2022-05-20T17:39:24.094Z">
<meta property="article:author" content="Mia Feng">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="SSL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104529719.png">
  
    <link rel="alternative" href="/atom.xml" title="Mia&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: #4d4d4d;
    }
  </style>
  

  

<meta name="generator" content="Hexo 5.3.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/assets/img/avatar.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/posts/uncategorized/2019-06-13-about.html">about</a></li>
	        
				<li><a href="/categories/notes">notes</a></li>
	        
				<li><a href="/categories/projects">projects</a></li>
	        
				<li><a href="/categories/talks">talks</a></li>
	        
				<li><a href="/categories/meetings">meetings</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">posts</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">papers</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">about</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/skaudrey" title="github"><i class="icon-github"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:skaudreymia@gmail.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/assets/img/avatar.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/skaudrey" title="github"><i class="icon-github"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:skaudreymia@gmail.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 20%"><a href="/posts/uncategorized/2019-06-13-about.html">about</a></li>
		        
					<li style="width: 20%"><a href="/categories/notes">notes</a></li>
		        
					<li style="width: 20%"><a href="/categories/projects">projects</a></li>
		        
					<li style="width: 20%"><a href="/categories/talks">talks</a></li>
		        
					<li style="width: 20%"><a href="/categories/meetings">meetings</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-notes-paper-ssl-milestones" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Important  Papers of SSL.
    </h1>
  

        
        <a href="/posts/notes/2022-04-24-notes-paper-ssl-milestones.html" class="archive-article-date">
  	<time datetime="2022-04-24T16:02:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2022-04-24</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><p>Paper 1: Vincent et al (2008) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume11%2Fvincent10a%2Fvincent10a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0SspBuh4yVzZEMYUyio3wn"><em>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</em></a>. JMLR 2008</p></li>
<li><p>Analysis of SSL methods</p>
<ul>
<li>Paper 2: Kolesnikov, Zhai and Beyer (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FKolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Kgfk_3VH635pQwvKeEBE0"><em>Revisiting Self-Supervised Visual Representation Learning</em></a>. CVPR 2019</li>
<li>Paper 3: Zhai et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.04867&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2ITTaWDSpqKWi3PBBkP5xL"><em>A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark</em></a> (A GLUE-like benchmark for images) ArXiv 2019</li>
<li>Paper 4: Asano et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1904.13132&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2Fb1pUVDy26_-64K2-T_oR"><em>A critical analysis of self-supervision, or what we can learn from a single image</em></a> ICLR 2020</li>
</ul></li>
<li><p>Contrastive methods</p>
<ul>
<li><p>Paper 5: van den Oord et al. (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1807.03748&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw33PilvMsYAOmPqiPojZcUY"><em>Representation Learning with Contrastive Predictive Coding</em></a> (CPC), ArXiv 2018</p></li>
<li><p>Paper 6: Hjelm et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1808.06670&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw31h2Nm4lF3JBWFDXbG1miu"><em>Learning deep representations by mutual information estimation and maximization</em></a> (DIM) ICLR 2019</p></li>
<li><p>Paper 7: Tian et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.05849&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0s1M_14J8haUyPCJRRILoQ"><em>Contrastive Multiview Coding</em></a> (CMC) ArXiv 2019</p></li>
<li><p>Paper 8: Hénaff et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.09272&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1GXq6QkuyYGzUQ2EqiWXnL"><em>Data-Efficient Image Recognition with Contrastive Predictive Coding</em></a> (CPC v2: Improved CPC evaluated on limited labelled data) ArXiv 2019</p></li>
<li><p>Paper 9: He et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FHe_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3wwgkDHILZxygHEqFpue3W"><em>Momentum Contrast for Unsupervised Visual Representation Learning</em></a> (MoCo, see also <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.04297&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0tEX1f7G3quzSTWL6rfgzw"><em>MoCo v2</em></a>). CVPR 2020</p></li>
<li><p>Paper 10: Chen T et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2002.05709.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3nN2zgMpeyBgpXG--pO5Kx"><em>A Simple Framework for Contrastive Learning of Visual Representations</em></a> (SimCLR). ICML 2020</p></li>
<li><p>Paper 11: Chen T et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.10029&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw263B6UxGrQeiE53_baVKp-"><em>Big Self-Supervised Models are Strong Semi-Supervised Learners</em></a> (SimCLRv2) ArXiv 2020</p></li>
<li><p>Paper 12: Caron et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.09882&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0IYamHH9j-b56C3eUiwNXP"><em>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</em></a> (SwAV) ArXiv 2020</p></li>
<li><p>Paper 13: Xiao et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.05659&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3gwV78jCI0fs0R9wrj-y7S"><em>What Should Not Be Contrastive in Contrastive Learning</em></a> ArXiv 2020</p></li>
<li><p>Paper 14: Misra and van der Maaten (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FMisra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3sER0wiTJlhLbi88owlXXE"><em>Self-Supervised Learning of Pretext-Invariant Representations</em></a><em>.</em> CVPR 2020</p></li>
</ul></li>
<li><p>Generative methods</p>
<ul>
<li>Paper 15: Dumoulin et al (2017) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1606.00704&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2xx-blDKLroqk6XrXTa7Tp">Adversarially Learned Inference</a> (ALI) ICLR 2017</li>
<li>Paper 16: Donahue, Krähenbühl and Darrell <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1605.09782&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Ea7cOor2n6pmoJ44576vf">Adversarial Feature Learning</a> (BiGAN, concurrent and similar to ALI) ICLR 2017</li>
<li>Paper 17: Donahue and Simonyan (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1907.02544&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3bxhaVdSwniyMV3EFLcUrl">Large Scale Adversarial Representation Learning</a> (Big BiGAN) ArXiv 2019</li>
<li>Paper 18: Chen et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fimage-gpt%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2hhRvO-SL2S08MmJ1VjBfw">Generative Pretraining from Pixels</a> (iGPT) ICML 2020</li>
</ul></li>
<li><p>BYoL: boostrap your own latents</p>
<ul>
<li>Paper 19: Tarvainen and Valpola (2017) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2-LuidEMZJI7fUiE-oWqoa"><em>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</em></a>. NeurIPS 2017</li>
<li>Paper 20: Grill et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.07733&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2igxNmTekOxWbWzL9y5IyY"><em>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</em></a> (BYoL). ArXiv 2020</li>
<li>Paper 21: Abe Fetterman, Josh Albrecht, (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Funtitled-ai.github.io%2Funderstanding-self-supervised-contrastive-learning.html&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0qgrINDjCLX30tzD20JOti"><em>Understanding self-supervised and contrastive learning with "Bootstrap Your Own Latent" (BYOL)</em></a> Blog post</li>
<li>Paper 22: Schwarzer and Anand et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19"><em>Data-Efficient Reinforcement Learning with Momentum Predictive Representations</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19">.</a> ArXiv 2020</li>
</ul></li>
<li><p>self-distillation methods</p>
<ul>
<li>Paper 23: Furlanello et al (2017) <a target="_blank" rel="noopener" href="http://www.google.com/url?q=http%3A%2F%2Fmetalearning.ml%2F2017%2Fpapers%2Fmetalearn17_furlanello.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3xt23sIkAffPt8Ym8vqZDS"><em>Born Again Neural Networks</em></a>. NeurIPS 2017</li>
<li>Paper 24: Yang et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.05551&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2HyME0KdJDI4vO-jOuk3Zp"><em>Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.05551&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2HyME0KdJDI4vO-jOuk3Zp">.</a> AAAI 2019</li>
<li>Paper 25: Ahn et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FAhn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3XI1Xtrprs4vWTy6AcLiYH"><em>Variational information distillation for knowledge transfer</em></a>. CVPR 2019</li>
<li>Paper 26: Zhang et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_ICCV_2019%2Fpapers%2FZhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3lmWaXmFh5PictqZWwKIG5"><em>Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation</em></a> ICCV 2019</li>
<li>Paper 27: Müller et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F8717-when-does-label-smoothing-help.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2aZPLQTJt3rY082Bdqc3K9"><em>When Does Label Smoothing Help?</em></a> NeurIPS 2019</li>
<li>Paper 28: Yuan et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FYuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3zJVfBf6c8hlwGdl32vMNP"><em>Revisiting Knowledge Distillation via Label Smoothing Regularization</em></a>. CVPR 2020</li>
<li>Paper 29: Zhang and Sabuncu (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2006.05065v1.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0MHYHwHxUixNKEr1znpFVs"><em>Self-Distillation as Instance-Specific Label Smoothing</em></a> ArXiv 2020</li>
<li>Paper 30: Mobahi et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2002.05715&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0SktJ4r4OuAXv0PwzzVEnu"><em>Self-Distillation Amplifies Regularization in Hilbert Space</em></a>. ArXiv 2020</li>
</ul></li>
<li><p>self-training / pseudo-labeling methods</p>
<ul>
<li>Paper 31: Xie et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1911.04252&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1DhfgYIRMPv6XislXO6ry_"><em>Self-training with Noisy Student improves ImageNet classification</em></a>. CVPR 2020</li>
<li>Paper 32: Sohn and Berthelot et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2001.07685&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3LFMCJQrLt8NabRsym2vQW"><em>FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</em></a>. ArXiv 2020</li>
<li>Paper 33: Chen et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2006.10032.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2yldjcUPMdKaLjP5mcG1Xp">Self-training Avoids Using Spurious Features Under Domain Shift</a>. ArXiv 2020</li>
</ul></li>
<li><p>Iterated learning/emergence of compositional structure</p>
<ul>
<li>Paper 34: Ren et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DHkePNpVKPB&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2kXj4HM38k2b1gLv7cLC9G">Compositional languages emerge in a neural iterated learning model</a>. ICLR 2020</li>
<li>Paper 35: Guo, S. et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.05291&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw26-y5c29PiVpbAZYCrsj-o">The emergence of compositional languages for numeric concepts through iterated learning in neural agents</a>. ArXiv 2020</li>
<li>Paper 36: Cogswell et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F1904.09067.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0I5RdaelW5fHhwWfXWf9V2">Emergence of Compositional Language with Deep Generational Transmission</a> ArXiv 2020</li>
<li>Paper 37: Kharitonov and Baroni (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2004.03420%23%3A~%3Atext%3DEmergent%20Language%20Generalization%20and%20Acquisition%20Speed%20are%20not%20tied%20to%20Compositionality%2C-Eugene%20Kharitonov%2C%20Marco%26text%3DStudies%20of%20discrete%20languages%20emerging%2Cfor%20evidence%20of%20compositional%20structure.&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw14HOo95bLPmafk6gFvaAOT">Emergent Language Generalization and Acquisition Speed are not tied to Compositionality</a> ArXiv 2020</li>
</ul></li>
<li><p>NLP</p>
<ul>
<li>Paper 38: Peters et al (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1802.05365&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw10eV0YxRuEz3vADuc94kFn"><em>Deep contextualized word representations</em></a> (ELMO), NAACL 2018</li>
<li>Paper 39: Devlin et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN19-1423%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1oPB9GeT2i05ilyj5OAlBE"><em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</em></a> (BERT) NAACL 2019</li>
<li>Paper 40: Brown et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.14165&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0k8WZSIpb3qv17TQpoA0HA"><em>Language Models are Few-Shot Learners</em></a> (GPT-3, see also GPT-<a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Flanguage-unsupervised%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0AevPs9tr7a9kx--G4mr7M"><em>1</em></a>and <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fbetter-language-models%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1A50m11hjqmW-6njiC3YUp"><em>2</em></a>for more context) ArXiv 2020</li>
<li>Paper 41: Clark et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.10555&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1FzuTywUomI9d2oAiBVFxm"><em>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</em></a> ICLR 2020</li>
<li>Paper 42: He and Gu et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DSJgdnAVKDH&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2BOCV5w8r7GPa0z64Xb1sC"><em>REVISITING SELF-TRAINING FOR NEURAL SEQUENCE GENERATION</em></a> (Unsupervised NMT) ICLR 2020</li>
</ul></li>
<li><p>video/multi-modal data</p>
<ul>
<li>Paper 43: Wang and Gupta (2015) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fxiaolonw.github.io%2Funsupervise.html&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3MTp7nv2L5RX59nyRPqMfM">Unsupervised Learning of Visual Representations using Videos</a> ICCV 2015</li>
<li>Paper 44: Misra, Zitnick and Hebert (2016) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1603.08561&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3oZeSlFwM0jkzrL2Mq_Hwf"><em>Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</em></a> ECCV 2016</li>
<li>Paper 45: Lu et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1908.02265&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw39ViPN0TptsZxtRypcY1Ya"><em>ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1908.02265&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw39ViPN0TptsZxtRypcY1Ya">,</a> NeurIPS 2019</li>
<li>Paper 46: Hjelm and Bachman (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.13278&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0fIuuJYkqfT84nt0_QyFfZ"><em>Representation Learning with Video Deep InfoMax</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN19-1423%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1oPB9GeT2i05ilyj5OAlBE"><em>.</em></a> (VDIM) Arxiv 2020</li>
</ul></li>
<li><p>the role of noise in representation learning</p>
<ul>
<li>Paper 47: Bachman, Alsharif and Precup (2014) <a target="_blank" rel="noopener" href="http://www.google.com/url?q=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5487-learning-with-pseudo-ensembles&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2hqOPAo5QSUwexnM7P62j4"><em>Learning with Pseudo-Ensembles</em></a> NeurIPS 2014</li>
<li>Paper 48: Bojanowski and Joulin (2017 ) <a target="_blank" rel="noopener" href="http://www.google.com/url?q=http%3A%2F%2Fproceedings.mlr.press%2Fv70%2Fbojanowski17a%2Fbojanowski17a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3_A9d13BdOXeC_QmofU3sV"><em>Unsupervised Learning by Predicting Noise.</em></a> ICML 2017</li>
</ul></li>
<li><p>SSL for RL, control and planning</p>
<ul>
<li>Paper 49: Pathak et al. (2017) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1705.05363&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1IrpWV_kOjuIQTMmDFS3wd"><em>Curiosity-driven Exploration by Self-supervised Prediction</em></a> (see also a <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fpathak22.github.io%2Flarge-scale-curiosity%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0ZeG_gMDAsY1Z_VMac-vsK"><em>large-scale follow-up</em></a>) ICML 2017</li>
<li>Paper 50: Aytar et al. (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.11592&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw35lA1NGt9l0sZszcrO-f6F"><em>Playing hard exploration games by watching YouTube</em></a> (TDC) NeurIPS 2018</li>
<li>Paper 51: Anand et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.08226&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1te1a2v-cmeICbUGL2_oav"><em>Unsupervised State Representation Learning in Atari</em></a> (ST-DIM) NeurIPS 2019</li>
<li>Paper 52: Sekar and Rybkin et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.05960&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1zh2h0rWm82Aq8AZKySxPl"><em>Planning to Explore via Self-Supervised World Models.</em></a> ICML 2020</li>
<li>Paper 53: Schwarzer and Anand et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19"><em>Data-Efficient Reinforcement Learning with Momentum Predictive Representations</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19">.</a> ArXiv 2020</li>
</ul></li>
<li><p>SSL theory</p>
<ul>
<li>Paper 54: Arora et al (2019) <a target="_blank" rel="noopener" href="http://www.google.com/url?q=http%3A%2F%2Fproceedings.mlr.press%2Fv97%2Fsaunshi19a%2Fsaunshi19a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1Jv6bPfprDaifia4UCyFhE"><em>A Theoretical Analysis of Contrastive Unsupervised Representation Learning</em></a>. ICML 2019</li>
<li>Paper 55: Lee et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.01064&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2cgSGuqGOXlczR6HPQRBwq"><em>Predicting What You Already Know Helps: Provable Self-Supervised Learning</em></a> ArXiv 2020</li>
<li>Paper 56: Tschannen, et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1907.13625&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Y3TCY3KlkCbEIXQ61liXo">On mutual information maximization for representation learning</a>. ArXiv 2019.</li>
</ul></li>
<li><p>Unsupervised domain adaption</p>
<ul>
<li>Paper 57: Shu et al (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DH1q-TM-AW&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1DzTvyS29_50se_tZob9IX"><em>A DIRT-T APPROACH TO UNSUPERVISED DOMAIN ADAPTATION</em></a>. ICLR 2018</li>
<li>Paper 58: Wilson and Cook (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1812.02849&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2n226swg-lx1H29furztaL"><em>A Survey of Unsupervised Deep Domain Adaptatio</em></a><em>n</em>. ACM Transactions on Intelligent Systems and Technology 2020<strong>.</strong></li>
<li>Paper 59: Mao et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.04215&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3tp1FCFSItdeJjpQUUvBJX"><em>Virtual Mixup Training for Unsupervised Domain Adaptation</em></a>. CVPR 2019</li>
<li>Paper 60: Vu et al. (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1811.12833&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1ZjGV5QwOyB0ZPMqgiSGXh"><em>ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</em></a> CVPR 2019</li>
</ul></li>
<li><p>Scaling</p>
<ul>
<li>Paper 61: Kaplan et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2001.08361&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0pY71xR2C_948Bp-wDv-aA"><em>Scaling Laws for Neural Language Models</em></a>. ArXiv 2020</li>
</ul></li>
</ul>
<a id="more"></a>
<h2 id="paper-1-stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion">Paper 1: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume11%2Fvincent10a%2Fvincent10a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0SspBuh4yVzZEMYUyio3wn"><em>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</em></a></h2>
<p>Codes:</p>
<h3 id="previous">Previous</h3>
<ul>
<li>What works much better is to initially use a local unsupervised criterion to (pre)train each layer in turn, with the goal of learning to produce a useful higher-level representation from the lower-level representation output by the previous layer.</li>
<li>Initializing a deep network by stacking autoencoders yields almost as good a classification performance as when stacking RBMs. But why is it almost as good?</li>
<li>looking for unsupervised learning principles likely to lead to the learning of feature detectors that detect important structure in the input patterns.</li>
</ul>
<h4 id="traditional-classifiers-training-with-noisy-inputs">Traditional classifiers training with noisy inputs</h4>
<ul>
<li>Training with noise is equivalent to applying generalized Tikhonov regularization (Bishop, 1995), which means L2 weight decay penalty but on linear regression with additive noise. For non-linear case, the regularization is more complex. Authors of this paper even show that different result when using DAEmon and when using regular autoencoders with a L2 weight decay.</li>
</ul>
<h4 id="pseudo-likelihood-and-dependency-networks">Pseudo-Likelihood and Dependency Networks</h4>
<ul>
<li>Pseudo-likelihood , dependency network paradigms etc.</li>
</ul>
<h3 id="ideas">Ideas</h3>
<p>stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. Find that denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. our results clearly establish the value of using a denoising criterion as an unsupervised objective to guide the learning of useful higher level representations.</p>
<h4 id="reasoning-what-makes-good-representations">Reasoning: what makes good representations</h4>
<ul>
<li>A good representation is those that retain a significant amount of information about the input, can be expressed in information-theoretic terms as maximizing the mutual information</li>
<li>Mutual information can be decomposed into an entropy and a conditional entropy term in two different ways
<ul>
<li>ICA: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104529719.png" alt="image-20220426104529719" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104552474.png" title="fig:" alt="image-20220426104552474" />
<ul>
<li>consider a parameterized distribution $p(X|Y;') $ that parameterized by <span class="math inline">\(\mathrm{\theta}&#39;\)</span>, then <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105305843.png" alt="image-20220426105305843" /> leads to maximizing a lower bound on<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105332503.png" alt="image-20220426105332503" />and thus on the mutual information.</li>
<li>ICA is when <span class="math inline">\(Y=f_\theta(X)\)</span>.</li>
<li>As <span class="math inline">\(q(X)\)</span> is unknown, but with samples, the empirical average over the training samples can be used instead as an unbiased estimate (i.e., replacing <span class="math inline">\(\mathbb{E}_{q(X)}\)</span> by $_{q^0(X)} $: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105801573.png" alt="image-20220426105801573" /></li>
</ul></li>
</ul></li>
<li>The equation above corresponds to the reconstruction error criterion used to train autoencoders
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111611299.png" alt="image-20220426111611299" />: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111628453.png" alt="image-20220426111628453" />To choose <span class="math inline">\(p(\mathrm{x|z}),L(\mathrm{x|z})\)</span>
<ul>
<li>For real-valued $ $ squared error: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111826403.png" alt="image-20220426111826403" style="zoom:80%;" /> on <span class="math inline">\(\sigma^2\)</span>, the variance of <span class="math inline">\(X\)</span>. Since gaussian, not to use a squashing nonlinearity in the decoder.</li>
<li>Binary $ $: finally become cross-entropy loss, which can be used when $ $ is not strictly binary but rather $^d $</li>
</ul></li>
<li><strong>training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between input X and learnt representation Y</strong></li>
</ul></li>
<li><strong>Merely Retaining Information is Not Enough</strong>
<ul>
<li>non-zero reconstruction error to separate useful information from noise: traditional AE uses bottleneck to produce an under-complete representation where <span class="math inline">\(d&#39;&lt;d\)</span>, and result in a lossy compressed representation of <span class="math inline">\(X\)</span>. When using affine encoder and decoder without any nonlinearity an a squared error loss, the AE performs PCA actually. But not in cross-entropy loss.</li>
<li>using over-complete (i.e., higher dimensional than the input) but sparse representations is popular now, which is a special case of imposing on <span class="math inline">\(Y\)</span> different constraints than that of a lower dimensionality. A sparse over-complete representations can be viewed as an alternative “compressed” representation.</li>
</ul></li>
</ul>
<h3 id="how">How?</h3>
<ul>
<li><p>DAE: denoising autoencoder</p>
<ul>
<li><p>Procedures</p>
<ul>
<li>first denoising: corrupt the initial input $ $ into <span class="math inline">\(\tilde{\mathrm{x}}\)</span> by a stochastic mapping. This will lead to force the learning of a mapping that extracts features useful for denoising.</li>
<li>Then map <span class="math inline">\(\tilde{\mathrm{x}}\)</span> to <span class="math inline">\(\mathrm{y}\)</span>. And reonstruct <span class="math inline">\(\mathrm{z}\)</span> to be close to the $ $ but a function of <span class="math inline">\(\mathrm{y}\)</span>.</li>
</ul></li>
<li><p>Geometric Interpretation</p>
<ul>
<li>Thus stochastic operator <span class="math inline">\(p(X|\tilde{X})\)</span> learns a map that tends to go from lower probability points <span class="math inline">\(\tilde{X}\)</span> to nearby high probability points <span class="math inline">\(X\)</span>, on or near the manifold.</li>
<li>Successful denoising implies that the operator maps even far away points to a small region close to the manifold</li>
<li>Think of <span class="math inline">\(Y=f(X)\)</span> as a representation of <span class="math inline">\(X\)</span> which is well suited to capture the main variations in the data, that is, those along the manifold.</li>
</ul></li>
<li><p>Types of corruption considered</p>
<ul>
<li>additive: natural</li>
<li>masking noise: natural for input domains which are interpretable as binary or near binary such as black an white images or the representation produced at the hidden layer after a signoid squashing function</li>
<li>salt and pepper noise: same as masking noise, only earse a changing subsets of the inputs components while leaving others untouched</li>
</ul></li>
<li><p>Emphasize corrupted dimensions</p>
<ul>
<li><p>only put an emphasis on the corrupted dimensions. Like use <span class="math inline">\(\alpha\)</span> for the Reconstruction error on corrupted components and <span class="math inline">\(\beta\)</span> for untouched components</p></li>
<li><p>Sqaured loss</p>
<figure>
<img src="C:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220427142848736.png" alt="image-20220427142848736" /><figcaption aria-hidden="true">image-20220427142848736</figcaption>
</figure></li>
<li><p>Cross-entropy loss</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220427142828464.png" alt="image-20220427142828464" /><figcaption aria-hidden="true">image-20220427142828464</figcaption>
</figure></li>
<li><p>If <span class="math inline">\(\alpha=1, \beta=0\)</span>, then this is full emphasis that only consider the error on the prediction of corrupted elements</p></li>
</ul></li>
<li><p>Stacking DAE for deep architecture</p></li>
</ul></li>
</ul>
<h3 id="experiments">Experiments</h3>
<ul>
<li><p>Single DAE</p>
<ul>
<li>feature detectors from natural image patches
<ul>
<li>The under-complete autoencoder appears to learn rather uninteresting local blob detectors. Filters obtained in the overcomplete case have no recognizable structure, looking entirely random</li>
<li>training with sufficiently large noise yields a qualitatively very different outcome than training with a weight decay regularization and this proves that the two are not equivalent for a non-linear autoencoder.</li>
<li>Salt-and-pepper noise yielded Gabor-like edge detectors, whereas masking noise yielded a mixture of edge detectors and grating filters. They all yield some potentially useful edge detectors.</li>
</ul></li>
<li>feature detectors from handwrite digits
<ul>
<li>With increased noise levels, a much larger proportion of interesting (visibly non random and with a clear structure) feature detectors are learnt. These include local oriented stroke detectors and detectors of digit parts such as loops.</li>
<li>But denoising a more corrupted input requires detecting bigger, less local structures.</li>
</ul></li>
</ul></li>
<li><p>Stacked DAE (SDAE): compare with SAE and DBN.</p>
<ul>
<li>Classification problem and experimental methodology: As increasing the noise level, denoising training forces the filters to differentiate more, and capture more distinctive features. Higher noise levels tend to induce less local filters, as expected.</li>
<li>Compared with other strategies: denoising pretraining with a non-zero noise level is a better strategy than pretraining with regular autoencoders</li>
<li>Influence of Number of Layers, Hidden Units per Layer, and Noise Level
<ul>
<li>Depth: denoising pretraining being better than autoencoder pretraining being better than no pretraining. The advantage appears to increase with the number of layers and with the number of hidden units</li>
<li>noise levels: SDAE appears to perform better than SAE (0 noise) for a rather wide range of noise levels, regardless of the number of hidden layers.</li>
</ul></li>
</ul></li>
<li><p>Denoising pretraining v.s. training with noisy input</p>
<ul>
<li>Note: SDAE uses a denoising criterion to learn good initial feature extractors at each layer that will be used as initialization for a noiseless supervised training; which is different from training with noisy inputs that amounts to training with a virtually expanded data set.</li>
<li>Denoising pretraining with SDAE, for a large range of noise levels, yields significantly improved performance, whereas training with noisy inputs sometimes degrades the performance, and sometimes improves it slightly but is clearly less beneficial than SDAE.</li>
</ul></li>
<li><p>Variations on the DAE, alternate corruption types and emphasizing</p>
<ul>
<li>An emphasized SDAE with salt-and-pepper noise appears to be the winning SDAE variant.</li>
<li>A judicious choice of noise type and added emphasis may often buy us a better performance.</li>
</ul></li>
<li><p>Are Features Learnt in an Unsupervised Fashion by SDAE Useful for SVMs?</p>
<ul>
<li>SVM performance can benefit significantly from using the higher level representation learnt by SDAE</li>
<li>linear SVMs can benefit from having the original input processed non-linearly</li>
</ul></li>
<li><p>Generating Samples from Stacked Denoising Autoencoder Networks</p>
<ul>
<li>Top-Down Generation of a Visible Sample Given a Top-Layer Representation: it is thus possible to generate samples at one layer from the representation of the layer above in the exact same way as in a DBN.</li>
<li>Bottom-up (infer representation of the top layer based on representation on the bottom layer) is similar as approximate inference of a factorial Bernoulli top-layer distribution given the low level input. The top-layer representation is to be understood as the parameters (the mean) of a factorial Bernoulli distribution for the actual binary units.</li>
<li>SDAE and DBN are able to resynthesize a variety of similarly good quality digits, whereas the SAE trained model regenerates patterns with much visible degradation in quality, this also prove that evidence of the qualitative difference resulting from optimizing a denoising criterion instead of mere reconstruction criterion.</li>
<li>Contrary to SAE, the regenerated patterns from SDAE or DBN look like they could be samples from the same unknown input distribution that yielded the training set.</li>
</ul></li>
</ul>
<h2 id="paper-2-revisiting-self-supervised-visual-representation-learning">Paper 2: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FKolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Kgfk_3VH635pQwvKeEBE0"><em>Revisiting Self-Supervised Visual Representation Learning</em></a></h2>
<p>https://github.com/google/revisiting-self-supervised</p>
<h3 id="previous-1">Previous</h3>
<ul>
<li>Compared with pretext task, the choice of CNN has not received equal attention</li>
<li>patch-based self-supervised visual representation learning methods: predicting the relative location of image patches; “jigsaw puzzle” created from the full image etc.</li>
<li>image-level classification tasks: randomly rotate an image by one of four possible angles and let the model predict that rotation; use clustering of the images</li>
<li>tasks with dense spatial outputs: image inpainting, image colorization, its improved variant split-brain and motion segmentation prediction.</li>
<li>equivariance relation to match the sum of multiple tiled representations to a single scaled representation; predict future patches in via autoregressive predictive coding.</li>
<li>many works have tried to combine multiple pretext tasks in one way or another</li>
</ul>
<h3 id="what">What?</h3>
<ul>
<li>Standard architecture design recipes do not necessarily translate from the fully-supervised to the self-supervised setting. Architecture choices which negligibly affect performance in the fully labeled setting, may significantly affect performance in the self-supervised setting.</li>
<li>The quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</li>
<li>Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations.</li>
<li>The <strong>evaluation</strong> procedure, where a linear model is trained on a fixed visual representation using <strong>stochastic gradient descent, is sensitive to the learning rate schedule</strong> and may take many epochs to converge.</li>
</ul>
<h3 id="how-1">How</h3>
<ul>
<li>revisit a prominent subset of the previously proposed pretext tasks and perform a large-scale empirical study using various architectures as base models.</li>
<li>The CNN candidates
<ul>
<li>ResNet:</li>
<li>RevNet: stronger invertibility guarantees while being structurally similar to ResNets. Set it to have the same depth and number of channels as the original Resnet50 model.</li>
<li>VGG: no skip, has BN</li>
</ul></li>
<li>The pretext tasks candidates
<ul>
<li>Rotation: 0，90，180，270</li>
<li>Exemplar: heavy random data augmentation such as translation, scaling, rotation, and contrast and color shifts</li>
<li>Jigsaw: recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches. They extract representations by averaging the representations of nine uniformly sampled, colorful, and normalized patches of an image.</li>
<li>Relative patch location: 8 possible relative spatial relations between two patches need to be predicted. use the same patch prepossessing as in the Jigsaw model and also extract final image representations by averaging representations of 9 cropped patches.</li>
</ul></li>
<li>Dataset candidates
<ul>
<li>ImageNet</li>
<li>Places205</li>
</ul></li>
<li>Evaluation protocol
<ul>
<li>measures representation quality as the accuracy of a linear regression model trained and evaluated on the ImageNet dataset</li>
<li>the pre-logits of the trained self-supervised networks as representation.</li>
</ul></li>
</ul>
<h3 id="experiments-1">Experiments</h3>
<ul>
<li><p>similar models often result in visual representations that have significantly different performance. Importantly, neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220429135409169.png" alt="image-20220429135409169" /><figcaption aria-hidden="true">image-20220429135409169</figcaption>
</figure></li>
<li><p>increasing the number of channels in CNN models improves performance of self-supervised models.</p></li>
<li><p>ranking of models evaluated on Places205 is consistent with that of models evaluated on ImageNet, indicating that our findings generalize to new datasets.</p></li>
<li><p>self-supervised learning architecture choice matters as much as choice of a pretext task</p></li>
<li><p>MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged. We thus conclude that the linear model is adequate for evaluation purposes.</p></li>
<li><p>Better performance on the pretext task does not always translate to better representations. For residual architectures, the pre-logits are always best.</p></li>
<li><p>Skip-connections prevent degradation of representation quality towards the end of CNNs. We hypothesize that this is a result of ResNet’s residual units being invertible under some conditions. RevNet, boosts performance by more than 5 % on the Rotation task, albeit it does not result in improvements across other tasks.</p></li>
<li><p><strong>Model width and representation size strongly influence the representation quality</strong>: disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer. self-supervised learning techniques are likely to benefit from using CNNs with increased number of channels across wide range of scenarios.</p></li>
<li><p>SGD optimization hyperparameters play an important role and need to be reported. very long training (≈ 500 epochs) results in higher accuracy. They decay lr at 480 epochs.</p></li>
<li></li>
</ul>
<h2 id="paper-3-zhai-et-al-2019-a-large-scale-study-of-representation-learning-with-the-visual-task-adaptation-benchmark-a-glue-like-benchmark-for-images">Paper 3: Zhai et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.04867&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2ITTaWDSpqKWi3PBBkP5xL"><em>A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark</em></a> (A GLUE-like benchmark for images)</h2>
<p>https://github.com/google-research/task_adaptation</p>
<h3 id="previous-2">Previous</h3>
<ul>
<li>the absence of a unified evaluation for general visual representations hinders progress. Each sub-domain has its own evaluation protocol, and the lack of a common benchmark impedes progress.</li>
<li>Popular protocols are often too constrained (linear classification), limited in diversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to representation quality (ELBO, reconstruction error)</li>
</ul>
<h3 id="what-1">What？</h3>
<ul>
<li>present the Visual Task Adaptation Benchmark (VTAB), which defines good representations as those that adapt to diverse, unseen tasks with few examples. (i) minimal constraints to encourage creativity, (ii) a focus on practical considerations, and (iii) make it challenging.</li>
<li>Conduct a large-scale study of many popular publicly-available representation learning algorithms on VTAB.</li>
<li>They found:
<ul>
<li>Supervised ImageNet pretraining yields excellent representations for natural image classification tasks</li>
<li>Self-supervised is less effective than supervised learning overall, but surprisingly, can improve structured understanding</li>
<li>Combining supervision and self-supervision is effective, and to a large extent, self-supervision can replace, or compliment labels.</li>
<li>Discriminative representations appear more effective than those trained as part of a generative model, with the exception of adversarially trained encoders.</li>
<li>GANs perform relatively better on data similar to their pre-training source (here, ImageNet), but worse on other tasks.</li>
<li>Evaluation using a linear classifier leads to poorer transfer and different conclusions.</li>
</ul></li>
</ul>
<h3 id="how-2">How？</h3>
<ul>
<li>To design, must ensure that the algorithms are not pre-exposed to specific evaluation samples.</li>
<li>VTAB benchmark
<ul>
<li>practical benchmark
<ul>
<li>Define task distribution as Tasks that a human can solve, from visual input alone.”</li>
<li>For each evaluation sample a new task.</li>
<li>mitigating meta-overfitting : treat the evaluation tasks unseen, and thus algorithms use pretraining must not pre-train on any of the evaluation tasks.</li>
<li>Unified implementation: algorithms must have no prior knowledge of the downstream tasks, and hyperparameter searches need to work well across the benchmark.</li>
<li>All tasks are classification in this paper, such as the detection task is mapped to the classification of the <span class="math inline">\((x,y,z)\)</span> coordinates. The diverse set of visual features are learnt from object identification, scene classification, pathology detection, counting, localization and 3D geometry.</li>
</ul></li>
<li>Tasks
<ul>
<li>NATURAL: classical vision problems, group includes Caltech101, CIFAR100, DTD, Flowers102, Pets, Sun397, and SVHN</li>
<li>SPECIALIZED: images are captured through specialist equipment, and human can recognize the structures. One is remote sensing, the other is medical</li>
<li>STRUCTURED: assesses comprehension of the structure of a scene, for example, object counting, or 3D depth prediction.</li>
</ul></li>
<li>pretraining on pretext tasks and then fine tune</li>
</ul></li>
<li>The methods are divided into five groups: Generative, training from-scratch, all methods using 10% labels (Semi Supervised), and all methods using 100% labels (Supervised),</li>
</ul>
<h3 id="experiments-2">Experiments</h3>
<ul>
<li>Upstream, control the data and architecture (pretrained on ImageNet). They find bigger architectures perform better on VTAB, and use resnet or resnet-similar nets for all models as the encoder.</li>
<li>Downstream, run VTAB in two modes: the light weight mode sweepstakes 2 initial learning rates and 2 learning rate schedules but fix other parameters while the heavy weights perform a large random search over learning rate, schedule, optimizers, batch size, train preprocessing functions, evaluation preprocessing and weight decay. The main study is done in lightweight.</li>
<li>Evaluate with top-1 accuracy. To aggregate scores across tasks, take the mean accuracy.</li>
<li>Lightweight
<ul>
<li>Generative models perform worst, GANs fit more strongly to ImageNet’s domain (natural images), than self-supervised alternatives.</li>
<li>All self-supervised representations outperform from-scratch training. Methods applied to the entire image outperform patch-based method, and these tasks require sensitivity to local textures. self supervised is worst than supervised on natural tasks, similar on specialized tasks and slightly better than structured tasks.</li>
<li>Supervised models perform the best. additional self-supervision even improves on top of 100% labelled ImageNet, particularly on STRUCTURED tasks</li>
</ul></li>
<li>Heavyweight
<ul>
<li>across all task groups, pre-trained representations are better than a tuned from-scratch model.</li>
<li>a combination supervision and self-supervision (SUP-EXEMPLAR-100%) getting the best performance.</li>
</ul></li>
<li>Frozen feature extractors
<ul>
<li>linear evaluation significantly lowers performance, even when downstream data is limited to 1000 examples. Linear transfer would not by used in practice unless infrastructural constraints required it. These self-supervised methods extract useful representations, just without linear separability. Linear evaluation results are sensitive to additional factors that we do not vary, such as ResNet version or pre-training regularization parameters.</li>
</ul></li>
<li>Vision benchmarks
<ul>
<li>The methods ranked according to VTAB are more likely to transfer to new tasks, than those ranked according to the Visual Decathlon</li>
<li>VTAB is more flexible than Facebook AI SSL challenge, such as the diversity of domain, the evaluation form.</li>
<li>Meta-dataset: designed for few-shot learning rather than 1000 examples which may entail different solutions.</li>
</ul></li>
<li>Some details
<ul>
<li><strong>while training from scratch , inception crop, horizontal flip preprocessing</strong>, 1e-3 in weight decay, 0.1~1 lr in SGD mostly give good results.</li>
<li>While doing <strong>SSL by ImageNet and then finetuning</strong>, <strong>Inception crop, non-horizontal flip</strong> give good results.</li>
</ul></li>
</ul>
<h3 id="discussion">Discussion</h3>
<ul>
<li>how effective are supervised ImageNet representations? ImageNet labels are indeed effective for natural tasks.</li>
<li>how do representations trained via generative and discriminative models compare? The generative losses seem less promising as means towards learning how to represent data. BigBiGAN is notable.</li>
<li>To what extent can self-supervision replace labels?
<ul>
<li>self-supervision can almost (but not quite) replace 90% of ImageNet labels; the gap between pre-training on 10% labels with self-supervison, and 100% labels, is small</li>
<li>self-supervision adds value on top of ImageNet labels on the same data.</li>
<li>simply adding more data on the SPECIALIZED and STRUCTURED tasks is better than the pre-training strategies we evaluated</li>
</ul></li>
<li>Varying other factors to improve VTAB is valuable future research. The only approach that is out-of-bounds is to condition the algorithm explicitly on the VTAB tasks</li>
</ul>
<h2 id="paper-4-a-critical-analysis-of-self-supervision-or-what-we-can-learn-from-a-single-image">Paper 4: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1904.13132&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2Fb1pUVDy26_-64K2-T_oR"><em>A critical analysis of self-supervision, or what we can learn from a single image</em></a></h2>
<p>https://github.com/yukimasano/linear-probes</p>
<h3 id="previous-3">Previous</h3>
<ul>
<li>For a given model complexity, pre-training by using an off-the-shelf annotated image datasets such as ImageNet remains much more efficient.</li>
<li>Methods often modify information in the images and require the network to recover them. However, features are learned on modified images which potentially harms the generalization to unmodified ones.</li>
<li>Learning from a single sample
<ul>
<li>Object tracking: max margin correlation filters learn robust tracking templates from a single sample of the patch.</li>
<li>learn and interpolate multi-scale textures with a GAN framework</li>
<li>semi-parametric exemplar SVM model</li>
<li>we do not use a large collection of negative images to train our model. Instead we restrict ourselves to a single or a few images with a systematic augmentation strategy.</li>
</ul></li>
<li>Classical learned and hand-crafted low-level feature extractors: insufficient to clarify the power and limitation of self-supervision in deep networks.</li>
</ul>
<h3 id="what-2">What</h3>
<ul>
<li>Aim to investigate the effectiveness of current self-supervised approaches by characterizing how much information they can extract from a given dataset of images. Then try to answer <strong>whether a large dataset is beneficial to unsupervised learning, especially for learning early convolutional features</strong></li>
<li>Three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used.</li>
<li>For deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training.</li>
<li>Conclusion
<ul>
<li>the weights of the early layers of deep networks contain limited information about the statistics of natural images</li>
<li>such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that</li>
<li>the low-level statistics can be captured via synthetic transformations instead of using a large image dataset. (training these layers with self-supervision and a single image already achieves as much as two thirds of the performance that can be achieved by using a million different images.)</li>
</ul></li>
</ul>
<h3 id="how-3">How</h3>
<ul>
<li>Data: use DAA augmentation to replace some source images.
<ul>
<li>Augmentations: involving cropping, scaling, rotation, contrast changes, and adding noise. Augmentation can be seen as imposing a prior on how we expect the manifold of natural images to look like
<ul>
<li>limit the size of cropped patch: the smallest size of the crops is limited to be at least βWH and at most the whole image. Additionally, changes to the aspect ratio are limited by γ. In practice we use β = 10e−3 and γ = 3/4 .</li>
<li>before cropping, rotate in <span class="math inline">\((-35,35)\)</span> degrees. And also flip images in 50% possibility</li>
<li>linear transformation in RGB space, color jitter with additive brightness, contrast and saturation</li>
</ul></li>
<li>Real samples: give some drawn samples which are not real captured but has plenty of textures and in small size, and a real picture in large size but with large areas no objects.</li>
</ul></li>
<li>Representation learning methods
<ul>
<li>BiGAN with leaky ReLU nonlinearities in discriminators</li>
<li>Rotation: do it on horizontal flips and non-scaled random crops to 224 × 224</li>
<li>clustering:</li>
</ul></li>
</ul>
<h3 id="experiments-3">Experiments</h3>
<ul>
<li>ImageNet and CIFAR-10/100 using linear probes</li>
<li>Base encoder: AlexNet. They insert the probes right after the ReLU layer in each block</li>
<li>Learning lasts for 36 epochs and the learning rate schedule starts from 0.01 and is divided by five at epochs 5, 15 and 25</li>
<li>extracting 10 crops for each validation image (four at the corners and one at the center along with their horizontal flips) and averaging the prediction scores before the accuracy is computed.</li>
<li>Effect of augmentations: <strong>Random rescaling</strong> adds at least ten points at every depth (see Table 1 (f,h,i)) and is the most important single augmentation. <strong>Color jittering and rotation</strong> slightly improve the performance of all probes by 1- 2% points</li>
<li>Benchmark evaluation
<ul>
<li>Mono is enough: Mono means train with one source image and its augmented images.</li>
<li>Image contents:
<ul>
<li>RotNet cannot extract photographic bias from a single image. the method can extract rotation from low level image features such as patches which is at first counter intuitive=&gt; <strong>lighting and shadows even in small patches</strong> can indeed give important cues on the up direction which can be learned even from a single (real) image.</li>
<li><strong>the augmentations can even compensate for large untextured areas and the exact choice of image is not critical.</strong> A trivial image without any image gradient (e.g. picture of a white wall) would not provide enough signal for any method.</li>
</ul></li>
<li>More than one image
<ul>
<li>for conv1 and conv2, a single image is enough</li>
<li>In deeper layers, DeepCluster seems to require large amounts of source images to yield the reported results as the deka- and kilo- variants start improving over the single image case</li>
</ul></li>
<li>Generalization
<ul>
<li>GAN trained on the smaller Image B outperforms all other methods including the fully-supervised trained one for the first convolutional layer</li>
<li>our method allows learning very generalizable early features that are not domain dependent.</li>
</ul></li>
</ul></li>
<li>the neural network is only extracting patterns and not semantic information because we do not find any neurons particularly specialized to certain objects even in higher levels as for example dog faces or similar which can be fund in supervised networks</li>
</ul>
<h2 id="paper-5-representation-learning-with-contrastive-predictive-coding-cpc-arxiv-2018">Paper 5: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1807.03748&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw33PilvMsYAOmPqiPojZcUY"><em>Representation Learning with Contrastive Predictive Coding</em></a> (CPC), ArXiv 2018</h2>
<h3 id="previous-4">Previous</h3>
<ul>
<li>It is not always clear what the ideal representation is and if it is possible that one can learn such a representation without additional supervision or specialization to a particular data modality.</li>
<li>One of the most common strategies for unsupervised learning has been to predict future, missing or contextual information
<ul>
<li>Recently in unsupervised learning some learn word representations by predicting neighboring words.</li>
<li>For images, predicting color from grey-scale or the relative position of image patches</li>
</ul></li>
<li>predicting high-dimensional data
<ul>
<li>unimodal losses such as mean-squared error and cross-entropy are not very useful</li>
<li>powerful conditional generative models which need to reconstruct every detail in the data are usually required.</li>
</ul></li>
</ul>
<h3 id="what-3">What</h3>
<ul>
<li>Main contributions
<ul>
<li>Compress features into a latent embedding space in which conditional predictions are easier to model.</li>
<li>predict the future in latent space by autoregressive models</li>
<li>Use NCE loss</li>
</ul></li>
<li>Intuition
<ul>
<li>learn the representations that encode the underlying shared information between parts of the signal</li>
<li>meanwhile discard low-level information and noise that is more local.</li>
</ul></li>
</ul>
<h3 id="how-4">How</h3>
<ul>
<li>use a NCE which induces the latent space to capture information that is maximally useful to predict future samples</li>
<li>Model a density ratio which preserves the mutual information between <span class="math inline">\(x_{t+k},c_t\)</span> as <span class="math inline">\(f_k(x_{t+k},c_t)\propto \frac{p(x_{t+k}|c_t)}{p(x_{t+k})}\)</span>, they choose <span class="math inline">\(f_k(x_{t+k},c_t)=\exp(z_{t+k}^TW_kc_t)\)</span>, the log-bilinear model.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504131635374.png" title="fig:" alt="image-20220504131635374" /></li>
</ul>
<h3 id="experiments-4">Experiments</h3>
<p>For every domain we train CPC models and probe what the representations contain with either a linear classification task or qualitative evaluations.</p>
<ul>
<li>Audio
<ul>
<li>use a 100-hour subset of the publicly available LibriSpeech dataset</li>
<li>use a GRU for the autoregressive part of the model</li>
<li>We found that not all the information encoded is linearly accessible.</li>
<li>CPCs capture both speaker identity and speech contents</li>
<li></li>
</ul></li>
<li>Vision
<ul>
<li>Use ImageNet and ResNet v2 101, no BN. use the outputs from the third residual block, and spatially mean-pool to get a single 1024-d vector per 64x64 patch. This results in a 7x7x1024 tensor. Next, we use a PixelCNN-style autoregressive model to make predictions about the latent activations in following rows top-to-bottom.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504133549850.png" title="fig:" alt="image-20220504133549850" /></li>
<li>CPCs improve upon state-of-the-art by 9% absolute in top-1 accuracy, and 4% absolute in top-5 accuracy.</li>
</ul></li>
<li>Natural language
<ul>
<li>a linear mapping is constructed between word2vec and the word embeddings learned by the model. A L2 regularization weight was chosen via cross-validation (therefore nested cross-validation for the first 4 datasets)</li>
<li>found that more advanced sentence encoders did not significantly improve the results, which may be due to the simplicity of the transfer tasks, and the fact that bag-of-words models usually perform well on many NLP tasks.</li>
<li>The performance of our method is very similar to the skip-thought vector model, with the advantage that it does not require a powerful LSTM as word-level decoder, therefore much faster to train</li>
</ul></li>
<li>Reinforcement learning
<ul>
<li>take the standard batched A2C agent as base model and add CPC as an auxiliary loss</li>
<li>The unroll length for the A2C is 100 steps and we predict up to 30 steps in the future to derive the contrastive loss</li>
<li>4 out of the 5 games performance of the agent improves significantly with the contrastive loss after training on 1 billion frames.</li>
</ul></li>
</ul>
<h2 id="paper-6-learning-deep-representations-by-mutual-information-estimation-and-maximization-dim-deep-infomax-iclr-2019">Paper 6: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1808.06670&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw31h2Nm4lF3JBWFDXbG1miu"><em>Learning deep representations by mutual information estimation and maximization</em></a> (DIM Deep InfoMax ) ICLR 2019</h2>
<p>https://github.com/rdevon/DIM</p>
<h3 id="previous-5">Previous</h3>
<ul>
<li>in typical settings, models with reconstruction-type objectives provide some guarantees on the amount of information encoded in their intermediate representations.</li>
<li>MI estimation
<ul>
<li>MINE: strongly consistent, can be used to learn better implicit bidirectional generative models</li>
<li>DIM: follow MINE, but they find the generator is unnecessary. And no necessary for the exact KL-divergence, alternatively the JSD is more stable and provides better results.</li>
</ul></li>
<li>CPC and DIM
<ul>
<li>CPC: make predictions about specific local features in the “future” of each summary feature. This equates to ordered autoregression over the local features, and requires training separate estimators for each temporal offset at which one would like to predict the future.</li>
<li>DIM uses a single summary feature that is a function of all local features, and this “global” feature predicts all local features simultaneously in a single step using a single estimator.</li>
</ul></li>
</ul>
<h3 id="what-4">What</h3>
<ul>
<li>structure matters: maximizing the average MI between the representation and local regions of the input can improve performance while maximizing MI between the complete input and the encoder output not always do this.</li>
<li>JSD helps MI estimation. JSD and DVD all maximize the expected log-ratio of the joint over the product of marginals.</li>
</ul>
<h3 id="how-5">How</h3>
<h4 id="mutual-information-estimation-and-maximization">Mutual information estimation and maximization</h4>
<ul>
<li><p>Basic MI maximization framework</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506132933037.png" alt="image-20220506132933037" /><figcaption aria-hidden="true">image-20220506132933037</figcaption>
</figure></li>
<li><p>share layers between encoder and mutual information</p></li>
<li><p>The different losses of DIM with different estimator</p>
<ul>
<li>With non-KL divergences such as JSD: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125714185.png" alt="image-20220506125714185" /></li>
<li>With NCE, <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125757738.png" alt="image-20220506125757738" /></li>
<li>For DIM, a key difference between the DV, JSD, and infoNCE formulations is whether an expectation over <span class="math inline">\(\mathrm{\mathbb{P/ \tilde{P}}}\)</span> appears inside or outside of a <span class="math inline">\(\log\)</span>. DIM sets the noise distribution to the product of marginals over <span class="math inline">\(X/Y\)</span> , and the data distribution to the true joint.</li>
</ul></li>
<li><p>infoNCE often outperforms JSD on downstream tasks, though this effect diminishes with more challenging data and also requires more negative samples compare with the JSD version.</p></li>
<li><p>DIM with the JSD loss is insensitive to the number of negative samples, and in fact outperforms infoNCE as the number of negative samples becomes smaller.</p></li>
</ul>
<h4 id="local-mutual-information-maximization">Local mutual information maximization</h4>
<ul>
<li>To obtain a representation more suitable for classification, one can maximize the average MI between the high-level representation and local patches of the image.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220505143215455.png" title="fig:" alt="image-20220505143215455" />
<ul>
<li>summarize this local feature map into a global feature</li>
<li>then the MI estimator on global/local pairs, maximizing the average estimated MI: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506133343254.png" alt="image-20220506133343254" /></li>
</ul></li>
</ul>
<h4 id="matching-representation-to-a-prior-distribution">Matching representation to a prior distribution</h4>
<ul>
<li><p>A good representation can be compact, independent, disentangled or independently controllable.</p></li>
<li><p>DIM imposes statistical constraints onto learned representations by implicitly training the encoder so that the push-forward distribution, <span class="math inline">\(\mathrm{\mathbb{U}}_{\psi,\mathrm{\mathbb{P}}}\)</span>, matches a prior, <span class="math inline">\(\mathrm{\mathbb{V}}\)</span>.</p></li>
<li><p>This is done by training a discriminator <span class="math inline">\(D_\phi: \mathcal{Y}\rightarrow \mathrm{\mathbb{R}}\)</span> to estimate the divergence, <span class="math inline">\(\mathcal{D}(\mathrm{\mathbb{V}}|\mathrm{\mathbb{U}}_{\psi,\mathrm{\mathbb{P}}})\)</span>. Then training the encoder to minimize</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506134524556.png" alt="image-20220506134524556" /><figcaption aria-hidden="true">image-20220506134524556</figcaption>
</figure></li>
<li><p>trains the encoder to match the noise implicitly rather than using a priori noise samples as targets</p></li>
</ul>
<h4 id="complete-loss">Complete loss</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506140132378.png" alt="image-20220506140132378" /><figcaption aria-hidden="true">image-20220506140132378</figcaption>
</figure>
<h3 id="experiments-5">Experiments</h3>
<ul>
<li>Datasets: CIFAR10+CIFAR100, Tiny ImageNet, STL-10, CelebA (a face image dataset )</li>
<li>Compared methods: VAE, <span class="math inline">\(\beta\)</span>-VAE, adversarial AE, BiGAN, NAT, and CPC.</li>
</ul>
<h4 id="evaluate-the-quality-of-a-representation">Evaluate the quality of a representation</h4>
<ul>
<li>Linear separability has no help in showing the representation has high MI with the class labels when the representation is not disentangled.</li>
<li>To measure:
<ul>
<li>They use MINE to more directly measure the MI between the input the the output of the encoder.</li>
<li>NDM (neural dependency measure): Then measure the independence of the representation using a discriminator. train a discriminator to estimate the KL-divergence between the original representations (joint distribution of the factors) and the shuffled representations. The higher the KL-divergence, the more dependent the factors. NDM is sensible and empirically consistent.</li>
</ul></li>
<li>The classification
<ul>
<li>Linear classification</li>
<li>Non-linear classification (with a single hidden layer NN)</li>
<li>Semi-supervised learning: finetuning the encoder by adding a small NN.</li>
<li>MS-SSIM: decoder trained on the L2 Reconstruction loss.</li>
<li>MINE: maximized the DV estimator of the KL-divergence</li>
<li>NDM: using a second discriminator to measure the KL between <span class="math inline">\(E_\psi(x)\)</span> and a batch-wise shuffled version of <span class="math inline">\(E_\psi(x)\)</span>.</li>
</ul></li>
</ul>
<h4 id="representation-learning-comparison-across-models">Representation learning comparison across models</h4>
<ul>
<li>Test DIM(G) the global only, DIM (L) the local only and ablation study.</li>
<li>Classification:
<ul>
<li>DIM(L) outperforms all models. The representations are as good as or better than the raw pixels given the model constraints in this setting</li>
<li>infoNCE tends to perform best, but differences between infoNCE and JSD diminish with larger datasets</li>
<li>Overall DIM only slightly outperforms CPC in this setting, which suggests that the <strong>strictly ordered autoregression of CPC may be unnecessary for some tasks.</strong></li>
</ul></li>
<li>Extended comparison: For MI, DIM combining local and global DIM objectives had very high scores .</li>
<li>Adding coordinate information and occlusions
<ul>
<li>can be interpreted as context prediction and generalizations of inpainting respectively.</li>
<li>For occlusion: occluded part of input for global representations, but no occlusion for local representations. Maximizing MI between occluded global features and unoccluded local features aggressively encourages the global features to encode information which is shared across the entire image.</li>
</ul></li>
</ul>
<h3 id="appendix">Appendix</h3>
<ul>
<li>KL (traditional definition of mutual information) and the JSD have an approximately monotonic relationship. Overall, the distributions with the highest mutual information also have the highest JSD.</li>
<li>We found both infoNCE and the DV-based estimators were sensitive to negative sampling strategies, while the JSD-based estimator was insensitive.</li>
<li>DIM with a local-only objective, DIM(L), learns a representation with a much more interpretable structure across the image.</li>
<li>In general, good classification performance is highly dependent on the local term, <span class="math inline">\(\beta\)</span>, while good reconstruction is highly dependent on the global term, <span class="math inline">\(\alpha\)</span>. the local objective is crucial, the global objective plays a stronger role here than with other datasets.</li>
</ul>
<h2 id="paper-7-contrastive-multiview-coding-cmc-arxiv-2019">Paper 7: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.05849&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0s1M_14J8haUyPCJRRILoQ"><em>Contrastive Multiview Coding</em></a> (CMC) ArXiv 2019</h2>
<p>http://github.com/HobbitLong/CMC/</p>
<h3 id="previous-6">Previous</h3>
<ul>
<li>some bits are in fact better than others.</li>
<li>In these models, an input <span class="math inline">\(X\)</span> to the model is transformed into an output <span class="math inline">\(\hat{X}\)</span>, which is supposed to be close to another signal <span class="math inline">\(Y\)</span> (usually in Euclidean space), which itself is related to <span class="math inline">\(X\)</span> in some meaningful way. and provides us with nearly infinite amounts of training data.</li>
<li>The objective functions are usually reconstruction-based loss or contrastive losses.</li>
<li>CPC learns from the past and the future view simultaneously, while Deep InfoMax takes the past as the input and the future as the output. They both use instance discrimination learns to match two sub-crops of the same image.</li>
<li>They extend the objective to the case of more than two views and explore a different set of view definitions, architectures and application settings.</li>
</ul>
<h3 id="what-5">What</h3>
<ul>
<li>Idea: a powerful representation is one that models view-invariant factor. We learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact.</li>
<li>study the setting where the different views are different image channels, such as luminance, chrominance, depth, and optical flow. The fundamental supervisory signal we exploit is the co-occurrence, in natural data, of multiple views of the same scene.</li>
<li>Goal: learn information shared between multiple sensory channels but that are otherwise compact (i.e. discard channel-specific nuisance factors). we learn a feature embedding such that views of the same scene map to nearby points (measured with Euclidean distance in representation space) while views of different scenes map to far apart points.</li>
<li>Use CPC as the backbone but remove the recurrent network part.</li>
<li>We find that the quality of the representation improves as a function of the number of views used for training.</li>
<li>demonstrate that the contrastive objective is superior to cross-view prediction.</li>
</ul>
<h3 id="how-6">How</h3>
<ul>
<li><p>Suppose a dataset of <span class="math inline">\(V_1, V_2\)</span> that consists of a collection of samples <span class="math inline">\(\{v_1 ^i, v_2 ^i\}_{i=1}^N\)</span>, consider $x={v_1 ^i, v_2 ^i} $ as the positive and the <span class="math inline">\(y=\{v_1 ^i, v_2 ^j\}\)</span> as the negatives, aka the positive are the same picture in different views while the negative are the dissimilar images in different views.</p></li>
<li><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143138612.png" alt="image-20220507143138612" />, simply fix one view and enumerate positives and negatives from the other view, then the objective is written as</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143236840.png" alt="image-20220507143236840" /><figcaption aria-hidden="true">image-20220507143236840</figcaption>
</figure></li>
<li><p>But directly minimizing the above function is infeasible since <span class="math inline">\(k\)</span> is pretty large. To approximate,</p>
<ul>
<li>Implementing the critic: implement <span class="math inline">\(h_\theta(\cdot)\)</span> as a neural network. For each view, build a NN as the encoder, then compute the features cosine similarity as score and adjust its dynamic range by a hyper-parameter <span class="math inline">\(\tau\)</span>. The two view loss is then <span class="math inline">\(\mathcal{L} (V_1，V_2) = \mathcal{L}_{constrast}^{V_1，V_2}+\mathcal{L}_{constrast}^{V_2，V_1}\)</span>.</li>
<li>Connecting to mutual information: minimizing the objective L maximizes the lower bound on the mutual information. But recent works show that the bound can be very weak.</li>
</ul></li>
<li><p>Multi-views</p>
<ul>
<li>the full graph formulation is that it can handle missing information (e.g. missing views) in a natural manner. <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507144936733.png" alt="image-20220507144936733" /></li>
<li>The core view is <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507145012695.png" alt="image-20220507145012695" /></li>
</ul></li>
<li><p>To estimate, they use memory bank to store latent features for each training sample.</p></li>
</ul>
<h3 id="experiments-6">Experiments</h3>
<ul>
<li>Benchmarks: ImageNet and STL-10.</li>
</ul>
<h4 id="benchmarking-cmc-on-imagenet">Benchmarking CMC on ImageNet</h4>
<ul>
<li>Convert the RGB images to the Lab image color space and split each image into L and ab channels.</li>
<li>L and ab from the same image are treated as the positive pair, and ab channels from other randomly selected images are treated as a negative pair (for a given L)</li>
<li>set the temperature τ as 0.07 and use a momentum 0.5 for memory update.</li>
<li>learning from luminance and chrominance views in two colorspaces, {L, ab} and {Y, DbDr}.</li>
<li>{Y, DbDr} provides <span class="math inline">\(0.7\%\)</span> improvement, strengthening data augmentation with RandAugment yields better or comparable results to other SOTA methods.</li>
</ul>
<h4 id="cmc-on-videos">CMC on videos</h4>
<ul>
<li>given an image it that is a frame centered at time <span class="math inline">\(t\)</span>, the ventral stream associates it with a neighbouring frame <span class="math inline">\(i_{t+k}\)</span>, while the dorsal stream connects it to optical flow <span class="math inline">\(f_t\)</span> centered at <span class="math inline">\(t\)</span>.</li>
<li>extract <span class="math inline">\(i_t\)</span>, <span class="math inline">\(i_{t+k}\)</span> and <span class="math inline">\(f_t\)</span> from two modalities as three views of a video.
<ul>
<li>Take <span class="math inline">\((i_t,i_{t+k})\)</span> as the positive, and negative pairs for <span class="math inline">\(i_t\)</span> is chosen as a random frame from another randomly chosen video;</li>
<li>Take <span class="math inline">\((i_t,f_t)\)</span> as the positive, then negative pairs for <span class="math inline">\(i_t\)</span> are those flow corresponding to a random frame in another randomly chosen video.</li>
</ul></li>
<li>Pretrain the encoder on UCF101 and use two CaffeNets for extracting features from images and optical flows.</li>
<li>Increasing the number of views of the data from 2 to 3 (using both streams instead of one) provides a boost for UCF-101</li>
</ul>
<h4 id="extending-cmc-to-more-views">Extending CMC to more views</h4>
<ul>
<li>Consider views: luminance (L channel), chrominance (ab channel), depth, surface normal, and semantic labels.</li>
<li>the sub-patch based contrastive objective to increase the number of negative pairs</li>
<li>Does representation quality improve as number of views increases?
<ul>
<li>UNet style architecture</li>
<li>The 2-4 view cases contrast L with ab, and then sequentially add depth and surface normals.</li>
<li>measured by mean IoU over all classes and pixel accuracy.</li>
<li><strong>performance steadily improves as new views are added</strong></li>
</ul></li>
<li>Is CMC improving all views?
<ul>
<li>train these encoders following the full graph paradigm, where each view is contrasted with all other views.</li>
<li>evaluate the representation of each view v by predicting the semantic labels from only the representation of v, where v is L, ab, depth or surface normals.</li>
<li>the full-graph representation provides a good representation learnt for all views.</li>
</ul></li>
</ul>
<h4 id="predictive-learning-vs.-contrastive-learning">Predictive Learning vs. Contrastive Learning</h4>
<ul>
<li>consider three view pairs on the NYU-Depth dataset: (1) L and depth, (2) L and surface normals, and (3) L and segmentation map. For each of them, we train two identical encoders for L, one using contrastive learning and the other with predictive learning.</li>
<li>evaluate the representation quality by training a linear classifier on top of these encoders on the STL-10 dataset</li>
<li>For predictive learning, pixel-wise reconstruction losses usually impose an independence assumption on the modeling. While contrastive learning does not assume conditional independence across dimensions of <span class="math inline">\(v_2\)</span>. Also the use of random jittering and cropping between views allows the contrastive learning approach to benefit from spatial co-occurrence (contrasting in space) in addition to contrasting across views.</li>
</ul>
<h4 id="how-does-mutual-information-affect-representation-quality">How does mutual information affect representation quality?</h4>
<ul>
<li><p><strong>cross-view representation learning is effective because it results in a kind of information minimization</strong>, discarding nuisance factors that are not shared between the views.</p></li>
<li><p>a good collection of views is one that shares some information but not too much</p></li>
<li><p>To test, build two domains: learning representations on images with different colorspaces forming the two views; and learning representations on pairs of patches extracted from an image, separated by varying spatial distance. (use high resolution images to avoid overlapping and cropped patches around boundary.)</p></li>
<li><p>using colorspaces with minimal mutual information give the best downstream accuracy</p></li>
<li><p>For patches with different offset with each other, views with too little or too much MI perform worse.</p></li>
<li><p>the relationship between mutual information and representation quality is meaningful but not direct.</p></li>
<li><p>patch-based contrastive loss is computed within each mini-batch and does not require a memory bank, but usually yields suboptimal results compared to NCE-based contrastive loss, according to our experiments</p></li>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220508144638659.png" alt="image-20220508144638659" /><figcaption aria-hidden="true">image-20220508144638659</figcaption>
</figure></li>
<li><p>combining CMC with the MoCo mechanism or JigSaw branch in PIRL can consistently improve the performance, verifying that they are compatible.</p></li>
</ul>
<h2 id="paper-8-data-efficient-image-recognition-with-contrastive-predictive-coding-cpc-v2-improved-cpc-evaluated-on-limited-labelled-data">Paper 8: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.09272&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1GXq6QkuyYGzUQ2EqiWXnL"><em>Data-Efficient Image Recognition with Contrastive Predictive Coding</em></a> (CPC v2: Improved CPC evaluated on limited labelled data)</h2>
<h3 id="previous-7">Previous</h3>
<ul>
<li><p>CPC only requires in its definition that observations be ordered along e.g. temporal or spatial dimensions. It learns representations by training neural networks to predict the representations of future observations from those of past ones.</p>
<ul>
<li><p>Loss</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509105849555.png" alt="image-20220509105849555" /><figcaption aria-hidden="true">image-20220509105849555</figcaption>
</figure></li>
<li><p>The loss is inspired by NCE, called as InfoNCE.</p></li>
<li><p>The negative samples <span class="math inline">\(\{z_l\}\)</span> are taken from other locations in the image and other images in the mini-batch.</p></li>
</ul></li>
<li><p>AMDIM is most similar to CPC in that it makes predictions across space, but differs in that it also predicts representations across layers in the model.</p></li>
<li><p>For improving data efficiency , one way is label-propogation.</p>
<ul>
<li>a classifier is trained on a subset of labeled data</li>
<li>then used to label parts of the unlabeled dataset</li>
</ul></li>
<li><p>Representation learning and label propagation have been shown to be complementary and can be combined to great effect (Zhai et al., 2019).</p></li>
<li><p>This work focus on representation learning.</p></li>
</ul>
<h3 id="what-6">What</h3>
<ul>
<li>hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable</li>
<li><strong>removing low-level cues which might lead to degenerate solutions.</strong></li>
</ul>
<h3 id="how-7">How</h3>
<ul>
<li>Pretrain the encoder by CPC on local patches, and during test, apply the encoder on the entire image.</li>
<li>Evaluation
<ul>
<li>Linear classification: mean pooling followed by a single linear layer as the classifier. Use cross-entropy loss.</li>
<li>Efficient classification: fix / fine tune the pretrained encoder, and then train the classifier (ResNet-33). Use a smaller learning rate and early-stopping for fine tuning incase the encoder deviates too much from the solution by the CPC objective.</li>
<li>transfer learning: transfer the pretrained encoder to faster-RCNN to do classification. This is a multi-task.</li>
<li>supervised training: directly train classifier by the input data, use cross-entropy loss.</li>
</ul></li>
<li></li>
</ul>
<h3 id="experiments-7">Experiments</h3>
<ul>
<li>Compare <span class="math inline">\(1\%\)</span> supervised DNN based on CPC encoder with Semi-supervised methods and then supervised ResNets.</li>
</ul>
<h4 id="from-cpc-v1-to-cpc-v2">From CPC v1 to CPC v2</h4>
<ul>
<li>Four axes for model capacity
<ul>
<li>increasing depth and width: <span class="math inline">\(+5\%\)</span>. $+2% $ Top-1 accuracy with larger patches.</li>
<li>improves training efficiency by importing layer normalization: can reclaim much of batch normalization’s training efficiency by using layer normalization (<span class="math inline">\(+2\%\)</span> accuracy)</li>
<li>making predictions in all four direction: Additional predictions tasks incrementally increased accuracy (adding bottom-up predictions: +2% accuracy; using all four spatial directions: +2.5% accuracy).</li>
<li>perform patch-based augmentation: ‘color dropping’ (+3% accuracy); adding a fixed, generic augmentation scheme using the primitives from (shearing, rotation etc.), as well as random elastic deformations and color transforms +4.5% accuracy in total.</li>
</ul></li>
</ul>
<h4 id="efficient-image-classification">Efficient image classification</h4>
<ul>
<li>fine-tune the entire stack hψ ◦ fθ for the supervised objective, for a small number of epochs (chosen by cross-validation)</li>
<li>with only 1% of the labels, our classifier surpasses the supervised baseline given 5% of the labels</li>
<li>the family of ResNet-50, -101, and -200 architectures are designed for supervised learning, and their capacity is calibrated for the amount of training signal present in ImageNet labels; larger architectures only run a greater risk of overfitting.</li>
<li>fine-tuned representations yield only marginal gains over fixed ones</li>
<li>we find that CPC provides gains in data efficiency that were previously unseen from representation learning methods, and rival the performance of the more elaborate label-propagation algorithms.</li>
</ul>
<h4 id="transfer-learning-image-detection-on-pascal-voc-2007">Transfer learning: image detection on PASCAL VOC 2007</h4>
<ul>
<li>unsupervised pre-training surpasses supervised pretraining for transfer learning</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<ul>
<li>images are far from the only domain where unsupervised representation learning is important.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509121402548.png" alt="image-20220509121402548" /><figcaption aria-hidden="true">image-20220509121402548</figcaption>
</figure>
<h2 id="paper-9-momentum-contrast-for-unsupervised-visual-representation-learning-moco-see-also-moco-v2">Paper 9: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FHe_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3wwgkDHILZxygHEqFpue3W"><em>Momentum Contrast for Unsupervised Visual Representation Learning</em></a> (MoCo, see also <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.04297&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0tEX1f7G3quzSTWL6rfgzw"><em>MoCo v2</em></a>)</h2>
<p>Have read MoCo before, here only list MoCo v2.</p>
<h3 id="previous-8">Previous</h3>
<ul>
<li>Momentum Contrast (MoCo) shows that unsupervised pre-training can surpass its ImageNet-supervised counterpart in multiple detection and segmentation tasks.</li>
<li>the negative keys are maintained in a queue, and only the queries and positive keys are encoded in each training batch.</li>
<li>MoCo decouples the batch size from the number of negatives.</li>
<li>SimCLR further reduces the gap in linear classifier performance between unsupervised and supervised pre-training representations.</li>
<li>In an end-to-end mechanism, the negative keys are from the same batch and updated endto-end by back-propagation. SimCLR is based on this mechanism and requires a large batch to provide a large set of negatives.</li>
</ul>
<h3 id="what-7">What</h3>
<ul>
<li>verify the effectiveness of two of SimCLR’s design improvements by implementing them in the MoCo framework</li>
<li>using an MLP projection head and more data augmentation—we establish stronger baselines that outperform SimCLR and do not require large training batches.</li>
</ul>
<h3 id="how-8">How</h3>
<ul>
<li>Evaluation
<ul>
<li>ImageNet linear classification: features are frozen and a supervised linear classifier is trained</li>
<li>Transferring to VOC object detection: a Faster R-CNN detector (C4-backbone) is fine-tuned end-to-end on the VOC 07+12 <em>trainval</em> set and evaluated on the VOC 07 <em>test</em> set using the COCO suite of metrics</li>
</ul></li>
<li>Amending
<ul>
<li>replace the fc head in MoCo with a 2-layer MLP head (hidden layer 2048-d, with ReLU). This MLP only influences the unsupervised training stage; the linear classification or transferring stage does not use this MLP head.</li>
<li>including the blur augmentation because stronger color distortion in <em>A simple framework for contrastive learning of visual representations</em> has diminishing gains in our higher baselines.</li>
</ul></li>
</ul>
<h3 id="experiments-8">Experiments</h3>
<ul>
<li>pre-training with the MLP head improves from 60.6% to 62.9%. in contrast to the big leap on ImageNet, the detection gains are smaller</li>
<li><strong>linear classification accuracy is not monotonically related to transfer performance in detection</strong></li>
<li><strong>large batches are not necessary for good accuracy</strong>, and state-of-the-art results can be made more accessible.</li>
</ul>
<h2 id="paper-10-a-simple-framework-for-contrastive-learning-of-visual-representations-simclr.-icml-2020">Paper 10: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2002.05709.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3nN2zgMpeyBgpXG--pO5Kx"><em>A Simple Framework for Contrastive Learning of Visual Representations</em></a> (SimCLR). ICML 2020</h2>
<p>https://github.com/google-research/simclr</p>
<h3 id="previous-9">Previous</h3>
<ul>
<li>Many such approaches have relied on heuristics to design pretext tasks, which could limit the generality of the learned representations.</li>
<li>Types of augmentations
<ul>
<li>spatial/geometric transformation of data: cropping and resizing (with horizontal flipping), rotation and cutout</li>
<li>appearance transformation, such as color distortion (including color dropping, brightness, contrast, saturation, hue), Gaussian blur, and Sobel filtering.</li>
</ul></li>
<li>it is not clear if the success of contrastive approaches is determined by the mutual information, or by the specific form of the contrastive loss</li>
</ul>
<h3 id="what-8">What</h3>
<ul>
<li><strong>composition of multiple data augmentations</strong> plays a critical role in defining effective predictive tasks</li>
<li>introducing <strong>a learnable nonlinear transformation between the representation and the contrastive loss</strong> substantially improves the quality of the learned representations</li>
<li>contrastive learning benefits from <strong>larger batch sizes</strong> and more training steps compared to supervised learning</li>
<li>Representation learning with contrastive cross entropy loss benefits from <strong>normalized embeddings and an appropriately adjusted temperature parameter</strong>.</li>
</ul>
<h3 id="how-9">How</h3>
<ul>
<li><p>SimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510105556229.png" alt="image-20220510105556229" /><figcaption aria-hidden="true">image-20220510105556229</figcaption>
</figure>
<p>Where the <span class="math inline">\(x\)</span> is the input data from one image, <span class="math inline">\(t\sim\mathcal{T}\)</span> is the augmentation operator, and <span class="math inline">\(f(\cdot)\)</span> is the encoder operator, <span class="math inline">\(g(\cdot)\)</span> is the projection head. In downstream task, the <span class="math inline">\(h\)</span> is used as the extracted features for classification.</p>
<ul>
<li>define the contrastive prediction task on pairs of augmented examples derived from the minibatch. Negative samples are not sampled explicitly.</li>
<li>Use dot product as the similarity measurement. The final loss is termed as NT-Xent (the normalized temperature-scaled cross entropy loss).</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510110242447.png" alt="image-20220510110242447" /><figcaption aria-hidden="true">image-20220510110242447</figcaption>
</figure>
<ul>
<li>vary the training batch size N from 256 to 8192. A batch size of 8192 gives us 16382 negative examples per positive pair from both augmentation views. Since large batch size with SGD will induce to unstable, use LARS optimizer instead.</li>
<li>as positive pairs are computed in the same device, use global BN by aggregating BN mean and variance over all devices during the training.</li>
<li>shuffling data examples across devices, replacing BN with layer norm</li>
</ul></li>
<li><p>Evaluation protocol</p>
<ul>
<li>Linear classifier, and compare with SOTA on Semi-supervised and transfer learning.</li>
<li>default settings
<ul>
<li>augmentation: random crop and resize (with random flip), color distortions, and Gaussian blur</li>
<li>use ResNet-50 as the base encoder network</li>
<li>a 2-layer MLP projection head to project the representation to a 128-dimensional latent space</li>
<li>train at batch size 4096 for 100 epochs</li>
<li>linear warmup for the first 10 epochs, and decay the learning rate with the cosine decay schedule without restarts</li>
</ul></li>
</ul></li>
</ul>
<h3 id="experiments-9">Experiments</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510104751609.png" alt="image-20220510104751609" /><figcaption aria-hidden="true">image-20220510104751609</figcaption>
</figure>
<h4 id="data-augmentation">Data augmentation</h4>
<ul>
<li>Found no single transformation suffices to learn good representations</li>
<li>One composition of augmentations stands out: random cropping and random color distortion
<ul>
<li>Though color histograms alone suffice to distinguish images, most patches from an image share a similar color distribution.</li>
<li><strong>Stronger color augmentation</strong> substantially <strong>improves the linear evaluation</strong> of the learned unsupervised models. But for supervised methods, the stronger color augmentation even hurt the performance.==&gt; unsupervised contrastive learning benefits from stronger (color) data augmentation than supervised learning</li>
</ul></li>
</ul>
<h4 id="architectures-for-encoder-and-head">Architectures for encoder and head</h4>
<ul>
<li>Unsupervised contrastive learning benefits (more) from bigger models
<ul>
<li>the gap between supervised models and linear classifiers trained on unsupervised models shrinks as the model size increases,
<ul>
<li>suggesting that unsupervised learning benefits more from bigger models than its supervised counterpart.</li>
<li>training logger does not improve supervised methods.</li>
</ul></li>
</ul></li>
<li>A nonlinear projection head improves the representation quality of the layer before it</li>
<li>a nonlinear projection is better than a linear projection (+3%), and much better than no projection (&gt;10%)</li>
<li>hidden layer before the projection head is a better representation than the layer after
<ul>
<li>They explain this is due to loss of information induced by the contrastive loss</li>
<li><span class="math inline">\(g\)</span> (projection head) can remove information that may be useful for the downstream task, such as the color or orientation of objects</li>
</ul></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220510114836113.png" title="fig:" alt="image-20220510114836113" /></li>
</ul>
<h4 id="loss-functions-and-batch-size">Loss functions and batch size</h4>
<ul>
<li>Normalized cross entropy loss with adjustable temperature works better than alternatives
<ul>
<li>NT-Xent loss against such as logistic loss and margin loss.
<ul>
<li><span class="math inline">\(\ell_2\)</span> normalization (i.e. cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives</li>
<li>unlike cross-entropy, other objective functions do not weigh the negatives by their relative hardness</li>
</ul></li>
<li>without normalization and proper temperature scaling, performance is significantly worse</li>
</ul></li>
<li>Contrastive learning benefits (more) from larger batch sizes and longer training
<ul>
<li>when the number of training epochs is small (e.g. 100 epochs), larger batch sizes have a significant advantage over the smaller ones</li>
<li>With more training steps/epochs, the gaps between different batch sizes decrease or disappear, provided the batches are randomly resampled</li>
<li>Training longer also provides more negative examples, improving the results.</li>
</ul></li>
</ul>
<h4 id="learning-rate-and-projection-matrix">Learning rate and projection matrix</h4>
<ul>
<li>square root learning rate scaling improves the performance for models trained with small batch sizes and in smaller number of epoch</li>
<li>The linear projection matrix for computing <span class="math inline">\(z\)</span> is approximately low rank, indicating by the plot of eigenvalues.</li>
</ul>
<h4 id="the-way-to-train-downstream-classifier">The way to train downstream classifier</h4>
<p>attaching the linear classifier on top of the base encoder (with a stop_gradient on the input to linear classifier to prevent the label information from influencing the encoder) and train them simultaneously during the pretraining achieves <strong>similar performance</strong></p>
<h4 id="the-best-temperature-parameter">The best temperature parameter</h4>
<ul>
<li>the optimal temperature in {0.1, 0.5, 1.0} is 0.5 and seems consistent regardless of the batch sizes.</li>
</ul>
<h4 id="compared-with-sota">Compared with SOTA</h4>
<ul>
<li>fine-tuning our pretrained ResNet-50 (2×, 4×) on full ImageNet are also significantly better then training from scratch (up to 2%)</li>
<li>our self-supervised model significantly outperforms the supervised baseline on 5 datasets, whereas the supervised baseline is superior on only 2</li>
</ul>
<h2 id="paper-11-big-self-supervised-models-are-strong-semi-supervised-learners-simclrv2-arxiv-2020">Paper 11: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.10029&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw263B6UxGrQeiE53_baVKp-"><em>Big Self-Supervised Models are Strong Semi-Supervised Learners</em></a> (SimCLRv2) ArXiv 2020</h2>
<h3 id="previous-10">Previous</h3>
<ul>
<li>semi-supervised learning involves unsupervised or self-supervised pretraining. It leverages unlabeled data in a task-agnostic way during pretraining, as the supervised labels are only used during fine-tuning.</li>
<li>An alternative approach, directly leverages unlabeled data during supervised learning, as a form of regularization. uses unlabeled data in a task-specific way to encourage class label prediction consistency on unlabeled data among different models or under different data augmentations.</li>
<li></li>
</ul>
<h3 id="what-9">What</h3>
<ul>
<li>Found the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network</li>
<li>The proposed semi-supervised learning algorithm in three steps:
<ul>
<li>unsupervised pretraining of a big ResNet model using SimCLRv2,</li>
<li>supervised fine-tuning on a few labeled examples,</li>
<li>and distillation with unlabeled examples for refining and transferring the task-specific knowledge.</li>
</ul></li>
<li>make use of unlabeled data for a second time to encourage the student network to mimic the teacher network’s label predictions</li>
</ul>
<h3 id="how-10">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220511122629288.png" alt="image-20220511122628256" /><figcaption aria-hidden="true">image-20220511122628256</figcaption>
</figure>
<ul>
<li>train the student network by the labels predicted by the teacher network, also can add some real labels, while the student and teacher network are both trained by unlabeled data.</li>
<li>SimCLR v2
<ul>
<li>With bigger backbone (encoder): The largest model we train is a 152-layer ResNet [25] with 3× wider channels and selective kernels (SK). From ResNet-50 to ResNet-152 (3×+SK), we obtain a 29% relative improvement in top-1 accuracy when fine-tuned on 1% of labeled examples</li>
<li>Increase the capacity of the projection head by making it deeper. Use a 3-layer projection head and fine tuning from the 1st layer of projection head. It results in as much as 14% relative improvement in top-1 accuracy when fine-tuned on 1% of labeled examples.</li>
<li>Incorporate the memory mechanism from MoCo. It yields an improvement of ∼1% for linear evaluation as well as when fine-tuning on 1% of labeled examples when the SimCLR is trained in larger batch size.</li>
</ul></li>
<li>Fine-tuning
<ul>
<li>Instead of throwing away the projection head directly, they fine-tune the model from a middle layer of the projection head, instead of the input layer of the projection head as in SimCLR.</li>
</ul></li>
<li>Self-training
<ul>
<li>leverage the unlabeled data directly for the target task</li>
<li>minimize the following distillation loss where no real labels are used: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220511130512584.png" alt="image-20220511130512584" style="zoom:50%;" /></li>
<li>during distillation the teacher network is fixed.</li>
<li>If do it in semi-supervised: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220511130823288.png" alt="image-20220511130823288" /></li>
<li>The student model can be the same as the teacher or be smaller.</li>
</ul></li>
</ul>
<h3 id="experiments-10">Experiments</h3>
<ul>
<li>Benchmarks: ImageNet ILSVRC-2012. only a randomly sub-sampled 1% (12811) or 10% (128116) of images are associated with labels.</li>
<li>with a batch size of 4096 and global batch normalization, for total of 800 epochs</li>
<li>The memory buffer is set to 64K, use the same augmentations as in SimCLR v1, namely random crop, color distortion, and Gaussian blur.</li>
<li>Two student networks, one is the same as the teacher, the other is smaller than the teacher to test the self-distillation and big-to-small distillation. <em>Only random crop and horizontal flips of training images are applied during fine-tuning and distillation</em>.</li>
</ul>
<h4 id="bigger-models-are-more-label-efficient">Bigger models are more label-efficient</h4>
<ul>
<li>train ResNet by varying width and depth as well as whether or not to use selective kernels (SK). If use SK, they use the ResNet-D version.</li>
<li>Use SK will cause larger model size.</li>
<li>increasing width and depth, as well as using SK, all improve the performance. But the benefits of width will plateau.</li>
<li>bigger models are more label-efficient for both supervised and semi-supervised learning, but gains appear to be larger for semi-supervised learning.</li>
</ul>
<h4 id="biggerdeeper-projection-heads-improve-representation-learning">Bigger/deeper projection heads improve representation learning</h4>
<ul>
<li>using a deeper projection head during pretraining is better when fine-tuning from the optimal layer of projection head, and this optimal layer is typically the first layer of projection head rather than the input (0th layer).</li>
<li>when using bigger ResNets, the improvements from having a deeper projection head are smaller.</li>
<li>it is possible that increasing the depth of the projection head has limited effect when the projection head is already relatively wide.</li>
<li>Correlation is higher when fine-tuning from <strong>the optimal middle layer of the projection head</strong> than when fine-tuning from the projection head input, which indicates the accuracy of fine-tuned models is more related with the optimal middle layer of the projection head.</li>
</ul>
<h4 id="distillation-using-unlabeled-data-improves-semi-supervised-learning">Distillation using unlabeled data improves semi-supervised learning</h4>
<ul>
<li><p>Two loss: distillation loss and an ordinary supervised cross-entropy loss on the labels.</p></li>
<li><p>Using the distillation loss alone works almost as well as balancing distillation and label losses when the labeled fraction is small (1%, 10%).</p></li>
<li><p>To get the best performance for smaller ResNets, the big model is self-distilled before distilling it to smaller models</p></li>
<li><p>when the student model has a smaller architecture than the teacher model, it improves the model efficiency by transferring task-specific knowledge to a student model; even when the student model has the same architecture as the teacher model (excluding the projection head after ResNet encoder), self-distillation can still meaningfully improve the semi-supervised learning performance.</p></li>
<li><p>task-agnostically learned general representations can be distilled into a more specialized and compact network using unlabeled examples.</p></li>
<li><p>The benefits are larger when (1) regularization techniques (such as augmentation, label smoothing) are used, or (2) the model is pretrained using unlabeled examples</p></li>
<li><p>a better fine-tuned model (measured by its top-1 accuracy), regardless their projection head settings, is a better teacher for transferring task specific knowledge to the student using unlabeled data.</p></li>
</ul>
<h2 id="paper-12-unsupervised-learning-of-visual-features-by-contrasting-cluster-assignments-swav-arxiv-2020">Paper 12: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.09882&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0IYamHH9j-b56C3eUiwNXP"><em>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</em></a> (SwAV) ArXiv 2020</h2>
<p>https://github.com/facebookresearch/swav</p>
<h3 id="previous-11">Previous</h3>
<ul>
<li>contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally expensive.</li>
<li>In contrastive methods, the loss function and augmentations both contribute to the performance.
<ul>
<li>most implementations approximate the loss by reducing the number of comparisons to random subsets of images during training</li>
<li>An alternative to approximate the loss is to approximate the task—that is to relax the instance discrimination problem. E.g., clustering-based methods discriminate between groups of images with similar features instead of individual images. But it does not scale well with the dataset as it requires a pass over the entire dataset to form image “codes” (i.e., cluster assignments) that are used as targets during training.</li>
<li>Amendment: <em>They avoid comparing every pair of images by mapping the image features to a set of trainable prototype vectors.</em></li>
</ul></li>
<li>Typical clustering-based methods are offline in the sense that they alternate between a cluster assignment step where image features of the entire dataset are clustered, and a training step where the cluster assignments, i.e., “codes” are predicted for different image views.</li>
</ul>
<h3 id="what-10">What</h3>
<ul>
<li>propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Their method can be interpreted as a way of contrasting between multiple image views by comparing their cluster assignments instead of their features.</li>
<li>simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image</li>
<li><strong>predict the code of a view from the representation of another view</strong>, and called it as swapped assignments between multiple views of the same image (SwAV)</li>
<li>memory efficient since it does not require a large memory bank or a special momentum network.</li>
<li>Propose multi-crop as a new augmentation, uses a mix of views with different resolutions in place of two full-resolution views. It consists in simply sampling multiple random crops with two different sizes: a standard size and a smaller one.</li>
<li>mapping small parts of a scene to more global views significantly boosts the performance.</li>
<li>Use ResNet as the backbone and ImageNet as the benchmark.</li>
</ul>
<h3 id="how-11">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512112020319.png" alt="image-20220512112020319" /><figcaption aria-hidden="true">image-20220512112020319</figcaption>
</figure>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512140025628.png" alt="image-20220512140025628" /><figcaption aria-hidden="true">image-20220512140025628</figcaption>
</figure>
<ul>
<li><p>Prototype vectors <span class="math inline">\(C\)</span> are learned along with the ConvNet parameters by backpropragation, and they are mapped with the predicted <span class="math inline">\(z\)</span> to search the codes <span class="math inline">\(Q\)</span>.</p>
<ul>
<li>For two image features <span class="math inline">\(z_t,z_s\)</span>, predict the codes <span class="math inline">\(q_t,q_s\)</span>, the the loss is <span class="math inline">\(L(z_t,z_s)=\ell(z_t,q_s)+\ell(z_s,q_t)\)</span>. <span class="math inline">\(\ell(z,q)\)</span> measures the fit between features <span class="math inline">\(z\)</span> and a code <span class="math inline">\(q\)</span>. <span class="math inline">\(\ell(z_t,q_s)=-\sum_k q_s^{(k)}\log p_t^{(k)}\)</span>, where <span class="math inline">\(p_t^{(k)}=\frac{\exp{(\frac{1}{\tau}}z_t^Tc_k)}{\sum_{k&#39;}\exp{(\frac{1}{\tau}z_t^Tc_{k&#39;}})}\)</span>.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512122436330.png" title="fig:" alt="image-20220512122436330" /></li>
</ul></li>
<li><p>Online codes computing: to get the soft predicted codes <span class="math inline">\(Q^*\)</span></p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512135955654.png" alt="image-20220512135955654" /><figcaption aria-hidden="true">image-20220512135955654</figcaption>
</figure>
<ul>
<li>optimize <span class="math inline">\(Q\)</span> to maximized the similarity between the features and the prototypes, i.e., <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512123010624.png" alt="image-20220512123010624" style="zoom: 33%;" /> <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512123038832.png" alt="image-20220512123038832" style="zoom: 33%;" /></li>
<li>the entropy <span class="math inline">\(H(Q)\)</span> controls the diversity of the codes, and a strong entropy regularization (i.e. using a high ε) generally leads to a trivial solution where all samples collapse into an unique representation and are all assigned uniformely to all prototypes. They keep <span class="math inline">\(\varepsilon\)</span> small.</li>
<li>Enforce that on average each prototype is selected at least <span class="math inline">\(\frac{B}{K}\)</span> times in the batch.
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512123621658.png" title="fig:" alt="image-20220512123621658" /></li>
</ul></li>
<li>In online version, the continuous codes work better than discrete codes (rounding the continuous codes)
<ul>
<li>An explanation is that the rounding needed to obtain discrete codes is a more aggressive optimization step than gradient updates. While it makes the model converge rapidly, it leads to a worse solution.</li>
</ul></li>
<li>The soft codes <span class="math inline">\(Q^*\)</span> is <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512124151867.png" alt="image-20220512124151867" style="zoom:50%;" />, where <span class="math inline">\(u,v\)</span> are renormalization vectors in <span class="math inline">\(\mathbb{R}^K, \mathbb{R}^B\)</span> respectively (prototypes and feature space).</li>
<li>In practice, we observe that using only 3 iterations is fast and sufficient to obtain good performance for solving <span class="math inline">\(Q^*\)</span>.</li>
<li>When working with small batches, we use features from the previous batches to augment the size of <span class="math inline">\(Z\)</span> to equally partition the batch into the <span class="math inline">\(K\)</span> prototypes. Then, we only use the codes of the batch features in our training loss</li>
</ul></li>
<li><p>In practice, the size of keep batches</p>
<ul>
<li>we store around 3K features, i.e., in the same range as the number of code vectors.</li>
<li>we only keep features from the last 15 batches with a batch size of 256, while contrastive methods typically need to store the last 65K instances obtained from the last 250 batches</li>
</ul></li>
<li><p>Multi-crop</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512140312637.png" alt="image-20220512140312637" /><figcaption aria-hidden="true">image-20220512140312637</figcaption>
</figure>
<ul>
<li>use two standard resolution crops and sample <span class="math inline">\(V\)</span> additional low resolution crops that cover only small parts of the image. The low resolution is for computation efficiency.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220512130430962.png" title="fig:" alt="image-20220512130430962" /></li>
<li>We compute codes <span class="math inline">\(q\)</span> using only the full resolution crops. using only partial information (small crops cover only small area of images) degrades the assignment quality</li>
<li>Why <span class="math inline">\(i=\{1,2\}\)</span> here? Because for each image, apply <span class="math inline">\(s,t\)</span> transformations? And swap just swap the predicted codes for the same source images (<span class="math inline">\(x_s,x_t\)</span>).</li>
</ul></li>
</ul>
<h3 id="experiments-11">Experiments</h3>
<ul>
<li><p>main results</p>
<ul>
<li><p>SwAV is not specifically designed for semi-supervised learning</p>
<p>Outperform other Self-supervised methods if using frozen features.</p></li>
<li><p>The performance of our model increases with the width of the model, and follows a similar trend to the one obtained with supervised learning</p></li>
<li><p>one epoch of MoCov2 or SimCLR is faster in wall clock time than one of SwAV, but these methods need more epochs for good downstream performance.</p></li>
</ul></li>
<li><p>Clustering-based SSL</p>
<ul>
<li>In DeepCluster-v2, instead of learning a classification layer predicting the cluster assignments, we perform explicit comparison between features and centroids.</li>
<li>SwAV and DeepCluster-v2 outperform SimCLR by 2% without multi-crop and by 3.5% with multi-crop.</li>
<li>DeepCluster-v2 is not online which makes it impractical for extremely large datasets. DeepCluster-v2 can be interpreted as a special case of our proposed swapping mechanism: swapping is done across epochs rather than within a batch.</li>
</ul></li>
<li><p>Multi-crop to different methods: <em>multi-crop seems to benefit more clustering-based methods than contrastive methods</em>. We note that multi-crop does not improve the supervised model</p></li>
<li><p>Longer training: While SwAV benefits from longer training, it already achieves strong performance after 100 epochs.</p></li>
<li><p>Unsupervised pretraining on a large uncurated dataset</p>
<ul>
<li>evaluate SwAV on random, uncurated images that have different properties from ImageNet which allows us to test if our online clustering scheme and multi-crop augmentation work out of the box: pretrain SwAV on an uncurated dataset of 1 billion random public non-EU images from Instagram. Then the trained model is used to evaluate ImageNet task.</li>
<li>SwAV maintains a similar gain of 6% over SimCLR as when pretrained on ImageNet==&gt; our improvements do not depend on the data distribution</li>
<li>Capacity
<ul>
<li>SwAV outperforms training from scratch by a significant margin showing that it can take advantage of the increased model capacity.</li>
</ul></li>
</ul></li>
<li><p>Other results</p>
<ul>
<li>start using a queue composed of the feature representations from previous batches <strong>after 15 epochs of training</strong>.</li>
<li>the prototypes in SwAV are not strongly encouraged to be categorical and <strong>random fixed prototypes work almost as well</strong>==&gt; they help contrasting different image views without relying on pairwise comparison with many negatives samples.</li>
</ul></li>
</ul>
<h2 id="paper-13-what-should-not-be-contrastive-in-contrastive-learning-arxiv-2020">Paper 13: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.05659&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3gwV78jCI0fs0R9wrj-y7S"><em>What Should Not Be Contrastive in Contrastive Learning</em></a> ArXiv 2020</h2>
<h3 id="previous-12">Previous</h3>
<ul>
<li>The methods of SSL contrastive methods assume a particular set of representational invariances.</li>
<li>The property of contrastive sampling like in MoCo negatively affects the learnt representations:
<ul>
<li>Generalizability and transferability are harmed if they are applied to the tasks where the discarded information is essential, e.g., color plays an important role in fine-grained classification of birds;</li>
<li>Adding an extra augmentation is complicated as the new operator may be helpful to certain classes while harmful to others, e.g., a rotated flower could be very similar to the original one, whereas it does not hold for a rotated car;</li>
<li>The hyper-parameters which control the strength of augmentations need to be carefully tuned for each augmentation to strike a delicate balance between leaving a short-cut open and completely invalidate one source of information.</li>
</ul></li>
</ul>
<h3 id="what-11">What</h3>
<ul>
<li>introduce a contrastive learning framework which does not require prior knowledge of specific, task-dependent invariances. The model learns to capture varying and invariant factors for visual representations by constructing separate embedding spaces, each of which is invariant to all but one augmentation.</li>
<li>We use a multi-head network with a shared backbone which captures information across each augmentation and alone outperforms all baselines on downstream tasks.</li>
<li>does not require hand-selection of data augmentation strategies, and achieves better performance against state-of-the-art MoCo baseline</li>
<li>Propose LOOC (leave-one-out contrastive learning).</li>
</ul>
<h3 id="how-12">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220513193229770.png" alt="image-20220513193229770" /><figcaption aria-hidden="true">image-20220513193229770</figcaption>
</figure>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220513193545502.png" title="fig:" alt="image-20220513193545502" /></li>
<li><span class="math inline">\(\mathcal{Z}_i\)</span> is dependent on the <span class="math inline">\(i^{th}\)</span> type of augmentation but invariant to other types of augmentations.
<ul>
<li>In <span class="math inline">\(\mathcal{Z}_0\)</span>, all features <span class="math inline">\(v\)</span> should be mapped to a single point</li>
<li>In <span class="math inline">\(\mathcal{Z}_i\)</span>, only <span class="math inline">\(v^q,v^{k_i}\)</span> should be mapped to a single point while <span class="math inline">\(v^{k_j} \forall j\neq i\)</span> should be mapped to <span class="math inline">\(n-1\)</span> separate points.</li>
</ul></li>
<li>Use MoCo as the framework, and ResNet50 as backbone. Use augmentations: color jittering (including random gray scale), random rotation (90，180，or 270) and texture randomization.</li>
<li>Use random-resized croppping, horizontal flipping and Gaussian blur as augmentations without designated embedding space.</li>
<li>In LooC++, use the concatenation of the output of Conv5 and the last layer of <span class="math inline">\(h\)</span> from each head to build a new input for the projection head.</li>
<li>After training, we adopt linear classification protocol by training a supervised linear classifier on frozen features of feature space <span class="math inline">\(\mathcal{V}\)</span> for LooC, or concatenated feature spaces <span class="math inline">\(\mathcal{Z}\)</span> for LooC++.</li>
</ul>
<h3 id="experiments-12">Experiments</h3>
<ul>
<li>Test on In-100 validation set, the iNaturalist (iNat-1k) dataset, VGG flowers, ObjectNet and ImageNet-C.</li>
<li>Use two-layer MLP head with a 2048-d hidden later and ReLU for each individual embedding space.</li>
<li>Inductive augmentation: Adding rotation augmentation into baseline MoCo significantly reduces its capacity to classify rotation angles while downgrades its performance on IN-100.
<ul>
<li>In contrast, our method better leverages the information gain of the new augmentation.</li>
<li>We can include all augmentations with our LooC multi-self-supervised method and obtain improved performance across all condition without any downstream labels or a prior knowledged invariance</li>
</ul></li>
<li>Fine-grained recognition results
<ul>
<li>MoCo has marginally superior performance on IN-100</li>
<li>But the original MoCo trails our LooC counterpart on all other datasets by a noticeable margin</li>
<li>our method can better preserve color information.</li>
<li>Rotation augmentation also boosts the performance on iNat-1k and Flowers-102, while yields smaller improvements on CUB-200, which supports the intuition that some categories benefit from rotation-invariant representations while some do not</li>
<li>The performance is further boosted by using LooC with both augmentations ==&gt; the effectiveness in simultaneously learning the information w.r.t. multiple augmentations</li>
<li>the benefits of explicit feature fusion without hand-crafting what should or should not be contrastive in the training objective</li>
</ul></li>
<li>Robustness learning results
<ul>
<li>The fully supervised network is most sensitive to perturbations</li>
<li>The rotation augmentation is beneficial for ON-13, but significantly downgrades the robustness to data corruptions in IN-C-100.</li>
<li><strong>texture randomization increases the robustness on IN-C-100 across all corruption types</strong></li>
<li>Combining rotation and texture augmentation yields improvements on both datasets</li>
</ul></li>
<li>Multiple heads
<ul>
<li>Using multiple heads boosts the performance of baseline MoCo</li>
<li>our method achieves better or comparable results compared with its baseline counterparts.</li>
</ul></li>
<li>Ablation of augmentation
<ul>
<li>Textures are (overly) strong cues for ImageNet classification (Geirhos et al., 2018), thus the linear classifier is prone to use texture-dependent features, loosing the gains of texture invariance</li>
<li>By checking the histograms of correct predictions (activations x weights of classifier). The classifier on IN-100 heavily relies on texture-dependent information, whereas it is much more balanced on iNat-1k.</li>
<li>when human or animal faces dominant an image ((a), bottom-left), LooC++ sharply prefers rotation-dependent features, which also holds for face recognition of humans.</li>
</ul></li>
</ul>
<h2 id="paper-14-self-supervised-learning-of-pretext-invariant-representations.-cvpr-2020">Paper 14: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FMisra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3sER0wiTJlhLbi88owlXXE"><em>Self-Supervised Learning of Pretext-Invariant Representations</em></a><em>.</em> CVPR 2020</h2>
<h3 id="previous-13">Previous</h3>
<ul>
<li>Pre-defined semantic annotations scale poorly to the long tail of visual concepts, while SSL tries to address this problem.</li>
<li>Representations ought to be invariant under image transformations to be useful for image recognition, because the transformations do not alter visual semantics.</li>
<li></li>
</ul>
<h3 id="what-12">What</h3>
<ul>
<li>focuses on image-based pretext tasks</li>
<li>Hint: semantic representations ought to be invariant under such transformations.==&gt; we develop Pretext-Invariant Representation Learning (PIRL, pronounced as “pearl”) that learns invariant representations based on pretext tasks</li>
<li>PIRL outperforms supervised pre-training in learning image representations for object detection.</li>
<li>adapt the “Jigsaw” pretext task to work with PIRL</li>
<li>PIRL even outperforms supervised pre-training in learning image representations suitable for object detection</li>
<li>PIRL can be viewed as extending the set of data augmentations to include prior pretext tasks and provides a new way to combine pretext tasks with contrastive learning.</li>
</ul>
<h3 id="how-13">How</h3>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516113317398.png" alt="image-20220516113310607" style="zoom: 80%;" /> <img src="C:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220516115120389.png" alt="image-20220516115120389" style="zoom: 80%;" /></p>
<ul>
<li><p>adopt the existing Jigsaw pretext task in a way that encourages the image representations to be invariant to the image patch perturbation</p>
<ul>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516114526058.png" alt="image-20220516114526058" /><figcaption aria-hidden="true">image-20220516114526058</figcaption>
</figure></li>
<li><p>Use NCE to control the similarity between the features of <span class="math inline">\(I\)</span> and the augmented patches of <span class="math inline">\(I_t\)</span>.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516121917957.png" alt="image-20220516121917957" /><figcaption aria-hidden="true">image-20220516121917957</figcaption>
</figure></li>
<li><p>apply different “heads” to the features before computing the score</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516121932767.png" alt="image-20220516121932767" /><figcaption aria-hidden="true">image-20220516121932767</figcaption>
</figure></li>
</ul></li>
<li><p>Use memory bank</p>
<ul>
<li>The representation <span class="math inline">\(m_I\)</span> in memory bank is an exponential moving average of feature representations <span class="math inline">\(f(v_I)\)</span> that were computed in prior epochs.</li>
</ul></li>
<li><p>Final loss</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220516122506492.png" title="fig:" alt="image-20220516122506492" /></li>
<li>The first term is the equation above, and the second term encourages the representation <span class="math inline">\(f(v_I)\)</span> to be similar to its memory representation so to control the parameters updating; and encourages the representations <span class="math inline">\(f(v_I),f(v_{I&#39;})\)</span> to be dissimilar.</li>
</ul></li>
<li><p>Backbone: ResNet50</p></li>
<li><p>Representations</p>
<ul>
<li><span class="math inline">\(f(v_I)\)</span> from <span class="math inline">\(I\)</span> is by extracting res5 features, average pooling and a linear projection to obtain a 128-dimensional representation.</li>
<li></li>
</ul></li>
</ul>
<h3 id="experiments-13">Experiments</h3>
<ul>
<li>Compare between PIRL (<span class="math inline">\(\lambda =0.5\)</span>): NPID (<span class="math inline">\(\lambda =0\)</span>) and NPID++ (with more negative samples, (<span class="math inline">\(\lambda =0\)</span>) by setting <span class="math inline">\(\lambda\)</span> in final loss.</li>
</ul>
<h4 id="tasks">Tasks</h4>
<ul>
<li>Object detection
<ul>
<li>benchmark: Pascal VOC</li>
<li>encoder: faster RCNN C4 object detection model implement in detectron 2 with a ResNet-50.</li>
<li>The success of PIRL underscore the importance of learning invariant (rather than covariant) image representations.</li>
<li>a self-supervised learner can outperform supervised pre-training for object detection.</li>
<li>PIRL learns image representations that are amenable to sample-efficient supervised learning.</li>
</ul></li>
<li>Image classification with linear models
<ul>
<li>PIRL outperforms all prior self-supervised learners on ImageNet in terms of the trade-off between model accuracy and size. Indeed, PIRL even outperforms most self-supervised learners that use much larger models</li>
<li>PIRL sets a new state-of-the-art for self-supervised representations in this learning setting on the VOC07, Places205, and iNaturalist datasets, while NPID++ performs well but is consistently outperformed by PIRL.</li>
</ul></li>
<li>Semi-supervised: finetuning the models on just 1% (∼13,000) labeled images leads to a top-5 accuracy of 57%.</li>
<li>If pretrained on another unlabeled dataset and then test on ImageNet, PIRL even outperforms Jigsaw and DeeperCluster models that were trained on 100× more data from the same distribution.</li>
<li>Other discussion
<ul>
<li>Does PIRL learn invariant representations? for PIRL, an image representation and the representation of a transformed version of that image are generally similar, measured by the histogram of distance of normalized features <span class="math inline">\(f(v_I), g(v_{I_t})\)</span>.</li>
<li>Which layer produces the best representations?
<ul>
<li>the quality of Jigsaw representations improves from the conv1 to the res4 layer but that their quality sharply decreases in the res5 layer==&gt; surmise this happens because the res5 representations in the last layer of the network covary with the image transformation t and are not encouraged to contain semantic information.</li>
<li>, the best image representations are extracted from the res5 layer of PIRL-trained networks</li>
</ul></li>
<li>learning invariance to Jigsaw is important for better representations</li>
<li>Loss functions
<ul>
<li>At λ= 1, the network does not compare untransformed images at training time and updates to the memory bank <span class="math inline">\(m_I\)</span> are not dampened</li>
<li>The performance of PIRL is sensitive to the setting of λ, and the best performance is obtained by setting λ= 0.5</li>
</ul></li>
<li>Effect of the number of image transforms
<ul>
<li>PIRL outperforms Jigsaw for all cardinalities of T . PIRL particularly benefits from being able to use very large numbers of image transformations (i.e., large |T |) during training</li>
</ul></li>
<li>Effect of the number of negative samples: increasing the number of negatives tends to have a positive influence on the quality of the image representations constructed by PIRL.</li>
</ul></li>
</ul>
<h2 id="paper-15-adversarially-learned-inference-ali-iclr-2017">Paper 15 <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1606.00704&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2xx-blDKLroqk6XrXTa7Tp">Adversarially Learned Inference</a> (ALI) ICLR 2017</h2>
<p>https://github.com/IshmaelBelghazi/ALI</p>
<h3 id="previous-14">Previous</h3>
<ul>
<li>three classes of algorithms for learning deep directed generative models, in practice they learn very different kinds of generative models on typical datasets.
<ul>
<li><strong>VAE</strong>: suffer from the issue of the maximum likelihood training paradigm when combined with a conditional independence assumption on the output given the latent variables==&gt; <strong>image samples from VAE-trained models tend to be blurry</strong></li>
<li>GANs: lack an efficient inference mechanism and thus cannot well reason about data at an abstract level.</li>
<li>autoregressive models: can produce good samples but in slow sampling speed and foregoing the learning of an abstract representation of the data</li>
</ul></li>
<li>Feedforward inference in GANs
<ul>
<li>InfoGAN: minimizes the mutual information between a subset <span class="math inline">\(c\)</span> of the latent code and <span class="math inline">\(x\)</span> through the use of an auxiliary distribution <span class="math inline">\(Q(c | x)\)</span>. But there are no inference of <span class="math inline">\(c\)</span> and requires <span class="math inline">\(Q(c | x)\)</span> to be tractable.</li>
<li>But the inverse mapping is not well learned. If decompose into two phases to train the encoder and decoder, the GAN’s decoder is frozen and an encoder is trained following the ALI procedure (i.e., a discriminator taking both x and z as input is introduced). We call this post-hoc learned inference.</li>
</ul></li>
</ul>
<h3 id="what-13">What</h3>
<ul>
<li>Introduce ALI (adversarially learned inference) jointly learns a generation network and an inference network using an adversarial process</li>
<li>Train the encoder (<span class="math inline">\(x\rightarrow z\)</span>) and decoder (<span class="math inline">\(z\rightarrow x\)</span>), these two are both the generator.</li>
<li>The discriminator is trained to distinguish between joint pairs <span class="math inline">\((x, \hat{z} = G_x(x))\)</span> and <span class="math inline">\((\tilde{x} = G_x(z), z)\)</span>, as opposed to marginal samples <span class="math inline">\(x ∼ q(x)\)</span> and <span class="math inline">\(\tilde{x} ∼ p(x)\)</span> in original GAN.</li>
<li></li>
</ul>
<h3 id="how-14">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517122848579.png" alt="image-20220517122848579" /><figcaption aria-hidden="true">image-20220517122848579</figcaption>
</figure>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517124109412.png" alt="image-20220517124109412" /><figcaption aria-hidden="true">image-20220517124109412</figcaption>
</figure>
<ul>
<li>For instance, if <span class="math inline">\(q(z | x) = \mathcal{N} (µ(x), σ^2 (x)I)\)</span>, one can draw samples by computing: <span class="math inline">\(z = µ(x) + σ(x) \bigodot \epsilon , \epsilon ∼ \mathcal{N} (0, I)\)</span>.</li>
<li>As a workaround, the generator is trained to maximize: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517131012759.png" alt="image-20220517131012759" /> which has the same fixed points but whose gradient is stronger when the discriminator’s output saturates.</li>
<li>ALI is not directly applicable to either applications with discrete data or to models with discrete latent variables.</li>
<li>The discriminator
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517132323518.png" title="fig:" alt="image-20220517132323518" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517132346424.png" title="fig:" alt="image-20220517132346424" /></li>
<li><img src="C:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220517133547079.png" title="fig:" alt="image-20220517133547079" /></li>
</ul></li>
</ul>
<h3 id="experiments-14">Experiments</h3>
<ul>
<li>Benchmarks: SVHN (Street View House Numbers), CIFAR-10, CelebA and a downsampled version of the ImageNet dataset.</li>
<li>Found reconstructions are not always faithful reproductions of the inputs. They retain the same crispness and quality characteristic to adversarially-trained models, but oftentimes make mistakes in capturing exact object placement, color, style and (in extreme cases) object identity</li>
<li>ALI is not concentrating its probability mass exclusively around training examples, but rather has learned latent features that generalize well</li>
<li>ALI did not require feature matching to obtain comparable results. Latent representation learned by ALI is better untangled with respect to the classification task and that it generalizes better.</li>
<li>The ALI encoder models a marginal distribution <span class="math inline">\(q(z)\)</span> that matches <span class="math inline">\(p(z)\)</span> fairly well (row 2, column a), while the GAN generator has more trouble reaching all the modes than the ALI generator.</li>
<li>Learning an inverse mapping from GAN samples does not work very well. reconstructions suffer from the generator dropping modes.</li>
<li>Learning inference post-hoc doesn’t work as well as training the encoder and the decoder jointly.</li>
<li>this experiment provides evidence that <strong>adversarial training benefits from learning an inference mechanism jointly with the decoder</strong>.</li>
</ul>
<h2 id="paper-16-adversarial-feature-learning-bigan-concurrent-and-similar-to-ali-iclr-2017">Paper 16: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1605.09782&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Ea7cOor2n6pmoJ44576vf">Adversarial Feature Learning</a> (BiGAN, concurrent and similar to ALI) ICLR 2017</h2>
<h3 id="previous-15">Previous</h3>
<ul>
<li>Way to learn inverse mapping
<ul>
<li>directly model <span class="math inline">\(p(z|G(z))\)</span>, predicting generator input <span class="math inline">\(z\)</span> given generated data <span class="math inline">\(G(z)\)</span>.</li>
<li>The BiGAN that they proposed.</li>
</ul></li>
<li>GAN model the data distribution as a transformation of a fixed latent distribution <span class="math inline">\(p_Z(z)\)</span> for <span class="math inline">\(z \in Ω_Z\)</span>, and the transformation is named as the generator.
<ul>
<li>The ideal discriminator of vanilla GAN is equivalent to the Jensen-Shannon divergence between the two distributions <span class="math inline">\(p_G, p_x\)</span>.</li>
</ul></li>
</ul>
<h3 id="what-14">What</h3>
<ul>
<li>learn the inverse mapping – projecting data back into the latent space.</li>
<li>propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping</li>
</ul>
<h3 id="how-15">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517151001418.png" alt="image-20220517151001418" /><figcaption aria-hidden="true">image-20220517151001418</figcaption>
</figure>
<ul>
<li><p>Include the generator G and the encoder E.</p>
<ul>
<li>a trained BiGAN encoder may serve as a useful feature representation for related semantic tasks, similar as what's obtained in the SSL.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517161725969.png" title="fig:" alt="image-20220517161725969" /></li>
</ul></li>
<li><p>The optimal discriminator, generator and decoder.</p>
<ul>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517153843819.png" alt="image-20220517153843819" /><figcaption aria-hidden="true">image-20220517153843819</figcaption>
</figure></li>
<li><p>With the optimal discriminator, then <img src="C:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220517162126472.png" alt="image-20220517162126472" /></p></li>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517162948011.png" alt="image-20220517162948011" /><figcaption aria-hidden="true">image-20220517162948011</figcaption>
</figure></li>
<li><p>The optimal generator &amp; encoder are inverses: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220517163454672.png" alt="image-20220517163454672" /></p>
<p>But these two are rarely exact inverses,while they two are approximate inverses.</p></li>
</ul></li>
<li><p>BiGAN is closely related to AEs with an <span class="math inline">\(\ell_0\)</span> loss.</p>
<ul>
<li><img src="C:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220517164044255.png" title="fig:" alt="image-20220517164044255" /></li>
<li><span class="math inline">\(1_{[G(E(x))=x]}\)</span> in the first term is equivalent to an autoencoder with <span class="math inline">\(\ell_0\)</span> loss, while the indicator <span class="math inline">\(1_{[G(E(z))=z]}\)</span> in the second term shows that the BiGAN encoder must invert the generator.</li>
<li>Also the <span class="math inline">\(\ell_0\)</span> loss does not make any assumptions about the structure or distribution of the data itself.</li>
</ul></li>
<li><p>BiGAN optimizes a Jensen-Shannon divergence between a joint distribution over both data <span class="math inline">\(X\)</span> and latent features <span class="math inline">\(Z\)</span>. This joint divergence allows us to further characterize properties of <span class="math inline">\(G\)</span> and <span class="math inline">\(E\)</span>, as shown below.</p></li>
<li><p>Learning process:</p>
<ul>
<li>first the D is updated and then E and G are updated together.</li>
<li>BiGAN training that an <strong>“inverse” objective provides stronger gradient signal to G and E</strong> (the real and generated labels Y are swapped).</li>
<li>update all modules D, G, and E simultaneously at each iteration, rather than alternating between D updates and G, E updates</li>
</ul></li>
<li><p>If <span class="math inline">\(G\)</span> successfully generates the true data distribution <span class="math inline">\(p_X(x)\)</span>, D may ignore the input data entirely and predict <span class="math inline">\(P(Y = 1) = P(Y = 1|x) = \frac{1}{2}\)</span> unconditionally, not learning any meaningful intermediate representations.</p></li>
</ul>
<h3 id="experiments-15">Experiments</h3>
<ul>
<li>Evaluate on n both permutation-invariant MNIST and on the high-resolution natural images of ImageNet.</li>
<li>in the all column, the entire network is “fine-tuned”</li>
<li>Encoder E follows AlexNet, and D also.
<ul>
<li>Found filters learned by the encoder E have clear Gabor-like structure, similar to those originally reported for the fully supervised AlexNet model</li>
</ul></li>
</ul>
<h2 id="paper-17-large-scale-adversarial-representation-learning-big-bigan-arxiv-2019">Paper 17: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1907.02544&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3bxhaVdSwniyMV3EFLcUrl">Large Scale Adversarial Representation Learning</a> (Big BiGAN) ArXiv 2019</h2>
<p>https://tfhub.dev/s?publisher=deepmind&amp;q=bigbigan</p>
<p>https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/bigbigan_with_tf_hub.ipynb</p>
<h3 id="previous-16">Previous</h3>
<ul>
<li>BiGAN behaves like an autoencoder minimizing <span class="math inline">\(\ell_0\)</span> reconstruction costs; however, the shape of the reconstruction error surface is dictated by a parametric discriminator, as opposed to simple pixel-level measures like the <span class="math inline">\(\ell_2\)</span> error.==&gt; it will induce an error surface which emphasizes “semantic” errors in reconstructions, rather than low-level details.</li>
<li>BiGAN used a DCGAN style generator, incapable of producing high-quality images on this dataset, so the semantics the encoder could model were in turn quite limited.</li>
</ul>
<h3 id="what-15">What</h3>
<ul>
<li>Based on BiGAN, adding an encoder and modifying the discriminator</li>
<li>demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation</li>
<li>The contributions
<ul>
<li>We show that BigBiGAN (BiGAN with BigGAN generator) matches the state of the art in unsupervised representation learning on ImageNet.</li>
<li>We propose a more stable version of the joint discriminator for BigBiGAN.</li>
<li>We perform a thorough empirical analysis and ablation study of model design choices.</li>
<li>We show that the representation learning objective also improves unconditional image generation, and demonstrate state-of-the-art results in unconditional ImageNet generation.</li>
<li>We open source pretrained BigBiGAN models on TensorFlow Hub.</li>
</ul></li>
</ul>
<h3 id="how-16">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220518155552625.png" alt="image-20220518155552625" /><figcaption aria-hidden="true">image-20220518155552625</figcaption>
</figure>
<ul>
<li>Modifications of BiGAN:
<ul>
<li>a joint discriminator D, which takes as input data-latent pairs <span class="math inline">\((x, z)\)</span> (rather than just data <span class="math inline">\(x\)</span> as in a standard GAN)</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220518161528661.png" alt="image-20220518161528661" /> where <span class="math inline">\(h(t)=\max (0,1-t)\)</span> is a hinge used to regularize the discriminator.</li>
<li>F is a convNet, H is an MLP, and J is the function of the outputs of F and H.</li>
<li>The discriminator loss <span class="math inline">\(\mathcal{L}_D\)</span> intuitively trains the discriminator to distinguish between the two joint data-latent distributions from the encoder and the generator, pushing it to predict positive values for encoder input pairs <span class="math inline">\((x, E(x))\)</span> and negative values for generator input pairs <span class="math inline">\((G(z), z)\)</span>.</li>
</ul></li>
<li>train a BigBiGAN on unlabeled ImageNet, freeze its learned representation, and then train a linear classifier on its outputs, fully supervised using all of the training set labels</li>
</ul>
<h3 id="experiments-16">Experiments</h3>
<ul>
<li>components of D – H and J – are 8-layer MLPs with ResNet-style skip connections (four residual blocks with two layers each) and size 2048 hidden layers</li>
<li>Ablation
<ul>
<li>latent distribution <span class="math inline">\(P_z\)</span> and stochastic <span class="math inline">\(\mathcal{E}\)</span>: the non-deterministic <span class="math inline">\(\mathcal{E}\)</span> model achieves significantly better classification performance (at no cost to generation)</li>
<li>Unary loss terms <span class="math inline">\(s_x,s_z\)</span>: The requirement of invertibility in a (Big)BiGAN could be encouraging the generator to produce distinguishable outputs across the entire latent space, rather than “collapsing” large volumes of latent space to a single mode of the data distribution. With the Base and x Unary Only rows having significantly better IS and FID than the z Unary Only and No Unaries rows.</li>
<li>G capacity: vary the capacity of G (with E and D fixed) in the Small G rows. A powerful image generator (with enough capacity, aka deep and wide) is indeed important for learning good representations via the encoder.</li>
<li>Standard GAN: to estimate the necessary of using the encoder <span class="math inline">\(\mathcal{E}\)</span> and <span class="math inline">\(F\)</span> convnet together. While the standard GAN achieves a marginally better IS, the BigBiGAN FID is about the same, indicating that the addition of the BigBiGAN <span class="math inline">\(\mathcal{E}\)</span> and joint <span class="math inline">\(D\)</span> does not compromise generation with the newly proposed unary loss terms. <span class="math inline">\(s_z\)</span> unary loss term leads to worse IS</li>
<li>High resolution <span class="math inline">\(\mathcal{E}\)</span> with varying resolution <span class="math inline">\(G\)</span>: as raising the resolution of <span class="math inline">\(G\)</span> to match the increased <span class="math inline">\(\mathcal{E}\)</span> input resolution, BigBiGAN achieves better representation learning results.</li>
<li><span class="math inline">\(\mathcal{E}\)</span> architecture: We find that the base ResNet-50 model (row High Res E (256)) outperforms RevNet-50 (row RevNet), but as the network widths are expanded, we begin to see improvements from RevNet-50, with double-width RevNet outperforming a ResNet of the same capacity (rows RevNet ×2 and ResNet ×2).</li>
<li>Decoupled E/G optimization: <strong>simply using a 10× higher learning rate for E dramatically accelerates training and improves final representation learning results.</strong></li>
</ul></li>
<li>Comparison with SOTA: models are selected with early stopping based on highest accuracy on our <span class="math inline">\(train_{val}\)</span> subset of 10K training set images
<ul>
<li>representation learning: The lighter augmentation from [27] results in better image generation performance under the IS and FID metrics. Due in part to the fact that this augmentation, on average, crops larger portions of the image, thus yielding generators that typically produce images encompassing most or all of a given object.</li>
<li>BigBiGAN significantly improves both IS and FID over the baseline unconditional BigGAN generation results with the same (unsupervised) “labels”</li>
<li>We see further improvements using a high resolution E (row BigBiGAN High Res E + SL), surpassing the previous unsupervised state of the art (row BigGAN + Clustering) under both IS and FID</li>
</ul></li>
<li>Reconstruction: These reconstructions are far from pixel-perfect, likely due in part to the fact that no reconstruction cost is explicitly enforced by the objective – reconstructions are not even computed at training time.</li>
<li>Found it helpful to monitor representation learning progress during BigBiGAN training by periodically rerunning this linear classification evaluation from scratch given the current E weights, resetting the classifier weights to 0 before each evaluation</li>
<li>Examining the convolutional filters of the input layer can serve as a diagnostic for undertrained models.</li>
</ul>
<h2 id="paper-18-generative-pretraining-from-pixels-igpt-icml-2020">Paper 18: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fimage-gpt%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2hhRvO-SL2S08MmJ1VjBfw">Generative Pretraining from Pixels</a> (iGPT) ICML 2020</h2>
<h3 id="previous-17">Previous</h3>
<ul>
<li>the state of the art increasingly relied on directly encoding prior structure into the model and utilizing abundant supervised data to directly learn representations</li>
<li>inspired by the success of generative pre-training methods developed for Natural Language Processing</li>
</ul>
<h3 id="what-16">What</h3>
<ul>
<li>examine whether similar models can learn useful representations for images</li>
<li>train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure</li>
<li>Their encoder does not encode the 2D spatial structure of images</li>
</ul>
<h3 id="how-17">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220519130518491.png" alt="image-20220519130518491" /><figcaption aria-hidden="true">image-20220519130518491</figcaption>
</figure>
<ul>
<li><p>Pre-training can be viewed as a favorable initialization or as a regularizer when used in combination with early stopping</p></li>
<li><p>consists of a pre-training stage followed by a fine-tuning stage.</p>
<ul>
<li>Pretraining: BERT+auto regressive , use the sequence transformer to predict pixels</li>
<li>Fine-tuning: small classification head, or using the pretrained model as a feature extractor.</li>
</ul></li>
<li><p>Pre-training</p>
<ul>
<li><p>Auto-regressive</p>
<p>pick a permutation π of the set [1, n] and model the density $p(x) $ auto-regressively as follows:<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220519134302563.png" alt="image-20220519134302563" style="zoom:67%;" />, <span class="math inline">\(L_{AR}=\mathbb{E}_{x\sim X}[-\log p(x)]\)</span>.</p></li>
<li><p>BERT: sample a sub-sequence M ⊂ <span class="math inline">\([1, n]\)</span> such that each index <span class="math inline">\(i\)</span> independently has probability <span class="math inline">\(0.15\)</span> of appearing in <span class="math inline">\(M\)</span> (BERT mask). The minimize the negative log-likelihood of the masked elements <span class="math inline">\(x_M\)</span> conditioned on the unmasked ones <span class="math inline">\(x_{[1,n]\backslash M}\)</span>: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220519134915317.png" alt="image-20220519134915317" /></p></li>
<li><p>In pretraining, pick one of <span class="math inline">\(L_{AR}\)</span> or <span class="math inline">\(L_{BERT}\)</span> and minimize the loss over our pretraining dataset.</p></li>
</ul></li>
<li><p>Architecture: the transformer head</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220519145227280.png" alt="image-20220519145227280" style="zoom:50%;" /></li>
<li>The only mixing across sequence elements occurs in the attention operation, and to ensure proper conditioning when training the AR objective, we apply the standard upper triangular mask to the n×n matrix of attention logits.</li>
<li>In the final transformer layer, apply a layer norm <span class="math inline">\(n^L = layer\_norm (h^L)\)</span></li>
<li>learn a projection from <span class="math inline">\(n^L\)</span> to logits parameterizing the conditional distributions at each sequence element</li>
<li>When training BERT, we simply ignore the logits at unmasked positions</li>
</ul></li>
<li><p>Fine-tuning</p>
<ul>
<li>average pool <span class="math inline">\(n^L\)</span> across the sequence dimension to extract a <span class="math inline">\(d\)</span>-dimensional vector of features per example.</li>
<li>Then learn a projection from the features to class logits, and used for minimizing a cross-entropy loss.</li>
<li>They found empirically that the joint objective <span class="math inline">\(L_{GEN}+L_{CLF}\)</span> works even better.</li>
</ul></li>
<li><p>Linear probing</p>
<ul>
<li>The average pooling is not always at the final layer.</li>
<li>In the experiments section that the best features often lie in the <strong>middle</strong> of the network.</li>
</ul></li>
<li><p>Final method</p>
<ul>
<li>using ImageNet as a proxy for a large unlabeled corpus, and small classic labeled datasets (CIFAR-10, CIFAR-100, STL-10) as proxies for downstream tasks</li>
<li>Rescale the image to decrease the context length
<ul>
<li>we first resize our image to a lower resolution, called as input resolution .</li>
<li>Further reduce context size by a factor of 3 by clustering (R, G, B) pixel values using k-means with k = 512. This breaks permutation invariance of the color channels, but keeps the model spatially invariant. Now the length is called as model resolution (MR).</li>
<li>Use VQ-VAE with a latent grid size to downsample images and stay at a MR.</li>
<li>we initialize weights in the layer-dependent fashion as in Sparse Transformer (Child et al., 2019) and zero-initialize all projections producing logits</li>
</ul></li>
<li>not employ a cosine schedule, and early stop once we reach the maximum validation accuracy. Again, no dropout is used.</li>
<li>While doing linear probe, start with a large learning rate (30) and train for 1000000 iterations with a cosine learning rate schedule</li>
</ul></li>
</ul>
<h3 id="experiments-17">Experiments</h3>
<ul>
<li>What Representation Works Best in a Generative Model Without Latent Variables: the best representations for these generative models lie in the middle of the network</li>
<li>Better generative models learn better representations: with higher capacity models achieving better validation losses.</li>
<li>fine-tuning should allow models trained at high IR to adjust to low resolution input.</li>
<li>Linear Probes on ImageNet
<ul>
<li>If using the concatenation from 11 layers centered at the best single layer, iGPT has a competitive performance</li>
<li>And if using larger IR, with VQ-VAE to preprocess the input, model finally get the best classification by the concatenation from 11 layers centered at the best single layer.</li>
<li>We also suspect that features from wider models will outperform concatenated layerwise features, <em>which tend to be correlated in residual networks</em></li>
</ul></li>
<li>Full fine-tuning: the pre-trained model is much quicker to fine-tune. the optimal learning rate on the joint training objective is often an order of magnitude smaller than that for pre-training</li>
<li>BERT: auto-regressive models produce much better features than BERT models after pre-training, but BERT models catch up after fine-tuning.</li>
<li>Although we have established that large models are necessary for producing good representations, large models are also difficult to fine-tune in the ultra-low data regime</li>
<li>Training skills
<ul>
<li>As we increase model size, the irrecoverable loss spike occurs at even lower learning rates. This motivates our procedure of sequentially searching learning rates from large to small and explains why <strong>larger models use lower learning rates than smaller models at fixed input resolution</strong>.</li>
<li>training an auto-regressive objective gives us this capability to generate high quality samples. L1 loss tends to produce slightly more diffuse images.</li>
</ul></li>
</ul>
<h2 id="paper-19-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.-neurips-2017">Paper 19: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2-LuidEMZJI7fUiE-oWqoa"><em>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</em></a>. NeurIPS 2017</h2>
<h3 id="previous-18">Previous</h3>
<ul>
<li>Temporal Ensembling: It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target==&gt; but targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets.</li>
<li>It is desirable to use regularization methods that exploit unlabeled data effectively to reduce over-fitting in semi-supervised learning.</li>
<li>A classification model should favor functions that give consistent output for similar data points.
<ul>
<li>One way is to add noise to the input of the model, such as Dropout.==&gt; the regularized model minimizes the cost on a manifold around each data point.</li>
<li>the noise regularization by itself does not aid in semi-supervised learning&lt;== because the classification cost is undefined for unlabeled examples</li>
</ul></li>
<li>To apply adding noise on SSL
<ul>
<li>model [20] evaluates each data point with and without noise, and then applies a consistency cost between the two predictions. ==&gt; A teacher and a student. The student leans as before while the teacher generates targets.</li>
<li>If too much weight is given to the generated targets, the cost of inconsistency outweighs that of misclassification, preventing the learning of new infomation.</li>
</ul></li>
<li>Two ways to improve the target quality
<ul>
<li>Choose the perturbation of the representations carefully instead of barely applying additive or multiplicative noise.</li>
<li>Choose the teacher model carefully instead of barely replicating the student model.==&gt; the way that this paper uses</li>
<li>The two methods are above and can be combined together.</li>
</ul></li>
<li>Consistency regularization can be seen as a form of label propagation. On the one hand consistency targets spread the labels according to the current distance metric, and on the other hand, they aid the network learn a better distance metric.</li>
</ul>
<h3 id="what-17">What</h3>
<ul>
<li>Goal: <strong>form a better teacher model from the student model without additional training</strong></li>
<li>averages model weights instead of label predictions.</li>
<li>Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling.
<ul>
<li>a noisy teacher by adding noise to predicted targets can yield more accurate targets</li>
<li>the EMA prediction of each example is formed by an ensemble of the model’s current version and those earlier versions that evaluated the same example. ==&gt; using them as the teacher predictions improves results.</li>
</ul></li>
<li>a good network architecture is crucial to performance</li>
</ul>
<h3 id="how-18">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220520122900354.png" alt="image-20220520122900354" /><figcaption aria-hidden="true">image-20220520122900354</figcaption>
</figure>
<ul>
<li>Instead of sharing the weights with the student model, the teacher model uses the EMA weights of the student model after every step (rather than every epoch)
<ul>
<li>Lead to weights improvement over all layers and thus the target model has better intermediate representations
<ul>
<li>The more accurate target labels lead to a faster feedback loop between the student and the teacher models, resulting in better test accuracy</li>
<li>The approach scales to large datasets and online learning</li>
</ul></li>
</ul></li>
<li>The loss is defined as the consistency cost between the teacher and the student:
<ul>
<li><img src="C:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220520124431616.png" alt="image-20220520124431616" />, where the former term is the teacher's and the latter one is the student's.</li>
<li>With EMA on weights: <span class="math inline">\(\theta&#39;_t=\alpha \theta&#39;_{t-1}+(1-\alpha) \theta_t\)</span>. For temporal ensembling and mean teacher, the <span class="math inline">\(\theta&#39;\)</span> is treated as a constant with regards to optimization.</li>
<li>approximate the consistency cost function <span class="math inline">\(J\)</span> by sampling noise <span class="math inline">\(\eta,\eta&#39;\)</span> at each training step with stochastic gradient descent.</li>
<li>use mean squared error (MSE) as the consistency cost in most of our experiments</li>
</ul></li>
</ul>
<h3 id="experiments-18">Experiments</h3>
<ul>
<li>Backbone: a 13-layer ConvNet.</li>
<li>Augmentations/noise: random translations, horizontal flips of the input images, gaussian noise on the input layer and dropout within the network</li>
<li>Benchmarks: SVNH, CIFAR-10</li>
<li>Mean Teacher improves test accuracy over the <span class="math inline">\(\prod\)</span> model and Temporal Ensembling on semi-supervised SVHN tasks. Mean Teacher also improves results on CIFAR-10 over our baseline <span class="math inline">\(\prod\)</span> model.</li>
<li>the EMA-weighted models give more accurate predictions than the bare student models after an initial period.
<ul>
<li>Mean Teacher helps when labels are scarce.</li>
<li>But in the all-labeled case (left column), Mean Teacher and the<span class="math inline">\(\prod\)</span> model behave virtually identically.</li>
</ul></li>
<li>Either input augmentation or dropout is necessary for passable performance of our model. And input noise does not help when augmentation is in use. Dropout on the teacher side provides only a marginal benefit over just having it on the student side, at least when input augmentation is in use.</li>
<li>Sensitivity to EMA decay and consistency weight.
<ul>
<li>In each case the good values span roughly an order of magnitude and outside these ranges the performance degrades quickly.</li>
<li>In the evaluation runs we used EMA decay <span class="math inline">\(\alpha\)</span> = 0.99 during the ramp-up phase, and <span class="math inline">\(\alpha\)</span> = 0.999 for the rest of the training. We chose this strategy because the student improves quickly early in the training, and thus the teacher should forget the old, inaccurate, student weights quickly. Later the student improvement slows, and the teacher benefits from a longer memory.</li>
</ul></li>
<li>Decoupling classification and consistency: The consistency to teacher predictions may not necessarily be a good proxy for the classification task, especially early in the training.
<ul>
<li>To investigate, we changed the model to have two top layers and produce two outputs. . We then trained one of the outputs for classification and the other for consistency</li>
<li>a moderate decoupling seems to have the benefit of making the consistency ramp-up redundant.</li>
</ul></li>
<li>Changing from MSE to KL-divergence: in this setting MSE performs better than the other cost functions.</li>
<li>the results improve remarkably with the better network architecture. (ResNet)</li>
</ul>
<h2 id="paper-20-bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning-byol.-arxiv-2020">Paper 20: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.07733&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2igxNmTekOxWbWzL9y5IyY"><em>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</em></a> (BYoL). ArXiv 2020</h2>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">paper</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">SSL</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/notes//" class="article-tag-list-link color1">notes</a>
        		</li>
      		
		</ul>
	</div>


      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
  
    <a href="/posts/uncategorized/2021-11-20-tmp-cat.html" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title"></div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-1-stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion"><span class="toc-number">1.</span> <span class="toc-text">Paper 1: Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous"><span class="toc-number">1.1.</span> <span class="toc-text">Previous</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#traditional-classifiers-training-with-noisy-inputs"><span class="toc-number">1.1.1.</span> <span class="toc-text">Traditional classifiers training with noisy inputs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pseudo-likelihood-and-dependency-networks"><span class="toc-number">1.1.2.</span> <span class="toc-text">Pseudo-Likelihood and Dependency Networks</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ideas"><span class="toc-number">1.2.</span> <span class="toc-text">Ideas</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#reasoning-what-makes-good-representations"><span class="toc-number">1.2.1.</span> <span class="toc-text">Reasoning: what makes good representations</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how"><span class="toc-number">1.3.</span> <span class="toc-text">How?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments"><span class="toc-number">1.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-2-revisiting-self-supervised-visual-representation-learning"><span class="toc-number">2.</span> <span class="toc-text">Paper 2: Revisiting Self-Supervised Visual Representation Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-1"><span class="toc-number">2.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what"><span class="toc-number">2.2.</span> <span class="toc-text">What?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-1"><span class="toc-number">2.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-1"><span class="toc-number">2.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-3-zhai-et-al-2019-a-large-scale-study-of-representation-learning-with-the-visual-task-adaptation-benchmark-a-glue-like-benchmark-for-images"><span class="toc-number">3.</span> <span class="toc-text">Paper 3: Zhai et al (2019) A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark (A GLUE-like benchmark for images)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-2"><span class="toc-number">3.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-1"><span class="toc-number">3.2.</span> <span class="toc-text">What？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-2"><span class="toc-number">3.3.</span> <span class="toc-text">How？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-2"><span class="toc-number">3.4.</span> <span class="toc-text">Experiments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#discussion"><span class="toc-number">3.5.</span> <span class="toc-text">Discussion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-4-a-critical-analysis-of-self-supervision-or-what-we-can-learn-from-a-single-image"><span class="toc-number">4.</span> <span class="toc-text">Paper 4: A critical analysis of self-supervision, or what we can learn from a single image</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-3"><span class="toc-number">4.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-2"><span class="toc-number">4.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-3"><span class="toc-number">4.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-3"><span class="toc-number">4.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-5-representation-learning-with-contrastive-predictive-coding-cpc-arxiv-2018"><span class="toc-number">5.</span> <span class="toc-text">Paper 5: Representation Learning with Contrastive Predictive Coding (CPC), ArXiv 2018</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-4"><span class="toc-number">5.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-3"><span class="toc-number">5.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-4"><span class="toc-number">5.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-4"><span class="toc-number">5.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-6-learning-deep-representations-by-mutual-information-estimation-and-maximization-dim-deep-infomax-iclr-2019"><span class="toc-number">6.</span> <span class="toc-text">Paper 6: Learning deep representations by mutual information estimation and maximization (DIM Deep InfoMax ) ICLR 2019</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-5"><span class="toc-number">6.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-4"><span class="toc-number">6.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-5"><span class="toc-number">6.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mutual-information-estimation-and-maximization"><span class="toc-number">6.3.1.</span> <span class="toc-text">Mutual information estimation and maximization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#local-mutual-information-maximization"><span class="toc-number">6.3.2.</span> <span class="toc-text">Local mutual information maximization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#matching-representation-to-a-prior-distribution"><span class="toc-number">6.3.3.</span> <span class="toc-text">Matching representation to a prior distribution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#complete-loss"><span class="toc-number">6.3.4.</span> <span class="toc-text">Complete loss</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-5"><span class="toc-number">6.4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#evaluate-the-quality-of-a-representation"><span class="toc-number">6.4.1.</span> <span class="toc-text">Evaluate the quality of a representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#representation-learning-comparison-across-models"><span class="toc-number">6.4.2.</span> <span class="toc-text">Representation learning comparison across models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#appendix"><span class="toc-number">6.5.</span> <span class="toc-text">Appendix</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-7-contrastive-multiview-coding-cmc-arxiv-2019"><span class="toc-number">7.</span> <span class="toc-text">Paper 7: Contrastive Multiview Coding (CMC) ArXiv 2019</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-6"><span class="toc-number">7.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-5"><span class="toc-number">7.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-6"><span class="toc-number">7.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-6"><span class="toc-number">7.4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#benchmarking-cmc-on-imagenet"><span class="toc-number">7.4.1.</span> <span class="toc-text">Benchmarking CMC on ImageNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cmc-on-videos"><span class="toc-number">7.4.2.</span> <span class="toc-text">CMC on videos</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#extending-cmc-to-more-views"><span class="toc-number">7.4.3.</span> <span class="toc-text">Extending CMC to more views</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#predictive-learning-vs.-contrastive-learning"><span class="toc-number">7.4.4.</span> <span class="toc-text">Predictive Learning vs. Contrastive Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#how-does-mutual-information-affect-representation-quality"><span class="toc-number">7.4.5.</span> <span class="toc-text">How does mutual information affect representation quality?</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-8-data-efficient-image-recognition-with-contrastive-predictive-coding-cpc-v2-improved-cpc-evaluated-on-limited-labelled-data"><span class="toc-number">8.</span> <span class="toc-text">Paper 8: Data-Efficient Image Recognition with Contrastive Predictive Coding (CPC v2: Improved CPC evaluated on limited labelled data)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-7"><span class="toc-number">8.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-6"><span class="toc-number">8.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-7"><span class="toc-number">8.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-7"><span class="toc-number">8.4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#from-cpc-v1-to-cpc-v2"><span class="toc-number">8.4.1.</span> <span class="toc-text">From CPC v1 to CPC v2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#efficient-image-classification"><span class="toc-number">8.4.2.</span> <span class="toc-text">Efficient image classification</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transfer-learning-image-detection-on-pascal-voc-2007"><span class="toc-number">8.4.3.</span> <span class="toc-text">Transfer learning: image detection on PASCAL VOC 2007</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#conclusion"><span class="toc-number">8.5.</span> <span class="toc-text">Conclusion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-9-momentum-contrast-for-unsupervised-visual-representation-learning-moco-see-also-moco-v2"><span class="toc-number">9.</span> <span class="toc-text">Paper 9: Momentum Contrast for Unsupervised Visual Representation Learning (MoCo, see also MoCo v2)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-8"><span class="toc-number">9.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-7"><span class="toc-number">9.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-8"><span class="toc-number">9.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-8"><span class="toc-number">9.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-10-a-simple-framework-for-contrastive-learning-of-visual-representations-simclr.-icml-2020"><span class="toc-number">10.</span> <span class="toc-text">Paper 10: A Simple Framework for Contrastive Learning of Visual Representations (SimCLR). ICML 2020</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-9"><span class="toc-number">10.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-8"><span class="toc-number">10.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-9"><span class="toc-number">10.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-9"><span class="toc-number">10.4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#data-augmentation"><span class="toc-number">10.4.1.</span> <span class="toc-text">Data augmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#architectures-for-encoder-and-head"><span class="toc-number">10.4.2.</span> <span class="toc-text">Architectures for encoder and head</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#loss-functions-and-batch-size"><span class="toc-number">10.4.3.</span> <span class="toc-text">Loss functions and batch size</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#learning-rate-and-projection-matrix"><span class="toc-number">10.4.4.</span> <span class="toc-text">Learning rate and projection matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-way-to-train-downstream-classifier"><span class="toc-number">10.4.5.</span> <span class="toc-text">The way to train downstream classifier</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-best-temperature-parameter"><span class="toc-number">10.4.6.</span> <span class="toc-text">The best temperature parameter</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#compared-with-sota"><span class="toc-number">10.4.7.</span> <span class="toc-text">Compared with SOTA</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-11-big-self-supervised-models-are-strong-semi-supervised-learners-simclrv2-arxiv-2020"><span class="toc-number">11.</span> <span class="toc-text">Paper 11: Big Self-Supervised Models are Strong Semi-Supervised Learners (SimCLRv2) ArXiv 2020</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-10"><span class="toc-number">11.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-9"><span class="toc-number">11.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-10"><span class="toc-number">11.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-10"><span class="toc-number">11.4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#bigger-models-are-more-label-efficient"><span class="toc-number">11.4.1.</span> <span class="toc-text">Bigger models are more label-efficient</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#biggerdeeper-projection-heads-improve-representation-learning"><span class="toc-number">11.4.2.</span> <span class="toc-text">Bigger&#x2F;deeper projection heads improve representation learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#distillation-using-unlabeled-data-improves-semi-supervised-learning"><span class="toc-number">11.4.3.</span> <span class="toc-text">Distillation using unlabeled data improves semi-supervised learning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-12-unsupervised-learning-of-visual-features-by-contrasting-cluster-assignments-swav-arxiv-2020"><span class="toc-number">12.</span> <span class="toc-text">Paper 12: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (SwAV) ArXiv 2020</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-11"><span class="toc-number">12.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-10"><span class="toc-number">12.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-11"><span class="toc-number">12.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-11"><span class="toc-number">12.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-13-what-should-not-be-contrastive-in-contrastive-learning-arxiv-2020"><span class="toc-number">13.</span> <span class="toc-text">Paper 13: What Should Not Be Contrastive in Contrastive Learning ArXiv 2020</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-12"><span class="toc-number">13.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-11"><span class="toc-number">13.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-12"><span class="toc-number">13.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-12"><span class="toc-number">13.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-14-self-supervised-learning-of-pretext-invariant-representations.-cvpr-2020"><span class="toc-number">14.</span> <span class="toc-text">Paper 14: Self-Supervised Learning of Pretext-Invariant Representations. CVPR 2020</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-13"><span class="toc-number">14.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-12"><span class="toc-number">14.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-13"><span class="toc-number">14.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-13"><span class="toc-number">14.4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tasks"><span class="toc-number">14.4.1.</span> <span class="toc-text">Tasks</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-15-adversarially-learned-inference-ali-iclr-2017"><span class="toc-number">15.</span> <span class="toc-text">Paper 15 Adversarially Learned Inference (ALI) ICLR 2017</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-14"><span class="toc-number">15.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-13"><span class="toc-number">15.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-14"><span class="toc-number">15.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-14"><span class="toc-number">15.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-16-adversarial-feature-learning-bigan-concurrent-and-similar-to-ali-iclr-2017"><span class="toc-number">16.</span> <span class="toc-text">Paper 16: Adversarial Feature Learning (BiGAN, concurrent and similar to ALI) ICLR 2017</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-15"><span class="toc-number">16.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-14"><span class="toc-number">16.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-15"><span class="toc-number">16.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-15"><span class="toc-number">16.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-17-large-scale-adversarial-representation-learning-big-bigan-arxiv-2019"><span class="toc-number">17.</span> <span class="toc-text">Paper 17: Large Scale Adversarial Representation Learning (Big BiGAN) ArXiv 2019</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-16"><span class="toc-number">17.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-15"><span class="toc-number">17.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-16"><span class="toc-number">17.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-16"><span class="toc-number">17.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-18-generative-pretraining-from-pixels-igpt-icml-2020"><span class="toc-number">18.</span> <span class="toc-text">Paper 18: Generative Pretraining from Pixels (iGPT) ICML 2020</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-17"><span class="toc-number">18.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-16"><span class="toc-number">18.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-17"><span class="toc-number">18.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-17"><span class="toc-number">18.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-19-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.-neurips-2017"><span class="toc-number">19.</span> <span class="toc-text">Paper 19: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. NeurIPS 2017</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-18"><span class="toc-number">19.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-17"><span class="toc-number">19.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-18"><span class="toc-number">19.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-18"><span class="toc-number">19.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-20-bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning-byol.-arxiv-2020"><span class="toc-number">20.</span> <span class="toc-text">Paper 20: Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning (BYoL). ArXiv 2020</span></a></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>



  
  
  

  

  

  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2022 Mia Feng
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


</footer>

    </div>
    <script>
	var yiliaConfig = {
		mathjax: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">posts</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">papers</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">about</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">notes</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Sensor</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">GPR</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">summer school</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">mathematics</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">cnn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">compress</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">infrared</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">hyperspectral</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">blog</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">logistic</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">applications</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">tendency</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">ML</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">naive</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">da</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">data assimilation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">machine learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">book</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">gcn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">math</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">paper</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SSL</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">survey</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">anomaly</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">skeleton</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">cv</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">3d shape</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">fall</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">remote sensing images</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">interpolation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">compression</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">reconstruction</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">AI</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Data assimilation</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>wind interpolation</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">&lt;br&gt; &amp;#160; &amp;#160; &amp;#160; &amp;#160;Mia, &lt;/br&gt;&lt;br&gt; &amp;#160; &amp;#160; &amp;#160; &amp;#160;a master student in NUDT.&lt;/br&gt; &lt;br&gt;  &amp;#160; &amp;#160; &amp;#160; &amp;#160;I love reading, exercising and cooking. &lt;br /&gt; &lt;br&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;Science fictions, detective novels are my favorite.&lt;/br&gt;</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>