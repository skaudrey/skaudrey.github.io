<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <link rel="dns-prefetch" href="//cdn.bootcss.com" />
  <link rel="dns-prefetch" href="//cdn.mathjax.org" />
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>Important  Papers of SSL. | Mia&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper 1: Vincent et al (2008) Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. JMLR 2008 Analysis of SSL methods  Paper 2: Koles">
<meta property="og:type" content="article">
<meta property="og:title" content="Important  Papers of SSL.">
<meta property="og:url" content="http://yoursite.com/posts/notes/2022-04-24-notes-paper-ssl-milestones.html">
<meta property="og:site_name" content="Mia&#39;s Blog">
<meta property="og:description" content="Paper 1: Vincent et al (2008) Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. JMLR 2008 Analysis of SSL methods  Paper 2: Koles">
<meta property="og:locale">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104529719.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104552474.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105305843.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105332503.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105801573.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111611299.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111628453.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111826403.png">
<meta property="og:image" content="c:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220427142848736.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220427142828464.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220429135409169.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504131635374.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504133549850.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506132933037.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125714185.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125757738.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220505143215455.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506133343254.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506134524556.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506140132378.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143138612.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143236840.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507144936733.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507145012695.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220508144638659.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509105849555.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509121402548.png">
<meta property="article:published_time" content="2022-04-24T16:02:00.000Z">
<meta property="article:modified_time" content="2022-05-09T22:13:17.446Z">
<meta property="article:author" content="Mia Feng">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="SSL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104529719.png">
  
    <link rel="alternative" href="/atom.xml" title="Mia&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: #4d4d4d;
    }
  </style>
  

  

<meta name="generator" content="Hexo 5.3.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/assets/img/avatar.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/posts/uncategorized/2019-06-13-about.html">about</a></li>
	        
				<li><a href="/categories/notes">notes</a></li>
	        
				<li><a href="/categories/projects">projects</a></li>
	        
				<li><a href="/categories/talks">talks</a></li>
	        
				<li><a href="/categories/meetings">meetings</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">posts</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">papers</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">about</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/skaudrey" title="github"><i class="icon-github"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:skaudreymia@gmail.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/assets/img/avatar.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/skaudrey" title="github"><i class="icon-github"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:skaudreymia@gmail.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 20%"><a href="/posts/uncategorized/2019-06-13-about.html">about</a></li>
		        
					<li style="width: 20%"><a href="/categories/notes">notes</a></li>
		        
					<li style="width: 20%"><a href="/categories/projects">projects</a></li>
		        
					<li style="width: 20%"><a href="/categories/talks">talks</a></li>
		        
					<li style="width: 20%"><a href="/categories/meetings">meetings</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-notes-paper-ssl-milestones" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Important  Papers of SSL.
    </h1>
  

        
        <a href="/posts/notes/2022-04-24-notes-paper-ssl-milestones.html" class="archive-article-date">
  	<time datetime="2022-04-24T16:02:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2022-04-24</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><p>Paper 1: Vincent et al (2008) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume11%2Fvincent10a%2Fvincent10a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0SspBuh4yVzZEMYUyio3wn"><em>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</em></a>. JMLR 2008</p></li>
<li><p>Analysis of SSL methods</p>
<ul>
<li>Paper 2: Kolesnikov, Zhai and Beyer (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FKolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Kgfk_3VH635pQwvKeEBE0"><em>Revisiting Self-Supervised Visual Representation Learning</em></a>. CVPR 2019</li>
<li>Paper 3: Zhai et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.04867&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2ITTaWDSpqKWi3PBBkP5xL"><em>A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark</em></a> (A GLUE-like benchmark for images) ArXiv 2019</li>
<li>Paper 4: Asano et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1904.13132&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2Fb1pUVDy26_-64K2-T_oR"><em>A critical analysis of self-supervision, or what we can learn from a single image</em></a> ICLR 2020</li>
</ul></li>
<li><p>Contrastive methods</p>
<ul>
<li><p>Paper 5: van den Oord et al. (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1807.03748&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw33PilvMsYAOmPqiPojZcUY"><em>Representation Learning with Contrastive Predictive Coding</em></a> (CPC), ArXiv 2018</p></li>
<li><p>Paper 6: Hjelm et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1808.06670&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw31h2Nm4lF3JBWFDXbG1miu"><em>Learning deep representations by mutual information estimation and maximization</em></a> (DIM) ICLR 2019</p></li>
<li><p>Paper 7: Tian et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.05849&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0s1M_14J8haUyPCJRRILoQ"><em>Contrastive Multiview Coding</em></a> (CMC) ArXiv 2019</p></li>
<li><p>Paper 8: Hénaff et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.09272&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1GXq6QkuyYGzUQ2EqiWXnL"><em>Data-Efficient Image Recognition with Contrastive Predictive Coding</em></a> (CPC v2: Improved CPC evaluated on limited labelled data) ArXiv 2019</p></li>
<li><p>Paper 9: He et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FHe_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3wwgkDHILZxygHEqFpue3W"><em>Momentum Contrast for Unsupervised Visual Representation Learning</em></a> (MoCo, see also <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.04297&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0tEX1f7G3quzSTWL6rfgzw"><em>MoCo v2</em></a>). CVPR 2020</p></li>
<li><p>Paper 10: Chen T et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2002.05709.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3nN2zgMpeyBgpXG--pO5Kx"><em>A Simple Framework for Contrastive Learning of Visual Representations</em></a> (SimCLR). ICML 2020</p></li>
<li><p>Paper 11: Chen T et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.10029&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw263B6UxGrQeiE53_baVKp-"><em>Big Self-Supervised Models are Strong Semi-Supervised Learners</em></a> (SimCLRv2) ArXiv 2020</p></li>
<li><p>Paper 12: Caron et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.09882&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0IYamHH9j-b56C3eUiwNXP"><em>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</em></a> (SwAV) ArXiv 2020</p></li>
<li><p>Paper 13: Xiao et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.05659&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3gwV78jCI0fs0R9wrj-y7S"><em>What Should Not Be Contrastive in Contrastive Learning</em></a> ArXiv 2020</p></li>
<li><p>Paper 14: Misra and van der Maaten (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FMisra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3sER0wiTJlhLbi88owlXXE"><em>Self-Supervised Learning of Pretext-Invariant Representations</em></a><em>.</em> CVPR 2020</p></li>
</ul></li>
<li><p>Generative methods</p>
<ul>
<li>Paper 15: Dumoulin et al (2017) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1606.00704&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2xx-blDKLroqk6XrXTa7Tp">Adversarially Learned Inference</a> (ALI) ICLR 2017</li>
<li>Paper 16: Donahue, Krähenbühl and Darrell <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1605.09782&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Ea7cOor2n6pmoJ44576vf">Adversarial Feature Learning</a> (BiGAN, concurrent and similar to ALI) ICLR 2017</li>
<li>Paper 17: Donahue and Simonyan (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1907.02544&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3bxhaVdSwniyMV3EFLcUrl">Large Scale Adversarial Representation Learning</a> (Big BiGAN) ArXiv 2019</li>
<li>Paper 18: Chen et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fimage-gpt%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2hhRvO-SL2S08MmJ1VjBfw">Generative Pretraining from Pixels</a> (iGPT) ICML 2020</li>
</ul></li>
<li><p>BYoL: boostrap your own latents</p>
<ul>
<li>Paper 19: Tarvainen and Valpola (2017) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2-LuidEMZJI7fUiE-oWqoa"><em>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</em></a>. NeurIPS 2017</li>
<li>Paper 20: Grill et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.07733&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2igxNmTekOxWbWzL9y5IyY"><em>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</em></a> (BYoL). ArXiv 2020</li>
<li>Paper 21: Abe Fetterman, Josh Albrecht, (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Funtitled-ai.github.io%2Funderstanding-self-supervised-contrastive-learning.html&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0qgrINDjCLX30tzD20JOti"><em>Understanding self-supervised and contrastive learning with "Bootstrap Your Own Latent" (BYOL)</em></a> Blog post</li>
<li>Paper 22: Schwarzer and Anand et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19"><em>Data-Efficient Reinforcement Learning with Momentum Predictive Representations</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19">.</a> ArXiv 2020</li>
</ul></li>
<li><p>self-distillation methods</p>
<ul>
<li>Paper 23: Furlanello et al (2017) <a target="_blank" rel="noopener" href="http://www.google.com/url?q=http%3A%2F%2Fmetalearning.ml%2F2017%2Fpapers%2Fmetalearn17_furlanello.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3xt23sIkAffPt8Ym8vqZDS"><em>Born Again Neural Networks</em></a>. NeurIPS 2017</li>
<li>Paper 24: Yang et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.05551&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2HyME0KdJDI4vO-jOuk3Zp"><em>Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.05551&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2HyME0KdJDI4vO-jOuk3Zp">.</a> AAAI 2019</li>
<li>Paper 25: Ahn et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FAhn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3XI1Xtrprs4vWTy6AcLiYH"><em>Variational information distillation for knowledge transfer</em></a>. CVPR 2019</li>
<li>Paper 26: Zhang et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_ICCV_2019%2Fpapers%2FZhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3lmWaXmFh5PictqZWwKIG5"><em>Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation</em></a> ICCV 2019</li>
<li>Paper 27: Müller et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F8717-when-does-label-smoothing-help.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2aZPLQTJt3rY082Bdqc3K9"><em>When Does Label Smoothing Help?</em></a> NeurIPS 2019</li>
<li>Paper 28: Yuan et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FYuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3zJVfBf6c8hlwGdl32vMNP"><em>Revisiting Knowledge Distillation via Label Smoothing Regularization</em></a>. CVPR 2020</li>
<li>Paper 29: Zhang and Sabuncu (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2006.05065v1.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0MHYHwHxUixNKEr1znpFVs"><em>Self-Distillation as Instance-Specific Label Smoothing</em></a> ArXiv 2020</li>
<li>Paper 30: Mobahi et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2002.05715&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0SktJ4r4OuAXv0PwzzVEnu"><em>Self-Distillation Amplifies Regularization in Hilbert Space</em></a>. ArXiv 2020</li>
</ul></li>
<li><p>self-training / pseudo-labeling methods</p>
<ul>
<li>Paper 31: Xie et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1911.04252&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1DhfgYIRMPv6XislXO6ry_"><em>Self-training with Noisy Student improves ImageNet classification</em></a>. CVPR 2020</li>
<li>Paper 32: Sohn and Berthelot et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2001.07685&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3LFMCJQrLt8NabRsym2vQW"><em>FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</em></a>. ArXiv 2020</li>
<li>Paper 33: Chen et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2006.10032.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2yldjcUPMdKaLjP5mcG1Xp">Self-training Avoids Using Spurious Features Under Domain Shift</a>. ArXiv 2020</li>
</ul></li>
<li><p>Iterated learning/emergence of compositional structure</p>
<ul>
<li>Paper 34: Ren et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DHkePNpVKPB&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2kXj4HM38k2b1gLv7cLC9G">Compositional languages emerge in a neural iterated learning model</a>. ICLR 2020</li>
<li>Paper 35: Guo, S. et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.05291&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw26-y5c29PiVpbAZYCrsj-o">The emergence of compositional languages for numeric concepts through iterated learning in neural agents</a>. ArXiv 2020</li>
<li>Paper 36: Cogswell et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F1904.09067.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0I5RdaelW5fHhwWfXWf9V2">Emergence of Compositional Language with Deep Generational Transmission</a> ArXiv 2020</li>
<li>Paper 37: Kharitonov and Baroni (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2004.03420%23%3A~%3Atext%3DEmergent%20Language%20Generalization%20and%20Acquisition%20Speed%20are%20not%20tied%20to%20Compositionality%2C-Eugene%20Kharitonov%2C%20Marco%26text%3DStudies%20of%20discrete%20languages%20emerging%2Cfor%20evidence%20of%20compositional%20structure.&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw14HOo95bLPmafk6gFvaAOT">Emergent Language Generalization and Acquisition Speed are not tied to Compositionality</a> ArXiv 2020</li>
</ul></li>
<li><p>NLP</p>
<ul>
<li>Paper 38: Peters et al (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1802.05365&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw10eV0YxRuEz3vADuc94kFn"><em>Deep contextualized word representations</em></a> (ELMO), NAACL 2018</li>
<li>Paper 39: Devlin et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN19-1423%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1oPB9GeT2i05ilyj5OAlBE"><em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</em></a> (BERT) NAACL 2019</li>
<li>Paper 40: Brown et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.14165&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0k8WZSIpb3qv17TQpoA0HA"><em>Language Models are Few-Shot Learners</em></a> (GPT-3, see also GPT-<a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Flanguage-unsupervised%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0AevPs9tr7a9kx--G4mr7M"><em>1</em></a>and <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fbetter-language-models%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1A50m11hjqmW-6njiC3YUp"><em>2</em></a>for more context) ArXiv 2020</li>
<li>Paper 41: Clark et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.10555&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1FzuTywUomI9d2oAiBVFxm"><em>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</em></a> ICLR 2020</li>
<li>Paper 42: He and Gu et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DSJgdnAVKDH&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2BOCV5w8r7GPa0z64Xb1sC"><em>REVISITING SELF-TRAINING FOR NEURAL SEQUENCE GENERATION</em></a> (Unsupervised NMT) ICLR 2020</li>
</ul></li>
<li><p>video/multi-modal data</p>
<ul>
<li>Paper 43: Wang and Gupta (2015) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fxiaolonw.github.io%2Funsupervise.html&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3MTp7nv2L5RX59nyRPqMfM">Unsupervised Learning of Visual Representations using Videos</a> ICCV 2015</li>
<li>Paper 44: Misra, Zitnick and Hebert (2016) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1603.08561&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3oZeSlFwM0jkzrL2Mq_Hwf"><em>Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</em></a> ECCV 2016</li>
<li>Paper 45: Lu et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1908.02265&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw39ViPN0TptsZxtRypcY1Ya"><em>ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1908.02265&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw39ViPN0TptsZxtRypcY1Ya">,</a> NeurIPS 2019</li>
<li>Paper 46: Hjelm and Bachman (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.13278&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0fIuuJYkqfT84nt0_QyFfZ"><em>Representation Learning with Video Deep InfoMax</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FN19-1423%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1oPB9GeT2i05ilyj5OAlBE"><em>.</em></a> (VDIM) Arxiv 2020</li>
</ul></li>
<li><p>the role of noise in representation learning</p>
<ul>
<li>Paper 47: Bachman, Alsharif and Precup (2014) <a target="_blank" rel="noopener" href="http://www.google.com/url?q=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5487-learning-with-pseudo-ensembles&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2hqOPAo5QSUwexnM7P62j4"><em>Learning with Pseudo-Ensembles</em></a> NeurIPS 2014</li>
<li>Paper 48: Bojanowski and Joulin (2017 ) <a target="_blank" rel="noopener" href="http://www.google.com/url?q=http%3A%2F%2Fproceedings.mlr.press%2Fv70%2Fbojanowski17a%2Fbojanowski17a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3_A9d13BdOXeC_QmofU3sV"><em>Unsupervised Learning by Predicting Noise.</em></a> ICML 2017</li>
</ul></li>
<li><p>SSL for RL, control and planning</p>
<ul>
<li>Paper 49: Pathak et al. (2017) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1705.05363&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1IrpWV_kOjuIQTMmDFS3wd"><em>Curiosity-driven Exploration by Self-supervised Prediction</em></a> (see also a <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fpathak22.github.io%2Flarge-scale-curiosity%2F&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0ZeG_gMDAsY1Z_VMac-vsK"><em>large-scale follow-up</em></a>) ICML 2017</li>
<li>Paper 50: Aytar et al. (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.11592&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw35lA1NGt9l0sZszcrO-f6F"><em>Playing hard exploration games by watching YouTube</em></a> (TDC) NeurIPS 2018</li>
<li>Paper 51: Anand et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.08226&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1te1a2v-cmeICbUGL2_oav"><em>Unsupervised State Representation Learning in Atari</em></a> (ST-DIM) NeurIPS 2019</li>
<li>Paper 52: Sekar and Rybkin et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.05960&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1zh2h0rWm82Aq8AZKySxPl"><em>Planning to Explore via Self-Supervised World Models.</em></a> ICML 2020</li>
<li>Paper 53: Schwarzer and Anand et al. (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19"><em>Data-Efficient Reinforcement Learning with Momentum Predictive Representations</em></a><a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2007.05929&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0Ovzcgk2ZTj3PedNgT9L19">.</a> ArXiv 2020</li>
</ul></li>
<li><p>SSL theory</p>
<ul>
<li>Paper 54: Arora et al (2019) <a target="_blank" rel="noopener" href="http://www.google.com/url?q=http%3A%2F%2Fproceedings.mlr.press%2Fv97%2Fsaunshi19a%2Fsaunshi19a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1Jv6bPfprDaifia4UCyFhE"><em>A Theoretical Analysis of Contrastive Unsupervised Representation Learning</em></a>. ICML 2019</li>
<li>Paper 55: Lee et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.01064&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2cgSGuqGOXlczR6HPQRBwq"><em>Predicting What You Already Know Helps: Provable Self-Supervised Learning</em></a> ArXiv 2020</li>
<li>Paper 56: Tschannen, et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1907.13625&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Y3TCY3KlkCbEIXQ61liXo">On mutual information maximization for representation learning</a>. ArXiv 2019.</li>
</ul></li>
<li><p>Unsupervised domain adaption</p>
<ul>
<li>Paper 57: Shu et al (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DH1q-TM-AW&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1DzTvyS29_50se_tZob9IX"><em>A DIRT-T APPROACH TO UNSUPERVISED DOMAIN ADAPTATION</em></a>. ICLR 2018</li>
<li>Paper 58: Wilson and Cook (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1812.02849&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2n226swg-lx1H29furztaL"><em>A Survey of Unsupervised Deep Domain Adaptatio</em></a><em>n</em>. ACM Transactions on Intelligent Systems and Technology 2020<strong>.</strong></li>
<li>Paper 59: Mao et al. (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.04215&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3tp1FCFSItdeJjpQUUvBJX"><em>Virtual Mixup Training for Unsupervised Domain Adaptation</em></a>. CVPR 2019</li>
<li>Paper 60: Vu et al. (2018) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1811.12833&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1ZjGV5QwOyB0ZPMqgiSGXh"><em>ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</em></a> CVPR 2019</li>
</ul></li>
<li><p>Scaling</p>
<ul>
<li>Paper 61: Kaplan et al (2020) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2001.08361&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0pY71xR2C_948Bp-wDv-aA"><em>Scaling Laws for Neural Language Models</em></a>. ArXiv 2020</li>
</ul></li>
</ul>
<a id="more"></a>
<h2 id="paper-1-stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion">Paper 1: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume11%2Fvincent10a%2Fvincent10a.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0SspBuh4yVzZEMYUyio3wn"><em>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</em></a></h2>
<p>Codes:</p>
<h3 id="previous">Previous</h3>
<ul>
<li>What works much better is to initially use a local unsupervised criterion to (pre)train each layer in turn, with the goal of learning to produce a useful higher-level representation from the lower-level representation output by the previous layer.</li>
<li>Initializing a deep network by stacking autoencoders yields almost as good a classification performance as when stacking RBMs. But why is it almost as good?</li>
<li>looking for unsupervised learning principles likely to lead to the learning of feature detectors that detect important structure in the input patterns.</li>
</ul>
<h4 id="traditional-classifiers-training-with-noisy-inputs">Traditional classifiers training with noisy inputs</h4>
<ul>
<li>Training with noise is equivalent to applying generalized Tikhonov regularization (Bishop, 1995), which means L2 weight decay penalty but on linear regression with additive noise. For non-linear case, the regularization is more complex. Authors of this paper even show that different result when using DAEmon and when using regular autoencoders with a L2 weight decay.</li>
</ul>
<h4 id="pseudo-likelihood-and-dependency-networks">Pseudo-Likelihood and Dependency Networks</h4>
<ul>
<li>Pseudo-likelihood , dependency network paradigms etc.</li>
</ul>
<h3 id="ideas">Ideas</h3>
<p>stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. Find that denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. our results clearly establish the value of using a denoising criterion as an unsupervised objective to guide the learning of useful higher level representations.</p>
<h4 id="reasoning-what-makes-good-representations">Reasoning: what makes good representations</h4>
<ul>
<li>A good representation is those that retain a significant amount of information about the input, can be expressed in information-theoretic terms as maximizing the mutual information</li>
<li>Mutual information can be decomposed into an entropy and a conditional entropy term in two different ways
<ul>
<li>ICA: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104529719.png" alt="image-20220426104529719" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426104552474.png" title="fig:" alt="image-20220426104552474" />
<ul>
<li>consider a parameterized distribution $p(X|Y;') $ that parameterized by <span class="math inline">\(\mathrm{\theta}&#39;\)</span>, then <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105305843.png" alt="image-20220426105305843" /> leads to maximizing a lower bound on<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105332503.png" alt="image-20220426105332503" />and thus on the mutual information.</li>
<li>ICA is when <span class="math inline">\(Y=f_\theta(X)\)</span>.</li>
<li>As <span class="math inline">\(q(X)\)</span> is unknown, but with samples, the empirical average over the training samples can be used instead as an unbiased estimate (i.e., replacing <span class="math inline">\(\mathbb{E}_{q(X)}\)</span> by $_{q^0(X)} $: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426105801573.png" alt="image-20220426105801573" /></li>
</ul></li>
</ul></li>
<li>The equation above corresponds to the reconstruction error criterion used to train autoencoders
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111611299.png" alt="image-20220426111611299" />: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111628453.png" alt="image-20220426111628453" />To choose <span class="math inline">\(p(\mathrm{x|z}),L(\mathrm{x|z})\)</span>
<ul>
<li>For real-valued $ $ squared error: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220426111826403.png" alt="image-20220426111826403" style="zoom:80%;" /> on <span class="math inline">\(\sigma^2\)</span>, the variance of <span class="math inline">\(X\)</span>. Since gaussian, not to use a squashing nonlinearity in the decoder.</li>
<li>Binary $ $: finally become cross-entropy loss, which can be used when $ $ is not strictly binary but rather $^d $</li>
</ul></li>
<li><strong>training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between input X and learnt representation Y</strong></li>
</ul></li>
<li><strong>Merely Retaining Information is Not Enough</strong>
<ul>
<li>non-zero reconstruction error to separate useful information from noise: traditional AE uses bottleneck to produce an under-complete representation where <span class="math inline">\(d&#39;&lt;d\)</span>, and result in a lossy compressed representation of <span class="math inline">\(X\)</span>. When using affine encoder and decoder without any nonlinearity an a squared error loss, the AE performs PCA actually. But not in cross-entropy loss.</li>
<li>using over-complete (i.e., higher dimensional than the input) but sparse representations is popular now, which is a special case of imposing on <span class="math inline">\(Y\)</span> different constraints than that of a lower dimensionality. A sparse over-complete representations can be viewed as an alternative “compressed” representation.</li>
</ul></li>
</ul>
<h3 id="how">How?</h3>
<ul>
<li><p>DAE: denoising autoencoder</p>
<ul>
<li><p>Procedures</p>
<ul>
<li>first denoising: corrupt the initial input $ $ into <span class="math inline">\(\tilde{\mathrm{x}}\)</span> by a stochastic mapping. This will lead to force the learning of a mapping that extracts features useful for denoising.</li>
<li>Then map <span class="math inline">\(\tilde{\mathrm{x}}\)</span> to <span class="math inline">\(\mathrm{y}\)</span>. And reonstruct <span class="math inline">\(\mathrm{z}\)</span> to be close to the $ $ but a function of <span class="math inline">\(\mathrm{y}\)</span>.</li>
</ul></li>
<li><p>Geometric Interpretation</p>
<ul>
<li>Thus stochastic operator <span class="math inline">\(p(X|\tilde{X})\)</span> learns a map that tends to go from lower probability points <span class="math inline">\(\tilde{X}\)</span> to nearby high probability points <span class="math inline">\(X\)</span>, on or near the manifold.</li>
<li>Successful denoising implies that the operator maps even far away points to a small region close to the manifold</li>
<li>Think of <span class="math inline">\(Y=f(X)\)</span> as a representation of <span class="math inline">\(X\)</span> which is well suited to capture the main variations in the data, that is, those along the manifold.</li>
</ul></li>
<li><p>Types of corruption considered</p>
<ul>
<li>additive: natural</li>
<li>masking noise: natural for input domains which are interpretable as binary or near binary such as black an white images or the representation produced at the hidden layer after a signoid squashing function</li>
<li>salt and pepper noise: same as masking noise, only earse a changing subsets of the inputs components while leaving others untouched</li>
</ul></li>
<li><p>Emphasize corrupted dimensions</p>
<ul>
<li><p>only put an emphasis on the corrupted dimensions. Like use <span class="math inline">\(\alpha\)</span> for the Reconstruction error on corrupted components and <span class="math inline">\(\beta\)</span> for untouched components</p></li>
<li><p>Sqaured loss</p>
<figure>
<img src="C:/Users/Skaud/AppData/Roaming/Typora/typora-user-images/image-20220427142848736.png" alt="image-20220427142848736" /><figcaption aria-hidden="true">image-20220427142848736</figcaption>
</figure></li>
<li><p>Cross-entropy loss</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220427142828464.png" alt="image-20220427142828464" /><figcaption aria-hidden="true">image-20220427142828464</figcaption>
</figure></li>
<li><p>If <span class="math inline">\(\alpha=1, \beta=0\)</span>, then this is full emphasis that only consider the error on the prediction of corrupted elements</p></li>
</ul></li>
<li><p>Stacking DAE for deep architecture</p></li>
</ul></li>
</ul>
<h3 id="experiments">Experiments</h3>
<ul>
<li><p>Single DAE</p>
<ul>
<li>feature detectors from natural image patches
<ul>
<li>The under-complete autoencoder appears to learn rather uninteresting local blob detectors. Filters obtained in the overcomplete case have no recognizable structure, looking entirely random</li>
<li>training with sufficiently large noise yields a qualitatively very different outcome than training with a weight decay regularization and this proves that the two are not equivalent for a non-linear autoencoder.</li>
<li>Salt-and-pepper noise yielded Gabor-like edge detectors, whereas masking noise yielded a mixture of edge detectors and grating filters. They all yield some potentially useful edge detectors.</li>
</ul></li>
<li>feature detectors from handwrite digits
<ul>
<li>With increased noise levels, a much larger proportion of interesting (visibly non random and with a clear structure) feature detectors are learnt. These include local oriented stroke detectors and detectors of digit parts such as loops.</li>
<li>But denoising a more corrupted input requires detecting bigger, less local structures.</li>
</ul></li>
</ul></li>
<li><p>Stacked DAE (SDAE): compare with SAE and DBN.</p>
<ul>
<li>Classification problem and experimental methodology: As increasing the noise level, denoising training forces the filters to differentiate more, and capture more distinctive features. Higher noise levels tend to induce less local filters, as expected.</li>
<li>Compared with other strategies: denoising pretraining with a non-zero noise level is a better strategy than pretraining with regular autoencoders</li>
<li>Influence of Number of Layers, Hidden Units per Layer, and Noise Level
<ul>
<li>Depth: denoising pretraining being better than autoencoder pretraining being better than no pretraining. The advantage appears to increase with the number of layers and with the number of hidden units</li>
<li>noise levels: SDAE appears to perform better than SAE (0 noise) for a rather wide range of noise levels, regardless of the number of hidden layers.</li>
</ul></li>
</ul></li>
<li><p>Denoising pretraining v.s. training with noisy input</p>
<ul>
<li>Note: SDAE uses a denoising criterion to learn good initial feature extractors at each layer that will be used as initialization for a noiseless supervised training; which is different from training with noisy inputs that amounts to training with a virtually expanded data set.</li>
<li>Denoising pretraining with SDAE, for a large range of noise levels, yields significantly improved performance, whereas training with noisy inputs sometimes degrades the performance, and sometimes improves it slightly but is clearly less beneficial than SDAE.</li>
</ul></li>
<li><p>Variations on the DAE, alternate corruption types and emphasizing</p>
<ul>
<li>An emphasized SDAE with salt-and-pepper noise appears to be the winning SDAE variant.</li>
<li>A judicious choice of noise type and added emphasis may often buy us a better performance.</li>
</ul></li>
<li><p>Are Features Learnt in an Unsupervised Fashion by SDAE Useful for SVMs?</p>
<ul>
<li>SVM performance can benefit significantly from using the higher level representation learnt by SDAE</li>
<li>linear SVMs can benefit from having the original input processed non-linearly</li>
</ul></li>
<li><p>Generating Samples from Stacked Denoising Autoencoder Networks</p>
<ul>
<li>Top-Down Generation of a Visible Sample Given a Top-Layer Representation: it is thus possible to generate samples at one layer from the representation of the layer above in the exact same way as in a DBN.</li>
<li>Bottom-up (infer representation of the top layer based on representation on the bottom layer) is similar as approximate inference of a factorial Bernoulli top-layer distribution given the low level input. The top-layer representation is to be understood as the parameters (the mean) of a factorial Bernoulli distribution for the actual binary units.</li>
<li>SDAE and DBN are able to resynthesize a variety of similarly good quality digits, whereas the SAE trained model regenerates patterns with much visible degradation in quality, this also prove that evidence of the qualitative difference resulting from optimizing a denoising criterion instead of mere reconstruction criterion.</li>
<li>Contrary to SAE, the regenerated patterns from SDAE or DBN look like they could be samples from the same unknown input distribution that yielded the training set.</li>
</ul></li>
</ul>
<h2 id="paper-2-revisiting-self-supervised-visual-representation-learning">Paper 2: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FKolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3Kgfk_3VH635pQwvKeEBE0"><em>Revisiting Self-Supervised Visual Representation Learning</em></a></h2>
<p>https://github.com/google/revisiting-self-supervised</p>
<h3 id="previous-1">Previous</h3>
<ul>
<li>Compared with pretext task, the choice of CNN has not received equal attention</li>
<li>patch-based self-supervised visual representation learning methods: predicting the relative location of image patches; “jigsaw puzzle” created from the full image etc.</li>
<li>image-level classification tasks: randomly rotate an image by one of four possible angles and let the model predict that rotation; use clustering of the images</li>
<li>tasks with dense spatial outputs: image inpainting, image colorization, its improved variant split-brain and motion segmentation prediction.</li>
<li>equivariance relation to match the sum of multiple tiled representations to a single scaled representation; predict future patches in via autoregressive predictive coding.</li>
<li>many works have tried to combine multiple pretext tasks in one way or another</li>
</ul>
<h3 id="what">What?</h3>
<ul>
<li>Standard architecture design recipes do not necessarily translate from the fully-supervised to the self-supervised setting. Architecture choices which negligibly affect performance in the fully labeled setting, may significantly affect performance in the self-supervised setting.</li>
<li>The quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</li>
<li>Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations.</li>
<li>The <strong>evaluation</strong> procedure, where a linear model is trained on a fixed visual representation using <strong>stochastic gradient descent, is sensitive to the learning rate schedule</strong> and may take many epochs to converge.</li>
</ul>
<h3 id="how-1">How</h3>
<ul>
<li>revisit a prominent subset of the previously proposed pretext tasks and perform a large-scale empirical study using various architectures as base models.</li>
<li>The CNN candidates
<ul>
<li>ResNet:</li>
<li>RevNet: stronger invertibility guarantees while being structurally similar to ResNets. Set it to have the same depth and number of channels as the original Resnet50 model.</li>
<li>VGG: no skip, has BN</li>
</ul></li>
<li>The pretext tasks candidates
<ul>
<li>Rotation: 0，90，180，270</li>
<li>Exemplar: heavy random data augmentation such as translation, scaling, rotation, and contrast and color shifts</li>
<li>Jigsaw: recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches. They extract representations by averaging the representations of nine uniformly sampled, colorful, and normalized patches of an image.</li>
<li>Relative patch location: 8 possible relative spatial relations between two patches need to be predicted. use the same patch prepossessing as in the Jigsaw model and also extract final image representations by averaging representations of 9 cropped patches.</li>
</ul></li>
<li>Dataset candidates
<ul>
<li>ImageNet</li>
<li>Places205</li>
</ul></li>
<li>Evaluation protocol
<ul>
<li>measures representation quality as the accuracy of a linear regression model trained and evaluated on the ImageNet dataset</li>
<li>the pre-logits of the trained self-supervised networks as representation.</li>
</ul></li>
</ul>
<h3 id="experiments-1">Experiments</h3>
<ul>
<li><p>similar models often result in visual representations that have significantly different performance. Importantly, neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220429135409169.png" alt="image-20220429135409169" /><figcaption aria-hidden="true">image-20220429135409169</figcaption>
</figure></li>
<li><p>increasing the number of channels in CNN models improves performance of self-supervised models.</p></li>
<li><p>ranking of models evaluated on Places205 is consistent with that of models evaluated on ImageNet, indicating that our findings generalize to new datasets.</p></li>
<li><p>self-supervised learning architecture choice matters as much as choice of a pretext task</p></li>
<li><p>MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged. We thus conclude that the linear model is adequate for evaluation purposes.</p></li>
<li><p>Better performance on the pretext task does not always translate to better representations. For residual architectures, the pre-logits are always best.</p></li>
<li><p>Skip-connections prevent degradation of representation quality towards the end of CNNs. We hypothesize that this is a result of ResNet’s residual units being invertible under some conditions. RevNet, boosts performance by more than 5 % on the Rotation task, albeit it does not result in improvements across other tasks.</p></li>
<li><p><strong>Model width and representation size strongly influence the representation quality</strong>: disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer. self-supervised learning techniques are likely to benefit from using CNNs with increased number of channels across wide range of scenarios.</p></li>
<li><p>SGD optimization hyperparameters play an important role and need to be reported. very long training (≈ 500 epochs) results in higher accuracy. They decay lr at 480 epochs.</p></li>
<li></li>
</ul>
<h2 id="paper-3-zhai-et-al-2019-a-large-scale-study-of-representation-learning-with-the-visual-task-adaptation-benchmark-a-glue-like-benchmark-for-images">Paper 3: Zhai et al (2019) <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.04867&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2ITTaWDSpqKWi3PBBkP5xL"><em>A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark</em></a> (A GLUE-like benchmark for images)</h2>
<p>https://github.com/google-research/task_adaptation</p>
<h3 id="previous-2">Previous</h3>
<ul>
<li>the absence of a unified evaluation for general visual representations hinders progress. Each sub-domain has its own evaluation protocol, and the lack of a common benchmark impedes progress.</li>
<li>Popular protocols are often too constrained (linear classification), limited in diversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to representation quality (ELBO, reconstruction error)</li>
</ul>
<h3 id="what-1">What？</h3>
<ul>
<li>present the Visual Task Adaptation Benchmark (VTAB), which defines good representations as those that adapt to diverse, unseen tasks with few examples. (i) minimal constraints to encourage creativity, (ii) a focus on practical considerations, and (iii) make it challenging.</li>
<li>Conduct a large-scale study of many popular publicly-available representation learning algorithms on VTAB.</li>
<li>They found:
<ul>
<li>Supervised ImageNet pretraining yields excellent representations for natural image classification tasks</li>
<li>Self-supervised is less effective than supervised learning overall, but surprisingly, can improve structured understanding</li>
<li>Combining supervision and self-supervision is effective, and to a large extent, self-supervision can replace, or compliment labels.</li>
<li>Discriminative representations appear more effective than those trained as part of a generative model, with the exception of adversarially trained encoders.</li>
<li>GANs perform relatively better on data similar to their pre-training source (here, ImageNet), but worse on other tasks.</li>
<li>Evaluation using a linear classifier leads to poorer transfer and different conclusions.</li>
</ul></li>
</ul>
<h3 id="how-2">How？</h3>
<ul>
<li>To design, must ensure that the algorithms are not pre-exposed to specific evaluation samples.</li>
<li>VTAB benchmark
<ul>
<li>practical benchmark
<ul>
<li>Define task distribution as Tasks that a human can solve, from visual input alone.”</li>
<li>For each evaluation sample a new task.</li>
<li>mitigating meta-overfitting : treat the evaluation tasks unseen, and thus algorithms use pretraining must not pre-train on any of the evaluation tasks.</li>
<li>Unified implementation: algorithms must have no prior knowledge of the downstream tasks, and hyperparameter searches need to work well across the benchmark.</li>
<li>All tasks are classification in this paper, such as the detection task is mapped to the classification of the <span class="math inline">\((x,y,z)\)</span> coordinates. The diverse set of visual features are learnt from object identification, scene classification, pathology detection, counting, localization and 3D geometry.</li>
</ul></li>
<li>Tasks
<ul>
<li>NATURAL: classical vision problems, group includes Caltech101, CIFAR100, DTD, Flowers102, Pets, Sun397, and SVHN</li>
<li>SPECIALIZED: images are captured through specialist equipment, and human can recognize the structures. One is remote sensing, the other is medical</li>
<li>STRUCTURED: assesses comprehension of the structure of a scene, for example, object counting, or 3D depth prediction.</li>
</ul></li>
<li>pretraining on pretext tasks and then fine tune</li>
</ul></li>
<li>The methods are divided into five groups: Generative, training from-scratch, all methods using 10% labels (Semi Supervised), and all methods using 100% labels (Supervised),</li>
</ul>
<h3 id="experiments-2">Experiments</h3>
<ul>
<li>Upstream, control the data and architecture (pretrained on ImageNet). They find bigger architectures perform better on VTAB, and use resnet or resnet-similar nets for all models as the encoder.</li>
<li>Downstream, run VTAB in two modes: the light weight mode sweepstakes 2 initial learning rates and 2 learning rate schedules but fix other parameters while the heavy weights perform a large random search over learning rate, schedule, optimizers, batch size, train preprocessing functions, evaluation preprocessing and weight decay. The main study is done in lightweight.</li>
<li>Evaluate with top-1 accuracy. To aggregate scores across tasks, take the mean accuracy.</li>
<li>Lightweight
<ul>
<li>Generative models perform worst, GANs fit more strongly to ImageNet’s domain (natural images), than self-supervised alternatives.</li>
<li>All self-supervised representations outperform from-scratch training. Methods applied to the entire image outperform patch-based method, and these tasks require sensitivity to local textures. self supervised is worst than supervised on natural tasks, similar on specialized tasks and slightly better than structured tasks.</li>
<li>Supervised models perform the best. additional self-supervision even improves on top of 100% labelled ImageNet, particularly on STRUCTURED tasks</li>
</ul></li>
<li>Heavyweight
<ul>
<li>across all task groups, pre-trained representations are better than a tuned from-scratch model.</li>
<li>a combination supervision and self-supervision (SUP-EXEMPLAR-100%) getting the best performance.</li>
</ul></li>
<li>Frozen feature extractors
<ul>
<li>linear evaluation significantly lowers performance, even when downstream data is limited to 1000 examples. Linear transfer would not by used in practice unless infrastructural constraints required it. These self-supervised methods extract useful representations, just without linear separability. Linear evaluation results are sensitive to additional factors that we do not vary, such as ResNet version or pre-training regularization parameters.</li>
</ul></li>
<li>Vision benchmarks
<ul>
<li>The methods ranked according to VTAB are more likely to transfer to new tasks, than those ranked according to the Visual Decathlon</li>
<li>VTAB is more flexible than Facebook AI SSL challenge, such as the diversity of domain, the evaluation form.</li>
<li>Meta-dataset: designed for few-shot learning rather than 1000 examples which may entail different solutions.</li>
</ul></li>
<li>Some details
<ul>
<li><strong>while training from scratch , inception crop, horizontal flip preprocessing</strong>, 1e-3 in weight decay, 0.1~1 lr in SGD mostly give good results.</li>
<li>While doing <strong>SSL by ImageNet and then finetuning</strong>, <strong>Inception crop, non-horizontal flip</strong> give good results.</li>
</ul></li>
</ul>
<h3 id="discussion">Discussion</h3>
<ul>
<li>how effective are supervised ImageNet representations? ImageNet labels are indeed effective for natural tasks.</li>
<li>how do representations trained via generative and discriminative models compare? The generative losses seem less promising as means towards learning how to represent data. BigBiGAN is notable.</li>
<li>To what extent can self-supervision replace labels?
<ul>
<li>self-supervision can almost (but not quite) replace 90% of ImageNet labels; the gap between pre-training on 10% labels with self-supervison, and 100% labels, is small</li>
<li>self-supervision adds value on top of ImageNet labels on the same data.</li>
<li>simply adding more data on the SPECIALIZED and STRUCTURED tasks is better than the pre-training strategies we evaluated</li>
</ul></li>
<li>Varying other factors to improve VTAB is valuable future research. The only approach that is out-of-bounds is to condition the algorithm explicitly on the VTAB tasks</li>
</ul>
<h2 id="paper-4-a-critical-analysis-of-self-supervision-or-what-we-can-learn-from-a-single-image">Paper 4: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1904.13132&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2Fb1pUVDy26_-64K2-T_oR"><em>A critical analysis of self-supervision, or what we can learn from a single image</em></a></h2>
<p>https://github.com/yukimasano/linear-probes</p>
<h3 id="previous-3">Previous</h3>
<ul>
<li>For a given model complexity, pre-training by using an off-the-shelf annotated image datasets such as ImageNet remains much more efficient.</li>
<li>Methods often modify information in the images and require the network to recover them. However, features are learned on modified images which potentially harms the generalization to unmodified ones.</li>
<li>Learning from a single sample
<ul>
<li>Object tracking: max margin correlation filters learn robust tracking templates from a single sample of the patch.</li>
<li>learn and interpolate multi-scale textures with a GAN framework</li>
<li>semi-parametric exemplar SVM model</li>
<li>we do not use a large collection of negative images to train our model. Instead we restrict ourselves to a single or a few images with a systematic augmentation strategy.</li>
</ul></li>
<li>Classical learned and hand-crafted low-level feature extractors: insufficient to clarify the power and limitation of self-supervision in deep networks.</li>
</ul>
<h3 id="what-2">What</h3>
<ul>
<li>Aim to investigate the effectiveness of current self-supervised approaches by characterizing how much information they can extract from a given dataset of images. Then try to answer <strong>whether a large dataset is beneficial to unsupervised learning, especially for learning early convolutional features</strong></li>
<li>Three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used.</li>
<li>For deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training.</li>
<li>Conclusion
<ul>
<li>the weights of the early layers of deep networks contain limited information about the statistics of natural images</li>
<li>such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that</li>
<li>the low-level statistics can be captured via synthetic transformations instead of using a large image dataset. (training these layers with self-supervision and a single image already achieves as much as two thirds of the performance that can be achieved by using a million different images.)</li>
</ul></li>
</ul>
<h3 id="how-3">How</h3>
<ul>
<li>Data: use DAA augmentation to replace some source images.
<ul>
<li>Augmentations: involving cropping, scaling, rotation, contrast changes, and adding noise. Augmentation can be seen as imposing a prior on how we expect the manifold of natural images to look like
<ul>
<li>limit the size of cropped patch: the smallest size of the crops is limited to be at least βWH and at most the whole image. Additionally, changes to the aspect ratio are limited by γ. In practice we use β = 10e−3 and γ = 3/4 .</li>
<li>before cropping, rotate in <span class="math inline">\((-35,35)\)</span> degrees. And also flip images in 50% possibility</li>
<li>linear transformation in RGB space, color jitter with additive brightness, contrast and saturation</li>
</ul></li>
<li>Real samples: give some drawn samples which are not real captured but has plenty of textures and in small size, and a real picture in large size but with large areas no objects.</li>
</ul></li>
<li>Representation learning methods
<ul>
<li>BiGAN with leaky ReLU nonlinearities in discriminators</li>
<li>Rotation: do it on horizontal flips and non-scaled random crops to 224 × 224</li>
<li>clustering:</li>
</ul></li>
</ul>
<h3 id="experiments-3">Experiments</h3>
<ul>
<li>ImageNet and CIFAR-10/100 using linear probes</li>
<li>Base encoder: AlexNet. They insert the probes right after the ReLU layer in each block</li>
<li>Learning lasts for 36 epochs and the learning rate schedule starts from 0.01 and is divided by five at epochs 5, 15 and 25</li>
<li>extracting 10 crops for each validation image (four at the corners and one at the center along with their horizontal flips) and averaging the prediction scores before the accuracy is computed.</li>
<li>Effect of augmentations: <strong>Random rescaling</strong> adds at least ten points at every depth (see Table 1 (f,h,i)) and is the most important single augmentation. <strong>Color jittering and rotation</strong> slightly improve the performance of all probes by 1- 2% points</li>
<li>Benchmark evaluation
<ul>
<li>Mono is enough: Mono means train with one source image and its augmented images.</li>
<li>Image contents:
<ul>
<li>RotNet cannot extract photographic bias from a single image. the method can extract rotation from low level image features such as patches which is at first counter intuitive=&gt; <strong>lighting and shadows even in small patches</strong> can indeed give important cues on the up direction which can be learned even from a single (real) image.</li>
<li><strong>the augmentations can even compensate for large untextured areas and the exact choice of image is not critical.</strong> A trivial image without any image gradient (e.g. picture of a white wall) would not provide enough signal for any method.</li>
</ul></li>
<li>More than one image
<ul>
<li>for conv1 and conv2, a single image is enough</li>
<li>In deeper layers, DeepCluster seems to require large amounts of source images to yield the reported results as the deka- and kilo- variants start improving over the single image case</li>
</ul></li>
<li>Generalization
<ul>
<li>GAN trained on the smaller Image B outperforms all other methods including the fully-supervised trained one for the first convolutional layer</li>
<li>our method allows learning very generalizable early features that are not domain dependent.</li>
</ul></li>
</ul></li>
<li>the neural network is only extracting patterns and not semantic information because we do not find any neurons particularly specialized to certain objects even in higher levels as for example dog faces or similar which can be fund in supervised networks</li>
</ul>
<h2 id="paper-5-representation-learning-with-contrastive-predictive-coding-cpc-arxiv-2018">Paper 5: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1807.03748&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw33PilvMsYAOmPqiPojZcUY"><em>Representation Learning with Contrastive Predictive Coding</em></a> (CPC), ArXiv 2018</h2>
<h3 id="previous-4">Previous</h3>
<ul>
<li>It is not always clear what the ideal representation is and if it is possible that one can learn such a representation without additional supervision or specialization to a particular data modality.</li>
<li>One of the most common strategies for unsupervised learning has been to predict future, missing or contextual information
<ul>
<li>Recently in unsupervised learning some learn word representations by predicting neighboring words.</li>
<li>For images, predicting color from grey-scale or the relative position of image patches</li>
</ul></li>
<li>predicting high-dimensional data
<ul>
<li>unimodal losses such as mean-squared error and cross-entropy are not very useful</li>
<li>powerful conditional generative models which need to reconstruct every detail in the data are usually required.</li>
</ul></li>
</ul>
<h3 id="what-3">What</h3>
<ul>
<li>Main contributions
<ul>
<li>Compress features into a latent embedding space in which conditional predictions are easier to model.</li>
<li>predict the future in latent space by autoregressive models</li>
<li>Use NCE loss</li>
</ul></li>
<li>Intuition
<ul>
<li>learn the representations that encode the underlying shared information between parts of the signal</li>
<li>meanwhile discard low-level information and noise that is more local.</li>
</ul></li>
</ul>
<h3 id="how-4">How</h3>
<ul>
<li>use a NCE which induces the latent space to capture information that is maximally useful to predict future samples</li>
<li>Model a density ratio which preserves the mutual information between <span class="math inline">\(x_{t+k},c_t\)</span> as <span class="math inline">\(f_k(x_{t+k},c_t)\propto \frac{p(x_{t+k}|c_t)}{p(x_{t+k})}\)</span>, they choose <span class="math inline">\(f_k(x_{t+k},c_t)=\exp(z_{t+k}^TW_kc_t)\)</span>, the log-bilinear model.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504131635374.png" title="fig:" alt="image-20220504131635374" /></li>
</ul>
<h3 id="experiments-4">Experiments</h3>
<p>For every domain we train CPC models and probe what the representations contain with either a linear classification task or qualitative evaluations.</p>
<ul>
<li>Audio
<ul>
<li>use a 100-hour subset of the publicly available LibriSpeech dataset</li>
<li>use a GRU for the autoregressive part of the model</li>
<li>We found that not all the information encoded is linearly accessible.</li>
<li>CPCs capture both speaker identity and speech contents</li>
<li></li>
</ul></li>
<li>Vision
<ul>
<li>Use ImageNet and ResNet v2 101, no BN. use the outputs from the third residual block, and spatially mean-pool to get a single 1024-d vector per 64x64 patch. This results in a 7x7x1024 tensor. Next, we use a PixelCNN-style autoregressive model to make predictions about the latent activations in following rows top-to-bottom.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220504133549850.png" title="fig:" alt="image-20220504133549850" /></li>
<li>CPCs improve upon state-of-the-art by 9% absolute in top-1 accuracy, and 4% absolute in top-5 accuracy.</li>
</ul></li>
<li>Natural language
<ul>
<li>a linear mapping is constructed between word2vec and the word embeddings learned by the model. A L2 regularization weight was chosen via cross-validation (therefore nested cross-validation for the first 4 datasets)</li>
<li>found that more advanced sentence encoders did not significantly improve the results, which may be due to the simplicity of the transfer tasks, and the fact that bag-of-words models usually perform well on many NLP tasks.</li>
<li>The performance of our method is very similar to the skip-thought vector model, with the advantage that it does not require a powerful LSTM as word-level decoder, therefore much faster to train</li>
</ul></li>
<li>Reinforcement learning
<ul>
<li>take the standard batched A2C agent as base model and add CPC as an auxiliary loss</li>
<li>The unroll length for the A2C is 100 steps and we predict up to 30 steps in the future to derive the contrastive loss</li>
<li>4 out of the 5 games performance of the agent improves significantly with the contrastive loss after training on 1 billion frames.</li>
</ul></li>
</ul>
<h2 id="paper-6-learning-deep-representations-by-mutual-information-estimation-and-maximization-dim-deep-infomax-iclr-2019">Paper 6: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1808.06670&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw31h2Nm4lF3JBWFDXbG1miu"><em>Learning deep representations by mutual information estimation and maximization</em></a> (DIM Deep InfoMax ) ICLR 2019</h2>
<p>https://github.com/rdevon/DIM</p>
<h3 id="previous-5">Previous</h3>
<ul>
<li>in typical settings, models with reconstruction-type objectives provide some guarantees on the amount of information encoded in their intermediate representations.</li>
<li>MI estimation
<ul>
<li>MINE: strongly consistent, can be used to learn better implicit bidirectional generative models</li>
<li>DIM: follow MINE, but they find the generator is unnecessary. And no necessary for the exact KL-divergence, alternatively the JSD is more stable and provides better results.</li>
</ul></li>
<li>CPC and DIM
<ul>
<li>CPC: make predictions about specific local features in the “future” of each summary feature. This equates to ordered autoregression over the local features, and requires training separate estimators for each temporal offset at which one would like to predict the future.</li>
<li>DIM uses a single summary feature that is a function of all local features, and this “global” feature predicts all local features simultaneously in a single step using a single estimator.</li>
</ul></li>
</ul>
<h3 id="what-4">What</h3>
<ul>
<li>structure matters: maximizing the average MI between the representation and local regions of the input can improve performance while maximizing MI between the complete input and the encoder output not always do this.</li>
<li>JSD helps MI estimation. JSD and DVD all maximize the expected log-ratio of the joint over the product of marginals.</li>
</ul>
<h3 id="how-5">How</h3>
<h4 id="mutual-information-estimation-and-maximization">Mutual information estimation and maximization</h4>
<ul>
<li><p>Basic MI maximization framework</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506132933037.png" alt="image-20220506132933037" /><figcaption aria-hidden="true">image-20220506132933037</figcaption>
</figure></li>
<li><p>share layers between encoder and mutual information</p></li>
<li><p>The different losses of DIM with different estimator</p>
<ul>
<li>With non-KL divergences such as JSD: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125714185.png" alt="image-20220506125714185" /></li>
<li>With NCE, <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506125757738.png" alt="image-20220506125757738" /></li>
<li>For DIM, a key difference between the DV, JSD, and infoNCE formulations is whether an expectation over <span class="math inline">\(\mathrm{\mathbb{P/ \tilde{P}}}\)</span> appears inside or outside of a <span class="math inline">\(\log\)</span>. DIM sets the noise distribution to the product of marginals over <span class="math inline">\(X/Y\)</span> , and the data distribution to the true joint.</li>
</ul></li>
<li><p>infoNCE often outperforms JSD on downstream tasks, though this effect diminishes with more challenging data and also requires more negative samples compare with the JSD version.</p></li>
<li><p>DIM with the JSD loss is insensitive to the number of negative samples, and in fact outperforms infoNCE as the number of negative samples becomes smaller.</p></li>
</ul>
<h4 id="local-mutual-information-maximization">Local mutual information maximization</h4>
<ul>
<li>To obtain a representation more suitable for classification, one can maximize the average MI between the high-level representation and local patches of the image.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220505143215455.png" title="fig:" alt="image-20220505143215455" />
<ul>
<li>summarize this local feature map into a global feature</li>
<li>then the MI estimator on global/local pairs, maximizing the average estimated MI: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506133343254.png" alt="image-20220506133343254" /></li>
</ul></li>
</ul>
<h4 id="matching-representation-to-a-prior-distribution">Matching representation to a prior distribution</h4>
<ul>
<li><p>A good representation can be compact, independent, disentangled or independently controllable.</p></li>
<li><p>DIM imposes statistical constraints onto learned representations by implicitly training the encoder so that the push-forward distribution, <span class="math inline">\(\mathrm{\mathbb{U}}_{\psi,\mathrm{\mathbb{P}}}\)</span>, matches a prior, <span class="math inline">\(\mathrm{\mathbb{V}}\)</span>.</p></li>
<li><p>This is done by training a discriminator <span class="math inline">\(D_\phi: \mathcal{Y}\rightarrow \mathrm{\mathbb{R}}\)</span> to estimate the divergence, <span class="math inline">\(\mathcal{D}(\mathrm{\mathbb{V}}|\mathrm{\mathbb{U}}_{\psi,\mathrm{\mathbb{P}}})\)</span>. Then training the encoder to minimize</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506134524556.png" alt="image-20220506134524556" /><figcaption aria-hidden="true">image-20220506134524556</figcaption>
</figure></li>
<li><p>trains the encoder to match the noise implicitly rather than using a priori noise samples as targets</p></li>
</ul>
<h4 id="complete-loss">Complete loss</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220506140132378.png" alt="image-20220506140132378" /><figcaption aria-hidden="true">image-20220506140132378</figcaption>
</figure>
<h3 id="experiments-5">Experiments</h3>
<ul>
<li>Datasets: CIFAR10+CIFAR100, Tiny ImageNet, STL-10, CelebA (a face image dataset )</li>
<li>Compared methods: VAE, <span class="math inline">\(\beta\)</span>-VAE, adversarial AE, BiGAN, NAT, and CPC.</li>
</ul>
<h4 id="evaluate-the-quality-of-a-representation">Evaluate the quality of a representation</h4>
<ul>
<li>Linear separability has no help in showing the representation has high MI with the class labels when the representation is not disentangled.</li>
<li>To measure:
<ul>
<li>They use MINE to more directly measure the MI between the input the the output of the encoder.</li>
<li>NDM (neural dependency measure): Then measure the independence of the representation using a discriminator. train a discriminator to estimate the KL-divergence between the original representations (joint distribution of the factors) and the shuffled representations. The higher the KL-divergence, the more dependent the factors. NDM is sensible and empirically consistent.</li>
</ul></li>
<li>The classification
<ul>
<li>Linear classification</li>
<li>Non-linear classification (with a single hidden layer NN)</li>
<li>Semi-supervised learning: finetuning the encoder by adding a small NN.</li>
<li>MS-SSIM: decoder trained on the L2 Reconstruction loss.</li>
<li>MINE: maximized the DV estimator of the KL-divergence</li>
<li>NDM: using a second discriminator to measure the KL between <span class="math inline">\(E_\psi(x)\)</span> and a batch-wise shuffled version of <span class="math inline">\(E_\psi(x)\)</span>.</li>
</ul></li>
</ul>
<h4 id="representation-learning-comparison-across-models">Representation learning comparison across models</h4>
<ul>
<li>Test DIM(G) the global only, DIM (L) the local only and ablation study.</li>
<li>Classification:
<ul>
<li>DIM(L) outperforms all models. The representations are as good as or better than the raw pixels given the model constraints in this setting</li>
<li>infoNCE tends to perform best, but differences between infoNCE and JSD diminish with larger datasets</li>
<li>Overall DIM only slightly outperforms CPC in this setting, which suggests that the <strong>strictly ordered autoregression of CPC may be unnecessary for some tasks.</strong></li>
</ul></li>
<li>Extended comparison: For MI, DIM combining local and global DIM objectives had very high scores .</li>
<li>Adding coordinate information and occlusions
<ul>
<li>can be interpreted as context prediction and generalizations of inpainting respectively.</li>
<li>For occlusion: occluded part of input for global representations, but no occlusion for local representations. Maximizing MI between occluded global features and unoccluded local features aggressively encourages the global features to encode information which is shared across the entire image.</li>
</ul></li>
</ul>
<h3 id="appendix">Appendix</h3>
<ul>
<li>KL (traditional definition of mutual information) and the JSD have an approximately monotonic relationship. Overall, the distributions with the highest mutual information also have the highest JSD.</li>
<li>We found both infoNCE and the DV-based estimators were sensitive to negative sampling strategies, while the JSD-based estimator was insensitive.</li>
<li>DIM with a local-only objective, DIM(L), learns a representation with a much more interpretable structure across the image.</li>
<li>In general, good classification performance is highly dependent on the local term, <span class="math inline">\(\beta\)</span>, while good reconstruction is highly dependent on the global term, <span class="math inline">\(\alpha\)</span>. the local objective is crucial, the global objective plays a stronger role here than with other datasets.</li>
</ul>
<h2 id="paper-7-contrastive-multiview-coding-cmc-arxiv-2019">Paper 7: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1906.05849&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0s1M_14J8haUyPCJRRILoQ"><em>Contrastive Multiview Coding</em></a> (CMC) ArXiv 2019</h2>
<p>http://github.com/HobbitLong/CMC/</p>
<h3 id="previous-6">Previous</h3>
<ul>
<li>some bits are in fact better than others.</li>
<li>In these models, an input <span class="math inline">\(X\)</span> to the model is transformed into an output <span class="math inline">\(\hat{X}\)</span>, which is supposed to be close to another signal <span class="math inline">\(Y\)</span> (usually in Euclidean space), which itself is related to <span class="math inline">\(X\)</span> in some meaningful way. and provides us with nearly infinite amounts of training data.</li>
<li>The objective functions are usually reconstruction-based loss or contrastive losses.</li>
<li>CPC learns from the past and the future view simultaneously, while Deep InfoMax takes the past as the input and the future as the output. They both use instance discrimination learns to match two sub-crops of the same image.</li>
<li>They extend the objective to the case of more than two views and explore a different set of view definitions, architectures and application settings.</li>
</ul>
<h3 id="what-5">What</h3>
<ul>
<li>Idea: a powerful representation is one that models view-invariant factor. We learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact.</li>
<li>study the setting where the different views are different image channels, such as luminance, chrominance, depth, and optical flow. The fundamental supervisory signal we exploit is the co-occurrence, in natural data, of multiple views of the same scene.</li>
<li>Goal: learn information shared between multiple sensory channels but that are otherwise compact (i.e. discard channel-specific nuisance factors). we learn a feature embedding such that views of the same scene map to nearby points (measured with Euclidean distance in representation space) while views of different scenes map to far apart points.</li>
<li>Use CPC as the backbone but remove the recurrent network part.</li>
<li>We find that the quality of the representation improves as a function of the number of views used for training.</li>
<li>demonstrate that the contrastive objective is superior to cross-view prediction.</li>
</ul>
<h3 id="how-6">How</h3>
<ul>
<li><p>Suppose a dataset of <span class="math inline">\(V_1, V_2\)</span> that consists of a collection of samples <span class="math inline">\(\{v_1 ^i, v_2 ^i\}_{i=1}^N\)</span>, consider $x={v_1 ^i, v_2 ^i} $ as the positive and the <span class="math inline">\(y=\{v_1 ^i, v_2 ^j\}\)</span> as the negatives, aka the positive are the same picture in different views while the negative are the dissimilar images in different views.</p></li>
<li><p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143138612.png" alt="image-20220507143138612" />, simply fix one view and enumerate positives and negatives from the other view, then the objective is written as</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507143236840.png" alt="image-20220507143236840" /><figcaption aria-hidden="true">image-20220507143236840</figcaption>
</figure></li>
<li><p>But directly minimizing the above function is infeasible since <span class="math inline">\(k\)</span> is pretty large. To approximate,</p>
<ul>
<li>Implementing the critic: implement <span class="math inline">\(h_\theta(\cdot)\)</span> as a neural network. For each view, build a NN as the encoder, then compute the features cosine similarity as score and adjust its dynamic range by a hyper-parameter <span class="math inline">\(\tau\)</span>. The two view loss is then <span class="math inline">\(\mathcal{L} (V_1，V_2) = \mathcal{L}_{constrast}^{V_1，V_2}+\mathcal{L}_{constrast}^{V_2，V_1}\)</span>.</li>
<li>Connecting to mutual information: minimizing the objective L maximizes the lower bound on the mutual information. But recent works show that the bound can be very weak.</li>
</ul></li>
<li><p>Multi-views</p>
<ul>
<li>the full graph formulation is that it can handle missing information (e.g. missing views) in a natural manner. <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507144936733.png" alt="image-20220507144936733" /></li>
<li>The core view is <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220507145012695.png" alt="image-20220507145012695" /></li>
</ul></li>
<li><p>To estimate, they use memory bank to store latent features for each training sample.</p></li>
</ul>
<h3 id="experiments-6">Experiments</h3>
<ul>
<li>Benchmarks: ImageNet and STL-10.</li>
</ul>
<h4 id="benchmarking-cmc-on-imagenet">Benchmarking CMC on ImageNet</h4>
<ul>
<li>Convert the RGB images to the Lab image color space and split each image into L and ab channels.</li>
<li>L and ab from the same image are treated as the positive pair, and ab channels from other randomly selected images are treated as a negative pair (for a given L)</li>
<li>set the temperature τ as 0.07 and use a momentum 0.5 for memory update.</li>
<li>learning from luminance and chrominance views in two colorspaces, {L, ab} and {Y, DbDr}.</li>
<li>{Y, DbDr} provides <span class="math inline">\(0.7\%\)</span> improvement, strengthening data augmentation with RandAugment yields better or comparable results to other SOTA methods.</li>
</ul>
<h4 id="cmc-on-videos">CMC on videos</h4>
<ul>
<li>given an image it that is a frame centered at time <span class="math inline">\(t\)</span>, the ventral stream associates it with a neighbouring frame <span class="math inline">\(i_{t+k}\)</span>, while the dorsal stream connects it to optical flow <span class="math inline">\(f_t\)</span> centered at <span class="math inline">\(t\)</span>.</li>
<li>extract <span class="math inline">\(i_t\)</span>, <span class="math inline">\(i_{t+k}\)</span> and <span class="math inline">\(f_t\)</span> from two modalities as three views of a video.
<ul>
<li>Take <span class="math inline">\((i_t,i_{t+k})\)</span> as the positive, and negative pairs for <span class="math inline">\(i_t\)</span> is chosen as a random frame from another randomly chosen video;</li>
<li>Take <span class="math inline">\((i_t,f_t)\)</span> as the positive, then negative pairs for <span class="math inline">\(i_t\)</span> are those flow corresponding to a random frame in another randomly chosen video.</li>
</ul></li>
<li>Pretrain the encoder on UCF101 and use two CaffeNets for extracting features from images and optical flows.</li>
<li>Increasing the number of views of the data from 2 to 3 (using both streams instead of one) provides a boost for UCF-101</li>
</ul>
<h4 id="extending-cmc-to-more-views">Extending CMC to more views</h4>
<ul>
<li>Consider views: luminance (L channel), chrominance (ab channel), depth, surface normal, and semantic labels.</li>
<li>the sub-patch based contrastive objective to increase the number of negative pairs</li>
<li>Does representation quality improve as number of views increases?
<ul>
<li>UNet style architecture</li>
<li>The 2-4 view cases contrast L with ab, and then sequentially add depth and surface normals.</li>
<li>measured by mean IoU over all classes and pixel accuracy.</li>
<li><strong>performance steadily improves as new views are added</strong></li>
</ul></li>
<li>Is CMC improving all views?
<ul>
<li>train these encoders following the full graph paradigm, where each view is contrasted with all other views.</li>
<li>evaluate the representation of each view v by predicting the semantic labels from only the representation of v, where v is L, ab, depth or surface normals.</li>
<li>the full-graph representation provides a good representation learnt for all views.</li>
</ul></li>
</ul>
<h4 id="predictive-learning-vs.-contrastive-learning">Predictive Learning vs. Contrastive Learning</h4>
<ul>
<li>consider three view pairs on the NYU-Depth dataset: (1) L and depth, (2) L and surface normals, and (3) L and segmentation map. For each of them, we train two identical encoders for L, one using contrastive learning and the other with predictive learning.</li>
<li>evaluate the representation quality by training a linear classifier on top of these encoders on the STL-10 dataset</li>
<li>For predictive learning, pixel-wise reconstruction losses usually impose an independence assumption on the modeling. While contrastive learning does not assume conditional independence across dimensions of <span class="math inline">\(v_2\)</span>. Also the use of random jittering and cropping between views allows the contrastive learning approach to benefit from spatial co-occurrence (contrasting in space) in addition to contrasting across views.</li>
</ul>
<h4 id="how-does-mutual-information-affect-representation-quality">How does mutual information affect representation quality?</h4>
<ul>
<li><p><strong>cross-view representation learning is effective because it results in a kind of information minimization</strong>, discarding nuisance factors that are not shared between the views.</p></li>
<li><p>a good collection of views is one that shares some information but not too much</p></li>
<li><p>To test, build two domains: learning representations on images with different colorspaces forming the two views; and learning representations on pairs of patches extracted from an image, separated by varying spatial distance. (use high resolution images to avoid overlapping and cropped patches around boundary.)</p></li>
<li><p>using colorspaces with minimal mutual information give the best downstream accuracy</p></li>
<li><p>For patches with different offset with each other, views with too little or too much MI perform worse.</p></li>
<li><p>the relationship between mutual information and representation quality is meaningful but not direct.</p></li>
<li><p>patch-based contrastive loss is computed within each mini-batch and does not require a memory bank, but usually yields suboptimal results compared to NCE-based contrastive loss, according to our experiments</p></li>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220508144638659.png" alt="image-20220508144638659" /><figcaption aria-hidden="true">image-20220508144638659</figcaption>
</figure></li>
<li><p>combining CMC with the MoCo mechanism or JigSaw branch in PIRL can consistently improve the performance, verifying that they are compatible.</p></li>
</ul>
<h2 id="paper-8-data-efficient-image-recognition-with-contrastive-predictive-coding-cpc-v2-improved-cpc-evaluated-on-limited-labelled-data">Paper 8: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1905.09272&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw1GXq6QkuyYGzUQ2EqiWXnL"><em>Data-Efficient Image Recognition with Contrastive Predictive Coding</em></a> (CPC v2: Improved CPC evaluated on limited labelled data)</h2>
<h3 id="previous-7">Previous</h3>
<ul>
<li><p>CPC only requires in its definition that observations be ordered along e.g. temporal or spatial dimensions. It learns representations by training neural networks to predict the representations of future observations from those of past ones.</p>
<ul>
<li><p>Loss</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509105849555.png" alt="image-20220509105849555" /><figcaption aria-hidden="true">image-20220509105849555</figcaption>
</figure></li>
<li><p>The loss is inspired by NCE, called as InfoNCE.</p></li>
<li><p>The negative samples <span class="math inline">\(\{z_l\}\)</span> are taken from other locations in the image and other images in the mini-batch.</p></li>
</ul></li>
<li><p>AMDIM is most similar to CPC in that it makes predictions across space, but differs in that it also predicts representations across layers in the model.</p></li>
<li><p>For improving data efficiency , one way is label-propogation.</p>
<ul>
<li>a classifier is trained on a subset of labeled data</li>
<li>then used to label parts of the unlabeled dataset</li>
</ul></li>
<li><p>Representation learning and label propagation have been shown to be complementary and can be combined to great effect (Zhai et al., 2019).</p></li>
<li><p>This work focus on representation learning.</p></li>
</ul>
<h3 id="what-6">What</h3>
<ul>
<li>hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable</li>
<li><strong>removing low-level cues which might lead to degenerate solutions.</strong></li>
</ul>
<h3 id="how-7">How</h3>
<ul>
<li>Pretrain the encoder by CPC on local patches, and during test, apply the encoder on the entire image.</li>
<li>Evaluation
<ul>
<li>Linear classification: mean pooling followed by a single linear layer as the classifier. Use cross-entropy loss.</li>
<li>Efficient classification: fix / fine tune the pretrained encoder, and then train the classifier (ResNet-33). Use a smaller learning rate and early-stopping for fine tuning incase the encoder deviates too much from the solution by the CPC objective.</li>
<li>transfer learning: transfer the pretrained encoder to faster-RCNN to do classification. This is a multi-task.</li>
<li>supervised training: directly train classifier by the input data, use cross-entropy loss.</li>
</ul></li>
<li></li>
</ul>
<h3 id="experiments-7">Experiments</h3>
<ul>
<li>Compare <span class="math inline">\(1\%\)</span> supervised DNN based on CPC encoder with Semi-supervised methods and then supervised ResNets.</li>
</ul>
<h4 id="from-cpc-v1-to-cpc-v2">From CPC v1 to CPC v2</h4>
<ul>
<li>Four axes for model capacity
<ul>
<li>increasing depth and width: <span class="math inline">\(+5\%\)</span>. $+2% $ Top-1 accuracy with larger patches.</li>
<li>improves training efficiency by importing layer normalization: can reclaim much of batch normalization’s training efficiency by using layer normalization (<span class="math inline">\(+2\%\)</span> accuracy)</li>
<li>making predictions in all four direction: Additional predictions tasks incrementally increased accuracy (adding bottom-up predictions: +2% accuracy; using all four spatial directions: +2.5% accuracy).</li>
<li>perform patch-based augmentation: ‘color dropping’ (+3% accuracy); adding a fixed, generic augmentation scheme using the primitives from (shearing, rotation etc.), as well as random elastic deformations and color transforms +4.5% accuracy in total.</li>
</ul></li>
</ul>
<h4 id="efficient-image-classification">Efficient image classification</h4>
<ul>
<li>fine-tune the entire stack hψ ◦ fθ for the supervised objective, for a small number of epochs (chosen by cross-validation)</li>
<li>with only 1% of the labels, our classifier surpasses the supervised baseline given 5% of the labels</li>
<li>the family of ResNet-50, -101, and -200 architectures are designed for supervised learning, and their capacity is calibrated for the amount of training signal present in ImageNet labels; larger architectures only run a greater risk of overfitting.</li>
<li>fine-tuned representations yield only marginal gains over fixed ones</li>
<li>we find that CPC provides gains in data efficiency that were previously unseen from representation learning methods, and rival the performance of the more elaborate label-propagation algorithms.</li>
</ul>
<h4 id="transfer-learning-image-detection-on-pascal-voc-2007">Transfer learning: image detection on PASCAL VOC 2007</h4>
<ul>
<li>unsupervised pre-training surpasses supervised pretraining for transfer learning</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<ul>
<li>images are far from the only domain where unsupervised representation learning is important.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20220509121402548.png" alt="image-20220509121402548" /><figcaption aria-hidden="true">image-20220509121402548</figcaption>
</figure>
<h2 id="paper-9-momentum-contrast-for-unsupervised-visual-representation-learning-moco-see-also-moco-v2">Paper 9: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2020%2Fpapers%2FHe_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3wwgkDHILZxygHEqFpue3W"><em>Momentum Contrast for Unsupervised Visual Representation Learning</em></a> (MoCo, see also <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2003.04297&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw0tEX1f7G3quzSTWL6rfgzw"><em>MoCo v2</em></a>)</h2>
<p>Have read MoCo before, here only list MoCo v2.</p>
<h3 id="previous-8">Previous</h3>
<ul>
<li>Momentum Contrast (MoCo) shows that unsupervised pre-training can surpass its ImageNet-supervised counterpart in multiple detection and segmentation tasks.</li>
<li>the negative keys are maintained in a queue, and only the queries and positive keys are encoded in each training batch.</li>
<li>MoCo decouples the batch size from the number of negatives.</li>
<li>SimCLR further reduces the gap in linear classifier performance between unsupervised and supervised pre-training representations.</li>
<li>In an end-to-end mechanism, the negative keys are from the same batch and updated endto-end by back-propagation. SimCLR is based on this mechanism and requires a large batch to provide a large set of negatives.</li>
</ul>
<h3 id="what-7">What</h3>
<ul>
<li>verify the effectiveness of two of SimCLR’s design improvements by implementing them in the MoCo framework</li>
<li>using an MLP projection head and more data augmentation—we establish stronger baselines that outperform SimCLR and do not require large training batches.</li>
</ul>
<h3 id="how-8">How</h3>
<ul>
<li>Evaluation
<ul>
<li>ImageNet linear classification: features are frozen and a supervised linear classifier is trained</li>
<li>Transferring to VOC object detection: a Faster R-CNN detector (C4-backbone) is fine-tuned end-to-end on the VOC 07+12 <em>trainval</em> set and evaluated on the VOC 07 <em>test</em> set using the COCO suite of metrics</li>
</ul></li>
<li>Amending
<ul>
<li>replace the fc head in MoCo with a 2-layer MLP head (hidden layer 2048-d, with ReLU). This MLP only influences the unsupervised training stage; the linear classification or transferring stage does not use this MLP head.</li>
<li>including the blur augmentation because stronger color distortion in <em>A simple framework for contrastive learning of visual representations</em> has diminishing gains in our higher baselines.</li>
</ul></li>
</ul>
<h3 id="experiments-8">Experiments</h3>
<ul>
<li>pre-training with the MLP head improves from 60.6% to 62.9%. in contrast to the big leap on ImageNet, the detection gains are smaller</li>
<li><strong>linear classification accuracy is not monotonically related to transfer performance in detection</strong></li>
<li><strong>large batches are not necessary for good accuracy</strong>, and state-of-the-art results can be made more accessible.</li>
</ul>
<h2 id="paper-10-a-simple-framework-for-contrastive-learning-of-visual-representations-simclr.-icml-2020">Paper 10: <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2002.05709.pdf&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw3nN2zgMpeyBgpXG--pO5Kx"><em>A Simple Framework for Contrastive Learning of Visual Representations</em></a> (SimCLR). ICML 2020</h2>
<h3 id="previous-9">Previous</h3>
<h3 id="what-8">What</h3>
<h3 id="how-9">How</h3>
<h3 id="experiments-9">Experiments</h3>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">paper</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">SSL</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/notes//" class="article-tag-list-link color1">notes</a>
        		</li>
      		
		</ul>
	</div>


      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
  
    <a href="/posts/uncategorized/2021-11-20-tmp-cat.html" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title"></div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-1-stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion"><span class="toc-number">1.</span> <span class="toc-text">Paper 1: Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous"><span class="toc-number">1.1.</span> <span class="toc-text">Previous</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#traditional-classifiers-training-with-noisy-inputs"><span class="toc-number">1.1.1.</span> <span class="toc-text">Traditional classifiers training with noisy inputs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pseudo-likelihood-and-dependency-networks"><span class="toc-number">1.1.2.</span> <span class="toc-text">Pseudo-Likelihood and Dependency Networks</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ideas"><span class="toc-number">1.2.</span> <span class="toc-text">Ideas</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#reasoning-what-makes-good-representations"><span class="toc-number">1.2.1.</span> <span class="toc-text">Reasoning: what makes good representations</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how"><span class="toc-number">1.3.</span> <span class="toc-text">How?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments"><span class="toc-number">1.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-2-revisiting-self-supervised-visual-representation-learning"><span class="toc-number">2.</span> <span class="toc-text">Paper 2: Revisiting Self-Supervised Visual Representation Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-1"><span class="toc-number">2.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what"><span class="toc-number">2.2.</span> <span class="toc-text">What?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-1"><span class="toc-number">2.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-1"><span class="toc-number">2.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-3-zhai-et-al-2019-a-large-scale-study-of-representation-learning-with-the-visual-task-adaptation-benchmark-a-glue-like-benchmark-for-images"><span class="toc-number">3.</span> <span class="toc-text">Paper 3: Zhai et al (2019) A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark (A GLUE-like benchmark for images)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-2"><span class="toc-number">3.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-1"><span class="toc-number">3.2.</span> <span class="toc-text">What？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-2"><span class="toc-number">3.3.</span> <span class="toc-text">How？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-2"><span class="toc-number">3.4.</span> <span class="toc-text">Experiments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#discussion"><span class="toc-number">3.5.</span> <span class="toc-text">Discussion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-4-a-critical-analysis-of-self-supervision-or-what-we-can-learn-from-a-single-image"><span class="toc-number">4.</span> <span class="toc-text">Paper 4: A critical analysis of self-supervision, or what we can learn from a single image</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-3"><span class="toc-number">4.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-2"><span class="toc-number">4.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-3"><span class="toc-number">4.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-3"><span class="toc-number">4.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-5-representation-learning-with-contrastive-predictive-coding-cpc-arxiv-2018"><span class="toc-number">5.</span> <span class="toc-text">Paper 5: Representation Learning with Contrastive Predictive Coding (CPC), ArXiv 2018</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-4"><span class="toc-number">5.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-3"><span class="toc-number">5.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-4"><span class="toc-number">5.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-4"><span class="toc-number">5.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-6-learning-deep-representations-by-mutual-information-estimation-and-maximization-dim-deep-infomax-iclr-2019"><span class="toc-number">6.</span> <span class="toc-text">Paper 6: Learning deep representations by mutual information estimation and maximization (DIM Deep InfoMax ) ICLR 2019</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-5"><span class="toc-number">6.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-4"><span class="toc-number">6.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-5"><span class="toc-number">6.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mutual-information-estimation-and-maximization"><span class="toc-number">6.3.1.</span> <span class="toc-text">Mutual information estimation and maximization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#local-mutual-information-maximization"><span class="toc-number">6.3.2.</span> <span class="toc-text">Local mutual information maximization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#matching-representation-to-a-prior-distribution"><span class="toc-number">6.3.3.</span> <span class="toc-text">Matching representation to a prior distribution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#complete-loss"><span class="toc-number">6.3.4.</span> <span class="toc-text">Complete loss</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-5"><span class="toc-number">6.4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#evaluate-the-quality-of-a-representation"><span class="toc-number">6.4.1.</span> <span class="toc-text">Evaluate the quality of a representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#representation-learning-comparison-across-models"><span class="toc-number">6.4.2.</span> <span class="toc-text">Representation learning comparison across models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#appendix"><span class="toc-number">6.5.</span> <span class="toc-text">Appendix</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-7-contrastive-multiview-coding-cmc-arxiv-2019"><span class="toc-number">7.</span> <span class="toc-text">Paper 7: Contrastive Multiview Coding (CMC) ArXiv 2019</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-6"><span class="toc-number">7.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-5"><span class="toc-number">7.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-6"><span class="toc-number">7.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-6"><span class="toc-number">7.4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#benchmarking-cmc-on-imagenet"><span class="toc-number">7.4.1.</span> <span class="toc-text">Benchmarking CMC on ImageNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cmc-on-videos"><span class="toc-number">7.4.2.</span> <span class="toc-text">CMC on videos</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#extending-cmc-to-more-views"><span class="toc-number">7.4.3.</span> <span class="toc-text">Extending CMC to more views</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#predictive-learning-vs.-contrastive-learning"><span class="toc-number">7.4.4.</span> <span class="toc-text">Predictive Learning vs. Contrastive Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#how-does-mutual-information-affect-representation-quality"><span class="toc-number">7.4.5.</span> <span class="toc-text">How does mutual information affect representation quality?</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-8-data-efficient-image-recognition-with-contrastive-predictive-coding-cpc-v2-improved-cpc-evaluated-on-limited-labelled-data"><span class="toc-number">8.</span> <span class="toc-text">Paper 8: Data-Efficient Image Recognition with Contrastive Predictive Coding (CPC v2: Improved CPC evaluated on limited labelled data)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-7"><span class="toc-number">8.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-6"><span class="toc-number">8.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-7"><span class="toc-number">8.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-7"><span class="toc-number">8.4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#from-cpc-v1-to-cpc-v2"><span class="toc-number">8.4.1.</span> <span class="toc-text">From CPC v1 to CPC v2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#efficient-image-classification"><span class="toc-number">8.4.2.</span> <span class="toc-text">Efficient image classification</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transfer-learning-image-detection-on-pascal-voc-2007"><span class="toc-number">8.4.3.</span> <span class="toc-text">Transfer learning: image detection on PASCAL VOC 2007</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#conclusion"><span class="toc-number">8.5.</span> <span class="toc-text">Conclusion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-9-momentum-contrast-for-unsupervised-visual-representation-learning-moco-see-also-moco-v2"><span class="toc-number">9.</span> <span class="toc-text">Paper 9: Momentum Contrast for Unsupervised Visual Representation Learning (MoCo, see also MoCo v2)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-8"><span class="toc-number">9.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-7"><span class="toc-number">9.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-8"><span class="toc-number">9.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-8"><span class="toc-number">9.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-10-a-simple-framework-for-contrastive-learning-of-visual-representations-simclr.-icml-2020"><span class="toc-number">10.</span> <span class="toc-text">Paper 10: A Simple Framework for Contrastive Learning of Visual Representations (SimCLR). ICML 2020</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-9"><span class="toc-number">10.1.</span> <span class="toc-text">Previous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-8"><span class="toc-number">10.2.</span> <span class="toc-text">What</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-9"><span class="toc-number">10.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#experiments-9"><span class="toc-number">10.4.</span> <span class="toc-text">Experiments</span></a></li></ol></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>



  
  
  

  

  

  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2022 Mia Feng
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


</footer>

    </div>
    <script>
	var yiliaConfig = {
		mathjax: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">posts</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">papers</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">about</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">notes</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Sensor</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">GPR</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">summer school</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">mathematics</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">cnn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">compress</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">infrared</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">hyperspectral</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">blog</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">applications</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">tendency</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">logistic</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">da</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">ML</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">naive</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">book</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">gcn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">data assimilation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">machine learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">math</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">paper</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SSL</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">survey</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">anomaly</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">skeleton</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">cv</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">3d shape</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">fall</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">remote sensing images</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">interpolation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">compression</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">reconstruction</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">AI</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Data assimilation</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>wind interpolation</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">&lt;br&gt; &amp;#160; &amp;#160; &amp;#160; &amp;#160;Mia, &lt;/br&gt;&lt;br&gt; &amp;#160; &amp;#160; &amp;#160; &amp;#160;a master student in NUDT.&lt;/br&gt; &lt;br&gt;  &amp;#160; &amp;#160; &amp;#160; &amp;#160;I love reading, exercising and cooking. &lt;br /&gt; &lt;br&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;Science fictions, detective novels are my favorite.&lt;/br&gt;</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>