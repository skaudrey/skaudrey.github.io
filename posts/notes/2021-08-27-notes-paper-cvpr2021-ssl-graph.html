<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <link rel="dns-prefetch" href="//cdn.bootcss.com" />
  <link rel="dns-prefetch" href="//cdn.mathjax.org" />
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>Papers of CVPR2021, SSL and graph. | Mia&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper 1: Jigsaw Clustering for Unsupervised Visual Representation Learning Paper 2: Self-supervised Motion Learning from Static Images Paper 3: Self-supervised Video Representation Learning by Con">
<meta property="og:type" content="article">
<meta property="og:title" content="Papers of CVPR2021, SSL and graph.">
<meta property="og:url" content="http://yoursite.com/posts/notes/2021-08-27-notes-paper-cvpr2021-ssl-graph.html">
<meta property="og:site_name" content="Mia&#39;s Blog">
<meta property="og:description" content="Paper 1: Jigsaw Clustering for Unsupervised Visual Representation Learning Paper 2: Self-supervised Motion Learning from Static Images Paper 3: Self-supervised Video Representation Learning by Con">
<meta property="og:locale">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827135623377.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827151850175.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172717585.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172910767.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827174808696.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181344187.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181534056.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827225903818.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901091727352.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901093537848.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901130744146.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901141700865.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901142059270.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901152032964.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901225711236.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901230258180.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902113314940.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902120453628.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902133618545.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151200499.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151226323.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902160626221.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902164243961.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903184316148.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232718696.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232735446.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232837194.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903234025866.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210904162325031.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905202700652.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905223900318.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906193605112.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194329698.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194519932.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908111357002.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908112654724.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120140074.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120117673.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910122543720.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130142797.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130231035.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130246505.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131617208.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131811218.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910132656530.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160437825.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160719249.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913111201445.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913112252883.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913131247274.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913133851127.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913135843608.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914123913163.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914141059850.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914142505161.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914160553376.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914165125847.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914170432752.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914171121061.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210915222052643.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210917161308152.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210917161807256.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210918084309019.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919095719228.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919142229296.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919153758111.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919154140301.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919154511695.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920134338763.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920154704931.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920200555554.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920200744561.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920200927480.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920201450434.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920202809962.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920202919023.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921124235527.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921154102468.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921151131234.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921153951384.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922115927078.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922121759262.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922123207545.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922123705808.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922134834006.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922205836991.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922210144596.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923134844909.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923135511275.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923144941732.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923162244368.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923163653685.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923171508369.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924142204794.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924222135548.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924185808927.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924222304571.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926103840998.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926115930588.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926121026487.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927135417172.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927135237680.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926221807606.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926224512111.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926233345530.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927200926592.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927203812177.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928123213933.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928124909064.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928130901448.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928142136488.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928143615211.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928172156560.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928181830196.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928191439028.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928192439836.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211014151336939.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211014165232695.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211017124211442.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211017150147461.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211017221640577.png">
<meta property="article:published_time" content="2021-08-27T15:16:00.000Z">
<meta property="article:modified_time" content="2021-11-30T04:27:48.378Z">
<meta property="article:author" content="Mia Feng">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="SSL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827135623377.png">
  
    <link rel="alternative" href="/atom.xml" title="Mia&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: #4d4d4d;
    }
  </style>
  

  

<meta name="generator" content="Hexo 5.3.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/assets/img/avatar.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/posts/uncategorized/2019-06-13-about.html">about</a></li>
	        
				<li><a href="/categories/notes">notes</a></li>
	        
				<li><a href="/categories/projects">projects</a></li>
	        
				<li><a href="/categories/talks">talks</a></li>
	        
				<li><a href="/categories/meetings">meetings</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">posts</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">papers</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">about</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/skaudrey" title="github"><i class="icon-github"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:skaudreymia@gmail.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/assets/img/avatar.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/skaudrey" title="github"><i class="icon-github"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:skaudreymia@gmail.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 20%"><a href="/posts/uncategorized/2019-06-13-about.html">about</a></li>
		        
					<li style="width: 20%"><a href="/categories/notes">notes</a></li>
		        
					<li style="width: 20%"><a href="/categories/projects">projects</a></li>
		        
					<li style="width: 20%"><a href="/categories/talks">talks</a></li>
		        
					<li style="width: 20%"><a href="/categories/meetings">meetings</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-notes-paper-cvpr2021-ssl-graph" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Papers of CVPR2021, SSL and graph.
    </h1>
  

        
        <a href="/posts/notes/2021-08-27-notes-paper-cvpr2021-ssl-graph.html" class="archive-article-date">
  	<time datetime="2021-08-27T15:16:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-08-27</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Paper 1: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00323.pdf">Jigsaw Clustering for Unsupervised Visual Representation Learning</a></li>
<li>Paper 2: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00240.pdf">Self-supervised Motion Learning from Static Images</a></li>
<li>Paper 3: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00862.pdf">Self-supervised Video Representation Learning by Context and Motion Decoupling</a></li>
<li>Paper 4: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.11487.pdf">Skip-convolutions for Efficient Video Processing</a></li>
<li>Paper 5: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09496.pdf">Temporal Query Networks for Fine-grained Video Understanding</a></li>
<li>Paper 6: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></li>
<li>Paper 7:<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Bi-GCN_Binary_Graph_Convolutional_Network_CVPR_2021_paper.pdf">Bi-GCN: Binary Graph Convolutional Network</a></li>
<li>Paper 8: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.09711.pdf">An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</a></li>
<li>Paper 9: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.03087">Cascade Graph Neural Networks for RGB-D Salient Object Detection</a></li>
<li>Paper 10: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01302">Coarse-Fine Networks for Temporal Activity Detection in Videos</a></li>
<li>Paper 11: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.03851">CoCoNets: Continuous Contrastive 3D Scene Representations</a></li>
<li>Paper 12: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.04015">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</a></li>
<li>Paper 13: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.03662">Discriminative Latent Semantic Graph for Video Captioning</a></li>
<li>Paper 14: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.02183?context=cs">Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</a></li>
<li>Paper 15: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.10566">Exploring simple siamese representation learning</a></li>
<li>Paper 16: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.05475">GiT: Graph Interactive Transformer for Vehicle Re-identification</a></li>
<li>Paper 17: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01730">Graph-Time Convolutional Neural Networks</a></li>
<li>Paper 18: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02370">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</a></li>
<li>Paper 19: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.09787.pdf">Group Contrastive Self-Supervised Learning on Graphs</a></li>
<li>Paper 20: <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s10618-021-00750-y">Homophily outlier detection in non-IID categorical data</a></li>
<li>Paper 21: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.02113">Hyperparameter-free and Explainable Whole Graph Embedding</a></li>
<li>Paper 22: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.01000">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</a></li>
<li>Paper 23: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.12609">Iterative graph self-distillation</a></li>
<li>Paper 24: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.17260">Learning by Aligning Videos in Time</a></li>
<li>Paper 25: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.13125">Learning graph representation by aggregating subgraphs via mutual information maximization</a></li>
<li>Paper 26: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.09612">Mile: A multi-level framework for scalable graph embedding</a></li>
<li>Paper 27: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.03400.pdf">Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</a></li>
<li>Paper 28: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.02639">Multi-Level Graph Contrastive Learning</a></li>
<li>Paper 29: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.09856">Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</a></li>
<li>Paper 30: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.04575">PiNet: Attention Pooling for Graph Classification</a></li>
<li>Paper 31: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.04113">Self-supervised Graph-level Representation Learning with Local and Global Structure</a></li>
<li>Paper 32: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.09111">Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</a></li>
<li>Paper 33: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.01903">SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</a></li>
<li>Paper 34: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.14613">Space-time correspondence as a contrastive random walk</a></li>
<li>Paper 35: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.06122">Spatially consistent representation learning</a></li>
<li>Paper 36: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.03800">Spatiotemporal contrastive video representation learning</a></li>
<li>Paper 37: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.11452">SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</a></li>
<li>Paper 38: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.13033">SSAN: Separable Self-Attention Network for Video Representation Learning</a></li>
<li>Paper 39: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411953">tdgraphembed: Temporal dynamic graph-level embedding</a></li>
<li>Paper 40: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf">Videomoco: Contrastive video representation learning with temporally adversarial examples</a></li>
<li>Paper 41: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.01181">Visual Relationship Forecasting in Videos</a></li>
<li>Paper 42: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=AAes_3W-2z">Wasserstein embedding for graph learning</a></li>
<li>Paper 43: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.07491">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning</a></li>
<li>Paper 44: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.08482">Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning</a></li>
<li>Paper 45: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.16516">Recognizing Actions in Videos from Unseen Viewpoints</a></li>
<li>Paper 46: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.13537">Shot Contrastive Self-Supervised Learning for Scene Boundary Detection</a></li>
</ul>
<a id="more"></a>
<h2 id="paper-1-jigsaw-clustering-for-unsupervised-visual-representation-learning">Paper 1: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00323.pdf">Jigsaw Clustering for Unsupervised Visual Representation Learning</a></h2>
<p>Codes: https://github.com/dvlab-research/JigsawClustering</p>
<h3 id="previous-pretext-task">Previous pretext task</h3>
<ul>
<li>Intra-image tasks: including colorization and jigsaw puzzle, design a transform of one image and train the network to learn the transform.
<ul>
<li>Since only the training batch itself is forwarded each time, they name these methods as single -batch methods.</li>
<li>Can be achieved using only one image's information, limiting the learning ability of feature extractors.</li>
</ul></li>
<li>Inter-image tasks
<ul>
<li>require the network to discriminate among different images.</li>
<li>Try to reduce the distance between representations of positive pairs and enlarge the distance between representations of negative samples.</li>
<li>since each training batch and its augmented version are forwarded simultaneously, methods are named as dual-batches methods.</li>
</ul></li>
<li>The way to design an efficient single-batch based method with similar performance to dual-batches methods is still an open problem.</li>
</ul>
<h3 id="ideas">Ideas</h3>
<p>They propose a framework for efficient training of unsupervised models using Jigsaw clustering, which combines advantages of solving jigsaw puzzles and contrastive learning, and makes use of both intra- and inter-image information to guide feature extractor.</p>
<h4 id="jigsaw-clustering-task">Jigsaw Clustering task</h4>
<ul>
<li>Every image in a batch is split into different patches. They are randomly permuted and stitched to form a new batch for training.</li>
<li><strong>Goal</strong> : recover the disrupted parts back to the original images.</li>
<li>The patches are permuted in a batch</li>
<li>The network has to distinguish between different parts of one image and identifies their original positions to recover the original image from multiple montage input images.</li>
<li><strong><em>Why works?</em></strong>
<ul>
<li>Discriminating among different patches in one stitched image forces the model to <em>capture instance-level information inside an image</em>. This level of feature is missing in general in other contrastive learning methods.</li>
<li>Clustering different patches from multiple input images helps the model <em>learn image-level features across images.</em></li>
<li>arranging every patch to the correct location requires detailed location information, which was considered in single-batch methods.</li>
</ul></li>
</ul>
<h3 id="how">How?</h3>
<ul>
<li><p>Batches</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827135623377.png" alt="image-20210827135623377" style="zoom:80%;" /></p>
<p>Each batch will have <span class="math inline">\(n\)</span> images, and after splitting <strong>(patches split in images have a level of overlap)</strong>, there will be <span class="math inline">\(n\times m\times m\)</span> patches, the patches are latterly stitched into <span class="math inline">\(n\)</span> images. The cluster branch will cluster the <span class="math inline">\(n\times m\times m\)</span> patches into <span class="math inline">\(n\)</span> classes so as to define which original image that one patch comes from.</p>
<ul>
<li>Using montage images as input instead of every single patch is noteworthy, since directly using small patches as input leads to the solution with only global information.</li>
<li>the input images form only one batch with the same size as the original batch, which costs half of resource during training compared with recent methods.</li>
<li><strong><em>The choice of <span class="math inline">\(m\)</span> affects the difficulty of the task</em></strong>. They show that <span class="math inline">\(m=2\)</span> is good.</li>
</ul></li>
<li><p>Network</p>
<ul>
<li>The logits is in size <span class="math inline">\(n\times m \times m\)</span>. (in location branch)</li>
</ul></li>
<li><p>Loss function</p>
<ul>
<li><p>The target of clustering is pulling together objects from the same class and pushing away patches from different classes.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827151850175.png" alt="image-20210827151850175" /><figcaption aria-hidden="true">image-20210827151850175</figcaption>
</figure></li>
<li><p>The loss function of location branch is simply cross-entropy loss.</p></li>
<li><p>The final objective is the weighted summation of the two losses mentioned above. But in their experiments, when the two loss are simply summed, they get the best result.</p></li>
</ul></li>
</ul>
<h2 id="paper-2-self-supervised-motion-learning-from-static-images">Paper 2: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00240.pdf">Self-supervised Motion Learning from Static Images</a></h2>
<h3 id="why">Why</h3>
<ul>
<li>To well distinguish actions, correctly locating the prominent motion areas is of crucial importance.</li>
</ul>
<h3 id="previous-work">Previous work</h3>
<ul>
<li>Motion learning by architectures: two-stream networks and 3D convolutional networks. The two-stream networks extract motions representations explicitly from optical flows, while 3D structures apply convolutions on the temporal dimension or space-time cubics to extract motion cues implicitly.</li>
<li>Self-supervised image representation learning: patch-based approaches , image-level pretext tasks such as image inpainting, image colorization, motion segment prediction and predicting image rotations.</li>
<li>Self-supervised video representation learning: extend patch-based context prediction to spatial-temporal scenarios, e.g., spatio-temporal puzzles, video cloze procedure and frame/clip order prediction; learn representations by predicting future frames; generate supervision signals, such as speed up prediction and play back rate prediction.</li>
</ul>
<h3 id="how-1">How</h3>
<h4 id="idea">Idea</h4>
<ul>
<li>Learn <strong>M</strong>otion from <strong>S</strong>tatic <strong>I</strong>mages (MoSI), take images as our data source, and generate deterministic motion patterns.</li>
<li>Given the desired direction and the speed of the motions, MoSI generates pseudo motions from static images. By correctly classifying the direction and speed of the movement in the image sequence, models trained with MoSI is able to well encode motion patterns.</li>
<li>Furthermore, a static mask is applied to the pseudo motion sequences. This produces inconsistent motions between the masked area and the unmasked one, which guides the network to focus on the inconsistent local motions</li>
</ul>
<h4 id="motion-learning-from-static-images">Motion learning from static images</h4>
<h5 id="pseudo-motions">Pseudo motions</h5>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172717585.png" alt="image-20210827172717585" /><figcaption aria-hidden="true">image-20210827172717585</figcaption>
</figure>
<ul>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172910767.png" alt="image-20210827172910767" /><figcaption aria-hidden="true">image-20210827172910767</figcaption>
</figure></li>
<li><p>Label pool:</p>
<ul>
<li>For each label, a non-zero speed only exists on one axis.</li>
</ul></li>
<li><p>Pseudo motion generation</p>
<ul>
<li><p>To generate the samples with different speeds, the moving distance from the start to the end of the Pseudo sequences are need.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827174808696.png" alt="image-20210827174808696" /><figcaption aria-hidden="true">image-20210827174808696</figcaption>
</figure></li>
<li><p>The start location is randomly sampled from a certain area which ensures the end location is located completely within the source image.</p></li>
<li><p>For label <span class="math inline">\((x, y) = (0, 0)\)</span>, where the sampled image sequence is static on both axis, the start location is selected from the whole image with uniform distribution.</p></li>
</ul></li>
<li><p>Classification</p>
<ul>
<li>each batch contains all transformed image sequences generated from the same source image</li>
<li>The model is trained by cross entropy loss.</li>
</ul></li>
</ul>
<h5 id="static-masks">Static masks</h5>
<ul>
<li>The static masks creates local motion patterns that are inconsistent with the background.</li>
<li>introduce static masks as the second core component of the proposed MoSI since the model may possibly focus on just several pixels.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181344187.png" title="fig:" alt="image-20210827181344187" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181534056.png" title="fig:" alt="image-20210827181534056" /></li>
<li>the model is now required not only to recognize motion patterns, but also to spot where the motion is happening</li>
</ul>
<h5 id="implementation">Implementation</h5>
<ul>
<li>Data preparations: the source images need to be first sampled from the videos in the video datasets.</li>
<li>Augmentation: randomize the location and the size of the unmasked area. In addition, randomize the selection of the background frames in the MoSI.</li>
</ul>
<h2 id="paper-3-self-supervised-video-representation-learning-by-context-and-motion-decoupling">Paper 3: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00862.pdf">Self-supervised Video Representation Learning by Context and Motion Decoupling</a></h2>
<h3 id="why-1">Why</h3>
<ul>
<li>What to learn ?
<ul>
<li>Context representation can be used to classify certain actions, but also leads to background bias.</li>
<li>Motion representation</li>
</ul></li>
<li>Problem
<ul>
<li>The source of supervision: video in compressed format (such as MPEG-4) roughly decouples the context and motion information in its I-frames and motion vectors.
<ul>
<li>I-frames can represent relatively static and coarse-grained context information, while motion vectors depict dynamic and fine-grained movements</li>
</ul></li>
</ul></li>
</ul>
<h3 id="previous-work-1">Previous work</h3>
<ul>
<li>SS video representation learning
<ul>
<li>video specific pretext tasks: estimating video playback rates, verifying temporal order of clips, predicting video rotations, solving space-time cubic puzzles, and dense predictive coding.</li>
<li>Contrastive learning
<ul>
<li>clips from the same video are pulled together while clips from different videos are pushed away.</li>
<li>employ adaptive cluster assignment, where the representation and embedding clusters are simultaneously learned.</li>
<li>But they may suffer from the context bias problem.</li>
</ul></li>
<li>mutual supervision across modalities</li>
<li>DSM: enhance the learned video representation by decoupling the scene and the motion. It simply changes the construction of positive and negative pairs in contrastive learning.</li>
</ul></li>
<li>Action recognition in compressed videos
<ul>
<li>Video compression techniques (e.g., H.264 and MPEG4) usually store only a few key frames completely, and reconstruct other frames using motion vectors and residual errors from the key frames.</li>
<li>Some methods directly build models on the compressed data.
<ul>
<li>One replace the optical flow stream in two-stream action recognition models with a motion vector stream.</li>
<li>CoViAR: use all modalities, including I-frames, motion vectors and residuals.</li>
</ul></li>
</ul></li>
<li>Motion prediction
<ul>
<li>deduce the states of an object in a near future.</li>
<li>Typical models: RNNs, Transformers, and GNNs.</li>
</ul></li>
</ul>
<h3 id="goal">Goal</h3>
<p>Design a self-supervised video representation learning method that jointly learns motion prediction and context matching.</p>
<ul>
<li>The context matching task aims to give the video network a rough grasp of the environment in which actions take place. It casts a NCE loss between global features of video clips and I-frames, where clips and I-frames from the same videos are pulled together, while those from different videos are pushed away.</li>
<li>The motion prediction task requires the model to predict pointwise motion dynamics in a near future based on visual information of the current clip.
<ul>
<li>They use pointwise contrastive learning to compare predicted and real motion features at every spatial and temporal location <span class="math inline">\((x,y,t)\)</span>, which will lead to more stable pretraining and better transferring performance.</li>
<li>It works as a strong regularization for video networks, and it can also be regarded as an auxiliary task clearly improves the performance of supervised action recognition.</li>
</ul></li>
</ul>
<h3 id="how-2">How</h3>
<ul>
<li>Data: compressed videos, to be exactly, MPEG-2 Part2, where every I-frames is followed by 11 consecutive P-frames.</li>
<li>Methods: context matching task for coarse-grained and relatively static context representation, and a motion prediction task for learning fine-grained and high-level motion representation.
<ul>
<li>context matching
<ul>
<li>where (video clip, I-frame) pairs from the same videos are pulled together, while pairs from different videos are pushed away</li>
<li></li>
</ul></li>
<li>Motion prediction
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827225903818.png" title="fig:" alt="image-20210827225903818" /></li>
<li>Only feature points corresponding to the same video <span class="math inline">\(i\)</span> and at the same spatial and temporal position <span class="math inline">\((x, y, t)\)</span> are regarded as positive pairs, otherwise they are regarded as negative pairs.</li>
<li>The input and output for Transformer is considered as a 1-D sequence.</li>
<li>Some findings
<ul>
<li>Predicting future motion information leads to significantly better video retrieval performance compared with estimating current motion information;</li>
<li>Matching predicted and “groundtruth” motion features using the pointwise InfoNCE loss brings better results than directly estimating motion vector values;</li>
<li>Different encoder-decoder networks lead to similar results, while using Transformer performs slightly better.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h2 id="paper-4-skip-convolutions-for-efficient-video-processing">Paper 4: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.11487.pdf">Skip-convolutions for Efficient Video Processing</a></h2>
<p>Codes: https://github.com/Qualcomm-AI-research/Skip-Conv</p>
<h3 id="why-2">Why</h3>
<ul>
<li>Leverage the large amount of redundancies in video streams and save computations</li>
<li>The spiking nets is lack of efficient training algorithms</li>
<li>Residual frames provide a strong prior on the relevant regions, easing the design of effective gating functions</li>
</ul>
<h3 id="previous-work-2">Previous work</h3>
<ul>
<li>Efficient video models
<ul>
<li>feature propagation, which computes the expensive backbone features only on key-frames.</li>
<li>interleave deep and shallow backbones between consecutive frames: methods are mostly suitable for global prediction tasks where a single prediction is made for the whole clip.</li>
</ul></li>
<li>Efficient image models: The reduction of parameter redundancies
<ul>
<li>model compression: Skip-Conv leverages temporal redundancies in activations.</li>
<li>conditional computations in developing efficient models for images.</li>
</ul></li>
</ul>
<h3 id="goal-1">Goal</h3>
<p>To speed up any convolutional network for inference on video streams. Considering a video as a series of changes across frames and network activations, denotes as residual frames. They reformulate standard convolution to be efficiently computed over such residual frames by limiting the computation only to the regions with significant changes while skipping the others. The important residuals are learned by a gating function.</p>
<h3 id="how-3">How</h3>
<ul>
<li>The contributions
<ul>
<li>a simple reformulation of convolution, which computes features on highly sparse residuals instead of dense video frames</li>
<li>Two gating functions, Norm gate and Gumbel gate, to effectively decide whether to process or skip each location, where Gumbel gate is trainable.</li>
</ul></li>
</ul>
<h4 id="skip-convolutions">Skip Convolutions</h4>
<ul>
<li><p>Convolutions on residual frames</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901091727352.png" title="fig:" alt="image-20210901091727352" /></li>
<li>for every kernel support filled with zero values in <span class="math inline">\(\mathrm{r}_t\)</span>, the corresponding output will be trivially zero, and the convolution can be skipped by copying values from <span class="math inline">\(\mathrm{z}_{t−1}\)</span> to <span class="math inline">\(\mathrm{z}_{t}\)</span>.</li>
<li><em>Introduce a gating function for each convolutional layer to predict a binary mask indicating which locations should be processed, and taking only <span class="math inline">\(\mathrm{r}_t\)</span> as input.</em> <span class="math inline">\(\mathrm{r}_t\)</span> as input will provide a strong prior to the gating function.</li>
</ul></li>
<li><p>Gating functions</p>
<ul>
<li><p>Norm gate: decides to skip a residual if its magnitude (norm) is small enough, not learnable</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901093537848.png" title="fig:" alt="image-20210901093537848" /></li>
<li>indicate regions that change significantly across frames, but not all changes are equally important for the final prediction.</li>
</ul></li>
<li><p>Gumbel gate, trainable with the convolutional kernels.</p>
<ul>
<li><p>A higher efficiency can be gained by introducing a higher</p></li>
<li><p>pixel-wise Bernoulli distributions by applying a sigmoid function. During training, sample binary deisions from the Bernoulli distribution.</p></li>
<li><p>Employ the Gumbel reparametrization and a straight-through gradient estimator in order to backpropagate through the sampling procedure.</p></li>
<li><p>The Gating parameters are learned jointly with all model parameters by minimizing <span class="math inline">\(\mathcal{L}_{task}+\beta \mathcal{L}_{gate}\)</span>.</p></li>
<li><p>The gating loss is defined as the average multiply-accumulate (MAC) count needed to process <span class="math inline">\(T\)</span> consecutive frames as</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901130744146.png" alt="image-20210901130744146" style="zoom:80%;" /></p></li>
<li><p>train the model over a fixed-length of frames and do inference iteratively on an binary number of frames.</p></li>
<li><p>By simply adding a downsampling and an unsampling function on the predicted gates, the Skip-conv can be extended to generate structured sparsity. This structure will enable more efficient implementation with minimal effect on performance.</p></li>
</ul></li>
</ul></li>
<li><p>Generalization and future work</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901141700865.png" title="fig:" alt="image-20210901141700865" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901142059270.png" title="fig:" alt="image-20210901142059270" /></li>
</ul></li>
</ul>
<h2 id="paper-5-temporal-query-networks-for-fine-grained-video-understanding">Paper 5: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09496.pdf">Temporal Query Networks for Fine-grained Video Understanding</a></h2>
<p>Codes: http://www.robots.ox.ac.uk/~vgg/research/tqn/</p>
<h3 id="why-3">Why</h3>
<ul>
<li>For finer-grain classification which depends on subtle differences in pose, the specific sequence, duration and number of certain subactions, it requires reasoning about events at varying temporal scales and attention to fine details.</li>
<li>the constraints imposed by finite GPU memory. To overcome this, one way is to use pretrained features, but this relies on good initializations and ensures a small domain gap. Another solution focuses on extracting key frames from untrimmed videos.</li>
<li>VQA (visual question and answering)
<ul>
<li>Have queries which attends to relevant features for predicting the answers.</li>
<li>The problem in this paper is more interested in a common set of queries shared across the whole dataset.</li>
</ul></li>
</ul>
<h3 id="goal-2">Goal</h3>
<ul>
<li><p>Fine-grained classification of actions in untrimmed videos.</p></li>
<li><p>Propose a Transformer-based video network, namely the Temporal Query Network (TQN) for fine-grained action classification, which will take a video and a predefined set of queries as input and output responses for each query, where the response is query dependent.</p></li>
<li><p>The queries act as "experts" that are able to pick out from the video the temporal segments required for their response.</p>
<ul>
<li>Pick out relevant temporal segments and ignore irrelevant segments.</li>
<li>Since only relevant segments will help in classification, the excessive temporal aggregation may lose the signal in the noise.</li>
</ul></li>
<li><p>Introduce a stochastically updated feature bank to solve memory constraints.</p>
<ul>
<li>features from densely sampled contiguous temporal segments are cached over the course of training,</li>
<li>only a random subset of these features is computed online and backpropagated through in each training iteration</li>
</ul></li>
</ul>
<h3 id="how-4">How</h3>
<p>Train with weak supervision, meaning that at training time the temporal location information for the response is not proposed.</p>
<h4 id="tqn">TQN</h4>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901152032964.png" alt="image-20210901152032964" style="zoom:50%;" /></p>
<ul>
<li>Identifies rapidly occurring discriminative events in untrimmed videos and can be trained given only weak supervision.</li>
<li>Achieves by learning a set of permutation-invariant query vectors corresponding to predefined queries about events and their attributes, which are transformed into response vectors using Transformer decoder layers attending to visual features extracted from a 3DCNN backbone.</li>
<li>Given an untrimmed video, first visual features for <em>contiguous non-overlapping clips of 8 frames</em> are extracted using a 3D ConvNet.</li>
<li>multiple layers of a parallel non-autoregressive Transformer decoder</li>
<li>The training loss is a multi-task combination of individual classifier losses, which are Softmax cross-entropy, where the labels are the ground-truth attribute for the label query.</li>
</ul>
<h4 id="stochastically-updated-feature-bank">Stochastically updated feature bank</h4>
<ul>
<li>The memory bank caches the clip-level 3DCNN visual features.</li>
<li>In each training iteration, a fixed number <span class="math inline">\(n_{online}\)</span> of randomly samples consecutive clips are forwarded through the visual encoder, while the remaining <span class="math inline">\(t-n_{online}\)</span> clip features are retrieved from the memory bank.</li>
<li>The two sets mentioned above are then combined and input into the TQN decoder for final prediction and backpropagation.</li>
<li>During inference, all features are computed online without the memory bank.</li>
<li>Advantages: fixed number of clips to reduce the memory price. Also enables to extend temporal context and promotes diversity in each mini-batch as multiple different videos can be included instead of just a single long video.</li>
</ul>
<h4 id="factorizing-categories-into-attribute-queries">Factorizing categories into attribute queries</h4>
<ul>
<li>This factorization unpacks the monolithic category labels into their semantic constituents</li>
<li>The categories are factorized into multiple queries that with several attributes respectively.</li>
</ul>
<h4 id="implementation-1">Implementation</h4>
<ul>
<li>Use S3D as visual backbone, operating on non-overlapping contiguous video clips of 8 frames.</li>
<li>The decoder consists of 4 standard post-normalization Transformer decoder layers, each with 4 attention heads.</li>
<li>The visual encoder is pre-trained on Kinetics-400.</li>
</ul>
<h2 id="paper-6-unsupervised-disentanglement-of-linear-encoded-facial-semantics">Paper 6: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></h2>
<h3 id="why-4">Why</h3>
<ul>
<li>Sampling along the linear-encoded representation vector in latent space will change the associated facial semantics accordingly.</li>
<li>Current frameworks that maps a particular facial semantics to a latent representation vector relies on training offline classifiers with manually labeled datasets. Therefore they require artificially defined semantics and provide the associated labels for all facial images. If training with labeled facial semantics:
<ul>
<li>They demand extra effort on human annotations for each new attributes proposed</li>
<li>Each semantics is defined artificially</li>
<li>unable to give any insights on the connections among different semantics</li>
</ul></li>
<li>Previous work
<ul>
<li>Synthesizing faces by GAN, which changes the target attribute but keep other information ideally unchanged.
<ul>
<li>The comprehensive design of loss functions.</li>
<li>the involvement of additional attribute features</li>
<li>the architecture design</li>
</ul></li>
<li>To achieve meaningful representations, one should always introduce either supervision or inductive biases to the disentanglement method
<ul>
<li>Inductive bias: rise from the symmetry of natural objects and the 3D graphical information.</li>
<li>Reconstruct the face images by carefully remodeling the graphics of camera principal, which makes it possible to decompose the facial images into environmental semantics and other facial semantics.</li>
<li>It's unable to generate realistic faces and perform pixel-level face editing on it.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="goal-3">Goal</h3>
<ul>
<li>Photo-realistic images synthesizing, minimize the demand for human annotations</li>
<li>Capture linear-encoded facial semantics.</li>
</ul>
<h3 id="how-5">How</h3>
<p>With a given collection of coarsely aligned faces, a GAN is trained to mimic the overall distribution of the data. Then use the faces that the trained GAN generates as training data and trains a 3D deformable face reconstruction method. A mutual reconstruction strategy stabilizes the training significantly. Then they keep a record of the latent code from StyleGAN and apply linear regression to disentangle the target semantics in the latent space.</p>
<h4 id="decorrelating-latent-code-in-stylegan">Decorrelating latent code in StyleGAN</h4>
<ul>
<li><p>Enhance the disentangled representation by decorrelating latent codes.</p>
<ul>
<li><p>In order to maximizes the utilization of all dimensions, they use Pearson correlation coefficient to zero and variance of all dimension.</p></li>
<li><p>Introduce decorrelation regularization via a loss function</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901225711236.png" alt="image-20210901225711236" /><figcaption aria-hidden="true">image-20210901225711236</figcaption>
</figure></li>
<li><p>The mapping network is the only one to update with the new loss.</p></li>
</ul></li>
</ul>
<h4 id="stabilized-training-for-3d-face-reconstruction">Stabilized training for 3D face reconstruction</h4>
<ul>
<li><p>Use the decomposed semantics to reconstruct the original input image with the reconstruction loss</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901230258180.png" title="fig:" alt="image-20210901230258180" /></li>
<li>The 3D face reconstruction algorithm struggles to estimate the pose of profile or near-profile faces.</li>
<li>The algorithm tries to use extreme values to estimate the texture and shape of each face independently, which deviate far away from the actual texture and shape of the face. To solve this, the mutual reconstruction strategy is proposed to prevent the model from using extreme values to fit individual reconstruction, and the model learns to reconstruct faces with a minimum variance of the shape and texture among all samples.</li>
</ul></li>
<li><p>During training, they swap the albedo and depth map between two images with a probability <span class="math inline">\(\epsilon\)</span> to perform the reconstruction with the alternative loss.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902113314940.png" alt="image-20210902113314940" /><figcaption aria-hidden="true">image-20210902113314940</figcaption>
</figure></li>
<li><p>simply concatenate the two images channel-wise as input to the confidence network</p></li>
</ul>
<h4 id="disentangle-semantics-with-linear-regression">Disentangle semantics with linear regression</h4>
<ul>
<li>The Ultimate goal of disentangling semantics is to find a vector in StyleGAN, such that it only takes control of the target semantics.</li>
<li>Semantic gradient estimation
<ul>
<li>It's observed that with StyleGAN, many semantics can be linear-encoded. Therefore, the gradient is now independent of the input latent code.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902120453628.png" title="fig:" alt="image-20210902120453628" /></li>
</ul></li>
<li>Semantic linear regression
<ul>
<li>In real world scenario, the gradient is hard to estimate directly because back-propagation only captures local gradient, making it less robust to noises.</li>
<li>Propose a linear regression model to capture global linearity for gradient estimation.</li>
</ul></li>
</ul>
<h4 id="image-manipulation-for-data-augmentation">Image manipulation for data augmentation</h4>
<ul>
<li>One application is to perform data augmentation.</li>
<li>By extrapolating along <span class="math inline">\(\mathrm{v}\)</span> beyond its standard deviation, we can get samples with more extreme values for the associated semantics.</li>
</ul>
<h4 id="localized-representation-learning">Localized representation learning</h4>
<ul>
<li>Find the manipulation vectors <span class="math inline">\(\hat{\mathrm{v}}\)</span> that capture interpretable combinations of pixel value variations.</li>
<li>Start by defining a Jacobian matrix, which is the concatenation of all canonical pixel-level <span class="math inline">\(\mathrm{v}\)</span>.</li>
<li>interpolation along <span class="math inline">\(\hat{\mathrm{v}}\)</span> should result in significant but localized (i.e. sparse) change across the image domain.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902133618545.png" title="fig:" alt="image-20210902133618545" /></li>
</ul>
<h2 id="paper-7-bi-gcn-binary-graph-convolutional-network">Paper 7: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Bi-GCN_Binary_Graph_Convolutional_Network_CVPR_2021_paper.pdf">Bi-GCN: Binary Graph Convolutional Network</a></h2>
<p>Codes: https://github.com/bywmm/Bi-GCN</p>
<h3 id="why-5">Why</h3>
<ul>
<li>The current success of GNNs is attributed to an implicit assumption that the input of GNNs contains the entire attributed graph, which will collapse or the accuracy will decrease if the entire graph is too large.
<ul>
<li>One intuitive solution for the problem is sampling: neighbor sampling or graph sampling. The graph sampling will sample subgraphs and can avoid neighbor explosion. But not like neighbor sampling, it cannot guarantee that each node is sampled.
<ul>
<li>Neighbor sampling: GraphSAGE, VRGCN</li>
<li>Sampling subgraphs: Fast-GCN, ClusterGCN, DropEdge, DropConnection, GraphSAINT for edge sampling.</li>
</ul></li>
<li>Another solution is compressing the size of input graph data the the GNN model: such as pruning, shallow networks, designing compact layers and quantizing the parameters.</li>
</ul></li>
<li>The challenges of compressed GNN
<ul>
<li>The compression of the loaded data demands more attention</li>
<li>The original GNN is shallow and therefore the compression will be more difficult to be achieved.</li>
<li>Require the compressed GNNs to possess sufficient parameters for representations.</li>
</ul></li>
</ul>
<h3 id="goal-4">Goal</h3>
<ul>
<li>reduce the redundancies in the node representations while maintain the principle information.</li>
</ul>
<h3 id="how-6">How</h3>
<ul>
<li>Binarizes both the network parameters and input node features. The original matrices multiplications are revised to binary operations for accelerations. Design a new gradient approximation based back-propagation to train the proposed Bi-GCN.</li>
</ul>
<h4 id="gcn">GCN</h4>
<ul>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151200499.png" alt="image-20210902151200499" /><figcaption aria-hidden="true">image-20210902151200499</figcaption>
</figure>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151226323.png" alt="image-20210902151226323" /><figcaption aria-hidden="true">image-20210902151226323</figcaption>
</figure></li>
<li><p>Use task-dependent loss function, e.g. the cross-entropy.</p></li>
</ul>
<h4 id="bi-gcn">Bi-GCN</h4>
<p>Only focus on binarizing the feature extraction step, because the aggregation step possesses no learnable parameters and it only requires a few calculations. To reduce the computational complexities and accelerates the inference process, the XNOR and bit count operations are utilized.</p>
<h5 id="binarization-of-the-feature-extraction-step">Binarization of the feature extraction step</h5>
<ul>
<li><p>Binarization of the parameters</p>
<p>Each column if the parameter matrix is splitted as a bucket, and <span class="math inline">\(\alpha\)</span> maintain the scalars for each bucket.</p></li>
<li><p>Binarization of the node features</p>
<p>Processed by the graph convolutional layers.</p>
<ul>
<li>Split the hidden state at layer <span class="math inline">\(l\)</span> into row buckets based on the constraints of the matrix multiplication.</li>
<li>Let <span class="math inline">\(F^{(l)}\)</span> be the binarized buckets, the binary approximation of <span class="math inline">\(H^{(l)}\)</span> can be obtained via <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902160626221.png" alt="image-20210902160626221" style="zoom:67%;" /></li>
<li><span class="math inline">\(\beta\)</span> can be considered as the node-weights for the features representations. Each element of <span class="math inline">\(F^{(l)}\)</span> and <span class="math inline">\(B^{(l)}\)</span> is either -1 or 1.</li>
<li>This Binarization also possesses the ability of activation, therefore the activation operations can be eliminated.</li>
</ul></li>
</ul>
<h5 id="binary-gradient-approximation-based-back-propagation">Binary gradient approximation based back propagation</h5>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902164243961.png" alt="image-20210902164243961" /><figcaption aria-hidden="true">image-20210902164243961</figcaption>
</figure>
<h2 id="paper-8-an-attractor-guided-neural-networks-for-skeleton-based-human-motion-prediction">Paper 8: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.09711.pdf">An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</a></h2>
<h3 id="why-6">Why</h3>
<ul>
<li>Most existing methods tend to build the relations among joints, where local interactions between joint pairs are well learned. However, the global coordination of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously.</li>
<li>Most graphs are designed according to the kinematic structure of the human to extract motion features, but hardly do they learn the relations between spatial separated joint pairs directly.</li>
<li>Except for speed, other dynamic information like accelerated speed are not counted into previous work, which ignores important motion information.</li>
<li>Previous work
<ul>
<li>Human motion prediction
<ul>
<li>Many works suffer from discontinuities between the observed poses and the predicted future ones.</li>
<li>Consider global spatial and temporal features simultaneously, such as transform temporal space to trajectory space to take the global temporal information into account.</li>
</ul></li>
<li>Joint relation modeling
<ul>
<li>Focus on skeletal constraints to model correlation between joints.</li>
<li>adaptive graph: the existed works weak the global coordination of all joints since they are learned from parts.</li>
</ul></li>
<li>Dynamic representation of skeleton sequence
<ul>
<li>Many attempts proposed to extract enriching dynamic representation from raw data, but they only extract the dynamics from neighbor frames</li>
<li>Extract the dynamic features among frames through multiple timescale will extract more motion features.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="goal-5">Goal</h3>
<p>To characterize the global motion features and thus can learn both the local and global motion features simultaneously.</p>
<p>Generate predicted poses through proposed framework AGN and the historical 3D skeleton-based poses.</p>
<h3 id="how-7">How</h3>
<ul>
<li>Pipeline: A BA (balance attractor) is learned by calculating dynamic weighted aggregation of single joint feature. Then the difference between the BA and each joint feature is calculated. Later the resulting new joint features are used to calculate joints similarities to generate final joint relations.</li>
<li>Framework: Attractor-Guided Neural Network, which first learn an enriching dynamic representation from raw position information adaptively through MTDE (multi-timescale dynamics extractor). Then the AJRE (attractor-based joint relation extractor) is imported , including a LIE (local interaction extractor), a GCE (global coordination extractor) and an adaptive feature fusing module.
<ul>
<li>AJRE: a joint relation modeling = GCE+LIE. The GCE models the global coordination of all joints, while LIE mines the local interactions between joint pairs.</li>
<li>MTDE: extract enriching dynamic information from raw input data for effective prediction.</li>
</ul></li>
</ul>
<h4 id="mtde">MTDE</h4>
<ul>
<li><p>A combination of different time scales motion dynamics</p></li>
<li><p>Two stream, one path is the raw input poses, the other is the difference between adjacent frames in raw input. The dynamics of each joint separately is also modeled to avoid the interference of other joints.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903184316148.png" alt="image-20210903184316148" /><figcaption aria-hidden="true">image-20210903184316148</figcaption>
</figure></li>
<li><p>The MTDE uses three 1DCNN but in different kernel size (5，3，1) to extract the local (joint-level) dynamics.</p></li>
</ul>
<h4 id="ajre">AJRE</h4>
<ul>
<li>Consists of GCE and LIE to separately model global coordination of all joints and local interactions between joint pairs, and also AFFM which is used to fuse features according to channel-wise attention to improve the flexibility of joint relation modeling.</li>
<li>GCE and LIE work in parallel, and they are followed by AFFM.</li>
</ul>
<h5 id="gce">GCE</h5>
<p>Global coordination of all joints, so they learn a medium to build new joint relations indirectly.</p>
<ul>
<li>BA (balance attractor unit) unit calculates all joints' aggregation to characterize the global motion features. After transpose the input features, then BA unit applies <span class="math inline">\(1\times 1\)</span> conv to get a dynamic weighted feature aggregation of N joints features. And <span class="math inline">\(X_{new}\)</span> is the difference between the output features of BA and the original <span class="math inline">\(X\)</span>.</li>
<li>The new relations of all joints is built by the Cosine similarity unit, which measure between <span class="math inline">\(X_{new},X\)</span>. The cosine similarity between all row vector pairs to illustrate the correlation between joint pairs. The correlation matrix on each channel is calculated since each channel encodes specific spatiotemporal features and should focus on different correlations compared with other channels.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232718696.png" alt="image-20210903232718696" /><figcaption aria-hidden="true">image-20210903232718696</figcaption>
</figure>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232735446.png" alt="image-20210903232735446" /><figcaption aria-hidden="true">image-20210903232735446</figcaption>
</figure>
<h5 id="lie">LIE</h5>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232837194.png" alt="image-20210903232837194" /><figcaption aria-hidden="true">image-20210903232837194</figcaption>
</figure>
<ul>
<li>It's used to learn local interactions between joint pairs, including adjacent and distant joints.</li>
<li>To learn the relations between adjacent joint pairs, a pure <span class="math inline">\(3\times 3\)</span> convolution is adopted. To learn the relations between distant joint pairs, the self-attention is used.</li>
</ul>
<h5 id="affm">AFFM</h5>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903234025866.png" title="fig:" alt="image-20210903234025866" /></li>
<li>Channel attention to fuse features adaptively and reform more reliable representation.</li>
<li>After the sigmoid in the AFFM unit, the importance ratio of each channel is obtained. Then the channel-wise multiplication between ratio and raw input is done to reform features.</li>
</ul>
<h2 id="paper-9-cascade-graph-neural-networks-for-rgb-d-salient-object-detection">Paper 9: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.03087">Cascade Graph Neural Networks for RGB-D Salient Object Detection</a></h2>
<p>Codes: https://github.com/LA30/Cas-Gnn</p>
<h3 id="why-7">Why</h3>
<ul>
<li>How to leverage the two complementary data sources: color and depth information</li>
<li>Current works either simply distill prior knowledge from the corresponding depth map for handling the RGB-image or blindly fuse color and geometric information to generate the coarse depth-aware representations, hindering the performance of RGB-D saliency detectors</li>
<li>Identify saliency objects of varying shape and appearance, show robustness towards heavy occlusion, various illumination and background.</li>
<li>Network cascade is an effective scheme for a variety of high-level vision applications. It will ensemble a set of models to handle challenging tasks in a coarse-to-fine or easy-to-hard manner.</li>
</ul>
<h3 id="goal-6">Goal</h3>
<p>Salient object detection for RGB-D images. To distill and reason the mutual benefits between the color and depth data sources through a set of cascade graphs.</p>
<p>Predict a saliency Map given an input image and its corresponding depth image.</p>
<h3 id="how-8">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210904162325031.png" alt="image-20210904162325031" /><figcaption aria-hidden="true">image-20210904162325031</figcaption>
</figure>
<ul>
<li>CGR (cascade graph reasoning ) module to learn dense features, from which the saliency map can be easily inferred. It explicitly reasons about the 2D appearance and 3D geometry information for RGBD SOD.</li>
<li>Each graph consists of two types of nodes, geometry nodes storing depth features and appearance nodes storing RGB-related features.</li>
<li>Multiple-level graphs sequentially chained by coarsening the preceding graph into two domain-specific guidance nodes for the following cascade graph.</li>
</ul>
<h4 id="cross-modality-reasoning-with-gnns">Cross-modality reasoning with GNNs</h4>
<ul>
<li>Build a directed graph, where the edges connect i) the nodes from the same modality but different scales and ii) the nodes of the same scale from different modalities.</li>
<li>The backbone is VGG-16 plus dilated network technique, which will extract 2D appearance representations and 3D geometry representations. They also propose a graph-based reasoning (GR) module to reason about the cross-modality, high-order relations between them.</li>
<li>GRU module
<ul>
<li>Input 2D features and 3D features.</li>
<li>gated recurrent unit for node state updating</li>
</ul></li>
</ul>
<h4 id="cascade-graph-neural-networks">Cascade Graph Neural networks</h4>
<ul>
<li><p>To overcome the drawbacks of independent multilevel (graph-based) reasoning, propose cascade GNNs.</p></li>
<li><p>coarsening the preceding graph into two domain-specific guidance nodes for the following cascade graph to perform the joint reasoning</p></li>
<li><p>The guidance nodes only deliver the guidance information, and will stay fixed during the message passing process.</p></li>
<li><p>The guidance node is built by firstly concatenation and then the fusion via <span class="math inline">\(3\times 3\)</span> convolution layer.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905202700652.png" alt="image-20210905202700652" /><figcaption aria-hidden="true">image-20210905202700652</figcaption>
</figure></li>
<li><p>Each guidance node propagates the guidance information to other nodes of the same domain in the graph through the attention mechanism.</p></li>
<li><p>Multi-level feature fusion: The merge function is either element-wise addition or channel-wise concatenation.</p></li>
</ul>
<h2 id="paper-10-coarse-fine-networks-for-temporal-activity-detection-in-videos">Paper 10: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01302">Coarse-Fine Networks for Temporal Activity Detection in Videos</a></h2>
<p>Code: https://github.com/kkahatapitiya/Coarse-Fine-Networks</p>
<h3 id="why-8">Why</h3>
<ul>
<li>One main challenge for video representation learning is capturing long-term motion from a continuous video.</li>
<li>Use of frame striding or temporal pooling has been a successful strategy to cover a larger time interval without increasing the number of parameters</li>
<li>Previous work
<ul>
<li>Action localization: temporal action localization task, which tends to annotate every frame with multiple ongoing activities. Use of sequential models such as LSTMs have been popular.</li>
<li>Dynamic sampling: selective processing of information, like spatially, temporally or spatio-temporally sampling.</li>
</ul></li>
<li>Two challenges of the network
<ul>
<li>how to abstract the information at a lower temporal resolution meaningfully, and</li>
<li>how to utilize the fine-grained context information effectively.</li>
</ul></li>
</ul>
<h3 id="goal-7">Goal</h3>
<p>learn better video representations for long-term motion, works in multiple temporal resolutions of the input and selects frames dynamically.</p>
<h3 id="how-9">How</h3>
<ul>
<li>Grid Pool, a learned temporal downsampling layer to extract coarse features, which adaptively samples the most informative frame locations with a differentiable process.</li>
<li>Multi-stage Fusion, a spatio-temporal attention mechanism to fuse a fine-grained context with the coarse features.</li>
</ul>
<h4 id="grid-pool">Grid pool</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905223900318.png" alt="image-20210905223900318" /><figcaption aria-hidden="true">image-20210905223900318</figcaption>
</figure>
<ul>
<li>Samples by interpolating on a non-uniform grid with learnable grid locations. The intuition comes from that sampling frames at a higher frame rate where the confidence is high and at a lower frame rate where it is low. The stride between the interpolated frame locations should be small and vice-versa.</li>
<li>The confidence value is modeled as a function of the input representation.</li>
<li>To get a set of <span class="math inline">\(\alpha T\)</span> (an integer) grid locations based on confidence values, the CDF is considered.</li>
<li>When a grid location is non-integer, the corresponding sampled frame is a temporal interpolation between the adjacent frames.</li>
<li>A grid unpool operation is coupled with the grid locations learned by the Grid pool layer, which simply performs the inverse operation of the Grid pool. In this way, one will resamples with a low frame-rate in the regions where one used a high frame-rate in Grid pool, and vice-versa.</li>
</ul>
<h4 id="multi-stage-fusion">Multi-stage fusion</h4>
<ul>
<li>Fuse the context from the fine stream and the coarse stream. Aims: filter out what fine-grained information should be passed down to the coarse stream, have a calibration step to align the coarse features and fine features, learn and benefit from multiple abstraction-levels of fine-grained context at each fuse-location in the coarse stream</li>
<li>filtering fine-grained information: self-attention mask by processing the fine feature through a lightweight head consists of point-wise convolutional layers followed by a sigmoid non-linearity.</li>
<li>Fine to coarse correspondence: use a set of temporal Gaussian distributions centered at each coarse frame location which abstract a location dependent weighted average of the fine feature.</li>
<li>Multiple abstraction-levels: allow each fusion connection to look at the features from all abstraction levels by concatenating them channel-wise. The scale and shift features at each fusion location is calculated to finally fuse the features from any abstraction-levels.</li>
</ul>
<h4 id="model-details">Model details</h4>
<ul>
<li>Backbone: X3D, which follows ResNet structure but designed for efficiency in video models.</li>
<li>The coarse stream takes in segmented clips of <span class="math inline">\(T=64\)</span> frames to follow the standard X3D architecture after the Grid pool later during training, while the fine stream always process the entire input clip.</li>
<li>The main difference between the coarse and the fine stream is the Grid pool layer and the corresponding grid unpool operation.</li>
<li>The grid pool later is placed after the 1st residual block.</li>
<li>The peak magnitude of each mask is normalized to 1.</li>
<li>The standard deviation <span class="math inline">\(\sigma\)</span> is set to be <span class="math inline">\(\frac{T&#39;}{8}\)</span>, empirically.</li>
</ul>
<h2 id="paper-11-coconets-continuous-contrastive-3d-scene-representations">Paper 11: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.03851">CoCoNets: Continuous Contrastive 3D Scene Representations</a></h2>
<p>https://mihirp1998.github.io/project_pages/coconets/</p>
<h3 id="why-9">Why</h3>
<ul>
<li>Combine 3D voxel grids and implicit functions and learn to predict 3D scene and object 3D occupancy from a single view with unlimited spatial resolution</li>
</ul>
<h3 id="goal-8">Goal</h3>
<p>SSL learning of amodal 3D feature representations from RGB and RGBD posed images and videos, and finally generate the representations to help object detection, object tracks or visual correspondence.</p>
<p>Trained for contrastive view prediction by rendering 3D feature clouds in queried viewpoints and matching against the 3D feature point cloud predicted frim the query view.</p>
<p>Finally, the model forms plausible 3D completions of the scene given a single RGB-D image as input.</p>
<h3 id="how-10">How</h3>
<ul>
<li>3d feature grids as a 3D-informed neural bottleneck for contrastive view prediction, and implicit functions for handling the resolution limitations of 3D grids.</li>
<li>Propose CoCoNets (continuous contrastive 3D networks) that learns to map RGB-D images to infinite-resolution 3D scene representations by contrastively predicting views. Specifically, the model is trained to lift 2.5D images to 3D feature function grids of the scene by optimizing for view-contrastive prediction</li>
<li>There are two branches in CoCoNet, one is to encode RGB-D images into a 3D feature map, and the other is to encode the RGB-D of the target viewpoint into a 3D feature cloud.</li>
<li>Two branches, one adopts top-down idea which encodes the input RGB-D image and orienting the feature map to the target viewpoint, and predict the features for the target 3D points in target domain by querying, and later output a feature cloud for the target domain. The other branch is the bottom-up one, which simply encodes the target RGB-D image and predict the features for the target 3D position in target domain, obtaining the feature cloud for target domains</li>
<li>The positive samples are from the target domain that works in bottom-up branch.</li>
</ul>
<h4 id="result">Result</h4>
<ul>
<li>The scene representations learnt by CoCoNets can detect objects in 3D across large frame gaps</li>
<li>Using the learnt 3D point features as initialization boosts the performance of the SOTA Deep Hough Voting detector.</li>
<li>The learnt 3D feature representations can infer 6DoF alignment between the same object in different viewpoints, and across different objects of the same category.</li>
<li>optimize a contrastive view prediction objective but uses a 3D girder of implicit functions as its latent bottleneck.</li>
</ul>
<h2 id="paper-12-cutpaste-self-supervised-learning-for-anomaly-detection-and-localization">Paper 12: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.04015">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</a></h2>
<p>Codes: https://github.com/Runinho/pytorch-cutpaste</p>
<h3 id="why-10">Why</h3>
<ul>
<li>Difficult to obtain a large amount of anomalous data, and the difference between normal and anomalous patterns are often fine-grained.</li>
<li>The anomaly score defined as an aggregation of pixel-wise reconstruction error or probability densities lacks to capture a high-level semantic information.</li>
<li>deep one-class classifier outperforms, but most existing work focus on detecting semantic outliers, which cannot generalize well in detecting fine-grained anomalous patterns as in defet detection.</li>
<li>Naively applying existed methods such as rotation prediction or contrastive learning, is sub-optimal for detecting local defects.</li>
<li>Rotation and translation ect. lacks of irregularity.</li>
</ul>
<h3 id="goal-9">Goal</h3>
<ul>
<li>detects unknown anomalous patterns of an image without anomalous data</li>
<li>Design a pretext task that can identify local irregularity.</li>
<li>The pretext task is also amenable to combine with existing methods, such as transfer learning from pretrained models for better performance or patch-based models for more accurate localization</li>
</ul>
<h3 id="how-11">How</h3>
<ul>
<li>Learn representations by classifying normal data from the CutPaste (data augmentation strategy that cuts an image patch at a random location of a large image). First learn SS deep representations and then build a generative one-class classifier.</li>
<li>transfer learning on pretrained representations on ImageNet.</li>
<li>designing a novel proxy classification task between normal training data and the ones augmented by the CutPaste. The CutPaste motivated to produce a spatial irregularity to serve as a coarse approximation of real defects.</li>
<li>A two-stage framework to build an anomaly detector, where in the first stage they learn deep representations from normal data and then construct an one-class classifier using learned representations.</li>
</ul>
<h4 id="ssl-with-cutpaste">SSL with CutPaste</h4>
<ul>
<li>CutPaste augmentation as follows:
<ul>
<li>Cut a small rectangular area of variable sizes and aspect ratios from a normal training image.</li>
<li>Optionally, we rotate or jitter pixel values in the patch.</li>
<li>Paste a patch back to an image at a random location</li>
</ul></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906193605112.png" title="fig:" alt="image-20210906193605112" /></li>
<li>In practice , data augmentation or color jitter, are applied before feeding x into g or CP.</li>
</ul>
<h4 id="cutpaste-variants">CutPaste variants</h4>
<ul>
<li>CutPaste scar: a long-thin rectangular box filled with an image patch</li>
<li>Multi-class classification: formulate a finer-grained 3-way classification task among normal, CutPaste and CutPaste-Scar by treating CutPaste variants as two separate classes.</li>
<li>Similarity between CutPaste and real defects: outliers exposure. CutPaste creates examples preserving more local structures of the normal examples, while is more challenging for the model to learn to find this irregularity.</li>
<li>CutPaste does look similar to some real defects.</li>
</ul>
<h4 id="computing-anomaly-score">Computing anomaly score</h4>
<ul>
<li><p>A simple Gaussian density estimator whose log-density is computed as follows</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194329698.png" alt="image-20210906194329698" style="zoom:50%;" /></p></li>
<li></li>
</ul>
<h4 id="localization-with-patch-representation">Localization with patch representation</h4>
<ul>
<li><p>CutPaste prediction is readily applicable to learn a patch representation – all we need to do at training is to crop a patch before applying CutPaste augmentation.</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194519932.png" alt="image-20210906194519932" style="zoom:50%;" /></p></li>
</ul>
<h2 id="paper-13-discriminative-latent-semantic-graph-for-video-captioning">Paper 13: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.03662">Discriminative Latent Semantic Graph for Video Captioning</a></h2>
<p>https://github.com/baiyang4/D-LSG-Video-Caption</p>
<h3 id="why-11">Why</h3>
<ul>
<li>Key challenge of the video captioning task: no explicit mapping between video frames and captions, and the output sentence should be natural</li>
<li>GNNs show particular advantages in modeling relationships between objects, but they don't jointly consider the frame-based spatial-temporal contexts in the entire video sequence</li>
<li>discriminative modeling for caption generation suffers from stability issues and requires pre-trained generators.</li>
<li>Traditional GNNs for video captioning cannot take adequate information into consideration, while the work of this paper (conditional graph) jointly consider objects, contexts and motion information at both region and frame levels.</li>
</ul>
<h3 id="goal-10">Goal</h3>
<ul>
<li>Video captioning</li>
<li>Encode-decoder frameworks cannot explicitly explore the object-level interactions and frame-level information from complex spatio-temporal data to generate semantic-rich captions.</li>
<li>Contributions on three key sub-tasks in video captioning
<ul>
<li>Enhanced object proposal: propose a novel conditional graph that can fuse spatio-temporally information into latent object proposal.</li>
<li>visual knowledge: latent proposal aggregation to dynamically extract visual words</li>
<li>sentence validation: a novel discriminative language validator</li>
</ul></li>
<li>Propose D-LSG, where the graph model for feature fusion from multiple base models, the latent semantic refers to the higher-level semantic knowledge that can be extracted from the enhanced object proposals. The discriminative module is designed as a plug-in language validator, which uses the Multimodal Low-rank Bi-linear (MLB) pooling as metrics.</li>
</ul>
<h3 id="how-12">How</h3>
<ul>
<li>a semantic relevance discriminative graph based on Wasserstein gradient penalty.</li>
<li>Modeled as a sequence to sequence process.</li>
</ul>
<h4 id="architecture-design">Architecture Design</h4>
<ul>
<li>Multiple feature extraction: use 2D CNNs for appearance features and 3D CNNs for motion features. Then these two features are concatenated and apply LSTM on them.</li>
<li>Enhanced object proposal: enhanced by their visual contexts of appearance and motion respectively, which result in enhanced appearance proposals and enhanced motion proposal, together these two form the enhanced object proposals.</li>
<li>Visual knowledge: latent semantic proposals as <span class="math inline">\(K\)</span> dynamic visual words, after introducing the dynamic graph built by LPA to summarize the enhanced appearance and motion features.</li>
<li>Language decoder: language generation decoder will take the visual knowledge extracted by the LPA to generate captions. it consists of an attention LSTM for weighting dynamic visual words and a language LSTM for caption generation.</li>
</ul>
<h4 id="latent-semantic-graph">Latent Semantic graph</h4>
<ul>
<li><p>conditional graph operation : model the complex object-level interactions and relationships, and learn informative object-level features that are in context of frame-based background information.</p>
<ul>
<li>To build the graph, each region feature is regarded as a node. During message passing, the enhanced appearance proposal and object-level region features are handled with a kernel function to encode relations between them. The kernel is defined by linear functions followed by Tanh activation function.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908111357002.png" alt="image-20210908111357002" /><figcaption aria-hidden="true">image-20210908111357002</figcaption>
</figure></li>
<li><p>LPA</p>
<ul>
<li>to further summarize the enhanced object proposals</li>
<li>propose a latent proposal aggregation method to generate visual words dynamically based on the enhanced features.</li>
<li>Introduce a set of object visual words, which means potential object candidates in the given video, and then they summarize the enhanced proposals into informative dynamic visual words.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908112654724.png" title="fig:" alt="image-20210908112654724" /></li>
</ul></li>
</ul>
<h4 id="discriminative-language-validation">Discriminative language validation</h4>
<ul>
<li>The module is designed to as a language validation process that encourages the generated captions to contain more informative Semantic concepts via reconstructing the visual words or knowledge based on the input sentences under the condition of corresponding true visual words encoded by LSG.</li>
<li>Use WGAN-GP</li>
<li>Extract sentence features from given captions by 1DCNN,</li>
<li>The output of the discriminative model is weighted since sentences have different proportions of object and motion concepts
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120140074.png" title="fig:" alt="image-20210908120140074" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120117673.png" title="fig:" alt="image-20210908120117673" /></li>
</ul></li>
</ul>
<h2 id="paper-14-enhancing-self-supervised-video-representation-learning-via-multi-level-feature-optimization">Paper 14: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.02183?context=cs">Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</a></h2>
<p>https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization</p>
<h3 id="why-12">Why</h3>
<ul>
<li>most recent works have mainly focused on high-level semantics and neglected lower-level representations and their temporal relationship</li>
<li>The requirement of developing unsupervised video representation learning without resorting to manual labeling</li>
<li>Drawbacks
<ul>
<li>previous works only explore either instance-wise or semantic-wise distribution, lacking a comprehensive perspective over both sides.</li>
<li>less effort has been placed on low-level features than high-level representations, while the former is proven critical for knowledge transfer</li>
<li>Third, directly performing temporal augmentations, e.g., shuffle and reverse, at input level instead of feature level could impair feature learning</li>
</ul></li>
<li>High-level features are more representative towards instances or semantics but less feasible towards cross-task transfer, while low-level features are transfer-friendly but lack structural information over samples.</li>
<li>a line of works expanded contrastive learning pipeline to video domain
<ul>
<li>InfoNCE loss for dense future prediction</li>
<li>the temporal information in videos is not well leveraged</li>
<li>require a simple yet effective operation to apply temporal augmentations on extracted multi-level features</li>
</ul></li>
<li></li>
</ul>
<h3 id="goal-11">Goal</h3>
<ul>
<li>proposes a multi-level feature optimization framework to improve the generalization and temporal modeling ability of learned video representations</li>
<li>avoids forcing the backbone model to adapt to unnatural sequences which corrupts spatiotemporal statistics.</li>
<li>Jointly consider the instance and semantic-wise similarity distribution to form a reliable SS signal.</li>
</ul>
<h3 id="how-13">How</h3>
<ul>
<li>high-level features obtained from naive and prototypical contrastive learning are utilized to build distribution graphs</li>
<li>devise a simple temporal modeling module from multi-level view features to enhance motion pattern learning.</li>
<li>For low-level representation, apply temporal augmentation on multi-level features to construct contrastive pairs that have different motion patterns with the objective designed to distinguish the augmented samples and original ones. And one retrieval task is proposed to match the features in short and long time spans based on their semantic consistency.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910122543720.png" title="fig:" alt="image-20210910122543720" /></li>
</ul>
<h4 id="beyond-instance-discrimination">Beyond instance discrimination</h4>
<ul>
<li>The one-hot labels in InfoNCE loss neglect the relationship between different samples. But there exist some negative samples that may share similar characteristics.</li>
<li>besides instance-wise discrimination, we explicitly develop another branch on the projected high-level feature vectors for inter-sample relationship modeling.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130142797.png" title="fig:" alt="image-20210910130142797" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130231035.png" title="fig:" alt="image-20210910130231035" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130246505.png" title="fig:" alt="image-20210910130246505" /></li>
<li>design a queue to store the semantic-wise distributions from previous batches to ensure equal partition into K prototypes, but using only those from the current batch for gradient back-propagation</li>
<li>Finally, we jointly leverage <span class="math inline">\(\mathcal{L}_{ins}\)</span> and <span class="math inline">\(\mathcal{L}_{sem}\)</span> to form the self-supervisory objective for high-level representations:</li>
</ul>
<h4 id="graph-constraint-for-multi-level-features">Graph constraint for multi-level features</h4>
<ul>
<li>It is the lower-level features that mainly transfer from the pretrained network to downstream tasks. One can infer instance- and semantic-wise distribution from high-level features.</li>
<li>Denote the instance-wise similarity distribution as a <strong><em>directed</em></strong> graph <span class="math inline">\(\mathcal{G}_{ins}\)</span>, and semantic-wise distribution as an <strong><em>undirected</em></strong> graph <span class="math inline">\(\mathcal{G}_{sem}\)</span>. Each graph contains N nodes representing N different samples within a batch, and edges indicating the relationship between each sample.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131617208.png" title="fig:" alt="image-20210910131617208" /></li>
<li><span class="math inline">\(\mathcal{E}_{ins}\)</span> indicates the inferred instance-wise similarity distribution, which respects inter-sample relationship and is more realistic data distribution than one-hot encoding. <span class="math inline">\(\mathcal{E}_{sem}\)</span> to truncate the edges between nodes of different pseudo categories.</li>
<li>Jointly leverage <span class="math inline">\(\mathcal{G}_{ins}\)</span> and <span class="math inline">\(\mathcal{G}_{sem}\)</span> to form the combined graph <span class="math inline">\(\mathcal{G}\)</span>, whose edge weights serve as the final soft targets: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131811218.png" alt="image-20210910131811218" style="zoom:53%;" /></li>
<li>The cross-entropy between <span class="math inline">\(\mathcal{E}\)</span> and inferred similarity distribution to optimize lower-level features.</li>
</ul>
<h4 id="temporal-modeling">Temporal modeling</h4>
<ul>
<li><p>Use the temporal information at diverse time scales to enhance motion pattern modeling since the features at different layers possess different temporal characteristic.</p></li>
<li><p>A robust temporal model requires two aspects: semantic discrimination between different motion patters; semantic consistency under different temporal views.==&gt; Two learning objectives</p></li>
<li><p>perform temporal augmentation on multi-level features <span class="math inline">\(\mathrm{f}_r\)</span>, and then leverage a lightweight motion excitation module to extract motion enhanced feature representations</p></li>
<li><p>Temporal transformations that result in semantically inconsistent motion patterns can be regarded as a negative pair of the original sample</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910132656530.png" alt="image-20210910132656530" /><figcaption aria-hidden="true">image-20210910132656530</figcaption>
</figure></li>
<li><p>To boost the consistency, they propose to match feature of a specific timestamp from sequences of different lengths. For one short sequence <span class="math inline">\(v_s\)</span> that is contained in a long sequence <span class="math inline">\(v_l\)</span>, they retrieve the feature at each timestep of <span class="math inline">\(v_s\)</span> in the feature set of <span class="math inline">\(v_l\)</span>. The feature of corresponding timestamp in <span class="math inline">\(v_l\)</span> serves as the positive key, while others serve as negatives.</p></li>
</ul>
<h2 id="paper-15-exploring-simple-siamese-representation-learning">Paper 15: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.10566">Exploring simple siamese representation learning</a></h2>
<p>codes: https://github.com/facebookresearch/simsiam</p>
<h3 id="why-13">Why</h3>
<ul>
<li>Siamese networks are natural tools for comparing (including but not limited to “contrasting”) entities</li>
<li>Recent methods define the inputs as two augmentations of one image, and maximize the similarity subject to different conditions</li>
<li>An undesired trivial solution to Siamese networks is all outputs “collapsing” to a constant.
<ul>
<li>Methods like Contrastive learning, e.g., SimCLR etc. work to fix this.</li>
<li>Clustering is another way of avoiding constant output. While these methods do not define negative exemplars, this cluster centers can play as negative prototypes.</li>
<li>BYOL relies only on positive pairs but it does not collapse in case a momentum encoder is used. The momentum encoder is important for BYOL to avoid collapsing, and it reports failure results if removing the momentum encoder</li>
</ul></li>
<li>the weight-sharing Siamese networks can model invariance w.r.t. more complicated transformations</li>
</ul>
<h3 id="goal-12">Goal</h3>
<ul>
<li>report that simple Siamese networks can work surprisingly well with none of the above strategies (contrastive learning, clustering or BYOL) for preventing collapsing</li>
<li>our method ( SimSiam) can be thought of as “<em>BYOL without the momentum encoder”</em>. Directly shares the weights between the two branches, so it can also be thought of as “SimCLR without negative pairs”, and “SwAV without online clustering”.</li>
<li>SimSiam is related to each method by removing one of its core components.</li>
<li>The importance of stop-gradient suggests that <em>there should be a different underlying optimization problem that is being solved</em>.</li>
</ul>
<h3 id="how-14">How</h3>
<ul>
<li><p>The proposed architecture takes as input two randomly augmented views <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> from an image <span class="math inline">\(x\)</span>. The two views are processed by an encoder network <span class="math inline">\(f\)</span> consisting of a backbone and a project MLP head. A predict MLP head is denoted as <span class="math inline">\(h\)</span>.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160437825.png" alt="image-20210910160437825" /><figcaption aria-hidden="true">image-20210910160437825</figcaption>
</figure></li>
<li><p>The symmetrized loss is denoted as <span class="math inline">\(\mathcal{L}=\frac{1}{2}\mathcal{D}(p_1,p_2)+\frac{1}{2}\mathcal{D}(p_2,z_1)\)</span>. Its minimum possible value is −1.</p></li>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160719249.png" alt="image-20210910160719249" /><figcaption aria-hidden="true">image-20210910160719249</figcaption>
</figure></li>
<li><p>the encoder on <span class="math inline">\(x_2\)</span> receives no gradient from <span class="math inline">\(z_2\)</span>, but gradients from <span class="math inline">\(p_2\)</span>.</p></li>
<li><p>Use SGD as optimizer, with a base <span class="math inline">\(lr=0.05\)</span>, the learning rate is <span class="math inline">\(lr\times \mathrm{BatchSize}/256\)</span>.</p></li>
<li><p>Use ResNet50 as the default backbone.</p></li>
<li><p>Unsupervised pretraining on the 1000-class ImageNet training set without using labels.</p></li>
</ul>
<h2 id="paper-16-git-graph-interactive-transformer-for-vehicle-re-identification">Paper 16: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.05475">GiT: Graph Interactive Transformer for Vehicle Re-identification</a></h2>
<h3 id="why-14">Why</h3>
<ul>
<li>Vehicle re-identification aiming to retrieve a target vehicle from non-overlapping cameras. But there are challenges
<ul>
<li>vehicle images of different identifications usually have similar global appearances and subtle differences in local regions</li>
</ul></li>
<li>The technologies of vehicle re-identification
<ul>
<li>Early methods: pure CNNs, fail to catch local information</li>
<li>Based on CNNs, cooperate part divisions (uniform spatial division suffer from partition misalignment, part detection requires a high cost of extra manual part annotations) to learn global features and local features.</li>
<li>CNNs cooperate GNNs to learn global and local features: the CNN's downsampling and convolution operations reduce the resolution of feature maps, the CNN and GNN branches are supervised with two independent loss functions and lack interaction.</li>
<li>This paper: couple global and local features via transformer and local correction graph modules.</li>
</ul></li>
<li>The advantages of transformer
<ul>
<li>The transformer can use multi-head attention module to capture global context information to establish long-distance dependence on global features of vehicles.</li>
<li>The multi-head attention module of transformer does not require convolution and down-sampling operations, which retain more detailed vehicle information.</li>
</ul></li>
</ul>
<h3 id="goal-13">Goal</h3>
<p>Propose a graph interactive transformer (GiT) for vehicle-reidentification. Each GiT block employs a novel local correlation graph (LCG) module to extract discriminative local features within patches.</p>
<p>LCG Modules and transformer layers are in a coupled status.</p>
<h3 id="how-15">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913111201445.png" alt="image-20210913111201445" /><figcaption aria-hidden="true">image-20210913111201445</figcaption>
</figure>
<ul>
<li>The transformer later and LCG module interact each other by skip connection when these two components work in sequence.</li>
</ul>
<h4 id="lcg-module">LCG module</h4>
<ul>
<li><p>To aggregate and learn discriminative local features within every patch.</p></li>
<li><p>flatten <span class="math inline">\(n\)</span> local features <span class="math inline">\(d\)</span> dimensions and map to <span class="math inline">\(d&#39;\)</span> dimensions with a trainable linear projection in every patch</p></li>
<li><p>The spatial graph's edges are constructed as <span class="math inline">\(E_{v_{i,j}}=\frac{\exp (F_{cos}(v_i,v_j))}{\sum\limits_{k=1}^{n}\exp {(F_{cos}(v_i,v_k))}}\)</span>, where <span class="math inline">\(i,j\in[1，2,...,n]\)</span>. The score of the cosine distance is denoted as <span class="math inline">\(F_{cos}=\frac{v_i,v_j}{\|v_i\|\|v_j\|}\)</span>.</p></li>
<li><p>To aggregate and update nodes, the aggregation node <span class="math inline">\(U\)</span> of <span class="math inline">\(i\)</span>-th graph is updated according as follows</p>
<ul>
<li><p><span class="math inline">\(U=(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}X_i)\cdot W\)</span>,</p></li>
<li><p>Then <span class="math inline">\(U\)</span> is processed non-linearly as</p>
<p><span class="math inline">\(O=GELU(LN(U))\)</span>, where GELU represents the gaussian error linear units and LN denotes the layer normalization.</p></li>
</ul></li>
</ul>
<h4 id="transformer-layer">Transformer layer</h4>
<ul>
<li>Model the global features between the different patches.</li>
<li>Patches are the input for multi-head attention layer</li>
<li>Later, the output from the attention layer is normalized and then processed by MLP.</li>
</ul>
<h4 id="graph-interactive-transformer">Graph interactive Transformer</h4>
<ul>
<li>Each GiT block consists of a LCG module and a Transformer layer.</li>
</ul>
<h4 id="loss-function-design">Loss function design</h4>
<ul>
<li><p>The proposed GiT's total loss function is</p>
<p><span class="math inline">\(L_{total}=\alpha L_{CE}+\beta L_{Triplet}\)</span>, where <span class="math inline">\(L_{CE}\)</span> denote cross-entropy loss, and <span class="math inline">\(L_T\)</span> denotes triplet loss.</p></li>
<li><p>The <span class="math inline">\(L_{CE}\)</span> formulates the cross-entropy of each patch's label</p></li>
<li><p>The triplet loss is</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913112252883.png" alt="image-20210913112252883" /><figcaption aria-hidden="true">image-20210913112252883</figcaption>
</figure></li>
</ul>
<h2 id="paper-17-graph-time-convolutional-neural-networks">Paper 17: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01730">Graph-Time Convolutional Neural Networks</a></h2>
<p>Codes: https://github.com/gtcnnpaper/DSLW-Code</p>
<h3 id="why-15">Why</h3>
<ul>
<li>The key for learning on multivariate temporal data is to embed spatiotemporal relations into into its inner-working mechanism.</li>
<li>Spatiotemporal graph-base models
<ul>
<li>Hybrid: combine learning algorithms developed separately for the graph domain and the temporal domain
<ul>
<li>Such as a temporal RNN, CNN</li>
<li>their spatial and temporal blocks are modular and can be implemented efficiently</li>
<li>unclear how to best interleave these blocks for learning from spatiotemporal relationships</li>
</ul></li>
<li>Fused
<ul>
<li>force the graph structure into conventional spatiotemporal solutions and provide a single strategy to jointly capture the spatiotemporal relationships.</li>
<li>substitute the parameter matrices in these models with graph convolutional filters</li>
<li>fused models capture naturally these relationships as they have graph-time dependent inner-working mechanisms.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="goal-14">Goal</h3>
<ul>
<li>Represent spatiotemporal relations through product graphs and develop a first principle graph-time convolutional neural network (GTCNN).</li>
<li>For multivariate temporal data such as sensor or social networks</li>
</ul>
<h3 id="how-16">How</h3>
<ul>
<li>Each layer consists of a graph-time convolutional module, a graphtime pooling module, and a nonlinearity.</li>
<li>The product graph itself is parametric to learn the spatiotemporal coupling</li>
<li>The zero-pad pooling preserves the spatial graph while reducing the number of active noes and parameters</li>
</ul>
<h4 id="signals-over-product-graphs">Signals over product graphs</h4>
<ul>
<li><p>Two graphs:</p>
<ul>
<li>Spatial graph <span class="math inline">\(\mathcal{G}\)</span>, consider the original sensor network, each node has a time sequence</li>
<li>Temporal graph <span class="math inline">\(\mathcal{G}_T\)</span>: for each node, there is a line graph that take each timestep as a node.</li>
</ul></li>
<li><p>Given graphs <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}_T\)</span> , we can capture the spatiotemporal relations in <span class="math inline">\(\mathrm{X}\)</span> through the product graph <span class="math inline">\(\mathcal{G}_\diamond = \mathcal{G}_T\times \mathcal{G}=(\mathcal{V}_\diamond,\mathcal{E}_\diamond)\)</span> , where the vertex set <span class="math inline">\(\mathcal{V}_\diamond = \mathcal{V}_T \times \mathcal{V}\)</span> is the Kronecker product between <span class="math inline">\(\mathcal{V}_T\)</span> and <span class="math inline">\(\mathcal{V}\)</span>.</p></li>
<li><p>Product graphs</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913131247274.png" alt="image-20210913131247274" /><figcaption aria-hidden="true">image-20210913131247274</figcaption>
</figure>
<ul>
<li>The Kronecher product preserves the relations between different nodes along temporal dimension. The 1st node at the 1st timestep have influences for the 2nd node at the 2nd timestep. And the influence is bidirectional.</li>
<li>The cartesian product preserves the original spatial infromation and the temporal evolution on each node's temporal dimension.</li>
</ul></li>
<li><p>Goal: to learn spatiotemporal representations in a form akin to temporal or graph CNNs.</p></li>
</ul>
<h4 id="graph-time-cnns">Graph-time CNNs</h4>
<ul>
<li>A compositional architecture of <span class="math inline">\(L\)</span> layers each having a graph-time convolutional module, a graph-time pooling module and a nonlinearity.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913133851127.png" title="fig:" alt="image-20210913133851127" /></li>
<li>at convolutions allow for effective parameter sharing, inductive learning, and efficient implementation, while zero-pad pooling and pointwise nonlinearities make the architecture independent from graph-reduction techniques or other modules.</li>
<li>Graph-time convolutional filtering
<ul>
<li>The graph-time convolutional filter aggregates at the space-time location <span class="math inline">\((i, t)\)</span> information from space-time neighbors that are up to <span class="math inline">\(K\)</span> hops away over the product graph <span class="math inline">\(\mathcal{G}_\diamond\)</span>.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913135843608.png" title="fig:" alt="image-20210913135843608" /></li>
<li>Implement it recursively, and expand all polynomials of order <span class="math inline">\(k\)</span>. Then the computational cost is liner in the product graph dimensions.</li>
</ul></li>
<li>Graph-time pooling: The pooling approach has three steps: i) summarization; ii) slicing; iii) downsampling
<ul>
<li>Summarization: up to <span class="math inline">\(\alpha_l\)</span> hops away for each node . Use mean or max function.
<ul>
<li>Summarization is an implicit low-pass operation and the type of product graph has an impact on its severit</li>
</ul></li>
<li>Slicing: reduces the dimensionality across the temporal dimension.</li>
<li>Downsampling: reduces the number of active nodes across the spatial dimension from <span class="math inline">\(N_{l-1}\)</span> to <span class="math inline">\(N_l\)</span> without modifying the underlying spatial graph.</li>
</ul></li>
</ul>
<h2 id="paper-18-graphzoom-a-multi-level-spectral-approach-for-accurate-and-scalable-graph-embedding">Paper 18: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02370">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</a></h2>
<p>Codes: https://github.com/cornell-zhang/GraphZoom</p>
<h3 id="why-16">Why</h3>
<ul>
<li>Challenges
<ul>
<li>existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy</li>
<li>few of them <em>scale to large graphs</em> due to their high computational complexity and memory usage</li>
</ul></li>
<li>Graph embedding techniques
<ul>
<li>Random-walk-based embedding algorithms
<ul>
<li>embed a graph based on its topology without incorporating node attribute information==&gt; limits the embedding power</li>
<li>GCN with the basic notion that node embeddings should be smoothed over the entire graph and so can leverage both topology and node attribute information.==&gt; But may suffer from high-frenquency noise in the inital node features.</li>
</ul></li>
</ul></li>
<li>Only one of the solution for increasing the accuracy or improving the scalability of graph embedding methods is well-handeled. Not all of them</li>
<li>Previous work
<ul>
<li>Multi-level graph embedding: GraphZoom is motivated by theoretical results in spectral graph embedding.</li>
<li>Graph filtering: GCN model implicitly exploits graph filter to remove high-frequency noise from the node feature matrix. In GraphZoom we adopt graph filter to properly smooth the intermediate embedding results during the iterative refinement step.</li>
</ul></li>
</ul>
<h3 id="goal-15">Goal</h3>
<ul>
<li>Propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms.</li>
</ul>
<h3 id="how-17">How</h3>
<ul>
<li>GraphZoom consists of four major kernels: (1) graph fusion, (2) spectral graph coarsening, (3) graph embedding, and (4) embedding refinement.
<ul>
<li>The graph fusion kernel first converts the node feature matrix into a feature graph and then fuses it with the original topology graph.</li>
<li>Spectral graph coarsening produces a series of successively coarsened graphs by merging nodes based on their spectral similarities.</li>
<li>During the graph embedding step, any of the existing unsupervised graph embedding techniques can be applied to obtain node embeddings for the graph at the coarsest level.</li>
<li>Embedding refinement is then employed to refine the embeddings back to the original graph by applying a proper graph filter to ensure embeddings are smoothed over the graph.</li>
</ul></li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914123913163.png" alt="image-20210914123913163" /><figcaption aria-hidden="true">image-20210914123913163</figcaption>
</figure>
<h4 id="graph-fusion">Graph Fusion</h4>
<ul>
<li>To construct a weighted graph that has the same number of nodes as the original graph but potentially different set of edges (weights) that encapsulate the original graph topology as well as node attribute information</li>
<li>Firstly generate a KNN graph based on the <span class="math inline">\(\ell^2-norm\)</span> distance between the attribute vectors of each node pair so to convert initial attribute matrix <span class="math inline">\(X\)</span> into a weighted node attribute graph.</li>
<li>To implement in a linear cost, they start with coarsening the original graph <span class="math inline">\(\mathcal{G}\)</span> to obtain a substantially reduced graph that has much fewer nodes with an <span class="math inline">\(\mathcal{O}(|\mathcal{E}|)\)</span> , similar as to spectral graph clustering to group nodes into clusters of high conductance. After the attribute graph is formed, we assign a weight to each edge based on the cosine similarity between the attribute vectors of the two incident nodes. Finally, we can construct the fused graph by combining the topology graph and the attribute graph using a weighted sum: <span class="math inline">\(\mathrm{A}_{fusion}=\mathrm{A}_{topo}+\beta \mathrm{A}_{feat}\)</span>.</li>
</ul>
<h4 id="spectral-coarsening">Spectral coarsening</h4>
<ul>
<li>For graph coarsening via global spectral embedding: calculating eigenvectors of the original gragh Laplacian is very costly. To eliminate the cost, they propose a method for graph coarsening via local spectral embedding.
<ul>
<li>The analogies between the traditional signal processing (Fourier analysis) and graph signal processing.
<ul>
<li>The signals at different time points in classical Fourier analysis correspond to the signals at different nodes in an undirected graph;</li>
<li>The more slowly oscillating functions in time domain correspond to the graph Laplacian eigenvectors associated with lower eigenvalues or the more slowly varying (smoother) components across the graph.</li>
</ul></li>
<li>apply the <em>simple smoothing (low-pass graph filtering) function to <span class="math inline">\(k\)</span> random vectors to obtain smoothed vectors for <span class="math inline">\(k\)</span>-dimensional graph embedding</em>, which can be achieved in linear time.</li>
</ul></li>
<li>We adopt low-pass graph filters to quickly filter out the high-frequency components of the random graph signal or the eigenvectors corresponding to high eigenvalues of the graph Laplacian, and then get the smoothed vectors in <span class="math inline">\(T\)</span> (initial random vectors).</li>
<li>The aggregation scheme: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914141059850.png" alt="image-20210914141059850" /></li>
<li>Once the aggregation scheme is defined, the coarsening in multilevel is formed as a set of graph mapping matrices.</li>
</ul>
<h4 id="graph-embedding">Graph embedding</h4>
<p>Just obtain embedding according to the previously defined formulas.</p>
<h4 id="embedding-refinement">Embedding refinement</h4>
<ul>
<li>To get the embedding vectors for the original graph eventually. It's kind of like remapping the graph from the coarsest level to the finest level.</li>
<li>The refinement process is motivated by Tikhonov regularization to smooth the node embedding over the graph by minimizing <span class="math inline">\(\min\limits_{E_i}\{\|E_i-\hat{E}_i\|^2_2+tr(E_i^\top L_iE_i)\}\)</span>, where <span class="math inline">\(L_i\)</span> and <span class="math inline">\(E_i\)</span> are the normalized Laplacian matrix and mapped embedding matrix of the graph at the <span class="math inline">\(i\)</span>-th coarsening level, respectively. The solving the equation, the refined embedding matrix of edges are solved.</li>
<li>To solve the equation efficiently , they work in spectral domain, and approximate the graph filter by it first-order Taylor expansion.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914142505161.png" title="fig:" alt="image-20210914142505161" /></li>
</ul>
<h2 id="paper-19-group-contrastive-self-supervised-learning-on-graphs">Paper 19: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.09787.pdf">Group Contrastive Self-Supervised Learning on Graphs</a></h2>
<h3 id="why-17">Why</h3>
<ul>
<li>Capability : Contrasting graphs in multiple subspaces enables graph encoders to capture more abundant characteristics.</li>
<li>CL methods train models on pretext tasks that encode the agreement between two views of representations. These two views can be global-local pairs or differently transformed graph data. The learning goal is to make these two-view representations similar if they are from the same graph and dissimilar if they are from different graphs.</li>
<li>The idea of using groups has been shown to be effective in the image domain.</li>
</ul>
<h3 id="goal-16">Goal</h3>
<ul>
<li>Study SSL on graphs by contrastive learning.</li>
<li>Propose a group contrastive learning framework. Embed the given graph into multiple subspaces, of which each representation is prompted to encode specific characteristics of graphs.</li>
<li>Further develop an attention-based representor function to compute representations. Develop principled objectives that enable us to capture the relations among both intra-space and inter-space representations in groups.</li>
</ul>
<h3 id="how-18">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914160553376.png" alt="image-20210914160553376" /><figcaption aria-hidden="true">image-20210914160553376</figcaption>
</figure>
<ul>
<li>refer to a group as a set of representations of different graph views within the same subspace</li>
<li>propose to maximize the MI between two views of representations in the same group while minimizing the MI between the representations of one view across different groups.</li>
<li>a graph-level encoder usually consists of a node encoder which computes the node embedding matrix and a readout function summarize the mdoe embeddigs into the desired graph-level embedding.</li>
</ul>
<h4 id="the-proposed-group-contrastive-learning-framework">The proposed group contrastive learning framework</h4>
<ul>
<li>To build two views and their corresponding multiple graph-level representations. The used GNN encoder can be the same, or one can just duplicate the calculated representation for <span class="math inline">\(p\)</span> times. Then for view <span class="math inline">\(u,r\)</span>, they can be combined into <span class="math inline">\(p\)</span> pairs which are cross-view pairs and two groups that each group from one specific view.</li>
<li>So for two views: the main view <span class="math inline">\(u\)</span>, and auxiliary view <span class="math inline">\(r\)</span>, there are in total two encoders which are parameterized by <span class="math inline">\(\theta, \phi\)</span> respectively.</li>
</ul>
<h4 id="intra-space-objective-function">Intra-space objective function</h4>
<ul>
<li>For the intra-space objective, they seek to maximize the mutual information between representations of two views within each group. The optimization is done based on paramters <span class="math inline">\(\theta,\phi\)</span>.</li>
<li>To implement, maximizes the MI's lower-bound. Precisely, they adopt the Jensen-Shannon estimator of MI, and the disciriminator is the dot product between two representations.</li>
</ul>
<h4 id="inter-space-objective-function">Inter-space objective function</h4>
<ul>
<li>Constraint the pairwise relation across different groups of the same view to enforce the diversity of inter-space. They employ an inter-space optimization objective based on mutual information minimization.</li>
<li>To minimize the MI, we introduce an upper bound of MI as an efficient estimation, based on the contrastive log-ratio upper bound: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914165125847.png" alt="image-20210914165125847" style="zoom:33%;" />. To solve the key challenge of modeling <span class="math inline">\(P(y|x)\)</span>, there are two approaches based on whether the same dimensions of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> correspond to each other across different representations. Suppose the distribution <span class="math inline">\(y\)</span> conditional on <span class="math inline">\(x\)</span> is subject to a Gaussian distribution.
<ul>
<li>Non-parameterized estimation
<ul>
<li>Assume <span class="math inline">\(\mathbb{E}[y|x]=x\)</span>, and the variance <span class="math inline">\(\Sigma\)</span> is a diagonal matrix with the same values on its diagonal. Then after deduction, the goal minimizing CLUB is equivalent to minimize <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914170432752.png" alt="image-20210914170432752" style="zoom: 25%;" /></li>
<li>This goal enlarges the agreement between <span class="math inline">\(\mathrm{u}^{(k)},\mathrm{u}^{(l)}\)</span>, under the joint distribution so to keep the diversity among one group.</li>
</ul></li>
<li>Parameterized estimation
<ul>
<li>When there is no correspondence between dimensions of <span class="math inline">\(x,y\)</span>.</li>
<li>Via a parameterized variational distribution. Concretely, they use two independent MLP to generate the mean and variance respectively.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210914171121061.png" alt="image-20210914171121061" style="zoom: 25%;" /></li>
</ul></li>
</ul></li>
</ul>
<h4 id="the-overall-objective-function">The overall objective function</h4>
<ul>
<li>Combine the intra-space and the inter-space objectives together.</li>
<li>Either optimize based on non-parameterized way or parameterized way.</li>
</ul>
<h4 id="groupcl-graphcl-with-group-contrast">GroupCL: GraphCL with Group Contrast</h4>
<ul>
<li>the generation of multiple representations shares the same node encoder and envolves a parameterized representor function that computes multiple graph representations from node embeddings of a given graph.</li>
<li>For view <span class="math inline">\(u\)</span>, GroupCL performs a random data augmentation on the input graph to generate the view data, and then use encoder to encode each nodes of the view data into node embeddings <span class="math inline">\(\mathrm{U}\)</span>. Given <span class="math inline">\(\mathrm{U}\)</span>, use attention to capture the information from different node combinations where each graph representation depends greatly on the heavily attended nodes with respect to different queries, and thus propose the representor function.</li>
<li>the multiple representations are prompted to focus on different and informative combinations of nodes and thereupon encode different subgraph patterns.</li>
</ul>
<h4 id="graphig-infograph-with-group-contrast">GraphIG: InfoGraph with Group Contrast</h4>
<ul>
<li>In InfoGraph, view <span class="math inline">\(u,r\)</span> are the graph-level embedding an the node-level embedding, respectively.</li>
<li>In view <span class="math inline">\(u\)</span>, the encoder GNN and representor function <span class="math inline">\(\mathcal{R}_Q\)</span> is similar as GroupCL.</li>
<li>In view <span class="math inline">\(r\)</span>, they generate the node embeddings through encder GNN and duplicate them for <span class="math inline">\(p\)</span> times to obtain <span class="math inline">\(p\)</span> representations.</li>
</ul>
<h2 id="paper-20-homophily-outlier-detection-in-non-iid-categorical-data">Paper 20: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.11516.pdf">Homophily outlier detection in non-IID categorical data</a></h2>
<p><font color="red"> Will be completed later.</font></p>
<h3 id="why-18">Why</h3>
<h3 id="goal-17">Goal</h3>
<ul>
<li>identify outliers in <em>categorical data</em> by capturing non-IID outlier factors</li>
</ul>
<h3 id="how-19">How</h3>
<ul>
<li>first defines and incorporates distribution-sensitive outlier factors and their interdependence into a value-value graph-based representation. Then model an outlierness propagation process in the value graph to learn the outlierness of feature values.</li>
<li></li>
</ul>
<h2 id="paper-21-hyperparameter-free-and-explainable-whole-graph-embedding">Paper 21: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.02113">Hyperparameter-free and Explainable Whole Graph Embedding</a></h2>
<p>Codes: https://github.com/HW-HaoWang/DHC-E</p>
<h3 id="why-19">Why</h3>
<ul>
<li>most node embedding or whole graph embedding methods suffer from the problem of having more sophisticated methodology, hyperparameter optimization, and low explainability.</li>
<li>Previous work
<ul>
<li>Before 2000, traditional methods like PCA, LDS are linear, which cannot handle the nonlinear relationships within datasets properly.</li>
<li>Around 2000, like IsoMap and LLE (Laplacian eigenmaps) are manifold.</li>
<li>From practical view, like word2vec is popular.</li>
</ul></li>
<li>Whole graph embedding methods: previously they are mathematically intractable, sophisticated, and challenging to interpret, and suffer hyperparameter selection problems in comparing different graphs.
<ul>
<li>FGSD (family of graph spectral distances): calculate the Moore-Penrose spectrum of the normalized Laplacian and use the histogram of the spectral features as a whole graph representation.</li>
<li>Graph2vec: unsupervised, derive fixed-length task-agnostic embedding of graphs</li>
<li>NetLSD (Network Laplacian Spectral Descriptor) : calculate the heat kernel trace of the normalized Laplacian matrix over a vector of time scales.</li>
<li>IGE (Invariant graph embedding): compute a mixture of spectral and noes embedding based features and pool node feature embedding to create graph descriptor.</li>
<li>GL2vec: complement the edge label information or the structural information .</li>
</ul></li>
<li></li>
</ul>
<h3 id="goal-18">Goal</h3>
<ul>
<li>This paper only considers unweighted and undirected graphs with no self-loops or multiple edges.</li>
<li>Propose a hyperparameter free, extensible, and explainable whole graph embedding method, combining the DHC (Degree, H-index and Coreness) theorem and Shannon Entropy (E), abbreviated as DHC-E.</li>
<li>The new whole graph embedding scheme can obtain a trade-off between simplicity and quality under supervised classification learning tasks, using molecular, social, and brain networks. In addition, the proposed approach has a good performance in lower dimensional graph visualization</li>
</ul>
<h3 id="how-20">How</h3>
<ul>
<li>DHC-E can be regarded as an adaptive and self-converging system that depicts the information of complex systems from the fine-grained to coarse-scale.</li>
<li>Assume that different networks have different convergence steps and each iteration of H-index sequences encodes information that can distinguish the network’s properties.</li>
<li>metrics for vital node identification
<ul>
<li>Degree</li>
<li>H-index: The h-index of a node in the graph is defined as the maximum value <span class="math inline">\(h\)</span> such that there exist at least <span class="math inline">\(h\)</span> neighbors who have a degree no less than <span class="math inline">\(h\)</span>. The H-index of every node can be updated based on the previous H-indices of neighbors following the same procedure.</li>
<li>Coreness: take the location of a node in the graph to measure its influence based on the <span class="math inline">\(k\)</span>-core decomposition process. A larger coreness of a noes indicate that it is located more centrally in the graph.</li>
</ul></li>
</ul>
<h4 id="dhc-entropy">DHC-entropy</h4>
<ul>
<li><p>DHC theorem proves that each nodal H-index sequence eventually converges to the nodal coreness.</p></li>
<li><p>The zero-order H-index of every node is initialized to its degree, as shown in the initial state. The following updated states can be calculated by the DHC theorem</p></li>
<li><p>Combining Shannon entropy and DHC theorem:</p>
<ul>
<li>Problems: : (1) How to extract and integrate the information generated from the DHC updating process (i.e., the H-index sequences of all nodes) to construct whole graph embedding; (2) How to unify and align the whole graph embedding with different dimensions.</li>
<li>In the <span class="math inline">\(m\)</span>-th iteration, for the H-indices, calculate the probability distribution of each node's H-index, and then take the probability distribution to calculate Shannon-entropy, so to quantify the uncertainty of the graph in this state.</li>
<li>To align, for one graph set with different subgraphs to have same dimension of embeddings so as to perform whole graph embedding aggregation, the key is to keep the number of H-indices of each subgraphs to be the same. They propose to expand those with smaller H-indices to be expanded with their last element.</li>
</ul></li>
<li><p>The DHC-E algorithm</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210915222052643.png" alt="image-20210915222052643" /><figcaption aria-hidden="true">image-20210915222052643</figcaption>
</figure></li>
</ul>
<h2 id="paper-22-infograph-unsupervised-and-semi-supervised-graph-level-representation-learning-via-mutual-information-maximization">Paper 22: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.01000">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</a></h2>
<p>Codes: https://github.com/fanyun-sun/InfoGraph</p>
<h3 id="why-20">Why</h3>
<ul>
<li>Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs</li>
<li>one of the most difficult obstacles for supervised learning on graphs is that it is often very costly or even impossible to collect annotated labels.</li>
<li>the handcrafted features of graph kernels lead to high dimensional, sparse or non-smooth representations and thus result in poor generalization performance, especially on large datasets.</li>
<li>Deep Graph InfoMax (DGI) aims to train a node encoder that maximizes mutual information between node representations and the pooled global graph representation.</li>
<li>Mean Teacher adds a loss term which encourages the distance between the original network’s output and the teacher’s output to be small. The teacher’s predictions are made using an exponential moving average of parameters from previous training steps.</li>
<li>Explicitly extracting the graph can be more straightforward and optimal for graph-oriented tasks</li>
</ul>
<h3 id="goal-19">Goal</h3>
<ul>
<li>Graph-level representations learning</li>
<li>maximize the mutual information between the graph-level representation and the representations of substructures of different scales</li>
<li>Propose InfoGraph and InfoGraph* for unsupervised and semi-supervised separately. The InfoGraph* employs a student-teacher framework similar to Mean-Teacher method. It deploys two separate encoders but instead of explicitly encouraging the output of the student model to be similar to the teacher model’s output, they enable the student model to learn from the teacher model by maximizing mutual information between intermediate representations learned by two models.</li>
</ul>
<h3 id="how-21">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210917161308152.png" alt="image-20210917161308152" /><figcaption aria-hidden="true">image-20210917161308152</figcaption>
</figure>
<h4 id="infograph">InfoGraph</h4>
<ul>
<li>The encoder is GNN, finally the representations from different GNN layers are concatenated and readout (they use sum over mean ) as the global representations.</li>
<li>The MI is between global and local pairs, and is estimated by Jensen-Shannon MI estimator.</li>
<li>In practice, the negative samples are generated using all possible combinations of global and local patch representations across all graph instances in a batch.</li>
<li>Although similar to DGI, they show their difference in
<ol type="1">
<li>They extract global representations rather than node-level representations</li>
<li>For graph convolution encoders, they use GIN rather than GCN since GIN provides a better inductive bias for graph level representations.</li>
</ol></li>
</ul>
<h4 id="infograph-1">InfoGraph*</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210917161807256.png" alt="image-20210917161807256" /><figcaption aria-hidden="true">image-20210917161807256</figcaption>
</figure>
<ul>
<li><p>Simply combining the two loss functions using the same encoder may lead to “negative transfer" (transfer knowledge from a less related source and thus may hurt the target performance).</p></li>
<li><p>Therefore, they propose a simple way to alleviate this problem: deploy two encoder models: the encoder on the labeled data and the encoder on the unlabeled data. For transferring, they define a loss term that encourages the representations learned by the two encoders to have high mutual information, at all levels of representations.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210918084309019.png" alt="image-20210918084309019" /><figcaption aria-hidden="true">image-20210918084309019</figcaption>
</figure></li>
<li><p>The formulation can be seen as a special instance of the student-teacher framework.</p></li>
<li><p>To reduce computation cost, instead of enforcing the mutual-information maximization over all the layers of the encoders, at each training update, they enforce mutual-information maximization on a randomly chosen layer of the encoder.</p></li>
</ul>
<h2 id="paper-23-iterative-graph-self-distillation">Paper 23: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.12609">Iterative graph self-distillation</a></h2>
<h3 id="why-21">Why</h3>
<ul>
<li>The limitation of existing GNN architecture is that they often require a huge amount of labeled data to be competitive but annotating graphs.==&gt; unsupervised learning such as graph kernels and matrix factorization.</li>
<li>SSL
<ul>
<li>pretext task: needs meticulous designs of hand-crafted tasks</li>
<li>contrastive learning via InfoMax principle:
<ul>
<li>context-instance contrastive approaches usually need to sample subgraphs as local views to contrast with global graphs. And they usually require an additional discriminator for scoring local-global pairs and negative samples, which is computationally prohibitive. The performance is also very sensitive to the choice of encoders and MI estimators</li>
<li>context-instance contrastive approaches cannot be handily extended to the semi-supervised setting since local subgraphs lack labels that can be utilized for training.</li>
</ul></li>
</ul></li>
<li>In order to alleviate the dependency on negative samples mining and still be able to learn discriminative graph representations, they propose to use self-distillation as a strong regularization to guide the graph representation learning.</li>
<li>GCC (graph contrastive coding ): leverage instance discrimination as the pretext task for structural information pre-training.</li>
<li>GraphCL: learn generalizable, transferrable, and robust representations of graph data in an unsupervised manner. Focuses on the impact of various combination of graph augmentation on multiple datasets and studies unsupervised</li>
<li>Self-distillation is a special case when two architectures are identical, which can iteratively modify regularization and reduce over-fitting if perform suitable rounds.</li>
<li>Semi-supervised learning
<ul>
<li>multi-task learning: regularizing the learning process with unlabeled data</li>
<li>consistency training between two separate networks: student-teacher framework, : introducing a slow-moving average teacher network to measures consistency against a student one, thus providing a consistency-based training paradigm where two networks can be mutually improved</li>
</ul></li>
<li>Data augmentation: limited since defining views of graphs is a non-trivial task.
<ul>
<li>Feature-space augmentation</li>
<li>structure-space augmentation</li>
</ul></li>
</ul>
<h3 id="goal-20">Goal</h3>
<ul>
<li>Learn graph-level representations unsupervised by iteratively performing the teacher-student distillation with graph augmentations.</li>
<li>Propose IGSD (iterative graph self-distillation), which constructs the teacher with an exponential moving average of the student model and distills the knowledge of itself.</li>
<li>Extend IGSD to semi-supervised by jointly regularizing the network with both supervised and unsupervised contrastive loss.</li>
<li>Seeking an approach that learns the entire graph representation by contrasting the whole graph directly alleviates the need for MI estimation, discriminator and subgraph sampling.</li>
</ul>
<h3 id="how-22">How</h3>
<ul>
<li>Intuition: predict the teacher network representation of the graph pairs under different augmented views.</li>
<li>Define a similarity metric for consistency-based training. The parameters of the teacher network are iteratively updated as an exponential moving average of the student network parameters.</li>
<li>To extend to semi-supervised learning, they develop a self-training algorithm based on the supervised contrastive loss for fine-tuning.</li>
<li>To perform augmentation, they have two choices
<ul>
<li>Transform a graph with transition matrix via graph diffusion and sparsification into a new graph with adjacency matrix as an augmented view in their framework</li>
<li>Randomly remove edges of graphs to attain corrupted graphs as augmented views.</li>
</ul></li>
</ul>
<h4 id="iterative-graph-self-distillation-framework">Iterative graph self-distillation framework</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919095719228.png" alt="image-20210919095719228" /><figcaption aria-hidden="true">image-20210919095719228</figcaption>
</figure>
<ul>
<li>Introduce a teacher-student architecture comprises two networks in similar structure composed by encoder <span class="math inline">\(f_\theta\)</span>, projector <span class="math inline">\(g_\theta\)</span> and predictor <span class="math inline">\(h_\theta\)</span>. Denote the components of the teacher network and the student network as <span class="math inline">\(f_{\theta&#39;},g_{\theta&#39;}\)</span> and <span class="math inline">\(f_{\theta},g_{\theta},h_\theta\)</span> respectively.</li>
<li>The positive pairs: the original and augmented view of the same graph, they are input into two networks respectively.</li>
<li>The teacher's parameters are updated as an exponential moving average of the student parameters <span class="math inline">\(\theta\)</span> after weights of the student network have been updated using gradient descent.</li>
<li>The moving average network to produce prediction targets, enforcing the consistency of teacher and student for training the student network.</li>
</ul>
<h4 id="self-supervised-learning-with-igsd">Self-supervised learning with IGSD</h4>
<ul>
<li>Employ the Self-supervised InfoNCE objective.</li>
<li>Obtain the graph representation by interpolating the latent representations with Mixup function.</li>
</ul>
<h4 id="semi-supervised-learning-with-igsd">Semi-supervised learning with IGSD</h4>
<ul>
<li>The instance-wise supervision limited to standard supervised learning may lead to biased negative sampling problems.</li>
<li>enforce consistency constraints between latents from different views, which acts as a regularizer for learning directly from labels</li>
<li>train the model using a small amount of labeled data and then fine-tune it by iterating between assigning pseudo-labels to unlabeled examples and training models using the augmented dataset.</li>
</ul>
<h2 id="paper-24-learning-by-aligning-videos-in-time">Paper 24: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.17260">Learning by Aligning Videos in Time</a></h2>
<h3 id="why-22">Why</h3>
<ul>
<li>To learn perfect alignment of two videos, a learning algorithm must be able to disentangle phases of the activity in time while simultaneously associating visually similar frames in the two different videos.</li>
<li>Using temporal alignment for learning video representations
<ul>
<li>some use cycle-consistency losses to perform local alignment between individual frames.</li>
<li>some have explored global alignment for video classification and segmentation.</li>
</ul></li>
<li>Supervised methods require fine-grained annotations which can be prohibitively expensive.</li>
<li>DTW is a global alignment metric, taking into account entire sequences while aligning, but not differential. The soft-DTW is differential.</li>
<li>Video-based SSL representation learning
<ul>
<li>predict future frames or forecast their encoding features</li>
<li>leverage temporal information such as temporal order and temporal coherence.</li>
</ul></li>
<li>TCC (temporal cycle consistency ): learns Self-supervised representations by finding frame correspondences across videos.
<ul>
<li>But TCC aligns each frame separately, not like this paper which aligns the video as a whole.</li>
</ul></li>
</ul>
<h3 id="goal-21">Goal</h3>
<ul>
<li>SSL for learning video representations using temporal video alignment as a pretext task.</li>
</ul>
<h3 id="how-23">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919142229296.png" alt="image-20210919142229296" /><figcaption aria-hidden="true">image-20210919142229296</figcaption>
</figure>
<ul>
<li>Soft-DTW for the minimum cost for temporally aligning videos in the embedding space. And to avoid trivial solutions, they propose a temporal regularization term which encourages different frames to be mapped to different points in the embedding space</li>
<li>Denote the embedding function as <span class="math inline">\(f_\theta\)</span>. They methods takes as input two videos <span class="math inline">\(X,Y\)</span> with <span class="math inline">\(n,m\)</span> frames respectively.</li>
</ul>
<h4 id="temporal-alignment-loss">Temporal alignment loss</h4>
<ul>
<li>For two videos <span class="math inline">\(X,Y\)</span>, after obtaining their embedding videos, one can compute the distance matrix <span class="math inline">\(D\)</span>. DTW calculates the alignment loss by finding the minimum cost path in <span class="math inline">\(D\)</span>: <span class="math inline">\(dtw(X,Y)=\min_{A\in A_{n,m}}\langle A,D \rangle\)</span>, where <span class="math inline">\(A_{n,m}\subset \{0，1\}^{n\times m}\)</span> is the set of all possible alignment matrices, which correspond to paths from the top-left corner of <span class="math inline">\(D\)</span> to the bottom-right Corner of <span class="math inline">\(D\)</span> using only <span class="math inline">\(\downarrow,\rightarrow, \searrow\)</span> moves.
<ul>
<li><span class="math inline">\(A(i,j)=1\)</span> if <span class="math inline">\(x_i\)</span> in <span class="math inline">\(X\)</span> is aligned with <span class="math inline">\(y_i\)</span> in <span class="math inline">\(Y\)</span>.</li>
<li>This can be computed by dynamic programming.</li>
</ul></li>
<li>Because the <span class="math inline">\(\min\)</span> operator, the DYW is not differentiable, to change it, soft-DTW is proposed
<ul>
<li>Replace the <span class="math inline">\(\min\)</span> by the smoothed <span class="math inline">\(\min^\gamma\)</span> one, defined as <span class="math inline">\(\min^{\gamma}\{a_1,a_2,\cdots,a_n\}=-\gamma\log\sum\limits_{i=1}^{n}e^{-\frac{-a_i}{\gamma}}\)</span>.</li>
<li>It returns the alignment cost between <span class="math inline">\(X,Y\)</span> by finding the soft-minimum cost path in <span class="math inline">\(D\)</span>.</li>
<li>The smoothed <span class="math inline">\(\min^{\gamma}\)</span> operator converges to the discrete <span class="math inline">\(\min\)</span> when <span class="math inline">\(\gamma\)</span> approaches 0.</li>
<li><span class="math inline">\(\min^\gamma\)</span> help the optimization by enabling smooth gradients and providing better optimization landscapes.</li>
</ul></li>
</ul>
<h4 id="temporal-regularization">Temporal regularization</h4>
<ul>
<li><p>To avoid trivial solutions (<span class="math inline">\(X,Y\)</span> are mapped to a small cluster in the embedding space), they add a temporal regularization, which is separately applied on <span class="math inline">\(f_\theta(X),f_\theta(Y)\)</span>.</p></li>
<li><p>Adapt inverse difference moment (IDM) as their regularization, which is written as: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919153758111.png" alt="image-20210919153758111" style="zoom:30%;" /></p>
<ul>
<li>maximizes this equation encourages temporally close frames in <span class="math inline">\(X\)</span> to be mapped to nearby points in the embedding space.</li>
</ul></li>
<li><p>But the equation above treats temporally close and far away frames in similar ways</p>
<ul>
<li><p>They propose separate terms for temporally close and far away frames.</p></li>
<li><p>Introduce a contrastive version and named as contrastive-IDM.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919154140301.png" alt="image-20210919154140301" /><figcaption aria-hidden="true">image-20210919154140301</figcaption>
</figure></li>
<li><p>It encourages temporally close frames (positive pairs) to be nearby in the embedding space, while penalizing temporally far away frames (negative pairs) when the distance between them is smaller than margin <span class="math inline">\(\lambda\)</span> in the embedding space.</p></li>
<li><p>Leveraging temporal information by adding weights to different pairs based on their temporal gaps leads to performance gain.</p></li>
</ul></li>
</ul>
<h4 id="final-loss">Final loss</h4>
<ul>
<li>Combine soft-DTW alignments loss and contrastive-IDM: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210919154511695.png" alt="image-20210919154511695" style="zoom:33%;" /></li>
<li>It encourages embedding videos to have minimum alignment costs while encouraging discrepancies among embedding frames.</li>
</ul>
<h4 id="encoder-network">Encoder network</h4>
<ul>
<li>Use resnet-50 as backbone, stack <span class="math inline">\(k\)</span> context frame features along the temporal dimension for each frame.</li>
<li>Then the combined features are passed through two 3DCNN layers for aggregating temporal information.</li>
</ul>
<h2 id="paper-25-learning-graph-representation-by-aggregating-subgraphs-via-mutual-information-maximization">Paper 25: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.13125">Learning graph representation by aggregating subgraphs via mutual information maximization</a></h2>
<h3 id="why-23">Why</h3>
<ul>
<li>Labeling graphs procedurally using strong prior knowledge is costly.</li>
<li>Previous work
<ul>
<li>Graph kernel methods
<ul>
<li>Decompose the graph into several subgraphs and then measure similarities between them</li>
<li>much work on deciding the most suitable sub-structures by hand-craft similarity measures between sub-structures.</li>
</ul></li>
<li>Semi-supervised learning
<ul>
<li>pseudo label methods: regard the prediction of unlabeled data as a pseudo label of unlabeled data, then train the network with all data together, and use a low weight of the loss of the unlabeled data part</li>
<li>Laddar network: combined supervised learning with unsupervised learning in DNNs to apply unlabeled data information to supervised learning reasonably.</li>
</ul></li>
<li>learning by mutual information
<ul>
<li>express the amount of information shared between two random variables</li>
<li>But to alleviate computational cost, many approximation methods for mutual information are proposed.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="goal-22">Goal</h3>
<ul>
<li>SSL to enhance graph-level representations with a set of subgraphs</li>
<li>Separate the aggregation into three aspects and they design three information aggregators: <strong>attribute-conv, layer-conv, subgraph-conv</strong>. The attribute-conv aggregate the original information from graphs, the layer-conv aggregate the multi-hop node representations from different GNN layers. The subgraph-conv aggregate the information from the generated subgraphs.</li>
<li></li>
</ul>
<h3 id="how-24">How</h3>
<ul>
<li>To constraint, maximizes the mutual information between the reconstructed graph representations rather than between graph and node representations as in previous works.</li>
<li>a head-tail contrastive construction to provide abundant negative samples.</li>
<li>In subgraph-agg stage, they introduce an auto-regressive method which is a universal SSL framework for the graph generation.</li>
</ul>
<h4 id="node-agg-stage">Node-agg stage</h4>
<ul>
<li>Perform the MLP and AGG+MLP on node attributes and edge attributes firstly.</li>
<li>The kernel size is <span class="math inline">\((2,1)\)</span>, which can squeeze each perspective of embeddings to one channel.</li>
</ul>
<h4 id="layer-agg-stage">Layer-agg stage</h4>
<ul>
<li>After perform convolution on the initial node representations for <span class="math inline">\(L\)</span> layers, to aggregate the node representations of different scales so that local and global information can be combined organically, they propose to use a <span class="math inline">\((L,1)\)</span> convolution kernel, named as layer-conv.</li>
<li>Then the whole graph representations are obtained by a readout function.</li>
</ul>
<h4 id="subgraph-agg-stage">Subgraph-agg stage</h4>
<ul>
<li>Propose subgraph-agg, like an ensemble learning, since the nodes and graph representations do not express the same level of information</li>
<li>Maximizing the mutual information between nodes and graph representation is not good enough to achieve the purpose of graph representations to express more information</li>
<li>Steps
<ul>
<li>Build an autoregressive model to generate subgraphs from the original graph first</li>
<li>Assemble these subgraphs into a reconstructed graph <span class="math inline">\(\mathrm{G}^{rec}\)</span>. Use layer-conv, by maximizing the mutual information between two graphs representations in the same level, the graph representation is learned.
<ul>
<li>This mutual contrastive leads to lower variance than node-graph constrain.</li>
</ul></li>
</ul></li>
<li>Auto-regressive subgraph generation
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920134338763.png" title="fig:" alt="image-20210920134338763" /></li>
</ul></li>
<li>Graph reconstruction
<ul>
<li>The readout function is used to aggregate each subgraph's representation.</li>
<li>Since there are <span class="math inline">\(S\)</span> subgraphs, the kernel size of subgraph-conv is <span class="math inline">\((S,1)\)</span>, and the resulting representation is taken as the representation of the reconstructed graph <span class="math inline">\(\mathrm{G^{rec}}\)</span></li>
</ul></li>
</ul>
<h4 id="implementation-2">Implementation</h4>
<ul>
<li>By softmax, the output matrix <span class="math inline">\(P\)</span> divides the original graph into two subgraphs, since <span class="math inline">\(p_{ij}\)</span> denotes the probability that the node <span class="math inline">\(i\)</span> is in the subgraph <span class="math inline">\(j\)</span>.</li>
<li>To implement the auto-regressive paradigm of subgraph generation, there are two approaches
<ul>
<li>Tree-split generation
<ul>
<li>recursively utilize the basic operator to the newly generated subgraph. Just lie splitting a binary tree.</li>
<li>At each non-leaf node, they execute the softmax to partition graphs.</li>
<li>After <span class="math inline">\(T\)</span> rounds split, they will get <span class="math inline">\(2^T\)</span> subraphs.</li>
</ul></li>
<li>multi-head generation
<ul>
<li>Import <span class="math inline">\(S\)</span> learnable matrices to execute the softmax and get <span class="math inline">\(S\)</span> partition matrices.</li>
<li>In this way, <span class="math inline">\(S\)</span> subgraphs are selected in parallel but they break the rule of auto-regressive generation.</li>
<li>By assuming all subgraphs are conditionally independent concerning the original graph, one can take this method as an auto-regressive way.</li>
</ul></li>
</ul></li>
<li>Loss function
<ul>
<li>The mutual information estimator is Jensen-Shannon divergence</li>
<li>The positive samples are the output of subgraph-conv.</li>
<li>Ways to get easy negative samples
<ul>
<li>using different graphs in the batch</li>
<li>or use the corruption function to get negative samples from the original graph.</li>
</ul></li>
<li>For a graph, its hard negative pair sample can be built by shuffling the node embeddings.</li>
</ul></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920154704931.png" title="fig:" alt="image-20210920154704931" /></li>
</ul>
<h2 id="paper-26-mile-a-multi-level-framework-for-scalable-graph-embedding">Paper 26: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.09612">Mile: A multi-level framework for scalable graph embedding</a></h2>
<p>Codes: https://github.com/jiongqian/MILE</p>
<h3 id="why-24">Why</h3>
<ul>
<li>None of the existing efforts examines how to scale up graph embedding in a generic way.</li>
<li>HARP is familiar with this work: but it focuses on improving the quality of embeddings by using the learned embeddings from the previous level as the initialized embeddings for the next level. But the way it is extended to other graph embedding is not obvious.</li>
<li>the quality of such embedding methods be strengthened by incorporating the holistic view of the graph.</li>
</ul>
<h3 id="goal-23">Goal</h3>
<ul>
<li>Obtain graph embedding efficiently and effectively (save computation cost), introduce MILE (multi-level embedding framework), allow contemporary graph embedding methods to scale to large graphs.</li>
<li>Given a graph <span class="math inline">\(\mathcal{G} = (V, E)\)</span> and a graph embedding method <span class="math inline">\(f(\cdot)\)</span>, we aim to realize a strengthened graph embedding method <span class="math inline">\(\hat{f}(\cdot)\)</span> so that it is more scalable than <span class="math inline">\(f(\cdot)\)</span> while generating embeddings of comparable or even better quality.</li>
</ul>
<h3 id="how-25">How</h3>
<ul>
<li>repeatedly coarsen the graph into smaller ones by employing a hybrid matching strategy</li>
<li>compute the embeddings on the coarsest graph using an existing embedding technique</li>
<li>propose a novel refinement model based on learning a graph convolution network to refine the embedding from the coarsest graph to the original graph.</li>
</ul>
<h4 id="graph-coarsening">Graph coarsening</h4>
<ul>
<li>the set of nodes forming a super-node is called a matching</li>
<li>Structural Equivalence Matching (SEM): Given two vertices <span class="math inline">\(u,v\)</span> in an unweighted graph <span class="math inline">\(\mathcal{G}\)</span>, we call they are structurally equivalent if they are incident on the same set of neighborhoods. Since if two vertices are structurally equivalent, then their node embeddings will be similar.</li>
<li>Normalized Heavy Edge Matching (NHEM): select an unmatched node, say <span class="math inline">\(u\)</span>, in the graph and find a large weighted edge <span class="math inline">\((u, v)\)</span> incident on node <span class="math inline">\(u\)</span> such that node <span class="math inline">\(v\)</span> is also unmatched. Then collapse nodes <span class="math inline">\(u,v\)</span> into one super-node and mark them as matched. The edge weights are normalized. The normalized method penalizes the weights of edges connected with high-degree nodes.</li>
<li>Define matching matrix to store the matching information from graph <span class="math inline">\(\mathcal{G}_i\)</span> to <span class="math inline">\(\mathcal{G}_{i+1}\)</span> as a binary matrix, the <span class="math inline">\(r\)</span>-the row and <span class="math inline">\(c\)</span>-the column of <span class="math inline">\(M_{i,i+1}\)</span> is set to 1 if node <span class="math inline">\(r\)</span> in <span class="math inline">\(\mathcal{G}_i\)</span> collapse to super-node <span class="math inline">\(c\)</span> in <span class="math inline">\(\mathcal{G}_{i+1}\)</span>, and is set to 0 if otherwise.</li>
<li>vertices with a small number of neighbors have a limited choice of finding a match and should be given a higher priority for matching. Otherwise, once their neighbors are matched by others, these vertices cannot be matched.</li>
<li>The embeddings learned by base embedding method on the coarsened graph can act as an effective initialization for the graph-topology aware refinement model.</li>
<li>Choice of coarsening level: depends on the application domain and the graph properties</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920200555554.png" alt="image-20210920200555554" /><figcaption aria-hidden="true">image-20210920200555554</figcaption>
</figure>
<h4 id="base-embedding-on-coarsened-graph">Base embedding on coarsened graph</h4>
<ul>
<li>They can use any graph embedding algorithm for base embedding.</li>
</ul>
<h4 id="refinement-of-embeddings">Refinement of embeddings</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920200744561.png" alt="image-20210920200744561" /><figcaption aria-hidden="true">image-20210920200744561</figcaption>
</figure>
<ul>
<li><p>Given the series of coarsened graph and their corresponding matching matrix, and the node embeddings on <span class="math inline">\(\mathcal{G}_m\)</span>, they seek to develop an approach to derive the node embeddings of <span class="math inline">\(\mathcal{G}_0\)</span> from <span class="math inline">\(\mathcal{G}_m\)</span>.</p></li>
<li><p>Subtask: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920200927480.png" alt="image-20210920200927480" /></p>
<ul>
<li>Propose to use GCN. Since the simple project will induce <span class="math inline">\(\mathcal{E}_i^p\)</span> from <span class="math inline">\(\mathcal{E}_{i+1}\)</span>, but one limitation is that nodes will share the same embeddings if they are matched and collapsed into a super-node during the coarsening phase.</li>
<li>The GCN uses the projected embedding <span class="math inline">\(\mathcal{E}_i^p\)</span> and the adjacency matrix <span class="math inline">\(A_i\)</span> from the input graph. Define the embedding refinement model as a <span class="math inline">\(l\)</span>-layer graph convolution model: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920201450434.png" alt="image-20210920201450434" style="zoom:25%;" /></li>
<li>The intuition behind this refinement model is to integrate the structural information of the current graph <span class="math inline">\(\mathcal{G}_i\)</span> into the projected embedding <span class="math inline">\(\mathcal{E}_i^p\)</span> by repeatedly performing the spectral graph convolution.</li>
</ul></li>
<li><p>Choice of number of GCN layers: <span class="math inline">\(l\)</span> GCN layers correspond to aggregating structural information from all the <span class="math inline">\(l\)</span>-hop neighbours for each node.</p>
<ul>
<li>But large <span class="math inline">\(l\)</span> will make the node embeddings homogeneous and less distinguishable across the graph due to the small-world property of real-world graphs.</li>
<li>In practice, they use 2.</li>
</ul></li>
<li><p>Train refinement model</p>
<ul>
<li><p>learn <span class="math inline">\(\Theta^{(k)}\)</span> on the coarsest graph and reuse them across all the levels for refinement</p></li>
<li><p>The loss function is defined as the MSE, where the ground truth is a base embedding on <span class="math inline">\(\mathcal{G}_i\)</span>.</p>
<ul>
<li>But two drawbacks: The above loss function requires one more level of coarsening to construct <span class="math inline">\(\mathcal{G}_{m+1}\)</span> and an extra base embedding on <span class="math inline">\(\mathcal{G}_{m+1}\)</span>. Also, the embedding space of graph <span class="math inline">\(\mathcal{G}_{m}\)</span> and <span class="math inline">\(\mathcal{G}_{m+1}\)</span> can be totally different since the two embeddings are learned independently.</li>
<li>One possible solution for these drawbacks may be force the embeddings to be aligned between the two graphs.</li>
</ul></li>
<li><p>Finally, this paper propose to construct a dummy coarsened graph by simply copying <span class="math inline">\(\mathcal{G}_m\)</span>, i.e. <span class="math inline">\(M_{m,m+1}=I, \mathcal{G}_{m+1}=\mathcal{G}_m\)</span>.</p>
<ul>
<li>reduce one iteration of graph coarsening</li>
<li>avoid performing base embeddings on <span class="math inline">\(\mathcal{G}_{m+1}\)</span>.</li>
</ul>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920202809962.png" alt="image-20210920202809962" style="zoom:33%;" /></p>
<ul>
<li>Adopt gradient-descent with BP to learn parameters.</li>
</ul></li>
</ul></li>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210920202919023.png" alt="image-20210920202919023" /><figcaption aria-hidden="true">image-20210920202919023</figcaption>
</figure></li>
<li><p>The shared <span class="math inline">\(\Theta^{(k)}\)</span> values do much better than alternative <span class="math inline">\(\Theta^{(k)}\)</span>.</p></li>
</ul>
<h2 id="paper-27-missing-data-estimation-in-temporal-multilayer-position-aware-graph-neural-network-tmp-gnn">Paper 27: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.03400.pdf">Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</a></h2>
<p><font color="blue">Not clear for me.</font></p>
<h3 id="why-25">Why</h3>
<ul>
<li>The goal of node embedding methods: identify a vector representation that captures node location within a broader topological structure of the graph. But most focus on static graphs.</li>
<li>To deal with dynamic graphs
<ul>
<li>One way is to use a GNN based node embedding for each individual time layer and aggregate the results accordingly.==&gt; ignore inter-layer correlation.</li>
<li>P-GNN: learn position-aware node embedding that utilize local network structure and the global network position of a given node with respect to randomly selected nodes called anchor-sets that enables us to distinguish among isomorphic nodes.</li>
</ul></li>
</ul>
<h3 id="goal-24">Goal</h3>
<ul>
<li>Propose TMP-GNN (Temporal multi-layered positio-aware GNN), a node embedding method for dynamic graphs. (change over time: edges, nodes attributes etc.)</li>
<li>The method in this paper is an extension of static position-aware GNN (P-GNN).</li>
</ul>
<h3 id="how-26">How</h3>
<ul>
<li>exploit a supra-adjacency matrix to encode a temporal graph with its intra-layer and inter-layer coupling in a single graph.</li>
<li>use hidden states learned from bi-directional GRU (bi-GRU) to learn the long-term temporal dependencies in both forward and backward directions to estimate missing values.</li>
<li>conditional centrality derived from eigenvector based centrality to distinguish nodes of higher influence</li>
</ul>
<h4 id="notations-and-preliminaries">Notations and preliminaries</h4>
<ul>
<li>The existence of inter-layer edge is restricted between separate instances of same nodes from one layer to another. The inter0later edge weight between two layers is identical for all nodes in those layers.</li>
<li>To model the coupling between two layers, this paper couples the adjacent time layers by neglecting the directionality of time, considering the short term dependencies between time layers.</li>
<li>Supracentrality matrix
<ul>
<li>A temporal graph is represented through a sequence of adjacency matrices that each of which refers to one layer of a dynamic network at a specific point of time.</li>
<li>The supracentrality matrix <span class="math inline">\(\mathbb{C}(\omega)\)</span> is built by linking up centrality matrices across time layers through a weighted inter-layer parameter <span class="math inline">\(\omega\)</span> (used to adjust the extent of coupling strength among pair of time layers.)
<ul>
<li>When <span class="math inline">\(\omega\rightarrow\infin\)</span>, the centrality measures change slowly over time, <span class="math inline">\(CC_{v}(\omega)\)</span> reaches to stationary that is equal to average <span class="math inline">\(CC_{v}(\omega)\)</span> over all time layers.</li>
<li>On the other hand, small <span class="math inline">\(\omega\)</span> makes <span class="math inline">\(CC_{v}(\omega)\)</span> fluctuate fromega one time layer to the other.</li>
</ul></li>
<li>Each item of <span class="math inline">\(\mathbb{C}(\omega)\)</span> shows joint centrality, the importance of every node-layer pair <span class="math inline">\((v,t)\)</span>. Additionally marginal and conditional centralities are defined to represent the relative importance of a node compared to other nodes at time layer <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\mathbb{C}(\omega)\)</span> consists of <span class="math inline">\(\hat{\mathbb{C}}=diag [\mathrm{C^{(1)},\cdots,C^{(T)}}]\)</span> and <span class="math inline">\(\omega\hat{\mathbb{A}}\)</span>, where <span class="math inline">\(\hat{\mathbb{C}}\)</span> represents a set of <span class="math inline">\(T\)</span> weighted centrality matrix of individual layers, and the latter encoder the uniform and diagonal coupling with strength parameter <span class="math inline">\(\omega\)</span> between the time layers. Only consider the coupling among same nodes between consecutive pair of layers.</li>
</ul></li>
<li>To solve, the dominant eigenvector <span class="math inline">\(\mathbb{V}(\omega)\)</span> is corresponds to largest eigenvalues <span class="math inline">\(\lambda_{max}\)</span>, <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921124235527.png" alt="image-20210921124235527" style="zoom:25%;" />. The elements in <span class="math inline">\(\mathbb{V}(\omega)\)</span> are interpreted as scores that measure the importance of node-layer pairs <span class="math inline">\((v,t)\)</span>.</li>
<li>Conditional centrality of node <span class="math inline">\(v\)</span>, denoted as <span class="math inline">\(CC_{v}(\omega)\)</span>, shows the importance of the node relative to other nodes at layer <span class="math inline">\(t\)</span>.</li>
</ul>
<h4 id="tmp-gnn">TMP-GNN</h4>
<ul>
<li>Position-aware GNN rather than original one, which aggregates the positional and feature information of each node with randomly selected number of nodes called anchor-sets.</li>
<li>The goal is to find the best position-aware embedding <span class="math inline">\(\mathrm{z}_v^t\)</span> with minimum distortion for a given node <span class="math inline">\(v\)</span> at time layer <span class="math inline">\(t\)</span>.</li>
<li>Modifications to P-GNN:
<ul>
<li>Generalization of P-GNN to time varying graphs: Adopt the input of P-GNN as supracentrality matrix, the embedding <span class="math inline">\(\mathrm{z}_v^t\)</span> will then be aggregated from an RNN based representation to estimate missing data.</li>
<li>modification of <span class="math inline">\(\mathcal{M}_j\)</span>: Using attention while calculating a message from anchor-set containing node <span class="math inline">\(u\)</span> with respect to a given node <span class="math inline">\(v\)</span>. Attention is used to learn the relative weights between the feature vector of <span class="math inline">\(v\)</span> and its neighbor <span class="math inline">\(u\)</span> from the anchor-set.
<ul>
<li>The <span class="math inline">\(\mathcal{M}_j\)</span> for anchor-set, denoted as <span class="math inline">\(M_v[j]\)</span>, is obtained by the shortest path between a pair of nodes and mean of <span class="math inline">\(\mathcal{M}_j\)</span>.</li>
</ul></li>
<li>Modification of <span class="math inline">\(AGG_{\mathcal{R}}\)</span>: <span class="math inline">\(\mathcal{R}\)</span> denotes the anchor-set. Corresponding informative anchor-sets contain at least 1-hop neighbours of node <span class="math inline">\(v\)</span>. Of those neighbor nodes, the ones with higher <span class="math inline">\(CC_v(\omega)\)</span> deserve to have higher <span class="math inline">\(r_j\)</span> for aggregation.
<ul>
<li>Large anchor-sets have higher probability of hitting <span class="math inline">\(v\)</span>, but are less informative of positional information of the node, as <span class="math inline">\(v\)</span> hits at least one of many nodes in the anchor-sets.</li>
<li>Small anchor-sets have fewer chance of hitting <span class="math inline">\(v\)</span>, but provide positional information with high certainty.</li>
</ul></li>
</ul></li>
<li>Edge embedding is estimated by averaging the embedding of the ending nodes.</li>
</ul>
<h4 id="bi-directional-rnn">Bi-directional RNN</h4>
<ul>
<li>Take <span class="math inline">\(Z_e,M_e,\Delta_e\)</span> as input, where <span class="math inline">\(Z_e\)</span> is randomly masked by removing missing points and set it to zero. The <span class="math inline">\(\Delta_e\)</span> illustrates the time difference between the current and the last layer which the measurement is recorded. It's defined to handle the different sampling rate associated with data heterogeneity from different sources.</li>
<li>Goal: Find the best estimate <span class="math inline">\(\mathrm{\hat{x}_{e_d}^t}\)</span> with minimum RMSE for a particular missing point.</li>
<li>Versions
<ul>
<li>The E-TMP-GNN I aims at extracting additional features out of the embedding yields from TMP-GNN, and use it to further enrich the edge feature sets.</li>
<li>TMP-GNN II aims at reducing the number of feature streams by implementing a softmax layer.</li>
</ul></li>
</ul>
<h2 id="paper-28-multi-level-graph-contrastive-learning">Paper 28: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.02639">Multi-Level Graph Contrastive Learning</a></h2>
<h3 id="why-26">Why</h3>
<ul>
<li><p>Most graph representation learning methods using GNN focus on supervised fashion and heavily depend on label information.</p></li>
<li><p>Unsupervised methods</p>
<ul>
<li>many reconstruction-based unsupervised algorithms have been proposed, either reconstruct the features of graph nodes or the topology structure of the graph.</li>
</ul></li>
<li><p>SSL methods</p>
<ul>
<li><p>contrastive methods, propose many data augmentation methods</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921154102468.png" alt="image-20210921154102468" /><figcaption aria-hidden="true">image-20210921154102468</figcaption>
</figure>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921151131234.png" alt="image-20210921151131234" /><figcaption aria-hidden="true">image-20210921151131234</figcaption>
</figure>
<ul>
<li>shuffles node features to augment the training examples</li>
<li>multi-view, maximizes MI between node representations of one view and graph representation of another view</li>
<li>subgraphs: MI loss defined on central nodes and their sampled subgraphs representation.</li>
<li>GCA: adaptive augmentation, extract the important connective structures of the original graph to the augmentation data based on node centrality measures.</li>
<li>GraphCL: multiple augmentation, including edge perturbation, attribute masking, and maximizes the MI between the node and global semantic representations to learn node representations.</li>
</ul></li>
</ul></li>
<li><p>GNN does not perform well on disassortative graphs. Constructing a KNN graph by feature similarity or dropping the links between nodes indifferent classes are feasible methods.</p></li>
<li><p>Contrastive learning consists of three main components: data augmentation schemes, the learner model and loss function</p>
<ul>
<li>data augmentation: design according to the priors and properties of graph-structured data:
<ul>
<li>vertex and edge missing do not alter the graph semantics;</li>
<li>the missing partial attributes of each node does not alter the robustness of the graph semantics;</li>
<li>the subgraph structure of the graph can hint full semantics;</li>
<li>correlated views of the same graph possess semantic consistency.</li>
</ul></li>
<li>Loss function
<ul>
<li>Probability loss: Deepwalk, node2vec. Maximizes the probability of nearest vertices of the given vertex to learn graph representation. Nodes occurring in the same sequence are viewed as the positive samples, and they should have similar representation.</li>
<li>Adversarial loss: generative-contrastive</li>
<li>Triplet loss: an important role in deep metric learning. Given an anchor <span class="math inline">\(x\)</span>, a positive <span class="math inline">\(x_p\)</span> of the same class as the anchor, a negative <span class="math inline">\(x_f\)</span> of a different class, the triple loss targets at achieving that the distance of <span class="math inline">\(x,x_f\)</span> greater than the distance of <span class="math inline">\(x,x_p\)</span> plus margin <span class="math inline">\(m\)</span>. <span class="math inline">\(\max(d(x,x_p)-d(x,x_f)+m)\)</span>.</li>
<li>Contrastive loss: maximizing the similarity of the corresponding nodes between two views, or maximizes the MI between node and graph representation.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="goal-25">Goal</h3>
<ul>
<li>SSL for graph node representation learning</li>
<li>propose a Multi-Level Graph Contrastive Learning (MLGCL) framework for learning robust representation of graph data by contrasting space views of graphs. Introduce a contrastive view-topological and feature space views. Adopt KNN graph (generated by encoding features preserves high-order proximity).</li>
<li>Feature space+topology space. The correlated graphs have similar and intrinsic characteristics.</li>
<li>Contrastive learning aims to learn an encoder model <span class="math inline">\(f(\cdot)\)</span> which can learn a robust representation for each node <span class="math inline">\(v_i\)</span> that is insensitive to the perturbation caused by data augmentations <span class="math inline">\(\tau\)</span>.</li>
</ul>
<h3 id="how-27">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210921153951384.png" alt="image-20210921153951384" /><figcaption aria-hidden="true">image-20210921153951384</figcaption>
</figure>
<ul>
<li>First sample a pair of graph augmentation functions <span class="math inline">\(\tau_1,\tau_2\)</span> from augmentation pool <span class="math inline">\(\tau\)</span>, which is applied to the input graph to generate the augmented graph of two views. Then use a pair of the shared GNN-based encoder to extract node representation, and further employ the pooling layer to extract the graph representation. Then use the shared MLP layer to project the nodes representations from both views into the space where the node-level contrastive loss is computed. <span class="math inline">\(\tau_2\)</span> includes an encoder and KNN function.</li>
</ul>
<h4 id="data-augmentation">Data augmentation</h4>
<ul>
<li>exert perturbation on the input graph to generate two correlated graphs of the same graph. Extract augmentation graph structure from space view.</li>
<li>Given the graph structure of topology space <span class="math inline">\(G(A,X)\)</span> , extraction is done
<ul>
<li>First employ GNN encoder to extract the encoding features <span class="math inline">\(Z\)</span> of topology graph</li>
<li>Then apply kNN to <span class="math inline">\(Z\)</span> to construct KNN graph with community structure <span class="math inline">\(G_f(A_f,X)\)</span>, where <span class="math inline">\(A_f\)</span> is the adjacency matrix of KNN graph.
<ul>
<li>First calculate the similarity matrix <span class="math inline">\(S\)</span> based on <span class="math inline">\(N\)</span> encoding features <span class="math inline">\(Z\)</span>. The choice of similarity function can be Mahalanobis distance, cosine similar or gaussian kernel. They use cosine similarity in their experiments.</li>
<li>Then choose top <span class="math inline">\(k\)</span> similar node pairs for each node to set edges and finally get the adjacency matrix of KNN graph <span class="math inline">\(A_f\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="gnn-encoder">GNN Encoder</h4>
<ul>
<li>Learn node representation <span class="math inline">\(z_1,z_2\)</span> for two augmented graphs. In inference phase, only use GNN encoder to learn node representation for downstream tasks.</li>
<li>Given topology graph <span class="math inline">\(G(A,X)\)</span>, KNN graph <span class="math inline">\(G_f(A_f,X)\)</span>, employ a two-layer GCN as the encoder.</li>
<li>For node representation <span class="math inline">\(Z_a,Z_b\)</span> of each view, use a graph pooling layer to derive their graph representation.</li>
</ul>
<h4 id="mlp-and-graph-pooling">MLP and graph pooling</h4>
<ul>
<li>MLP: maps the representation to the space in which the contrastive loss is computed. Since two views, there are two MLP.</li>
<li>Graph pooling: readout layer, used to learn graph representation.</li>
</ul>
<h4 id="multi-level-loss-function">Multi-level loss function</h4>
<ul>
<li>Used to preserve the low-level "local" and high-level "global" agreement, simultaneously.</li>
<li>Consists of two parts: the contrast of low-level node representation between two views, and the contrast of high-level graph representation between two views.</li>
<li>Positive samples <span class="math inline">\((z_i^a,z_i^b)\)</span>, K-1 pair of negative samples <span class="math inline">\((z_i^a,z_j^a),(z_i^a,z_j^b)\)</span>.</li>
</ul>
<h2 id="paper-29-permutation-invariant-variational-autoencoder-for-graph-level-representation-learning">Paper 29: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.09856">Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</a></h2>
<h3 id="why-27">Why</h3>
<ul>
<li>unsupervised learning on graphs mainly focus on node-level representation learning, which aims at embedding the local graph structure into latent node representations.
<ul>
<li>Usually implemented by an autoencoder framework</li>
</ul></li>
<li>A graph-level representation should not depend on the order of the nodes in the input representation of a graph.</li>
</ul>
<h3 id="goal-26">Goal</h3>
<ul>
<li>Graph-level unsupervised learning</li>
<li>Propose a permutation-invariant variational autoencoder for graph structured data. The method indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching</li>
</ul>
<h3 id="how-28">How</h3>
<ul>
<li>Address the order ambiguity issue by training alongside the encoder and decoder model an additional permuter model that assigns to each input graph a permutation matrix that aligns the input graph node ordering with the output graph ordering.</li>
</ul>
<h4 id="problem-definition">Problem definition</h4>
<ul>
<li>Consider a dataset of graphs <span class="math inline">\(\mathrm{G}={\mathcal{G}^{(i)}}_{i=0}^N\)</span>, the goal is to represent in a low-dimensional continuous space. Assume the data is generated by a process <span class="math inline">\(p_\theta(\mathcal{G}|\mathrm{z})\)</span>, then approximate the intractable posterior by <span class="math inline">\(q_\phi(\mathcal{G}|\mathrm{z})\)</span> and minimize the lower bound on the marginal likelihood of graph <span class="math inline">\(G^{(i)}\)</span>: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922115927078.png" alt="image-20210922115927078" style="zoom: 67%;" />
<ul>
<li>The KL-divergence divergence term regularizes the encoded latent codes of graphs <span class="math inline">\(\mathcal{G}^{(i)}\)</span></li>
<li>The second term enforce high similarity of decoded graphs to their encoded counterparts</li>
<li>Parameters <span class="math inline">\(q_\phi,p_\theta\)</span> can be estimated by NNs that encode and decode node features <span class="math inline">\(\mathrm{X}_\pi^{(i)}\)</span> and adjacency matrices of graphs <span class="math inline">\(\mathcal{G}^{(i)}\)</span>.</li>
</ul></li>
<li>Considering the permutation invariant, <span class="math inline">\(\mathcal{G}_\pi,\hat{\mathcal{G}}_{\pi&#39;}\)</span> have to be brought in the same node ordering.</li>
<li>Previous work either fail to track <span class="math inline">\(\mathrm{P}_{\pi&#39;\rightarrow \pi}\)</span>, or is computational costly (by maximizing the similarity which involves up to <span class="math inline">\(O(n^4)\)</span> re-ordering operations at each training step).</li>
</ul>
<h4 id="permutation-invariant-variational-graph-autoencoder">Permutation-Invariant variational graph autoencoder</h4>
<ul>
<li><p>Propose to silver the reordering problem implicitly by inferring the permutation matrix <span class="math inline">\(\mathrm{P}_{\pi&#39;\rightarrow\pi}\)</span> from the input graph <span class="math inline">\(\mathcal{G}_\pi\)</span> by a model <span class="math inline">\(g_\psi(\mathrm{P}|\mathcal{G}_\pi)\)</span>.</p></li>
<li><p>Train this model jointly with the encoder model <span class="math inline">\(q_\phi(\mathrm{z}|\mathcal{G})\)</span> and decoder model <span class="math inline">\(p_\theta(\mathcal{G}|\mathrm{z})\)</span>.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922121759262.png" alt="image-20210922121759262" /><figcaption aria-hidden="true">image-20210922121759262</figcaption>
</figure></li>
<li><p>The permuter model has to learn how the ordering of nodes in the graph generated by the encoding model will differ from a specific node ordering present in the input graph. It predicts for each node <span class="math inline">\(i\)</span> of the input graph a score <span class="math inline">\(s_i\)</span> corresponding to its probability to have a lower node index in the decoded graph.</p></li>
<li><p>Next they derive the elements of the permutation matrix <span class="math inline">\(\mathrm{P}\)</span> by sorting the scored nodes: <span class="math inline">\(p_{ij}=\begin{cases}1,&amp; ifj=\arg sort(s)_i\\0,&amp;else\end{cases}\)</span>. The argsort is then replaced by the continuous relaxation: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922123207545.png" alt="image-20210922123207545" style="zoom:60%;" />， where the softmax operator is applied row-wise, <span class="math inline">\(d(x,y)\)</span> is the <span class="math inline">\(L_1\)</span>-norm and <span class="math inline">\(\tau\)</span> a temperature-parameter.</p></li>
<li><p>The permuter model <span class="math inline">\(g_\psi\)</span> is trained with stochastic gradient descent.</p></li>
<li><p>In order to push the relaxed permutation matrix towards a real permutation matrix (only one 1 in every row and column), add a row- and column-wise entropy term as additional penalty term to equation (5)</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922123705808.png" alt="image-20210922123705808" /><figcaption aria-hidden="true">image-20210922123705808</figcaption>
</figure></li>
<li><p>By enforcing <span class="math inline">\(C(\mathrm{P})=0\)</span>, the real permutation matrix is fulfilled.</p></li>
</ul>
<h4 id="details-of-the-model-architecture">Details of the model architecture</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922134834006.png" alt="image-20210922134834006" /><figcaption aria-hidden="true">image-20210922134834006</figcaption>
</figure>
<ul>
<li>Graph representation by directional messages: MPNN
<ul>
<li>Key idea: aggregation of neighborhood information by passing and receiving messages of each node to and from neighbouring noes in a graph. In <span class="math inline">\(k\)</span> hop.</li>
<li>The item <span class="math inline">\(m_{ij}\)</span> of message matrix <span class="math inline">\(\mathrm{M}\)</span> is <span class="math inline">\(m_{ij}=\sigma([x_i\|x_j\|e_{ij}]\mathrm{W+b})\)</span>.</li>
<li>Nodes in this view are represented by self-message <span class="math inline">\(\mathrm{diag(M)}\)</span>.</li>
</ul></li>
<li>Self-attention on directed messages
<ul>
<li>Embedded in MPNN, take the message matrix to compute key,value and query.</li>
<li>Let messages <span class="math inline">\(m_{ij}\)</span> only attend on incoming messages <span class="math inline">\(m_{ki}\)</span> to reduce the complexity.</li>
</ul></li>
<li>Encoder
<ul>
<li>A dummy node <span class="math inline">\(v_0\)</span> acts as an embedding node, the cumulated messages are updated in <span class="math inline">\(m_{0,0}\)</span>.</li>
<li>Then use the reparameterization trick and sample the latent representation <span class="math inline">\(\mathrm{z}\)</span> of a graph by sampling from a multivariate normal distribution.</li>
</ul></li>
<li>Permuter
<ul>
<li>to predict how to re-order the nodes in the output graph to match the ordering of nodes in the input graph.</li>
<li>After extract node embeddings represented by self messages on the main diagonal of the encoded message matrix, retrieve the permutation matrix <span class="math inline">\(\hat{\mathrm{P}}\)</span> by scoring these messages with a function which is parameterized by a linear layer and the soft-sort operator.</li>
</ul></li>
<li>Decoder
<ul>
<li>Add position embeddings to the initial node embeddings, same as what's defined in Bert. And then the permutation matrix is performed on these position embeddings. After obtaining the permutation matrix, extracted messages are permuted by the permutation matrix and then use self-attention in decoder to extract node and edge features.</li>
<li>Pad all graphs in a batch with empty nodes to match the number of nodes of the largest graph, attention on empty nodes is masked out at all time.</li>
</ul></li>
</ul>
<h2 id="paper-30-pinet-attention-pooling-for-graph-classification">Paper 30: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.04575">PiNet: Attention Pooling for Graph Classification</a></h2>
<p>Codes: http://github.com/meltzerpete/pinet</p>
<h3 id="why-28">Why</h3>
<ul>
<li>The essential to the success of CNNs is the process of pooling
<ul>
<li>Pooling is invariance to different orderings of the input vectors.</li>
<li>GCNs achieve invariance by pooling neighbors' feature vectors with symmetric operators such as feature-weighted mean, max, and self-attention weighted means.</li>
</ul></li>
<li>Previous work on permutation invariant
<ul>
<li>Propose a permutation invariant function <span class="math inline">\(f(\mathbb{X})\)</span> on the set <span class="math inline">\(\mathbb{X}\)</span> may be learned indirectly through decomposition in the form <span class="math inline">\(f(\mathbb{X})=\rho(\sum\limits_{x\in\mathrm{X}}\phi(x))\)</span>. The model is named as Janossy pooling, where <span class="math inline">\(\rho\)</span> is a normalization function, and the summation occurs over the set of all possible permutations of the input set.</li>
<li>The use of canonical orderings to tackle permutations in graph representation learning has been demonstrated to be effective in Patchy-SAN.</li>
<li>DGCNN uses a sorting method to introduce permutation invariance.</li>
<li>Variants may also be expressed as an instance of the WL graph isomorphism algorithm.</li>
</ul></li>
<li>A simple solution is to use a symmetric operator to combine vertex vectors to form a single graph vector.</li>
</ul>
<h3 id="goal-27">Goal</h3>
<ul>
<li>graph classification</li>
<li>A generalized differential attention-based pooling mechanism for utilizing graph convolution operations for graph-level classification.</li>
<li>The proposed pooling mechanism is differentiable, by which the vertex-level invariance to permutation achieved for vertex level tasks may be extended to the graph level.</li>
</ul>
<h3 id="how-29">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922205836991.png" alt="image-20210922205836991" /><figcaption aria-hidden="true">image-20210922205836991</figcaption>
</figure>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210922210144596.png" title="fig:" alt="image-20210922210144596" /></li>
<li>The inner softmax constrains the attention coefficients to sum to 1 and prevents them from all falling to 0. The outer softmax may be replaced for multi-label classification tasks (i.e. sigmoid).</li>
</ul>
<h2 id="paper-31-self-supervised-graph-level-representation-learning-with-local-and-global-structure">Paper 31: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.04113">Self-supervised Graph-level Representation Learning with Local and Global Structure</a></h2>
<p>Codes: https://github.com/DeepGraphLearning/GraphLoG</p>
<h3 id="why-29">Why</h3>
<ul>
<li>Existing methods mainly focus on preserving the local similarity structure between different graph instances but fail to discover the global semantic structure of the entire dataset.</li>
<li>a desirable graph representation should be able to preserve the local-instance structure. It should also reflect the global-semantic structure of the data.</li>
<li>Require a model that is sufficient to model both the local and global structure of a set of graphs</li>
</ul>
<h3 id="goal-28">Goal</h3>
<ul>
<li>Study the unsupervised/self supervised graph-level representation</li>
<li>propose a unified framework called Local-instance and Global-semantic Learning (GraphLoG) for self-supervised whole-graph representation learning</li>
<li>Introduce hierarchical prototypes to capture the global semantic clusters.</li>
<li>EM for training</li>
</ul>
<h3 id="how-30">How</h3>
<ul>
<li>To preserve local similarity between various graph instances, align the embeddings of correlated graphs/subgraphs by discriminating the correlated graph/subgraph pairs from the negative pairs.</li>
<li>Import hierarchical prototypes to depict the latent distribution of a graph dataset in a hierarchical way. Propose to maximizes the data likelihood with respect to both the GNN parameters and hierarchical prototypes via an online EM algorithm.
<ul>
<li>E-step, infer the embeddings of the mini-batch of graphs sampled from the data distribution with a GNN, and sample the latent variable of each graph from the posterior distribution defined by current model.</li>
<li>M-step, maximizes the expectation of complete-data likelihood with respect to the current model by optimizing with a mini-batch-induced objective function.</li>
</ul></li>
</ul>
<h4 id="problem-definition-1">Problem definition</h4>
<ul>
<li>Expect the graph embeddings <span class="math inline">\(\mathrm{H}=\{h_{\mathcal{G}_1},\cdots,h_{\mathcal{G}_M}\}\)</span> follow both the local-instance and global-semantic structure.
<ul>
<li>Local-instance structure: For a pair of similar graphs/subgraphs, <span class="math inline">\(\mathcal{G,G&#39;}\)</span>, their embeddings are expected to be nearby in the latent space.</li>
<li>Global-semantic structure: After mapping to the latent space, the embeddings of a set of graphs are expected to form some global structures reflecting the clustering patterns of the original data.</li>
</ul></li>
<li>Since <span class="math inline">\(h_v\)</span> summarizes the information of a subgraph centered around node <span class="math inline">\(v\)</span>, they refer <span class="math inline">\(h_v\)</span> as subgraph embedding. Then the entire graph's embedding is derived as <span class="math inline">\(h_\mathcal{G}=f_{\mathcal{R}}(\{h_v|v\in\mathcal{V}\})\)</span>, where <span class="math inline">\(f_{\mathcal{R}}\)</span> is a permutation-invariant readout function, like mean pooling.</li>
<li>EM
<ul>
<li>In E-step, using the observed <span class="math inline">\(\mathrm{X}\)</span> and pre-estimated <span class="math inline">\(\theta_{t-1}\)</span> to estimate the posterior <span class="math inline">\(p(\mathrm{Z|X,}\theta_{t-1})\)</span></li>
<li>In M-step, use the posterior to calculate the expectation of <span class="math inline">\(\log p(\mathrm{X,Z|}\theta)\)</span>, and <span class="math inline">\(\theta_t\)</span> is the solution of maximizing this expectation.</li>
</ul></li>
</ul>
<h4 id="graphlog">GraphLoG</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923134844909.png" alt="image-20210923134844909" /><figcaption aria-hidden="true">image-20210923134844909</figcaption>
</figure>
<ul>
<li><p>Learning local-instance structure of graph representation</p>
<ul>
<li>The problem is formulated as maximizing the similarity of correlated graph/subgraph pairs while minimizing that of negative pair.</li>
<li>Given a graph sampled from the data distribution, obtain its correlated counterpart through randomly masking a part of node/edge attributes in the graph.
<ul>
<li>Given a molecular graph, we randomly mask the attributes of 30% nodes (i.e. atoms) in it to obtain its correlated counterpart. Specifically, we add an extra dimension to the feature of atom type and atom chirality to indicate masked attribute, and the input features of all masked atoms are set to these extra dimension.</li>
</ul></li>
<li>The similarity function is cosine similarity.</li>
<li>The negative pairs: for a correlated graph pair <span class="math inline">\(\mathcal{G,G&#39;}\)</span> or correlated subgraph pair <span class="math inline">\(\mathcal{G_v,G&#39;_v}\)</span>, substitute <span class="math inline">\(\mathcal{G(G}_v)\)</span> randomly with another graph from the dataset (a subgraph centered around another node in the same graph) to construct negative pairs.</li>
<li>The local-instance structure of graph representations is then solved by minimizing <span class="math inline">\(\mathcal{L}_{graph}+\mathcal{L}_{sub}\)</span>.</li>
</ul></li>
<li><p>Leaning global-semantic structure of graph representation</p>
<ul>
<li><p>Hierarchical prototypes to represent the feature clusters in the latent space in a hierarchical way. Each layer has several prototypes.</p></li>
<li><p>encourage the graphs to be compactly embedded around corresponding prototypes and, at the same time, refine hierarchical prototypes to better represent the data. Formalize the problem as optimizing a latent variable model.</p></li>
<li><p>To solve parameters: hierarchical prototypes <span class="math inline">\(\mathrm{C}\)</span>, the online EM is applied.</p>
<ul>
<li><p>The online EM is base on i.i.d. assumption, where both the complete-data likelihood and the posterior probability of latent variables can be factorized over each observed-latent variable pair</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923135511275.png" alt="image-20210923135511275" /><figcaption aria-hidden="true">image-20210923135511275</figcaption>
</figure></li>
<li><p>The initialization of model parameters</p>
<p>Pretrained GNN by minimizing <span class="math inline">\(\mathcal{L}_{local}\)</span> and employ the derived GNN model as initialization, then use it to extract the embeddings of all graphs in the data set. After, k-means clustering is applied upon these graph embeddings to initialize the bottom layer prototypes with the output cluster centers.</p>
<p>The prototypes of upper layers are initialized by iteratively applying kmeans to the prototypes of the layer below.</p>
<p>Clusters with less than two samples are dropped.</p></li>
<li><p>E-step</p>
<ul>
<li>randomly sample a mini-batch of graphs. Each latent variable is a chain of prototypes from top layer that best represent graph <span class="math inline">\(\mathcal{G}_n\)</span> in the latent space.</li>
<li>To train, use stochastic EM algorithm, draw a sample for Monte Carlo estimation.</li>
<li>Formally, we first sample a prototype from top layer according to a categorical distribution over all the top layer prototypes. For the sampling at layer <span class="math inline">\(l\)</span> (<span class="math inline">\(l &gt; 2\)</span>), we draw a prototype from that layer based on a categorical distribution over the child nodes of prototype sampled from the layer above.</li>
</ul></li>
<li><p>M-step: Maximizes the expected log-likelihood on mini-batch to obtain <span class="math inline">\(\theta,\mathrm{C}\)</span>.</p></li>
</ul></li>
<li><p>This algorithm can indeed maximizes the marginal likelihood function <span class="math inline">\(p(\mathrm{G}|\theta,\mathrm{C})\)</span>.</p></li>
</ul></li>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923144941732.png" alt="image-20210923144941732" /><figcaption aria-hidden="true">image-20210923144941732</figcaption>
</figure></li>
</ul>
<h2 id="paper-32-self-supervised-heterogeneous-graph-neural-network-with-co-contrastive-learning">Paper 32: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.09111">Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</a></h2>
<p>Codes: https://github.com/liun-online/HeCo</p>
<h3 id="why-30">Why</h3>
<ul>
<li>HGNNs can effectively combine the mechanism of message passing with complex heterogeneity.</li>
<li>Most methods on HGNN are semi-supervised, but the requirement of some node labels are known is not always fullfilled.</li>
<li>Contrastive learning is able to learn the discriminative embeddings even without labels. But the following problems have to be addressed
<ul>
<li>How to design a heterogeneous contrastive mechanism
<ul>
<li>Different meta-paths (used to capture the long-rang structure in a HIN) represent different semantics.</li>
<li>Performing contrastive learning only on single meta-path view is actually is actually distant from sufficient.</li>
</ul></li>
<li>How to select proper views in a HIN:
<ul>
<li>The selected view should cover both of the local and high-order structures while meta-path is usually used to extract the high-order structure. Meta-path is the combination of multiple relations, so it contains complex semantics, which is regarded as high-order structure.</li>
<li>Both of the network schema and meta-path structure views should be carefully considered.</li>
</ul></li>
<li>How to set a difficult contrastive task
<ul>
<li>A proper contrastive task will further promote to learn a more discriminative embedding.</li>
<li>Need to make the contrastive learning on two not too similar views. One strategy is to enhance the information diversity in two views, the other is to generate harder negative samples of high quality.</li>
</ul></li>
</ul></li>
<li>There is a lack of methods contrasting across views in HIN so that the high-level factors can be captured.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923162244368.png" alt="image-20210923162244368" /><figcaption aria-hidden="true">image-20210923162244368</figcaption>
</figure>
<h3 id="goal-29">Goal</h3>
<ul>
<li>Self-supervised on Heterogeneous graph neural networks (HGNNs), <strong>learn node embeddings</strong>.</li>
<li>Propose a novel co-contrastive learning mechanism for HGNNs, named HeCo. It employs cross-view contrastive mechanism. Specifically , two views of a HIN, namely network schema and meta-path views are proposed to learn node embeddings.</li>
<li></li>
</ul>
<h3 id="how-31">How</h3>
<ul>
<li>Choose network schema and meta-path structure as two views.
<ul>
<li>In network schema, the node embedding is learned by aggregating information from its direct neighbors.</li>
<li>In meta-path view, the node embedding is learned by passing messages along multiple meta-paths to capture high-order structure.</li>
</ul></li>
<li>To make contrast harder, propose a view mask mechanism that hides different parts of network schema and meta-path, respectively.</li>
<li>Given the high correlation between nodes, define the positive samples of a node in HIN and design a optimization strategy specially.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923163653685.png" alt="image-20210923163653685" /><figcaption aria-hidden="true">image-20210923163653685</figcaption>
</figure>
<h4 id="node-feature-transformation">Node feature transformation</h4>
<ul>
<li>First need to project features of all types of nodes into a common latent vector space to avoid features of nodes distributed into different spaces.</li>
<li>For a node <span class="math inline">\(i\)</span> with type <span class="math inline">\(\phi_i\)</span>, design a type-specific mapping matrix <span class="math inline">\(W_{\phi_i}\)</span> to transform its feature <span class="math inline">\(x_i\)</span> into common space as <span class="math inline">\(h_i=\sigma(W_{\phi_i}\cdot x_i+b_{\phi_i})\)</span>, where <span class="math inline">\(h_i\)</span> is the projected feature of node.</li>
</ul>
<h4 id="network-schema-view-guided-encoder">Network schema view guided encoder</h4>
<ul>
<li>Now aim to learn the embedding of node <span class="math inline">\(i\)</span> under network schema view. For node <span class="math inline">\(i\)</span>, different types of neighbors contribute differently to its embedding, and so do the different nodes with the same type. Therefore, attention is used in node-level and type-level to hierarchically aggregate messages from other types of neighbors to target node <span class="math inline">\(i\)</span>.</li>
<li>For node-level (same type) attention, the projected features of nodes <span class="math inline">\(h_i\)</span> are used to calculate the attention map, then applied on projected features. Note that in practice, they randomly sample a part of neighbors (in the same type) for aggregation on one node. In this way, they ensure that every node aggregates the same amount of information from neighbors, and promote the diversity of embeddings in each epoch under this view.</li>
<li>After getting all type embeddings, they use type-level attention to fuse them together to get the final embedding for node <span class="math inline">\(i\)</span> under network-schema view. The attention here is apply tanh on linear combination of type embeddings.
<ul>
<li>Is <span class="math inline">\(\mathrm{a}_{sc}\)</span> trainable here?</li>
</ul></li>
</ul>
<h4 id="meta-path-view-guided-encoder">Meta-path view guided encoder</h4>
<ul>
<li>Each meta-path represents one semantic similarity.</li>
<li>For one meta-path <span class="math inline">\(\mathcal{P}_n\)</span> from <span class="math inline">\(M\)</span> meta-paths, apply meta-path specific GCN to encode, features will be <span class="math inline">\(h_j^{\mathcal{P}_n}=\frac{1}{d_i+1}h_i+\sum\limits_{j\in N_i^{\mathcal{P}_n}}\frac{1}{\sqrt{(d_i+1)(d_j+1)}}h_j\)</span>, where <span class="math inline">\(h_i,h_j\)</span> are their projected features. For each node, along <span class="math inline">\(M\)</span> meta-path, it will have <span class="math inline">\(M\)</span> embeddings.</li>
<li>Then to fuse <span class="math inline">\(M\)</span> embeddings for each node, use semantic-level attention to get the final embedding under the meta-path view.
<ul>
<li>Is <span class="math inline">\(\mathrm{a}_{mp}\)</span> trainable here?</li>
</ul></li>
</ul>
<h4 id="view-mask-mechanism">View mask mechanism</h4>
<ul>
<li><p>a view mask mechanism that hides different parts of network schema and meta-path views, respectively.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210923171508369.png" alt="image-20210923171508369" /><figcaption aria-hidden="true">image-20210923171508369</figcaption>
</figure></li>
<li><p><font color="blue">How to select nodes? Still unclear for me.</font></p></li>
</ul>
<h4 id="collaboratively-contrastive-optimization">Collaboratively contrastive optimization</h4>
<ul>
<li>Feed the obtained features from two views to MLP to map them into the space where contrastive loss is calculated. Both views use the same MLP.</li>
<li>Positive pairs and negative pairs
<ul>
<li>But consider that nodes are usually highly-correlated because of edges, propose a new positive selection strategy
<ul>
<li>if two nodes are connected by many meta-paths, they are positive samples. Then the selected positive samples can well reflect the local structure of the target node.</li>
</ul></li>
<li>Steps
<ul>
<li>First count the number of meta-paths connecting two nodes</li>
<li>Then sort the node set <span class="math inline">\(S_i\)</span> by this count in descending order and set a threshold <span class="math inline">\(T_{pos}\)</span>. If The count is larger than the threshold, select first <span class="math inline">\(T_{pos}\)</span> nodes from <span class="math inline">\(S_i\)</span> as positive samples of node <span class="math inline">\(i\)</span>.</li>
<li>Treat all left nodes as negative samples of <span class="math inline">\(i\)</span>.</li>
</ul></li>
<li>Different from traditional methods, they consider multiple positive pairs.</li>
<li>In contrastive loss <span class="math inline">\(\mathcal{L}_i^{sc}\)</span>, for two nodes in a pair, the target embedding is from the network schema view and the embeddings of positive and negative samples are from the meta-path view. This also realize the cross-view self-supervision.</li>
<li>In contrastive loss <span class="math inline">\(\mathcal{L}_i^{mp}\)</span>, the target embedding is from the meta-path view while the embeddings of positive and negative samples are from the network schema view.</li>
</ul></li>
<li>The final loss is the combination of <span class="math inline">\(\mathcal{L}_i^{sc},\mathcal{L}_i^{mp}\)</span>, optimized by back propagation. The <span class="math inline">\(\mathcal{L}_i^{mp}\)</span> is used to perform downstream tasks because nodes of target type explicitly participant into the generation of <span class="math inline">\(z^{mp}\)</span>.</li>
</ul>
<h4 id="model-extension">Model extension</h4>
<ul>
<li>HeCo_GAN
<ul>
<li>Sample additional negatives from a continuous Gaussian distribution.</li>
<li>Composed of three components: the proposed HeCo, a discriminatory D and a generator G.
<ul>
<li>Train D and G: first train D to identify the embedding from two views as positives and that generated from G as negatives. Then rain G to generate samples with high quality that fool D.</li>
<li>Use trained G to generate samples, which can be viewed as the new negative samples with high quality.</li>
</ul></li>
</ul></li>
<li>HeCo_MU
<ul>
<li>Proposed to improve results in supervised learning by adding arbitrary two samples to create a new one.</li>
<li>Build hard negative samples: get cosine similarities between noes <span class="math inline">\(i\)</span> and nodes from <span class="math inline">\(\mathbb{N}_i\)</span> during calculating, and sort them in the descending order. Then select first top <span class="math inline">\(k\)</span> negative samples as the hardest negatives, and randomly add them to create new <span class="math inline">\(k\)</span> negatives, which are involved in training.</li>
<li>No learnable parameters in this version.</li>
</ul></li>
</ul>
<h2 id="paper-33-sm-sge-a-self-supervised-multi-scale-skeleton-graph-encoding-framework-for-person-re-identification">Paper 33: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.01903">SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</a></h2>
<p>Codes: https://github.com/Kali-Hac/SM-SGE</p>
<h3 id="why-31">Why</h3>
<ul>
<li>Previous works just learn body and motion features from the body-joint trajectory, whereas lack a systematic way to model body structure and underlying relations of body components beyond the scale of body joints.
<ul>
<li>extract appearance-based features such as body texture and silhouettes from RGB or depth images: but vulnerability to illumination or appearance changes</li>
<li>skeleton-based models: robust to factors such as view and body shape changes</li>
</ul></li>
<li>still an open challenge to extract discriminative body and motion features with 3D skeletons for person Re-ID</li>
<li>Multi-scale skeleton graphs: many previous methods extract hand-crafted features from skeletons with a single spatial scale and topology</li>
<li>Multi-scale relation learning: previous works mostly encode body-joint trajectory or pre-defined pose descriptors into a feature vector, rarely explore the inherent relations between different body joints or components.</li>
<li>Multi-scale skeleton dynamics modeling: Previous methods mostly learn skeleton motion at a fixed scale of body joints, lack the flexibility to capture motion patterns at various levels==&gt; cause a loss of global motion features.</li>
</ul>
<h3 id="goal-30">Goal</h3>
<ul>
<li>Person re-identification. Propose a Self-supervised Multi-scale Skeleton Graph Encoding (SM-SGE) framework that models human body, component relations, and skeleton dynamics from unlabeled skeleton graphs of various scales to learn an effective skeleton representation for person Re-ID.</li>
<li>MGRN (multi-scale graph relation network) + MSR (multi-scale skeleton reconstruction), where MSR with two concurrent pretext tasks, namely skeleton subsequence reconstruction task and cross-scale skeleton inference task.</li>
<li>No need of ID labels during training.</li>
<li>Effective with the 3D skeletons estimated from RGB videos.</li>
</ul>
<h3 id="how-32">How</h3>
<ul>
<li>First devise multi-scale skeleton graphs with coarse-to-fine human body partitions, which enables to model body structure and skeleton dynamics at multiple levels.</li>
<li>To mine inherent correlations between body components in skeletal motion, propose a multi-scale graph relation network to learn structural relations between adjacent body-component nodes and collaborative relations among nodes of different scales.</li>
<li>Propose a multi-scale skeleton reconstruction mechanism to enable to encode skeleton dynamics and high-level semantics from unlabeled skeleton graphs.</li>
<li>The input is several clips of 3D skeleton sequence. First train to get representation <span class="math inline">\(\mathrm{H}\)</span>, then take the frozen <span class="math inline">\(\mathrm{H}\)</span> and corresponding ID labels to train a MLP for person Re-ID.</li>
</ul>
<h4 id="ms-skeleton-graph-construction">MS skeleton graph construction</h4>
<ul>
<li><p>Goal: learn a latent discriminative representation <span class="math inline">\(\mathrm{H}\)</span> from skeleton sequences <span class="math inline">\(\mathrm{S}\)</span> without using any label.</p></li>
<li><p>regard body joints as the basic components, and merge spatially nearby groups of joints to be a higher level body-component node at the center of their positions. The graphs at joint-scale, part-scale and body-scale are denoted as <span class="math inline">\(\mathcal{G^1,G^2,G^3}\)</span> respectively.</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924142204794.png" alt="image-20210924142204794" style="zoom:50%;" /></p></li>
<li><p>The hyper-joint-scale is implemented by interpolating nodes between adjacent nodes.</p></li>
</ul>
<h4 id="msgr">MSGR</h4>
<ul>
<li><p>Propose to learn relations of body components from two aspects: structural relations which provide a higher motion correlation than distant pairs, and collaborative relations that considers several action-related body components.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924222135548.png" alt="image-20210924222135548" /><figcaption aria-hidden="true">image-20210924222135548</figcaption>
</figure></li>
<li><p>Structural relation learning</p>
<ul>
<li>First compute the structural relation between adjacent nodes as follows: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924185808927.png" alt="image-20210924185808927" style="zoom:33%;" /></li>
<li>Then normalized relations with a temperature-based softmax (T-softmax) function to learn flexible structural relations. The higher value of temperature produces a softer relation distributed over nodes and retains more similar relation information.</li>
<li>The aggregate features of most relevant nodes to represent the node <span class="math inline">\(i\)</span>, in which both the <span class="math inline">\(v_j\)</span> and the relation matrix <span class="math inline">\(\mathrm{A}_{i,j}^m\)</span> are taken as the input.</li>
<li>The previous three steps are operated for <span class="math inline">\(P\)</span> times to obtain <span class="math inline">\(P\)</span> different structural relation matrices.</li>
<li>Then averagely aggregate features learned by these relation matrices to represent each node.</li>
</ul></li>
<li><p>Collaborative relation learning</p>
<ul>
<li>Unique walking patterns could be represented by the dynamic cooperation among body joints or between different body components.</li>
<li>Learn collaborative relations from two aspects: single-scale collaborative relations among nodes of the same scale, and cross-scale collaborative relations between a node and its spatially corresponding or motion-related higher level body component.</li>
<li>Use softmax and the nodes features at different scale to obtain collaborative relation matrix, still the T-softmax. But calculate the inner product of node feature representations to measure the degree of collaboration between two nodes.</li>
</ul></li>
<li><p>Multi-scale collaboration fusion</p>
<ul>
<li>Each node representation in the <span class="math inline">\(a\)</span>-th scale graph is updated by the feature fusion of collaborative nodes from different graphs as <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210924222304571.png" alt="image-20210924222304571" /> where the <span class="math inline">\(\lambda_C\)</span> is the fusion coefficient to fuse collaborative graph node features.</li>
<li>graph representations of each individual scale is retained to encourage their model to capture skeleton dynamics and pattern information at different levels</li>
</ul></li>
</ul>
<h4 id="multi-scale-skeleton-reconstruction-mechanism">Multi-scale skeleton reconstruction mechanism</h4>
<ul>
<li>Pretext tasks for capturing skeleton graph dynamics and high-level semantics from different scales of graphs.</li>
<li>Skeleton subsequence reconstruction task: MSR aims to reconstruct target multi-scale skeletons corresponding to multi-scale graphs in subsequences, instead of reconstructing the original subsequences.</li>
<li>Cross-scale subsequence reconstruction task: exploit fine skeleton graph representations to infer 3D positions of coarser body components. E..g., use joint-level graph representations to infer nodes of body-scale skeletons.</li>
<li>To simultaneously achieve above two pretext tasks, first sample <span class="math inline">\(k\)</span>-length subsequences by randomly discarding <span class="math inline">\((f-k)\)</span> skeletons from the input sequence. The sampling process is repeated for <span class="math inline">\(r\)</span> rounds and each round cover all possible lengths form 1 to <span class="math inline">\(f-1\)</span>.
<ul>
<li>Given a sampled skeleton subsequence, MGRN encodes its corresponding skeleton graphs of each scale into fused graph features as previously mentioned.</li>
<li>Then, leverage an LSTM to integrate the temporal dynamics of graphs at each scale into effective representations.</li>
<li>Last, use graph states at the <span class="math inline">\(𝑎^{𝑡ℎ}\)</span> scale and MLP to reconstruct the target skeleton at the <span class="math inline">\(b^{𝑡ℎ}\)</span> scale.</li>
<li>The reconstruction loss use <span class="math inline">\(\ell_1\)</span> norm.</li>
</ul></li>
</ul>
<h4 id="the-entire-framework">The entire framework</h4>
<ul>
<li>For the downstream task, extract encoded graph states (𝒉) learned from the pre-trained framework, and exploit an MLP <span class="math inline">\((g(\cdot))\)</span> to predict the sequence label .</li>
<li>For the <span class="math inline">\(t^{th}\)</span> skeleton in an input sequence, concatenate its corresponding encoded graph states of four scales as its skeleton-level representation of the sequence. Then train MLP with the frozen <span class="math inline">\(\mathrm{H}_t\)</span> and its label. The predicted label is <span class="math inline">\((g(\mathrm{H}_t))\)</span>.</li>
</ul>
<h2 id="paper-34-space-time-correspondence-as-a-contrastive-random-walk">Paper 34: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.14613">Space-time correspondence as a contrastive random walk</a></h2>
<p>Codes: https://github.com/ajabri/videowalk</p>
<h3 id="why-32">Why</h3>
<ul>
<li>Challenge: temporal correspondence: a physical point depicted at position <span class="math inline">\((x,y)\)</span> in frame <span class="math inline">\(t\)</span> might not have any relation to what we find at that same <span class="math inline">\((x,y)\)</span> in frame <span class="math inline">\(t+k\)</span>.</li>
<li>Cycle-consistency of time is a promising direction.
<ul>
<li>current methods rely on complex and greedy tracking that may lead to local optima, especially when applied recurrently in time.</li>
</ul></li>
</ul>
<h3 id="goal-31">Goal</h3>
<ul>
<li>self-supervised approach for learning a representation of visual correspondence from raw video, where correspondence means prediction of links in space-time graph constructed from video.</li>
<li>Take patches sampled from each frame as nodes, and nodes adjacent in time sahre a directed edge. The long-range correspondence is computed as a walk along the graph.</li>
<li>Optimize the representation to place high probability along paths of similarity.</li>
</ul>
<h3 id="how-33">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926103840998.png" alt="image-20210926103840998" /><figcaption aria-hidden="true">image-20210926103840998</figcaption>
</figure>
<ul>
<li>Turn training videos into palindromes: sequences where the first half is repeated backwards. View each step of the walk as a contrasstive learning problem where the walker's target provides supervision for entire chains of intermediate comparisons.</li>
</ul>
<h4 id="contrastive-random-walks-on-video">Contrastive random walks on video</h4>
<ul>
<li>An encoder <span class="math inline">\(\phi\)</span> maps nodes to <span class="math inline">\(\ell_2\)</span>-normalized <span class="math inline">\(d\)</span>-dimensional vectors. The similarity function is non-neative affinities, the whole function of encoder is based on <span class="math inline">\(T\)</span>-softmax, and the output is denoted as the stocastic matrix of affinities <span class="math inline">\(A_{t}^{t+1}\)</span>, which describes only the local affinity between the patches of two video frames <span class="math inline">\(\mathrm{q}_t,\mathrm{q}_{t+1}\)</span>. <span class="math inline">\(A_{t}^{t+1}\)</span> is then used to build transition probabilities of a random walk on the whole graph, which relates all nodes in the video as a Markov chain.
<ul>
<li>The step of random Walker Can be viewed as performing tracking by contrastive similarity of neighboring nodes.</li>
<li>The long-range correspondence as walking multiple steps along the graph is <span class="math inline">\(\bar{A}_{t}^{t+k}=\prod\limits_{i=0}^{k-1}A_{t+i}^{t+i+1}=P(X_{t+k}|X_t)\)</span>.</li>
<li>They use resnet.</li>
</ul></li>
<li>Guiding the walk
<ul>
<li>Aim: encourage the random Walker to follow paths of corresponding patches as it steps through time. ==&gt; <em>construct targets for free by choosing palindromes as sequences for learning (right)</em></li>
<li>Train: use labels to fit the embedding by maximizing the likelihood that a Walker beginning at a query node at <span class="math inline">\(t\)</span> ends at the target node at time <span class="math inline">\(t+k\)</span>. The loss is cross-entropy.</li>
<li>The walk is viewed as a chain of contrastive learning problems, providing supervision at every step amounts to maximizing similarity between query and target nodes adjacent in time, while minimizing similarity to all other neighbors.</li>
<li>By minimizing the loss function, they shift affinity to paths that link the query and target.</li>
</ul></li>
</ul>
<h4 id="ssl">SSL</h4>
<ul>
<li><p>Build labels by palindromes, where sequences that are identical when reversed, for which targets are known since the first and last frames are identical. Given a sequence of frames, they form training examples by simply concatenating the sequence with a temporally reversed version of itself, aka <span class="math inline">\((I_t,\cdots,I_{t+k})\rightarrow (I_t,\cdots,I_{t+k},\cdots I_t)\)</span>.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926115930588.png" alt="image-20210926115930588" /><figcaption aria-hidden="true">image-20210926115930588</figcaption>
</figure></li>
<li><p>The cycle-consistency objective is then built as <span class="math inline">\(\mathcal{L}_{cyc}^k=\mathcal{L}_{CE}(\bar{A}_t^{t+k}\bar{A}_{t+k}^t,I)=-\sum\limits_{i=1}^{N}\log P(X_{t+2k}=i|X_t=i)\)</span>.</p></li>
</ul>
<h4 id="edge-dropout">Edge dropout</h4>
<ul>
<li>Consider segments correspondence, where points within a segment might have strong affinity to all other points in the segment.==&gt; inspire a trivial extension, randomly dropping edges from the graph, thereby forcing the Walker to consider alternative paths.</li>
<li>Implementation: use dropout to the transition matrix <span class="math inline">\(A\)</span> and then re-normalize.</li>
<li>They argue that edge dropout improve object-centric correspondence.</li>
</ul>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926121026487.png" alt="image-20210926121026487" style="zoom:45%;" /></p>
<h2 id="paper-35-spatially-consistent-representation-learning">Paper 35: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.06122">Spatially consistent representation learning</a></h2>
<p>Codes: https://github.com/kakaobrain/scrl</p>
<h4 id="why-33">Why</h4>
<ul>
<li>Previous SSL methods are prone to overlook spatial consistency of local representations and therefore have a limitation in pretraining for localization tasks such as object detection.</li>
<li>Aggresively cropped views used in existing contrastive methods can minimize representation distances between the semantically different regions of a single image.</li>
<li>Contrastive SSL aims to obtain discriminative representations based on the semantically positive and negative image pairs.</li>
<li>Most exisiting contrastive methods explot consistent global representations on a per image basis, they are likely to generate inconsistent local representations with respect to the same spatial regions after image transformations.
<ul>
<li>Lead to performance degradation on localization tasks based on spatial representations.</li>
</ul></li>
<li>Similar previous work
<ul>
<li>SwAV: introduce cluster assignment and swapped prediction, also propose multi-crop where they compare multiple smaller patches cropped from the same image, but substantially different in that the spatial consistency on the feature map is not directly considered.</li>
<li>BYOL removes the necessity of negative pairs and have shown to be more robust against changes in batch size.</li>
<li>SSL methods to leverage geometric correspondence are limited to dense representation irself that comes with object structure learning.</li>
</ul></li>
</ul>
<h4 id="goal-32">Goal</h4>
<ul>
<li>Propose SCRL (spatially consistent representation learning) for multi-object and location-specific tasks.</li>
<li>Devise a new SSL objectIve that tries to produce coherent spatial representations of a randomly cropped local region according to geometric translations and zooming operations.</li>
<li>SCRL can possibly sample infinite number of pairs by pooling the variable-sized random regions with bilinear interpolation. VADeR is a specific instance of their methods where the sizes of the pooled box is fixed to a single pixel without a sophisticated pooling technique.</li>
</ul>
<h4 id="how-34">How</h4>
<ul>
<li>From a positive pair of cropped feature maps, apply RoIAlign to the respective maps and obtain equally-sized local representations. Optimize the encoding network to minimize the distance between these two local representations. Adpat BYOL learning framework.</li>
</ul>
<h4 id="spatially-consistent-representation-learning">Spatially consistent representation learning</h4>
<ul>
<li>Use two network, the online network parameterized by <span class="math inline">\(\theta\)</span> and target network parameterized by <span class="math inline">\(\xi\)</span>. The target network provides the regression target to train the online network while <span class="math inline">\(\xi\leftarrow\tau\xi+(1-\tau)\theta\)</span> (an exponential moving average with a decay parameter <span class="math inline">\(\tau\)</span>).</li>
<li><span class="math inline">\(\mathbb{T}_1,\mathbb{T}_2\)</span> denote two sets of image augmention strategies and then generate two augmented views <span class="math inline">\(v_1 =t_1(I),v_2=t_2(I)\)</span>. Then <span class="math inline">\(v_1,v_2\)</span> are respectively fed into the two encoder networks <span class="math inline">\(f_\theta,f_\xi\)</span> (without GAP layer) to obtain spatial feature maps <span class="math inline">\(m_1,m_2\)</span>.</li>
<li>To find spatial correspondence, first find the intersection regions, use IoU to reject a candidate box.</li>
<li>Then randomly sample an arbitrary box <span class="math inline">\(B=(x,y,w,h)\)</span> in intersection region, the box has to be translated to the coordinates in each view, denoted as <span class="math inline">\(B_1,B_2\)</span>.
<ul>
<li>Though the size, location, and internal color of each box may be different for each views, the semanttic meaning in the cropped box area does not change between <span class="math inline">\(B_1,B_2\)</span>.</li>
<li>The box are rectangular box, and to map one box to another box after geometrical transformations, exclude certain affine transformations such as shear operations and rotations.</li>
<li>To obtain the equally-sized local representations from both boxes, crop the corresponding sample regions, called RoI, on the spatial feature maps and locally pool the cropped feature maps by <span class="math inline">\(1\times 1\)</span> RoIAlign.</li>
<li>Sample <span class="math inline">\(K\)</span> boxes on a given spatial feature map, one image can generate multiple pairs of local representations <span class="math inline">\(p_i^k\)</span>, where <span class="math inline">\(i\)</span> denotes the view (<span class="math inline">\(i=1,2\)</span>), <span class="math inline">\(k\)</span> is the number of sampled boxes.</li>
</ul></li>
<li>In the online network (view 1), then perform the projection from <span class="math inline">\(p_1^k\)</span> to get <span class="math inline">\(z_1^k\)</span>, followed by the prediction <span class="math inline">\(q_\theta(z_1^k)\)</span>, and the target network gets <span class="math inline">\(z_2 ^k\)</span>.</li>
<li>The spatial consistency loss is MSE between the normalized prediction <span class="math inline">\(\bar{q_\theta(z_1^k)}=\frac{q_\theta(z_1^k)}{\|q_\theta(z_1^k)\|^2}\)</span>and the normalized target prediction <span class="math inline">\(\bar{z_2^k}=\frac{z_2^k}{\|z_2^k\|^2}\)</span>.
<ul>
<li>To symmetrize the loss, also feed <span class="math inline">\(v_2\)</span> to the online network and <span class="math inline">\(v_1\)</span> to the target network respectively.</li>
</ul></li>
<li>use resnet as the backbone.</li>
</ul>
<h2 id="paper-36-spatiotemporal-contrastive-video-representation-learning">Paper 36: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.03800">Spatiotemporal contrastive video representation learning</a></h2>
<p>Codes: https://github.com/tensorflow/models/tree/master/official/</p>
<h3 id="why-34">Why</h3>
<ul>
<li>Traditional hand-crafted local invariant features for images have their counterparts in videos.</li>
<li>The long-standing pursuit after temporal cues for self-supervised video representation learning has left selfsupervision signals in the spatial subspace under-exploited for videos.</li>
<li>Simply applying spatial augmentation independently to video frames actually hurts the learning because it breaks the natural motion along the time dimension</li>
<li>a pair of positive clips that are temporally distant may contain very different visual content, leading to a low similarity that could be indistinguishable from those of the negative pairs.</li>
<li>The recent wave of contrastive learning shares a similar loss objective as instance discrimination.</li>
<li></li>
</ul>
<h3 id="goal-33">Goal</h3>
<ul>
<li>Propose CVRL (contrastIve video representation learning) to learn spatiotemporal visual representations from unlabeled videos.</li>
<li>There are two augmented clips used for contrastive learning, augmented by their proposed temporally consistent spatial augmentation method and sampling-based temporal augmented method.</li>
</ul>
<h3 id="how-35">How</h3>
<ul>
<li>construct positive pairs as two augmented video clips sampled from the same input video.</li>
<li>CVRL is more effective with larger networks.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927135417172.png" alt="image-20210927135417172" /><figcaption aria-hidden="true">image-20210927135417172</figcaption>
</figure>
<h4 id="video-representation-learning-framework">Video representation learning framework</h4>
<ul>
<li>Use InfoNCE loss, which allows the postitive pair <span class="math inline">\((z_i,z_i&#39;)\)</span> to attract mutually while they repel the other items in the mini-batch.</li>
<li>Components: An encoder maps an input video clips to its representation <span class="math inline">\(z\)</span>, spatiotemporal augmentations to construct positive pairs <span class="math inline">\((z_i,z_i&#39;)\)</span> and the properties they induce, and methods to evaluate the learned representations.</li>
</ul>
<h4 id="video-encoder">Video encoder</h4>
<ul>
<li>Encode a video sequence using 3D-ResNets as backbones.
<ul>
<li>Only two changes on original 3D-ResNets: the temporal stride of 2 in the data layer, and the temporal kernel size of 5 and stride of 2 in the first CNN layer.</li>
<li>The video representation will be a 2048-dimensional feature vector.</li>
</ul></li>
<li>The add a multi-layer projection head (MLP) onto the backbone to obtain the encoded 128-dimensional feature vector <span class="math inline">\(z\)</span>.</li>
<li>During evaluation, the MLP is dropped.</li>
</ul>
<h4 id="data-augmentation-1">Data augmentation</h4>
<ul>
<li><p>Temporal augmentation: The main motivation is that two clips from the same video would be more distinct when their temporal interval is larger.</p>
<ul>
<li>If sample temporally distanct clips with smaller probabilities, the contrastive loss would focus more on the temporally close clips, pulling their features closer and imposing less penalty over the clips that are far away in time.==&gt; aligning lower sampling probability on larger temporal intervals.</li>
<li>Steps: First draw a time interval <span class="math inline">\(t\)</span> from a distribution <span class="math inline">\(P(t)\)</span> over <span class="math inline">\([0，T]\)</span>, then uniformly sample a clip from <span class="math inline">\([0，T-t]\)</span>, followed by the second clip which is delayed by <span class="math inline">\(t\)</span> after the first.</li>
<li>The descreasing distributions generally perform better than the unifom or increasing ones.</li>
</ul></li>
<li><p>Spatial augmentation: With fixed randomness across frames, the 3D video encoder is able to better utilize spatiotemporal cues.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927135237680.png" alt="image-20210927135237680" /><figcaption aria-hidden="true">image-20210927135237680</figcaption>
</figure></li>
</ul>
<h4 id="evaluation">Evaluation</h4>
<ul>
<li>evaluate the learned video representations by fixing the weights in the pre-trained video encoder and training a linear classifier on top of it.</li>
</ul>
<h2 id="paper-37-sportscap-monocular-3d-human-motion-capture-and-fine-grained-understanding-in-challenging-sports-videos">Paper 37: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.11452">SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</a></h2>
<p>Codes: https://github.com/ChenFengYe/SportsCap</p>
<h3 id="why-35">Why</h3>
<ul>
<li>Most existing action understanding solutions are limited to the pure high-level action assessment, where the abundant 3D motion capture information of sub-motions has been ignored.</li>
<li>Previous methods on pose and shape estimation do not consider rich semantic information embedded in sports, and the structure constraints within sub-motions.
<ul>
<li>They use the underlying semantic and ordering rules in sport to reduce the complexity of the problem, and use PCA to capture the similarities of poses in each sub-motion and constrain estimated poses in reasonable forms.</li>
<li>Previous methods rely on 2D poses under-estimate the complex sports actions.</li>
</ul></li>
<li>Action parsing
<ul>
<li>Competitive sports are a mixture of long and short dynamics.</li>
<li>Approaches rely on joint motions ignore the patterns of the human body in certain activities.</li>
</ul></li>
<li>Dataset: To their knowledge, the SMART dataset is the only one that provides the fine-grained semantic labels, 2D ad 3D annotated poses, and assessment information.</li>
</ul>
<h3 id="goal-34">Goal</h3>
<ul>
<li>Monocular 3D Human Motion Capture on sports videos, use the semantic and temporally structured sub-motion prior in the embedding space. Propose a motion embedding module to recover both the implict motion embedding and explicit 3D motion details via a corresponding mapping function and a sub-motion classifier.</li>
<li>reconstruct both the 3D human motion and the corresponding fine-grained action attributes from monocular professional sports video.</li>
<li>The approach will provide sub-actions, action types and rotation angles .</li>
</ul>
<h3 id="how-36">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926221807606.png" alt="image-20210926221807606" /><figcaption aria-hidden="true">image-20210926221807606</figcaption>
</figure>
<ul>
<li>The fine-grained semantic action attributes are predicted by ST-GCN, use both the original motion embedding stream and the recovered 3D detailed motion stream of the whole video clip.</li>
<li>The model comprises of two modules
<ul>
<li>Motion embedding module: Use PCA to extract embedding prior, the estimate the per-frame motion embedding parameters (pose, shape) so as to recover the 3D motion details. The embedding module consists of a sub-motion classifier (WS-DAN) , a CNN encoder to regress the embedding and the mapping function from the embedding space to the 3D motion output.</li>
<li>Actions parsing module: use ST-GCN, input both the implicit pose embedding and the explicit 3D joints. Finally map the predicted attributes to the final action label by a semantic attribute mapping block.</li>
</ul></li>
</ul>
<h4 id="motion-embedding-and-capturing">Motion embedding and capturing</h4>
<ul>
<li>Split one action into sub-motions. In each sub-motion, poses exhibit high resemblance across athletes.</li>
<li>The sub-motion classifier is WS-GAN.</li>
<li>Then use SMPL (skinned multi-person linear model) to build the motion embedding space, which represents the pose parameters as <span class="math inline">\(\theta\)</span>.
<ul>
<li>Use pose coefficient <span class="math inline">\(\boldsymbol{\alpha}\)</span> to leverage the rich semantic and temporally structural prior of sub-motions in the motion embedding space. <span class="math inline">\(\boldsymbol\theta=\sum\limits_{k=1}^K\alpha_k\boldsymbol{b}_k^m+\boldsymbol{a}^m\)</span>, where <span class="math inline">\(\boldsymbol{a}^m\)</span> is the mean of pose parameters, and <span class="math inline">\(\boldsymbol{\alpha}\)</span> are the pose coefficients.</li>
</ul></li>
<li>Then use PCA on pose parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>, the <span class="math inline">\(\boldsymbol{b}_k^m\)</span> in above are PCs, and <span class="math inline">\(\boldsymbol{a}^m\)</span> denotes the mean, and <span class="math inline">\(\alpha\)</span> means the coefficient assigned by std.</li>
<li>The module consists of a resnet-152 encoder followed by two FCC to regress the pose coefficients <span class="math inline">\(\boldsymbol{\alpha}\)</span> and then reconstruct joint positions
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926224512111.png" title="fig:" alt="image-20210926224512111" /></li>
<li>Use this network to estimate pose coefficients, shape parameters and camera parameters from images. Then recover 3D human body meshes from estimated pose and shape parameters of SMPL.</li>
</ul></li>
</ul>
<h4 id="action-parsing">Action parsing</h4>
<ul>
<li><p>Tasks include inferring semantic meaningful labels and the action number (code ) from sports videos.</p></li>
<li><p>Use SAs (semantic attributes, are learnt by ST-GCN ) to represent the semantic meaningful label. The action number represents a valid combination of SAs.</p>
<ul>
<li><p>SAs indicate the specific number of a motion (the number of frames for one type of motion?), like the rotation angle, take-off type and so on.</p></li>
<li><p>For one sequence, there maybe are many types of motions that intersect together?</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210926233345530.png" alt="image-20210926233345530" /><figcaption aria-hidden="true">image-20210926233345530</figcaption>
</figure></li>
</ul></li>
<li><p>ST-GCN for extract SAs.</p>
<ul>
<li>J- and B-Stream use 2s-AGCN.</li>
<li>In P-stream, represent pose coefficients as a 1D vector and use 1DCNN with residual to generate features.</li>
</ul></li>
<li><p>Semantic attributes mapping block</p>
<ul>
<li>Learn the mapping between the extracted ST features and the final action label.</li>
<li>Goal: partition the whole action sequence in terms of SAs, or to say how individual frames contribute to specific SAs.</li>
<li>Use two FCs to predict their contributions, then stack the resulting SAs and feed the results to another 2 layer FCs to infer the action number.</li>
</ul></li>
</ul>
<h4 id="multi-task-training">Multi-task training</h4>
<ul>
<li>Loss for motion embedding module
<ul>
<li>The prior loss: <span class="math inline">\(\mathcal{L}_{prior}=\|\mathrm{W}(\bar{\alpha}-\hat{\alpha})\|_2\)</span>, where <span class="math inline">\(\bar{\alpha}\)</span> is the mean of pose coefficients in training set, and <span class="math inline">\(\hat{\alpha}\)</span> is the predicted pose coefficients.</li>
<li>The data loss: On 2D pose, <span class="math inline">\(\mathcal{L}_{data}=\|\mathrm{V(J-\hat{J})}\|_2\)</span>, where <span class="math inline">\(\mathrm{J}\)</span> is the ground truth, and <span class="math inline">\(\hat{\mathrm{J}}\)</span> is the estimated 2D joints, which are projected from estimated 3D points. <span class="math inline">\(\mathrm{V}\)</span> denotes the visibility of the ground truth joint.</li>
<li>SMPL loss: <span class="math inline">\(\mathcal{L}_{smpl}=\|\theta-\hat{\theta}\|_2+\|\beta-\hat{\beta}\|_2\)</span>, where <span class="math inline">\(\theta,\beta\)</span> are the supervision of pose/shape parameters.</li>
<li>The final loss is the combination of previously listed 3 losses.</li>
</ul></li>
<li>Loss for action parsing module
<ul>
<li>Use the cross-entropy between the predicted and the ground truth attributes.</li>
<li>The acion labeling task also use the cross-entropy loss.</li>
<li>The final loss in this module is the combination of three losses.</li>
</ul></li>
<li>Training strategy:
<ul>
<li>Stage 1: train a motion embedding module for each sub-motion independently and fix its parameters</li>
<li>Stage 2: Then train the action attribute prediction and label classification modules in the action parsing module jointly.</li>
<li>Stage 3: Fine tune the entire network using the combined losses from two modules.</li>
</ul></li>
</ul>
<h2 id="paper-38-ssan-separable-self-attention-network-for-video-representation-learning">Paper 38: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.13033">SSAN: Separable Self-Attention Network for Video Representation Learning</a></h2>
<h3 id="why-36">Why</h3>
<ul>
<li>Self-attention succeed in video representation learning due to its ability of modeling long range dependencies. Existing methods build the dependencies merely by the pairwise correlations along spatial and temporal dimensions simultaneously.
<ul>
<li>However, spatial correlations and temporal correlation represent different contextual information of scenes and temporal reasoning.</li>
<li>Learning spatial contextual information will benefit temporal modeling.</li>
</ul></li>
<li>Learning strong and generic video representations is still challenging.
<ul>
<li>Videos contain rich semantic elements within individual frames, but also the temporal reasoning across time.</li>
<li>The long-range dependencies among pixels cannot be well-cared by CNN.</li>
<li>RNN does, but sufferes from the high computation cost, and it cannot establish the direct pairwise relationship between positions regardless of their distance.</li>
</ul></li>
<li>The correlations from space and time represent different contextual information. The former often relates to scenes and objects, and the latter often relates to temporal reasoning for actions (short-term) and events (long-term).
<ul>
<li>Learning correlations along spatial and temporal dimensions together might capture irrelevant information, leading to the ambiguity for action understanding</li>
<li>Short-term dependencies should also be considered for capturing episodes of complex activities.</li>
</ul></li>
<li>Video learning networks
<ul>
<li>Mainly two branches for video learning architectures: 2D based methods and 3D based methods.</li>
<li>But 3D based methods suffer from the overhead of parameters and complexity, while 2D based methods need careful temporal feature aggregation.</li>
</ul></li>
<li>Action recognition
<ul>
<li>video classification, early works try to use 2D based methods to video, the 3DCNN are used. But huge computation cost.</li>
<li>3D based methods always take several consecutive frames as input, so that videos with complex actions cannot be well handled.</li>
<li>2D convolution networks with temporal aggregation achieve significant improvements.</li>
</ul></li>
<li>Self-attention
<ul>
<li>The full connections among pixels also introduces irrelevant information.</li>
<li>The 3D based self-attention can model long dependencies from space and time simultaneously, but such dependencies are first-order correlations which mainly capture the similarity between single pixels, not semantic-level correlations.</li>
<li>Non-local methods consider more on position-wise correlations but less on channel-wise correlations.</li>
</ul></li>
<li>Visual-language learning
<ul>
<li>VideoBERT: propose to use a visual-linguistic model to learning high-level features without any explicit supervision.</li>
<li>Sun et al.: use contrastive bidirectional transformer (CBT) to perform SSL for video representations.</li>
<li>UniViLM: propose a joint video and language pre-training scheme by adding generation tasks in the pre-training process.</li>
<li>These methods only focus on the training of the transformer encoder and decoder, while take the video networks as feature extractors.</li>
</ul></li>
</ul>
<h3 id="goal-35">Goal</h3>
<ul>
<li>Propose separable self-attention (SSA) module, which models spatial and temporal correlations sequentially.</li>
<li>The proposed SSA learn spatial self-attention firstly. The attention maps are then aggregated along temporal dimension and sent to temporal attention module.</li>
</ul>
<h3 id="how-37">How</h3>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927200926592.png" alt="image-20210927200926592" style="zoom:67%;" /></p>
<h4 id="self-attention-in-vision">Self-attention in vision</h4>
<ul>
<li>If videos <span class="math inline">\(X\)</span> in <span class="math inline">\(T\)</span> frames, in size <span class="math inline">\(H,W,C\)</span>, then reshape the embedding for query, key and value to size <span class="math inline">\(THW\times C,C\times THW,THW \times C\)</span>, respectively, denoted by <span class="math inline">\(X_q,X_k,X_v\)</span>.</li>
<li>Then the attention map Y is <span class="math inline">\(Y=softmax(X_q\times X_k)\times X_v\)</span> in size <span class="math inline">\(THW \times C\)</span>.</li>
<li>Then it is transformed by <span class="math inline">\(1\times 1\times 1\)</span> convolution and added back to the original query feature <span class="math inline">\(X\)</span>. <span class="math inline">\(Z=W_z(Y)+X\)</span>.</li>
</ul>
<h4 id="separable-self-attention-module">Separable Self-attention module</h4>
<ul>
<li>The spatial and temporal attention are performed sequentially, so that temporal correlations can fully consider the spatial contexts. Second, sptial attention maps exploit as much context information as possible.</li>
<li>First use 2D <span class="math inline">\(1\times 1\)</span> convolutions to map input feature <span class="math inline">\(X\)</span> into <span class="math inline">\(X_q,X_v,X_k\)</span>.</li>
<li>After spatial attention, the 4D intermediate attention map <span class="math inline">\(\hat{X}\)</span> (after spatial attention) is then transformed to temporal embeddings <span class="math inline">\(\hat{X}_q,\hat{X}_v\)</span> using <span class="math inline">\(3\times1\times 1\)</span> convolution. The temporal attention still use same <span class="math inline">\(X_v\)</span> (same as what's used in spatial attention ).</li>
<li>Spatial attention
<ul>
<li>Both position-wise and channel-wise attention, so two similarity matrices <span class="math inline">\(M_s,M_c\)</span> are generated.</li>
<li>The final spatIAL maps for time <span class="math inline">\(t\)</span> is <span class="math inline">\(\hat{X_t}=(M_s\times X_{v(s)}^t)+(M_s\times X_{v(c)}^t)\)</span>, and the intermediate attention map <span class="math inline">\(\hat{X}=Cat[\hat{X}_0,\cdots,\hat{X}_T]\)</span>.</li>
</ul></li>
<li>Temporal attention
<ul>
<li><span class="math inline">\(3\times1\times 1\)</span> convolution alloes temporal fusion on spatial attention maps and builds the short range correlations along temporal dimension.</li>
</ul></li>
</ul>
<h4 id="network-architecture">Network architecture</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210927203812177.png" alt="image-20210927203812177" /><figcaption aria-hidden="true">image-20210927203812177</figcaption>
</figure>
<ul>
<li>Choose TSN as the baseline, insert SSA module into different layers to establish separable self-attention network (SSAN).</li>
</ul>
<h2 id="paper-39-tdgraphembed-temporal-dynamic-graph-level-embedding">Paper 39: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411953">tdgraphembed: Temporal dynamic graph-level embedding</a></h2>
<p>Codes: https://github.com/moranbel/tdGraphEmbed</p>
<h3 id="why-37">Why</h3>
<ul>
<li>Existing graph embedding methods focus on capturing the graph's nodes in a static mode and/or not model the graph in its entirety in temporal dynamic mode.</li>
<li>To the best of their knowledge, the novel task of embedding entire graphs into a single embedding in temporal domains has not been addressed yet.</li>
<li>Graph representation learning on Static graph context:
<ul>
<li>Matrix factorization: like SVM, LLE, decompose the Laplacian or higher-order adjacency matrix to produce node embedding that preserves the distance between ndoes.</li>
<li>Random walk based methods: create random path over the graph and use word2vec-like architecture for the node embedding task</li>
<li>Deep learning based methods: like autoencoders, GCN, GAEs</li>
</ul></li>
<li>Unsupervised graph embeddings
<ul>
<li>Graph2vec: embed the entire graph in a static mode and use an analogy between graphs and documents, take the rooted subgraphs around every node as the context terms that make up the document</li>
<li>Sub2vec, unsupervised, for arbitrary static subgraphs using gone random walk</li>
<li>UGraphEmbed: graph-level representation, the embeddings of two graphs preserve their graph-graph proximity based on GED measure.</li>
<li>But they encode the graph without considering the historical contexts.</li>
</ul></li>
<li>Temporal node/edge embedding methods
<ul>
<li>generate node embeddings at each time step of the graph using static embedding, and then align them to from a unified representation</li>
<li>DynGEM: use the learned embeddings from the previous graph to initialize the representation at current time.</li>
<li>Dyngraph2vec: recurrent layers to learn patterns over sequence of graphs.</li>
<li>They don't take the fact that the number of nodes changes over time into consideration.</li>
<li>DyRep: use two time scaling process that captures temporal node interactions ad topological evolution, but only supports the network's growth; DynamicTriad: analyze every two consecutive timesteps to create the embedding. ==&gt; But not graph-level embedding.</li>
</ul></li>
</ul>
<h3 id="goal-36">Goal</h3>
<ul>
<li>Temporal dynamic graphs, propose tdGraphEmbed to learn temporal graph-level embedding unsupervisedly, which is an extension of random walk that globally embed both the nodes of the graph and its representation at each step.</li>
</ul>
<h3 id="how-38">How</h3>
<ul>
<li>The proposed method embeds the entire graph at timestamp <span class="math inline">\(t\)</span> into a single vector <span class="math inline">\(G_t\)</span>. At each timestamp <span class="math inline">\(t\)</span>, it performs <span class="math inline">\(\gamma\)</span> random walks of length <span class="math inline">\(L\)</span> from each node in the graph. Then jointly learn the node embeddings <span class="math inline">\(v\)</span> along with the entire graph embedding <span class="math inline">\(G_t\)</span>.</li>
<li>The optimization becomes predicting a node embedding given the node's context in the random walks at time <span class="math inline">\(t\)</span> of the graph and the vector <span class="math inline">\(G_t\)</span>.</li>
</ul>
<h4 id="random-walk">Random walk</h4>
<ul>
<li>Modeling each graph as a sequence of sentences, and then model the graph's sentences using random walks.
<ul>
<li>Denote a random walk initiated at node <span class="math inline">\(v_i\)</span> as <span class="math inline">\(\mathcal{W}_{v_i}\)</span>. Then a random walk is a stochastic process with random variables <span class="math inline">\(\mathcal{W}_{v_i,1},\mathcal{W}_{v_i,2},\cdots,\mathcal{W}_{v_i,k}\)</span> such that <span class="math inline">\(\mathcal{W}_{v_i,k+1}\)</span> is a node chosen at random from the neighbors of node <span class="math inline">\(W_{v_i,k}\)</span>.</li>
<li>Implement a second order random walk with a return parameter <span class="math inline">\(p\)</span> and an in-out parameter <span class="math inline">\(q\)</span> to guide the walk.
<ul>
<li>Use <span class="math inline">\(p,q\)</span> to adjust the transition probability <span class="math inline">\(\alpha_{pq}(u,x)\)</span> from node <span class="math inline">\(u\)</span> to some node <span class="math inline">\(x\)</span>: <span class="math inline">\(\alpha_{pq}(u,x)=\begin{cases}\frac{1}{p} &amp; if d_{ux}=0\\1 &amp; if d_{ux}=1\\ \frac{1}{q}&amp; if d_{ux}=2\end{cases}\)</span>, where <span class="math inline">\(d_{ux}\)</span> indicates the distance between node <span class="math inline">\(u\)</span> and node <span class="math inline">\(x\)</span>.</li>
<li>Then this node2vec can bias the random walk closer or further away from the source node, creating different embedding types.</li>
<li>Precisely, <span class="math inline">\(p&lt;q\)</span> biases the random walk to noes closer to each other, then nodes from the same cluster to be embedded closer and nodes from different regions to be embedded further away; <span class="math inline">\(p&gt;q\)</span> biases the random walk to embed nodes of the same graph characteristics closer together while others are embedded further away.</li>
</ul></li>
</ul></li>
<li>Then the goal is to estimate the likelihood of observing <span class="math inline">\(G_t\)</span> in its entirety given all random walks at time <span class="math inline">\(t\)</span>, defined as <span class="math inline">\(Pr(G_t|(\mathcal{W}_{v_1},\mathcal{W}_{v_2},\cdots,\mathcal{W}_{v_k}))\)</span>.</li>
</ul>
<h4 id="framework">Framework</h4>
<ul>
<li>Each node is initialized by <span class="math inline">\(\gamma\)</span> random walks of length <span class="math inline">\(\mathcal{L}\)</span>.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928123213933.png" alt="image-20210928123213933" /><figcaption aria-hidden="true">image-20210928123213933</figcaption>
</figure>
<ul>
<li><p>After the step shown in figure 2, the goal is to predict the next node in a random walk. Take context nodes (sampled from a sliding window <span class="math inline">\(\omega\)</span> over random walks), also known as nodes in the neighborhood of the root node, and denote as <span class="math inline">\(N_s(v_i^t)\)</span>.</p></li>
<li><p>To learn the representation map <span class="math inline">\(\phi\)</span> that maps a given node and graph to a <span class="math inline">\(d\)</span>-dimensional space, then maximize <span class="math inline">\(\log p(v_i^t|N_s(v_i^t),G_t)\)</span> for each node in <span class="math inline">\(V_t\)</span>, which is defined by softmax. <span class="math inline">\(\log p(v_i^t|N_s(v_i^t),G_t)=\frac{\exp (\phi(v_i^t)\cdot h)}{\sum_{j\in V_t}\exp(\phi(v_j^t)\cdot h)}\)</span>, where <span class="math inline">\(h\)</span> is the combination of mapped node embedding and mapped graph embedding <span class="math inline">\(G_t\)</span>, and map means <span class="math inline">\(\phi\)</span>.</p></li>
<li><p>The graph’s time index can be thought of as another node which acts as a memory for the global context of the graph.</p></li>
<li><p>Finally, the objective is</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928124909064.png" alt="image-20210928124909064" /><figcaption aria-hidden="true">image-20210928124909064</figcaption>
</figure></li>
</ul>
<h4 id="tips">Tips</h4>
<ul>
<li>To decrease computation cost, use negative sampling for the denominator of softmax.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928130901448.png" title="fig:" alt="image-20210928130901448" /></li>
</ul>
<h2 id="paper-40-videomoco-contrastive-video-representation-learning-with-temporally-adversarial-examples">Paper 40: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf">Videomoco: Contrastive video representation learning with temporally adversarial examples</a></h2>
<h3 id="why-38">Why</h3>
<ul>
<li>The evolution of contrastive learning heavily focuses on feature representations from static images while leaving the temporal video representations less touched.
<ul>
<li>Large scale video data is difficult to store in memory.</li>
<li>Attempts on unsupervised video representation learning focus on proposing pretext tasks related to a sub-property of video content.</li>
</ul></li>
<li>MoCo treats the contribution of keys from the queue only based on their representation. But in fact, the longer the keys in the queue, the more different their representations are compared to those of the current input samples.</li>
</ul>
<h3 id="goal-37">Goal</h3>
<ul>
<li>An extension of MoCo to learn video representation unsupervisedly.</li>
<li>Two modifications given a video sequence as a sample
<ul>
<li>Introduce a generator to drop out several frames from the sample temporally.==&gt; by dropping during adversarial training, force the input sample to train a temporally robust encoder.</li>
<li>Use temporal decay to model key attenuation in the memory queue when computing contrastive loss. As the momentum encoder updates after keys enqueue, the representation ability of these keys degrades when we use the current input sample for contrastive learning.</li>
</ul></li>
<li>VideoMoCo utilizes color information and performs instance discrimination without bringing empirical pretext tasks.</li>
</ul>
<h3 id="how-39">How</h3>
<ul>
<li>Introduce adversarial learning to improve the temporal robustness of the encoder
<ul>
<li>Use a generator to adaptively drop out several frames</li>
<li>And the dropped samples are sent to discriminatory for differentiating.</li>
<li>After adversarial training, only D is kept to extract temporally robust features.</li>
</ul></li>
<li>The sample after dropping out some frames is then taken as a query sample and perform contrastive learning with keys in the memory queue.
<ul>
<li>Model the degradation of keys by proposing a temporal decay. If a key stays longer in the queue, its contribution is less (manipulated by the decay).</li>
</ul></li>
</ul>
<h4 id="moco">MoCo</h4>
<ul>
<li>The contrastive loss of MoCo <span class="math inline">\(L_q=-\log \frac{\exp (q\cdot k_+/\tau)}{\sum\limits_{i=0}^K\exp(q\cdot k_i/\tau)}\)</span>, which tends to classify <span class="math inline">\(q\)</span> as <span class="math inline">\(k_+\)</span>. The query <span class="math inline">\(q\)</span> is the representation of an input sample via the encoder network, while the keys <span class="math inline">\(k_i\)</span> are the representations of the other training samples in the queue.</li>
<li>The queue follows FIFO scheme. Denote the parameters of an encoder as <span class="math inline">\(\theta_q\)</span> and those of a momentum encoder as <span class="math inline">\(\theta_k\)</span>. The momentum encoder is updated as <span class="math inline">\(\theta_k\leftarrow m\theta_k+(1-\theta)\theta_q\)</span>, where <span class="math inline">\(m\)</span> is a momentum coefficient.</li>
<li>For VideoMoCo, give an input video clip with a fixed number of frames, the model sent it to a generator G and the encoder D to produce <span class="math inline">\(q\)</span>. Meanwhile, reweigh <span class="math inline">\(\exp(q\cdot k_i/\tau)\)</span> by <span class="math inline">\(t^i,t\in(0,1)\)</span>.</li>
</ul>
<h4 id="temporally-adversarial-learning">Temporally adversarial learning</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928142136488.png" alt="image-20210928142136488" /><figcaption aria-hidden="true">image-20210928142136488</figcaption>
</figure>
<ul>
<li>G, consisting of ConvLSTM, takes <span class="math inline">\(x\)</span> as input and produces a temporal mask. The mask will help drop out <span class="math inline">\(25\%\)</span> of the frames.</li>
<li>Regard the encoder of MoCo as the discriminator D, and take <span class="math inline">\(X^{query},x\)</span> as the input of D.</li>
<li>Tips for training: Initially, we train D without using G and only use contrastive learning. When D is learned to approach a semi-stable state, we train D by involving G. We empirically observe that <strong>utilizing adversarial learning at the initial stage makes D difficult to converge.</strong></li>
</ul>
<h4 id="temporal-decay">Temporal decay</h4>
<ul>
<li><p>For a key <span class="math inline">\(k_i\)</span> in the queue, set its corresponding temporal decay as <span class="math inline">\(t^i\)</span> where <span class="math inline">\(t\in(0,1)\)</span>, and <span class="math inline">\(i\)</span> gradually increases by 1 and <span class="math inline">\(t^i\)</span> decreases correspondingly.</p></li>
<li><p>Then with <span class="math inline">\(t^i\)</span>, the real loss function for the discriminator D is</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928143615211.png" alt="image-20210928143615211" style="zoom:50%;" /></p></li>
</ul>
<h4 id="visualization">Visualization</h4>
<ul>
<li>After obtaining classification scores, use entropy to measure the classifier's confidence when making the prediction. A high entropy value means the classifier is more uncertain on making the current predictions.</li>
<li>Even though both MoCo and VideoMoCo can resist temporally occluded video sequences, the feature representation of MoCo has become very fragile while that of VideoMoCo does not degrade significantly.</li>
<li>The features learned from VideoMoCo attend the network to the temporal motions.
<ul>
<li><font color="red">How to calculate attention maps?</font></li>
</ul></li>
</ul>
<h2 id="paper-41-visual-relationship-forecasting-in-videos">Paper 41: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.01181">Visual Relationship Forecasting in Videos</a></h2>
<h3 id="why-39">Why</h3>
<ul>
<li>Current efforts mostly focus on detecting the object interactions based on observation rather than reasoning about the spatiotemporal connections among predicates.</li>
<li>VRF requires additional spatiotemporal modeling among time series. This task emphasizes the importance of visual relationship reasoning.</li>
<li>Future prediction
<ul>
<li>generating future frames: usually generative models</li>
<li>predicting future labels or states: multiple methods, VRF can be classified into this group. It performs forecasting on the unknown future frames, and it focuses on the relationship forecasting among a specific pair of <subject-object>.</li>
</ul></li>
</ul>
<h3 id="goal-38">Goal</h3>
<ul>
<li>Deal with object interactions in unknown future. Propose VRF (visual relationship forecasting, a new task) in videos to explore the prediction of visual relationships in a reasoning manner.
<ul>
<li>Formally, given a subject-object pair with <span class="math inline">\(H\)</span> existing frames, it aims to predict their future interactions for the next <span class="math inline">\(T\)</span> frames without visual evidence.</li>
<li>Propose a graph convolutional transformer (GCT) framework, which captures both object-level and frame-level dependencies by ST-GCN (object-level ) and transformer (frame-level) .</li>
</ul></li>
</ul>
<h3 id="how-40">How</h3>
<ul>
<li>Propose GCT</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928172156560.png" alt="image-20210928172156560" /><figcaption aria-hidden="true">image-20210928172156560</figcaption>
</figure>
<h4 id="framework-1">Framework</h4>
<ul>
<li>GCT can be divided into three parts: feature representation, object-level reasoning, and frame-level reasoning.</li>
<li>In feature representation module, use visual feature, semantic feature and spatial features. For spatial reasoning, a spatial graph is constructed for every key fame and the corresponding GCN is taken to model the interactions between the given <subject-object> pairs.</li>
<li>Then, use a multi-head transformer for frame-level reasoning.</li>
</ul>
<h4 id="feature-representation">Feature representation</h4>
<ul>
<li>Visual features: Use the bounding boxes of one instance, <span class="math inline">\(b_s,b_{so},b_o\)</span> indicate the subject, predicate and object bounding box. Use Inception-ResNet-V2 as the backbone and extract the features of these bounding boxes from the fully connected layers.</li>
<li>Spatial features: Get the relative spatial feature of bounding boxes, adopt the idea of box regression.
<ul>
<li><span class="math inline">\(\Delta(b_i,b_j)\)</span> denote the box delta that regresses the bounding box <span class="math inline">\(b_i\)</span> to <span class="math inline">\(b_j\)</span>, <span class="math inline">\(dis(b_i,b_j)\)</span> and <span class="math inline">\(iou(b_i,b_j)\)</span> denote the normalized distance and IoU between <span class="math inline">\(b_i,b_j\)</span>, and the union region of <span class="math inline">\(b_i,b_j\)</span> is denoted as <span class="math inline">\(b_{ij}\)</span>.</li>
<li>The relative spatial location of subject and object can be defined as: <span class="math inline">\(l_{ij}=[\Delta(b_i,b_j);\Delta(b_i,b_{ij});\Delta(b_j,b_{ij});iou(b_i,b_j);dis(b_i,b_j)]\)</span>.</li>
</ul></li>
<li>Semantic features: Adopt a semantic embedding layer to map the object category <span class="math inline">\(c\)</span> into word embedding <span class="math inline">\(s\)</span>.
<ul>
<li>The parameters of object categories are initialized with the pre-trained word representations such as word2vec.</li>
</ul></li>
</ul>
<h4 id="object-level-reasoning">Object-level reasoning</h4>
<ul>
<li>The spatiotemporal graph <span class="math inline">\(\mathcal{G}=\{\mathcal{V}_o,\mathcal{V}_p,\mathcal{E}\}\)</span> is constructed, containing an object node set <span class="math inline">\(\mathcal{V}_o\)</span>, a predicate node set <span class="math inline">\(\mathcal{V}_p\)</span>, and an edge set.
<ul>
<li><span class="math inline">\(v_i\in \mathcal{V}_o\)</span> represents an object bounding box <span class="math inline">\(b_i\)</span> in a video fame, corresponding with object category <span class="math inline">\(c_i\)</span>.</li>
<li><span class="math inline">\(v_i\in \mathcal{V}_p\)</span> represents a predicate bounding box <span class="math inline">\(b_i\)</span> in a video fame, corresponding with predicate category <span class="math inline">\(c_i\)</span>.</li>
<li>There is an edge between an object node and a predicate node, but no links between two object nodes.</li>
<li>The objects and predicate nodes in the key frames at different moments are all included in the corresponding node set</li>
</ul></li>
<li>Then, perform GCN on the built graph.</li>
</ul>
<h4 id="transformer-for-temporal-modeling">Transformer for temporal modeling</h4>
<ul>
<li>The encoder layer encode the object-level features outputed by GCN, spatial features and semantic features into continuous representations. The encoder will output <span class="math inline">\(E\)</span> as the feature representation.</li>
<li>The decoder will use multi-head attention on <span class="math inline">\(E\)</span>, with two multi-head attention layers.</li>
<li>The final decoder embedding is sent to a FC layer and a softmax to get the final prediction.</li>
<li>Cross-entropy for optimization.</li>
</ul>
<h2 id="paper-42-wasserstein-embedding-for-graph-learning">Paper 42: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=AAes_3W-2z">Wasserstein embedding for graph learning</a></h2>
<p>Codes: https://github.com/navid-naderi/WEGL</p>
<h3 id="why-40">Why</h3>
<ul>
<li><p>The research on kernel approaches, perhaps most notably the random walk kernel and the WL kernel, remains an active field of study.</p></li>
<li><p>Recent works</p>
<ul>
<li>One propose a node embedding based on WL kernel, and combine the resulting node embeddings with the Wassertein distance.</li>
<li>Previous works all suffer from graphs in huge size.
<ul>
<li>On the GCN side, optimization is challenging.</li>
<li>On the graph kernel side, calculating the matrix of all pairwise similarities can be a burden.</li>
</ul></li>
<li>Graph kernels
<ul>
<li>The shortest-path kernels</li>
<li>The graphlets and WL subtree kernel.</li>
<li>Others like kernel methods using spectral approaches, assignment-based approaches, and graph decomposition algorithms.</li>
</ul></li>
</ul></li>
<li><p>Wassertein distance</p>
<ul>
<li><p>Due to Brenier's theorem , for absolutely continuous probability measures <span class="math inline">\(\mu_i,\mu_j\)</span>, the 2-wassertein distance can be equivalently obtained from</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928181830196.png" alt="image-20210928181830196" /><figcaption aria-hidden="true">image-20210928181830196</figcaption>
</figure></li>
<li><p>The mapping <span class="math inline">\(f\)</span> is referred to as a transport map, and the optimal transport map is called the Monger map.</p></li>
<li><p>But pairwise calculation of the Wassertein distance could be expensive.==&gt; The linear optimal transportation framework between the probability measures to define a Hilbertian embedding, in which the <span class="math inline">\(\ell_2\)</span> distance provides a true metric between the probability measures that approximates <span class="math inline">\(\mathcal{W}_2\)</span>.</p></li>
</ul></li>
</ul>
<h3 id="goal-39">Goal</h3>
<ul>
<li>learn graph-level representations, propose WEGL(wassertein embedding for graph learning). Define the similarity between graphs as a similarity function between their node embedding distributions. And use wassertein distance to measure the dissimilarity between node embeddings of different graphs.</li>
<li>WEGL calculates Monger maps from a reference distribution to each node embedding and, based on these maps, creates a fixed-sized vector representation of the graph.</li>
<li>WEGL embeds a graph into a Hilbert space, where the <span class="math inline">\(\ell_2\)</span> distance between two embedded graphs provides a true metric between the graphs that approximates their 2-Wassertein distance.</li>
</ul>
<h3 id="how-41">How</h3>
<h4 id="linear-wassertein-embedding">Linear Wassertein embedding</h4>
<ul>
<li>The probability distributions are mapped to the tangent space with respect to a fixed reference distribution. The framework below is for isometric Hilbertian embedding of probability measures such that the Euclidean distance between the embedded images approximates <span class="math inline">\(\mathcal{W}_2\)</span>.</li>
<li>By define <span class="math inline">\(\phi(\mu_i)\)</span> as a function of Jacobian equation <span class="math inline">\(p_0\)</span>, the identity function <span class="math inline">\(id(z)=z\)</span> and the Monge map <span class="math inline">\(f_i\)</span>, where <span class="math inline">\(f_i=\arg\min_{f\in MP(\mu_0,\mu_i)\int_{\mathcal{Z}}\|z-f(z)\|^2d\mu_0(z)}\)</span>. <span class="math inline">\(\phi(\mu_i)\)</span> provides an isometric embedding for probability measures.
<ul>
<li><span class="math inline">\(\|\phi(\mu_i)-\phi(\mu_0)\|_2=\|\phi(\mu_i)\|_2=\mathcal{W}_2(\mu_i,\mu_0)\)</span>, with <span class="math inline">\(\phi(\mu_0)=0\)</span></li>
<li><span class="math inline">\(\|\phi(\mu_i)-\phi(\mu_j)\|_2\approx\mathcal{W}_2(\mu_i,\mu_j)\)</span>.</li>
</ul></li>
<li>Since <span class="math inline">\(\phi(\mu_i)\)</span> provides a linear embedding for the probability measures, it's called the linear Wassertein embedding. It can also be thought as the RKHS embedding of the measure <span class="math inline">\(\mu_i\)</span>.</li>
<li>For the discrete distributions , the Monge coupling is used.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928191439028.png" title="fig:" alt="image-20210928191439028" /></li>
<li>The Monger map is the approximated from the optimal transport plan by barycentric projection via <span class="math inline">\(F_i=N(\pi_i^*Z_i)\in\mathbb{R}^{N\times d}\)</span>, and <span class="math inline">\(\phi(Z_i)=(F_i-Z_0)/\sqrt{N}\in\mathbb{R}^{N\times d}\)</span>.
<ul>
<li>Since the barycentric projection, <span class="math inline">\(\phi(\cdot)\)</span> is only pseudo-inverible.</li>
</ul></li>
</ul>
<h4 id="wegl">WEGL</h4>
<ul>
<li><p>Applying optimal transport to measure the dissimilarity between two graphs, the entire embedding for each graph <span class="math inline">\(G_i\)</span>, is obtained by composing <span class="math inline">\(\phi(\cdot)\)</span> and <span class="math inline">\(h(\cdot)\)</span>,i.e., <span class="math inline">\(\psi(G_i)=\phi(h(G_i))\)</span> where <span class="math inline">\(h\)</span> is an arbitrary node embedding process.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210928192439836.png" alt="image-20210928192439836" /><figcaption aria-hidden="true">image-20210928192439836</figcaption>
</figure></li>
<li><p>Node embedding</p>
<ul>
<li><p>They use a similar non-parametric propagation/diffusion-based encoder as in <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.01277">here</a> .</p></li>
<li><p>Precisely, given node features <span class="math inline">\(\{x_v\}_{v\in\mathcal{V}}\)</span> and scalar edge features <span class="math inline">\(\{w_{uv}\}_{(u,v)\in\mathcal{E}}\)</span>, <span class="math display">\[
x_v^{(l)}=\sum\limits_{u\in\mathcal{N}_v\cup\{v\}}\frac{w_{uv}}{\sqrt{\deg(u)\deg(v)}}\cdot x_u^{(l-1)},\forall l\in\{1,\cdots,L\},\forall v\in\mathcal{V}\\
z_v=g(\{x_v^{(l)}\}_{l=0}^L\})
\]</span></p></li>
</ul></li>
</ul>
<p>where <span class="math inline">\(g(\cdot)\)</span> is a local pooling process on a single node, <span class="math inline">\(z_v\)</span> is the result embedding for each node.</p>
<ul>
<li>Calculation of the reference distribution
<ul>
<li>Use k-means on <span class="math inline">\(\cup_{i=1}^MZ_i\)</span> with <span class="math inline">\(N=\lfloor\frac{1}{M}\sum\limits_{i=1}^MN_i\rfloor\)</span> centroids.</li>
<li>Or one can calculate the Wassertein barycenter of the node embeddings or simply use <span class="math inline">\(N\)</span> samples from a normal distribution.</li>
</ul></li>
</ul>
<h2 id="paper-43-anomaly-detection-in-video-via-self-supervised-and-multi-task-learning">Paper 43: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.07491">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning</a></h2>
<p>Codes: https://github.com/lilygeorgescu/AED-SSMTL. Need to fill the form firstly.</p>
<h3 id="why-41">Why</h3>
<ul>
<li>Previous work on anomaly detection
<ul>
<li>Prediction based, and reconstruction error as the clue.</li>
<li>Reconstruction error</li>
<li>The dissimilarity between patches in Siamese network.</li>
<li>Eliminate the reliance of anomaly detection on context and form the problem as HAR.</li>
</ul></li>
<li>Addressing anomaly detection through a single proxy task is suboptimal.</li>
<li></li>
</ul>
<h3 id="goal-40">Goal</h3>
<ul>
<li>anomalous event detection in video at object-level.</li>
<li>Use 3DCNN on three pretext tasks and one distillation task. The tasks are : discrimination of forward/backward moving objects (arrow of time), discrimination of objects in consecutive/intermittent frames, reconstruction of object-specific appearance information. The distillation task is based on teacher-student mechanism.</li>
<li></li>
</ul>
<h3 id="how-42">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211014151336939.png" alt="image-20211014151336939" /><figcaption aria-hidden="true">image-20211014151336939</figcaption>
</figure>
<ul>
<li>YOLOv3 and ResNet-50 as teachers.</li>
<li>Data preparation: use YOLOv3 to generate an object-centric temporal sequence by simply cropping the corresponding bounding box from frames.</li>
<li>The anomaly score is the average scores predicted fro each task.</li>
<li>Four tasks share the 3DCNN together, but each task has its prediction head.</li>
</ul>
<h4 id="task-1-arrow-of-time">Task 1: arrow of time</h4>
<ul>
<li>For one sequence, create a forward and a backward sample, then train.</li>
<li>Use cross-entropy</li>
</ul>
<h4 id="task-2-motion-irregularity">Task 2: motion irregularity</h4>
<ul>
<li>Use one consecutive clip and the other is composed of the centric frame and other sampled frame, with a step sampled from <span class="math inline">\(\{1,2,3,4\}\)</span>.</li>
<li>Use cross-entropy</li>
</ul>
<h4 id="task-3-middle-bounding-box-prediction">Task 3: middle bounding box prediction</h4>
<ul>
<li>Use the sequence with the centric frame missing and try to predict the missing one.</li>
<li>Use L1 loss.</li>
</ul>
<h4 id="task-4-model-distillation">Task 4: model distillation</h4>
<ul>
<li>Resnet 50 pretrained on ImageNet and YOLOv3 pretrained on MS COCO as the teacher who gives the GT, first YOLOv3 to get the middle box and the prediction <span class="math inline">\(Y_{yolo}\)</span>, then input the middle box to resnet 50 to get the prediction <span class="math inline">\(Y_{res}\)</span>, these two predictions are concatenated together as the GT.</li>
<li>The shared 3DCNN is the student who use the features before input in resnet 50.</li>
<li>Use L1 loss.</li>
</ul>
<h4 id="joint-loss">Joint loss</h4>
<ul>
<li>Summation of the four loss, but the 4th loss is weighted summed to regulate the importance of the knowledge distillation task.</li>
</ul>
<h4 id="inference">Inference</h4>
<ul>
<li>For an object, the abnormal score is the average from four tasks. The pixel-level anomaly map uses a 3D mean filter. For a certain frame, it is taken as the maximum score in the corresponding anomaly map. The final frame-level anomaly scores are obtained by a temporal Gaussian filter.</li>
<li>The failures of YOLOv3 are translated into false negatives.
<ul>
<li>By fusing the frame-level and object-level anomaly scores.</li>
</ul></li>
</ul>
<h2 id="paper-44-exponential-moving-average-normalization-for-self-supervised-and-semi-supervised-learning">Paper 44: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.08482">Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning</a></h2>
<p>Codes: https://github.com/amazon-research/exponential-moving-average-normalization</p>
<h3 id="why-42">Why</h3>
<ul>
<li>The BN used in both teachers and students can lead to
<ul>
<li>Cross-sample dependency: caused by the intrinsic property of BN where the output of a sample is dependent on all other samples in the same batch. To solve this, some use layer normalization, MoCo designs ShuffleBN where a mini-batch uses BN statistics from other randomly sampled mini-batch. SimCLR and BYOL use Synchronized BN.</li>
<li>Model parameter mismatch: parameters in a teachers comes from the student in previous iterations, while the batch-wise BN statistics are instantly collected at current iteration, which will lead to potential mismatch between the model parameters and the BN statistics in the parameter space.</li>
</ul></li>
<li>Normalization
<ul>
<li>BN introduces some issues: the large batch sizes for accurate statistics, mismatch between how BN is used during training and inference</li>
<li>Other normalization
<ul>
<li>Layer normalization (LN) : along the channel and spatial dimension.</li>
<li>Instance normalization (IN): along only the spatial dimension</li>
<li>Group normalization (GN): similar to LN, divides the channels into groups.</li>
<li>MABN: similar with EMAN, but focus on the stability of small batch size training and updates its statistics inside a single network.</li>
<li>ShuffleBN: a minibatch uses BN statistics from other randomly sampled minibatch.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="goal-41">Goal</h3>
<ul>
<li>Propose a plug-in replacement for BN called EMAN (exponential moving average normalization), for the improvement of student-teacher mechanism.</li>
<li>EMAN in the teacher side updates statistics by exponential moving average, from the BNN statistics of the student. This design reduces the intrinsic cross-sample dependency of BN and enhances the generalization of the teacher.</li>
</ul>
<h3 id="how-43">How</h3>
<ul>
<li><p>The EMAN statistics <span class="math inline">\(\mu,\sigma\)</span> in the teacher are exponentially moving averaged from the student BN statistics.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211014165232695.png" alt="image-20211014165232695" /><figcaption aria-hidden="true">image-20211014165232695</figcaption>
</figure></li>
</ul>
<h4 id="ema-teacher">EMA teacher</h4>
<ul>
<li><p>Both the student and the teacher use standard BN during training.</p></li>
<li><p>The teacher parameters <span class="math inline">\(\theta&#39;\)</span> are updated by exponential moving average from the student parameters <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta&#39;:=m\theta&#39;+(1-m)\theta\)</span>, where <span class="math inline">\(m\)</span> is the momentum. SGD for students, but no gradient back propagation through the teacher model.</p></li>
<li><p>BN</p>
<ul>
<li><p>In training, first compute the mean and the variance of the layer inputs for the current batch. Then every sample in the current batch is normalized using the batch-wise statistics, and then an affine transformation with learnable parameters <span class="math inline">\(\gamma,\beta\)</span> is applied <span class="math display">\[
\hat{x}=BN(x)=\gamma \frac{x-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2}+\epsilon}+\beta,
\]</span> where $$ is a small constant for numerical stability.</p></li>
<li><p>In inference, though the population statistics should be used instead, but since the additional stage, in many implementations, a more practical way is collecting the proxy statistics <span class="math inline">\(\mu,\sigma^2\)</span> by EMA during training <span class="math display">\[
\mu:=\alpha\mu+(1-\alpha)\mu_{\mathcal{B}},\\
\sigma^2:=\alpha\sigma^2+(1-\alpha)\sigma_{\mathcal{B}}^2,\\
\]</span> then the BN at inference becomes <span class="math inline">\(\hat{x}=BN(x)=\gamma \frac{x-\mu}{\sqrt{\sigma^2}+\epsilon}+\beta,\)</span></p></li>
</ul></li>
<li><p>When the teacher is updated by EMA, the standard BN is not well aligned with the model parameters</p>
<ul>
<li>The teacher is used to generate pseudo GY to guide the learning of the student, the batch-wise BN will make generated labels be cross-sample dependent.</li>
<li>Possible mismatch between the model parameters <span class="math inline">\(\theta&#39;\)</span> and batch-wise BN statistics in the teacher model since the non-alignment of iterations.==&gt; lead to non-smoothness in the parameter space.</li>
</ul></li>
</ul>
<h4 id="eman">EMAN</h4>
<ul>
<li><p>Compared with formula (2) in pervious section, they use EMAN for the teacher during training (student still uses BN ) where <span class="math inline">\(\hat{x}=EMAN(x)=\gamma \frac{x-\mu&#39;}{\sqrt{\sigma&#39;^2}+\epsilon}+\beta,\)</span> where <span class="math inline">\(\mu&#39;,\sigma&#39;^2\)</span> are also exponentially moving averaged from the student <span class="math inline">\(\mu,\sigma^2\)</span>: <span class="math display">\[
\mu&#39;:=m\mu&#39;+(1-m)\mu,\\
\sigma&#39;^2:=m\sigma&#39;^2+(1-m)\sigma^2,\\
\]</span></p>
<ul>
<li>This is a linear transform which is no longer dependent on batch statistics.</li>
</ul></li>
<li><p>Although the student is still cross-sample dependent, this is a less serious issue than the cross-sample dependency in the teacher.</p></li>
</ul>
<h2 id="paper-45-recognizing-actions-in-videos-from-unseen-viewpoints">Paper 45: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.16516">Recognizing Actions in Videos from Unseen Viewpoints</a></h2>
<h3 id="why-43">Why</h3>
<ul>
<li>CNNs are unable to recognize actions/data that are outside of the training data distribution, also unseen viewpoint.</li>
<li>PoseNet for 3D coordinates, and CalibNet for a limited extrinsic camera matrix.</li>
<li>But the 3D pose directly from PoseNet may not be the best feature since scale changes, speed of which the action occurs etc.</li>
<li></li>
</ul>
<h3 id="goal-42">Goal</h3>
<ul>
<li>introduce a new geometric convolutional layer that can learn viewpoint invariant representations. The core is to learn and represent the intrinsic camera matrix <span class="math inline">\(K\)</span>.</li>
<li>Estimating 3D poses, then explores using different representations of it for recognition.</li>
<li></li>
</ul>
<h3 id="how-44">How</h3>
<ul>
<li><p>Learn global 3D pose and 2d multi-view projections of it for classifying unseen viewpoints.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211017124211442.png" alt="image-20211017124211442" /><figcaption aria-hidden="true">image-20211017124211442</figcaption>
</figure></li>
<li><p>The 3d world coordinate system is the same for all videos, thus <span class="math inline">\(R\)</span> is different for each video, depending on the camera viewpoint. <span class="math inline">\(R\)</span> plays the role of aligning features in different scenes, so that the losses are minimized.</p></li>
<li><p>To enforce the similarity of the 3D representations from the same action and the dissimilarity of 2D view,</p>
<ul>
<li><span class="math inline">\(\mathrm{3d\_{loss}}(V,U)=\|F_W(V)-F_W(U)\|_F\)</span>,</li>
<li><span class="math inline">\(cam_reg(c_1,c_2)=\max(-\|c_1,c_2\|_F,\alpha)\)</span></li>
</ul></li>
<li><p>The final loss</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211017150147461.png" alt="image-20211017150147461" /><figcaption aria-hidden="true">image-20211017150147461</figcaption>
</figure></li>
</ul>
<h2 id="paper-46-shot-contrastive-self-supervised-learning-for-scene-boundary-detection">Paper 46: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.13537">Shot Contrastive Self-Supervised Learning for Scene Boundary Detection</a></h2>
<h3 id="why-44">Why</h3>
<ul>
<li>A shot: a series of frames captured from the same camera over an interrupted period of time. A scene: a series of shots depicting a semantically cohesive part of a story.</li>
<li>Scenes are more difficult to be localized.</li>
<li>Previous relatively simple data augmentation used by SSL cannot encode the complex temporal scene-structure.</li>
<li>Scene detection: define the locations in videos where different scenes begin and end.</li>
</ul>
<h3 id="goal-43">Goal</h3>
<ul>
<li>Find scene boundaries. Propose ShotCoL to Learn a shot representation that maximizes the similarity between nearby shots compared to randomly selected shots, and then use the shot representation to detect scene boundary.</li>
<li>Use the priors: nearby shots tend to have the same set of actors enacting a semantically cohesive story-arch, and therefore are more similar to each other compared with other randomly selected shots.==&gt;consider nearby shots as augmented versions of each other .</li>
<li>Specifically, given a shot (both the images and audios), they try to: (a) maximize its similarity with its most similar neighboring shot, and (b) minimize its similarity with a set of randomly selected shots.</li>
</ul>
<h3 id="how-45">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20211017221640577.png" alt="image-20211017221640577" /><figcaption aria-hidden="true">image-20211017221640577</figcaption>
</figure>
<h4 id="shot-level-representation-learning">Shot-level representation learning</h4>
<ul>
<li>Given a full-length input video, use standard shot detection techniques to divide it into its constituent set of shots.</li>
<li>Comprises of encoder network and momentum contrastive learning to contrast the similarity of the embedded shots.</li>
<li>Shot encoder network
<ul>
<li>The intra-shot frame dynamics is not as important since scene boundaries depend on inter-shot relationships.
<ul>
<li>Sample <span class="math inline">\(k\)</span> frames uniformly from each shot, and then reshape the 4D tensor to 3D by combining <span class="math inline">\(c\)</span> channels and <span class="math inline">\(k\)</span> frames.</li>
<li>Advantages: the usage of standard networks and resource efficiency.</li>
</ul></li>
<li>Audio modality: use a Wavegram-Longmel CNN which incorporates a 14-layer CNN similar in architecture to the VGG network.</li>
</ul></li>
<li>Shot contrastive learning
<ul>
<li>Pretext: given a query shot, first find the positive key as its most similar shot within a neighborhood around the query, and then maximizes the similarity between the query and the positive key, and minimize the similarity of the query with a set of randomly selected shots.</li>
<li>The positive key is the most similar shot compared with the query one in embedded space (after query encoder).</li>
<li>The contrastive loss is InfoNCE.</li>
<li>Momentum contrast: save the embedded keys in a fixed-sized queue as negative keys, and follow FIFO. And a momentum update scheme is used for the key encoder. <span class="math inline">\(\theta_k \leftarrow\alpha\theta_k+(1-\alpha)\theta_q\)</span>.</li>
</ul></li>
</ul>
<h4 id="supervised-learning">Supervised learning</h4>
<ul>
<li>Formulate the problem of scene boundary detection as a binary classification problem of determining if a shot boundary is also a scene boundary or not.</li>
<li>For each shot boundary, consider its <span class="math inline">\(2N\)</span> neighboring shots as a data-point to perform scene boundary detection.</li>
<li>Use MLP as the classifier of boundary, and take the features extracted from trained shot-level contrastive learning as the input.</li>
<li>While inference, directly use the features of audio and visual from shot encoder, and the trained MLP.</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">paper</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">SSL</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/notes//" class="article-tag-list-link color1">notes</a>
        		</li>
      		
		</ul>
	</div>


      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
    <a href="/posts/uncategorized/2021-11-20-tmp-cat.html" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/posts/notes/2021-08-23-notes-paper-SSL-survey.html" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Paper--Fall Surveys</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-1-jigsaw-clustering-for-unsupervised-visual-representation-learning"><span class="toc-number">1.</span> <span class="toc-text">Paper 1: Jigsaw Clustering for Unsupervised Visual Representation Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-pretext-task"><span class="toc-number">1.1.</span> <span class="toc-text">Previous pretext task</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ideas"><span class="toc-number">1.2.</span> <span class="toc-text">Ideas</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#jigsaw-clustering-task"><span class="toc-number">1.2.1.</span> <span class="toc-text">Jigsaw Clustering task</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how"><span class="toc-number">1.3.</span> <span class="toc-text">How?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-2-self-supervised-motion-learning-from-static-images"><span class="toc-number">2.</span> <span class="toc-text">Paper 2: Self-supervised Motion Learning from Static Images</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why"><span class="toc-number">2.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-work"><span class="toc-number">2.2.</span> <span class="toc-text">Previous work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-1"><span class="toc-number">2.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#idea"><span class="toc-number">2.3.1.</span> <span class="toc-text">Idea</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#motion-learning-from-static-images"><span class="toc-number">2.3.2.</span> <span class="toc-text">Motion learning from static images</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#pseudo-motions"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">Pseudo motions</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#static-masks"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">Static masks</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#implementation"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">Implementation</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-3-self-supervised-video-representation-learning-by-context-and-motion-decoupling"><span class="toc-number">3.</span> <span class="toc-text">Paper 3: Self-supervised Video Representation Learning by Context and Motion Decoupling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-1"><span class="toc-number">3.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-work-1"><span class="toc-number">3.2.</span> <span class="toc-text">Previous work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal"><span class="toc-number">3.3.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-2"><span class="toc-number">3.4.</span> <span class="toc-text">How</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-4-skip-convolutions-for-efficient-video-processing"><span class="toc-number">4.</span> <span class="toc-text">Paper 4: Skip-convolutions for Efficient Video Processing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-2"><span class="toc-number">4.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-work-2"><span class="toc-number">4.2.</span> <span class="toc-text">Previous work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-1"><span class="toc-number">4.3.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-3"><span class="toc-number">4.4.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#skip-convolutions"><span class="toc-number">4.4.1.</span> <span class="toc-text">Skip Convolutions</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-5-temporal-query-networks-for-fine-grained-video-understanding"><span class="toc-number">5.</span> <span class="toc-text">Paper 5: Temporal Query Networks for Fine-grained Video Understanding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-3"><span class="toc-number">5.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-2"><span class="toc-number">5.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-4"><span class="toc-number">5.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tqn"><span class="toc-number">5.3.1.</span> <span class="toc-text">TQN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#stochastically-updated-feature-bank"><span class="toc-number">5.3.2.</span> <span class="toc-text">Stochastically updated feature bank</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#factorizing-categories-into-attribute-queries"><span class="toc-number">5.3.3.</span> <span class="toc-text">Factorizing categories into attribute queries</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#implementation-1"><span class="toc-number">5.3.4.</span> <span class="toc-text">Implementation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-6-unsupervised-disentanglement-of-linear-encoded-facial-semantics"><span class="toc-number">6.</span> <span class="toc-text">Paper 6: Unsupervised disentanglement of linear-encoded facial semantics</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-4"><span class="toc-number">6.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-3"><span class="toc-number">6.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-5"><span class="toc-number">6.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#decorrelating-latent-code-in-stylegan"><span class="toc-number">6.3.1.</span> <span class="toc-text">Decorrelating latent code in StyleGAN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#stabilized-training-for-3d-face-reconstruction"><span class="toc-number">6.3.2.</span> <span class="toc-text">Stabilized training for 3D face reconstruction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#disentangle-semantics-with-linear-regression"><span class="toc-number">6.3.3.</span> <span class="toc-text">Disentangle semantics with linear regression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#image-manipulation-for-data-augmentation"><span class="toc-number">6.3.4.</span> <span class="toc-text">Image manipulation for data augmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#localized-representation-learning"><span class="toc-number">6.3.5.</span> <span class="toc-text">Localized representation learning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-7-bi-gcn-binary-graph-convolutional-network"><span class="toc-number">7.</span> <span class="toc-text">Paper 7: Bi-GCN: Binary Graph Convolutional Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-5"><span class="toc-number">7.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-4"><span class="toc-number">7.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-6"><span class="toc-number">7.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#gcn"><span class="toc-number">7.3.1.</span> <span class="toc-text">GCN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bi-gcn"><span class="toc-number">7.3.2.</span> <span class="toc-text">Bi-GCN</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#binarization-of-the-feature-extraction-step"><span class="toc-number">7.3.2.1.</span> <span class="toc-text">Binarization of the feature extraction step</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#binary-gradient-approximation-based-back-propagation"><span class="toc-number">7.3.2.2.</span> <span class="toc-text">Binary gradient approximation based back propagation</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-8-an-attractor-guided-neural-networks-for-skeleton-based-human-motion-prediction"><span class="toc-number">8.</span> <span class="toc-text">Paper 8: An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-6"><span class="toc-number">8.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-5"><span class="toc-number">8.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-7"><span class="toc-number">8.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mtde"><span class="toc-number">8.3.1.</span> <span class="toc-text">MTDE</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ajre"><span class="toc-number">8.3.2.</span> <span class="toc-text">AJRE</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#gce"><span class="toc-number">8.3.2.1.</span> <span class="toc-text">GCE</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#lie"><span class="toc-number">8.3.2.2.</span> <span class="toc-text">LIE</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#affm"><span class="toc-number">8.3.2.3.</span> <span class="toc-text">AFFM</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-9-cascade-graph-neural-networks-for-rgb-d-salient-object-detection"><span class="toc-number">9.</span> <span class="toc-text">Paper 9: Cascade Graph Neural Networks for RGB-D Salient Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-7"><span class="toc-number">9.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-6"><span class="toc-number">9.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-8"><span class="toc-number">9.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#cross-modality-reasoning-with-gnns"><span class="toc-number">9.3.1.</span> <span class="toc-text">Cross-modality reasoning with GNNs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cascade-graph-neural-networks"><span class="toc-number">9.3.2.</span> <span class="toc-text">Cascade Graph Neural networks</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-10-coarse-fine-networks-for-temporal-activity-detection-in-videos"><span class="toc-number">10.</span> <span class="toc-text">Paper 10: Coarse-Fine Networks for Temporal Activity Detection in Videos</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-8"><span class="toc-number">10.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-7"><span class="toc-number">10.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-9"><span class="toc-number">10.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#grid-pool"><span class="toc-number">10.3.1.</span> <span class="toc-text">Grid pool</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-stage-fusion"><span class="toc-number">10.3.2.</span> <span class="toc-text">Multi-stage fusion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#model-details"><span class="toc-number">10.3.3.</span> <span class="toc-text">Model details</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-11-coconets-continuous-contrastive-3d-scene-representations"><span class="toc-number">11.</span> <span class="toc-text">Paper 11: CoCoNets: Continuous Contrastive 3D Scene Representations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-9"><span class="toc-number">11.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-8"><span class="toc-number">11.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-10"><span class="toc-number">11.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#result"><span class="toc-number">11.3.1.</span> <span class="toc-text">Result</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-12-cutpaste-self-supervised-learning-for-anomaly-detection-and-localization"><span class="toc-number">12.</span> <span class="toc-text">Paper 12: CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-10"><span class="toc-number">12.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-9"><span class="toc-number">12.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-11"><span class="toc-number">12.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ssl-with-cutpaste"><span class="toc-number">12.3.1.</span> <span class="toc-text">SSL with CutPaste</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cutpaste-variants"><span class="toc-number">12.3.2.</span> <span class="toc-text">CutPaste variants</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#computing-anomaly-score"><span class="toc-number">12.3.3.</span> <span class="toc-text">Computing anomaly score</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#localization-with-patch-representation"><span class="toc-number">12.3.4.</span> <span class="toc-text">Localization with patch representation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-13-discriminative-latent-semantic-graph-for-video-captioning"><span class="toc-number">13.</span> <span class="toc-text">Paper 13: Discriminative Latent Semantic Graph for Video Captioning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-11"><span class="toc-number">13.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-10"><span class="toc-number">13.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-12"><span class="toc-number">13.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#architecture-design"><span class="toc-number">13.3.1.</span> <span class="toc-text">Architecture Design</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#latent-semantic-graph"><span class="toc-number">13.3.2.</span> <span class="toc-text">Latent Semantic graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#discriminative-language-validation"><span class="toc-number">13.3.3.</span> <span class="toc-text">Discriminative language validation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-14-enhancing-self-supervised-video-representation-learning-via-multi-level-feature-optimization"><span class="toc-number">14.</span> <span class="toc-text">Paper 14: Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-12"><span class="toc-number">14.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-11"><span class="toc-number">14.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-13"><span class="toc-number">14.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#beyond-instance-discrimination"><span class="toc-number">14.3.1.</span> <span class="toc-text">Beyond instance discrimination</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graph-constraint-for-multi-level-features"><span class="toc-number">14.3.2.</span> <span class="toc-text">Graph constraint for multi-level features</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#temporal-modeling"><span class="toc-number">14.3.3.</span> <span class="toc-text">Temporal modeling</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-15-exploring-simple-siamese-representation-learning"><span class="toc-number">15.</span> <span class="toc-text">Paper 15: Exploring simple siamese representation learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-13"><span class="toc-number">15.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-12"><span class="toc-number">15.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-14"><span class="toc-number">15.3.</span> <span class="toc-text">How</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-16-git-graph-interactive-transformer-for-vehicle-re-identification"><span class="toc-number">16.</span> <span class="toc-text">Paper 16: GiT: Graph Interactive Transformer for Vehicle Re-identification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-14"><span class="toc-number">16.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-13"><span class="toc-number">16.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-15"><span class="toc-number">16.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lcg-module"><span class="toc-number">16.3.1.</span> <span class="toc-text">LCG module</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformer-layer"><span class="toc-number">16.3.2.</span> <span class="toc-text">Transformer layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graph-interactive-transformer"><span class="toc-number">16.3.3.</span> <span class="toc-text">Graph interactive Transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#loss-function-design"><span class="toc-number">16.3.4.</span> <span class="toc-text">Loss function design</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-17-graph-time-convolutional-neural-networks"><span class="toc-number">17.</span> <span class="toc-text">Paper 17: Graph-Time Convolutional Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-15"><span class="toc-number">17.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-14"><span class="toc-number">17.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-16"><span class="toc-number">17.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#signals-over-product-graphs"><span class="toc-number">17.3.1.</span> <span class="toc-text">Signals over product graphs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graph-time-cnns"><span class="toc-number">17.3.2.</span> <span class="toc-text">Graph-time CNNs</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-18-graphzoom-a-multi-level-spectral-approach-for-accurate-and-scalable-graph-embedding"><span class="toc-number">18.</span> <span class="toc-text">Paper 18: Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-16"><span class="toc-number">18.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-15"><span class="toc-number">18.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-17"><span class="toc-number">18.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#graph-fusion"><span class="toc-number">18.3.1.</span> <span class="toc-text">Graph Fusion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spectral-coarsening"><span class="toc-number">18.3.2.</span> <span class="toc-text">Spectral coarsening</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graph-embedding"><span class="toc-number">18.3.3.</span> <span class="toc-text">Graph embedding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#embedding-refinement"><span class="toc-number">18.3.4.</span> <span class="toc-text">Embedding refinement</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-19-group-contrastive-self-supervised-learning-on-graphs"><span class="toc-number">19.</span> <span class="toc-text">Paper 19: Group Contrastive Self-Supervised Learning on Graphs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-17"><span class="toc-number">19.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-16"><span class="toc-number">19.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-18"><span class="toc-number">19.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#the-proposed-group-contrastive-learning-framework"><span class="toc-number">19.3.1.</span> <span class="toc-text">The proposed group contrastive learning framework</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#intra-space-objective-function"><span class="toc-number">19.3.2.</span> <span class="toc-text">Intra-space objective function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#inter-space-objective-function"><span class="toc-number">19.3.3.</span> <span class="toc-text">Inter-space objective function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-overall-objective-function"><span class="toc-number">19.3.4.</span> <span class="toc-text">The overall objective function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#groupcl-graphcl-with-group-contrast"><span class="toc-number">19.3.5.</span> <span class="toc-text">GroupCL: GraphCL with Group Contrast</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graphig-infograph-with-group-contrast"><span class="toc-number">19.3.6.</span> <span class="toc-text">GraphIG: InfoGraph with Group Contrast</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-20-homophily-outlier-detection-in-non-iid-categorical-data"><span class="toc-number">20.</span> <span class="toc-text">Paper 20: Homophily outlier detection in non-IID categorical data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-18"><span class="toc-number">20.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-17"><span class="toc-number">20.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-19"><span class="toc-number">20.3.</span> <span class="toc-text">How</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-21-hyperparameter-free-and-explainable-whole-graph-embedding"><span class="toc-number">21.</span> <span class="toc-text">Paper 21: Hyperparameter-free and Explainable Whole Graph Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-19"><span class="toc-number">21.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-18"><span class="toc-number">21.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-20"><span class="toc-number">21.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#dhc-entropy"><span class="toc-number">21.3.1.</span> <span class="toc-text">DHC-entropy</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-22-infograph-unsupervised-and-semi-supervised-graph-level-representation-learning-via-mutual-information-maximization"><span class="toc-number">22.</span> <span class="toc-text">Paper 22: Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-20"><span class="toc-number">22.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-19"><span class="toc-number">22.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-21"><span class="toc-number">22.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#infograph"><span class="toc-number">22.3.1.</span> <span class="toc-text">InfoGraph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#infograph-1"><span class="toc-number">22.3.2.</span> <span class="toc-text">InfoGraph*</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-23-iterative-graph-self-distillation"><span class="toc-number">23.</span> <span class="toc-text">Paper 23: Iterative graph self-distillation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-21"><span class="toc-number">23.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-20"><span class="toc-number">23.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-22"><span class="toc-number">23.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#iterative-graph-self-distillation-framework"><span class="toc-number">23.3.1.</span> <span class="toc-text">Iterative graph self-distillation framework</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#self-supervised-learning-with-igsd"><span class="toc-number">23.3.2.</span> <span class="toc-text">Self-supervised learning with IGSD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#semi-supervised-learning-with-igsd"><span class="toc-number">23.3.3.</span> <span class="toc-text">Semi-supervised learning with IGSD</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-24-learning-by-aligning-videos-in-time"><span class="toc-number">24.</span> <span class="toc-text">Paper 24: Learning by Aligning Videos in Time</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-22"><span class="toc-number">24.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-21"><span class="toc-number">24.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-23"><span class="toc-number">24.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#temporal-alignment-loss"><span class="toc-number">24.3.1.</span> <span class="toc-text">Temporal alignment loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#temporal-regularization"><span class="toc-number">24.3.2.</span> <span class="toc-text">Temporal regularization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#final-loss"><span class="toc-number">24.3.3.</span> <span class="toc-text">Final loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#encoder-network"><span class="toc-number">24.3.4.</span> <span class="toc-text">Encoder network</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-25-learning-graph-representation-by-aggregating-subgraphs-via-mutual-information-maximization"><span class="toc-number">25.</span> <span class="toc-text">Paper 25: Learning graph representation by aggregating subgraphs via mutual information maximization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-23"><span class="toc-number">25.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-22"><span class="toc-number">25.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-24"><span class="toc-number">25.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#node-agg-stage"><span class="toc-number">25.3.1.</span> <span class="toc-text">Node-agg stage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#layer-agg-stage"><span class="toc-number">25.3.2.</span> <span class="toc-text">Layer-agg stage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#subgraph-agg-stage"><span class="toc-number">25.3.3.</span> <span class="toc-text">Subgraph-agg stage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#implementation-2"><span class="toc-number">25.3.4.</span> <span class="toc-text">Implementation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-26-mile-a-multi-level-framework-for-scalable-graph-embedding"><span class="toc-number">26.</span> <span class="toc-text">Paper 26: Mile: A multi-level framework for scalable graph embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-24"><span class="toc-number">26.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-23"><span class="toc-number">26.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-25"><span class="toc-number">26.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#graph-coarsening"><span class="toc-number">26.3.1.</span> <span class="toc-text">Graph coarsening</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#base-embedding-on-coarsened-graph"><span class="toc-number">26.3.2.</span> <span class="toc-text">Base embedding on coarsened graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#refinement-of-embeddings"><span class="toc-number">26.3.3.</span> <span class="toc-text">Refinement of embeddings</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-27-missing-data-estimation-in-temporal-multilayer-position-aware-graph-neural-network-tmp-gnn"><span class="toc-number">27.</span> <span class="toc-text">Paper 27: Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-25"><span class="toc-number">27.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-24"><span class="toc-number">27.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-26"><span class="toc-number">27.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#notations-and-preliminaries"><span class="toc-number">27.3.1.</span> <span class="toc-text">Notations and preliminaries</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tmp-gnn"><span class="toc-number">27.3.2.</span> <span class="toc-text">TMP-GNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bi-directional-rnn"><span class="toc-number">27.3.3.</span> <span class="toc-text">Bi-directional RNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-28-multi-level-graph-contrastive-learning"><span class="toc-number">28.</span> <span class="toc-text">Paper 28: Multi-Level Graph Contrastive Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-26"><span class="toc-number">28.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-25"><span class="toc-number">28.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-27"><span class="toc-number">28.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#data-augmentation"><span class="toc-number">28.3.1.</span> <span class="toc-text">Data augmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gnn-encoder"><span class="toc-number">28.3.2.</span> <span class="toc-text">GNN Encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mlp-and-graph-pooling"><span class="toc-number">28.3.3.</span> <span class="toc-text">MLP and graph pooling</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-level-loss-function"><span class="toc-number">28.3.4.</span> <span class="toc-text">Multi-level loss function</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-29-permutation-invariant-variational-autoencoder-for-graph-level-representation-learning"><span class="toc-number">29.</span> <span class="toc-text">Paper 29: Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-27"><span class="toc-number">29.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-26"><span class="toc-number">29.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-28"><span class="toc-number">29.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#problem-definition"><span class="toc-number">29.3.1.</span> <span class="toc-text">Problem definition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#permutation-invariant-variational-graph-autoencoder"><span class="toc-number">29.3.2.</span> <span class="toc-text">Permutation-Invariant variational graph autoencoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#details-of-the-model-architecture"><span class="toc-number">29.3.3.</span> <span class="toc-text">Details of the model architecture</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-30-pinet-attention-pooling-for-graph-classification"><span class="toc-number">30.</span> <span class="toc-text">Paper 30: PiNet: Attention Pooling for Graph Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-28"><span class="toc-number">30.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-27"><span class="toc-number">30.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-29"><span class="toc-number">30.3.</span> <span class="toc-text">How</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-31-self-supervised-graph-level-representation-learning-with-local-and-global-structure"><span class="toc-number">31.</span> <span class="toc-text">Paper 31: Self-supervised Graph-level Representation Learning with Local and Global Structure</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-29"><span class="toc-number">31.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-28"><span class="toc-number">31.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-30"><span class="toc-number">31.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#problem-definition-1"><span class="toc-number">31.3.1.</span> <span class="toc-text">Problem definition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graphlog"><span class="toc-number">31.3.2.</span> <span class="toc-text">GraphLoG</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-32-self-supervised-heterogeneous-graph-neural-network-with-co-contrastive-learning"><span class="toc-number">32.</span> <span class="toc-text">Paper 32: Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-30"><span class="toc-number">32.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-29"><span class="toc-number">32.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-31"><span class="toc-number">32.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#node-feature-transformation"><span class="toc-number">32.3.1.</span> <span class="toc-text">Node feature transformation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#network-schema-view-guided-encoder"><span class="toc-number">32.3.2.</span> <span class="toc-text">Network schema view guided encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#meta-path-view-guided-encoder"><span class="toc-number">32.3.3.</span> <span class="toc-text">Meta-path view guided encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#view-mask-mechanism"><span class="toc-number">32.3.4.</span> <span class="toc-text">View mask mechanism</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#collaboratively-contrastive-optimization"><span class="toc-number">32.3.5.</span> <span class="toc-text">Collaboratively contrastive optimization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#model-extension"><span class="toc-number">32.3.6.</span> <span class="toc-text">Model extension</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-33-sm-sge-a-self-supervised-multi-scale-skeleton-graph-encoding-framework-for-person-re-identification"><span class="toc-number">33.</span> <span class="toc-text">Paper 33: SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-31"><span class="toc-number">33.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-30"><span class="toc-number">33.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-32"><span class="toc-number">33.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ms-skeleton-graph-construction"><span class="toc-number">33.3.1.</span> <span class="toc-text">MS skeleton graph construction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#msgr"><span class="toc-number">33.3.2.</span> <span class="toc-text">MSGR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-scale-skeleton-reconstruction-mechanism"><span class="toc-number">33.3.3.</span> <span class="toc-text">Multi-scale skeleton reconstruction mechanism</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-entire-framework"><span class="toc-number">33.3.4.</span> <span class="toc-text">The entire framework</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-34-space-time-correspondence-as-a-contrastive-random-walk"><span class="toc-number">34.</span> <span class="toc-text">Paper 34: Space-time correspondence as a contrastive random walk</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-32"><span class="toc-number">34.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-31"><span class="toc-number">34.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-33"><span class="toc-number">34.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#contrastive-random-walks-on-video"><span class="toc-number">34.3.1.</span> <span class="toc-text">Contrastive random walks on video</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ssl"><span class="toc-number">34.3.2.</span> <span class="toc-text">SSL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#edge-dropout"><span class="toc-number">34.3.3.</span> <span class="toc-text">Edge dropout</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-35-spatially-consistent-representation-learning"><span class="toc-number">35.</span> <span class="toc-text">Paper 35: Spatially consistent representation learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#why-33"><span class="toc-number">35.0.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#goal-32"><span class="toc-number">35.0.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#how-34"><span class="toc-number">35.0.3.</span> <span class="toc-text">How</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spatially-consistent-representation-learning"><span class="toc-number">35.0.4.</span> <span class="toc-text">Spatially consistent representation learning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-36-spatiotemporal-contrastive-video-representation-learning"><span class="toc-number">36.</span> <span class="toc-text">Paper 36: Spatiotemporal contrastive video representation learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-34"><span class="toc-number">36.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-33"><span class="toc-number">36.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-35"><span class="toc-number">36.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#video-representation-learning-framework"><span class="toc-number">36.3.1.</span> <span class="toc-text">Video representation learning framework</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#video-encoder"><span class="toc-number">36.3.2.</span> <span class="toc-text">Video encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#data-augmentation-1"><span class="toc-number">36.3.3.</span> <span class="toc-text">Data augmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#evaluation"><span class="toc-number">36.3.4.</span> <span class="toc-text">Evaluation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-37-sportscap-monocular-3d-human-motion-capture-and-fine-grained-understanding-in-challenging-sports-videos"><span class="toc-number">37.</span> <span class="toc-text">Paper 37: SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-35"><span class="toc-number">37.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-34"><span class="toc-number">37.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-36"><span class="toc-number">37.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#motion-embedding-and-capturing"><span class="toc-number">37.3.1.</span> <span class="toc-text">Motion embedding and capturing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#action-parsing"><span class="toc-number">37.3.2.</span> <span class="toc-text">Action parsing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-task-training"><span class="toc-number">37.3.3.</span> <span class="toc-text">Multi-task training</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-38-ssan-separable-self-attention-network-for-video-representation-learning"><span class="toc-number">38.</span> <span class="toc-text">Paper 38: SSAN: Separable Self-Attention Network for Video Representation Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-36"><span class="toc-number">38.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-35"><span class="toc-number">38.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-37"><span class="toc-number">38.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#self-attention-in-vision"><span class="toc-number">38.3.1.</span> <span class="toc-text">Self-attention in vision</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#separable-self-attention-module"><span class="toc-number">38.3.2.</span> <span class="toc-text">Separable Self-attention module</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#network-architecture"><span class="toc-number">38.3.3.</span> <span class="toc-text">Network architecture</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-39-tdgraphembed-temporal-dynamic-graph-level-embedding"><span class="toc-number">39.</span> <span class="toc-text">Paper 39: tdgraphembed: Temporal dynamic graph-level embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-37"><span class="toc-number">39.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-36"><span class="toc-number">39.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-38"><span class="toc-number">39.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#random-walk"><span class="toc-number">39.3.1.</span> <span class="toc-text">Random walk</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#framework"><span class="toc-number">39.3.2.</span> <span class="toc-text">Framework</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tips"><span class="toc-number">39.3.3.</span> <span class="toc-text">Tips</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-40-videomoco-contrastive-video-representation-learning-with-temporally-adversarial-examples"><span class="toc-number">40.</span> <span class="toc-text">Paper 40: Videomoco: Contrastive video representation learning with temporally adversarial examples</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-38"><span class="toc-number">40.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-37"><span class="toc-number">40.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-39"><span class="toc-number">40.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#moco"><span class="toc-number">40.3.1.</span> <span class="toc-text">MoCo</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#temporally-adversarial-learning"><span class="toc-number">40.3.2.</span> <span class="toc-text">Temporally adversarial learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#temporal-decay"><span class="toc-number">40.3.3.</span> <span class="toc-text">Temporal decay</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#visualization"><span class="toc-number">40.3.4.</span> <span class="toc-text">Visualization</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-41-visual-relationship-forecasting-in-videos"><span class="toc-number">41.</span> <span class="toc-text">Paper 41: Visual Relationship Forecasting in Videos</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-39"><span class="toc-number">41.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-38"><span class="toc-number">41.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-40"><span class="toc-number">41.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#framework-1"><span class="toc-number">41.3.1.</span> <span class="toc-text">Framework</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#feature-representation"><span class="toc-number">41.3.2.</span> <span class="toc-text">Feature representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#object-level-reasoning"><span class="toc-number">41.3.3.</span> <span class="toc-text">Object-level reasoning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformer-for-temporal-modeling"><span class="toc-number">41.3.4.</span> <span class="toc-text">Transformer for temporal modeling</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-42-wasserstein-embedding-for-graph-learning"><span class="toc-number">42.</span> <span class="toc-text">Paper 42: Wasserstein embedding for graph learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-40"><span class="toc-number">42.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-39"><span class="toc-number">42.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-41"><span class="toc-number">42.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#linear-wassertein-embedding"><span class="toc-number">42.3.1.</span> <span class="toc-text">Linear Wassertein embedding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#wegl"><span class="toc-number">42.3.2.</span> <span class="toc-text">WEGL</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-43-anomaly-detection-in-video-via-self-supervised-and-multi-task-learning"><span class="toc-number">43.</span> <span class="toc-text">Paper 43: Anomaly Detection in Video via Self-Supervised and Multi-Task Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-41"><span class="toc-number">43.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-40"><span class="toc-number">43.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-42"><span class="toc-number">43.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#task-1-arrow-of-time"><span class="toc-number">43.3.1.</span> <span class="toc-text">Task 1: arrow of time</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#task-2-motion-irregularity"><span class="toc-number">43.3.2.</span> <span class="toc-text">Task 2: motion irregularity</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#task-3-middle-bounding-box-prediction"><span class="toc-number">43.3.3.</span> <span class="toc-text">Task 3: middle bounding box prediction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#task-4-model-distillation"><span class="toc-number">43.3.4.</span> <span class="toc-text">Task 4: model distillation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#joint-loss"><span class="toc-number">43.3.5.</span> <span class="toc-text">Joint loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#inference"><span class="toc-number">43.3.6.</span> <span class="toc-text">Inference</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-44-exponential-moving-average-normalization-for-self-supervised-and-semi-supervised-learning"><span class="toc-number">44.</span> <span class="toc-text">Paper 44: Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-42"><span class="toc-number">44.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-41"><span class="toc-number">44.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-43"><span class="toc-number">44.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ema-teacher"><span class="toc-number">44.3.1.</span> <span class="toc-text">EMA teacher</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#eman"><span class="toc-number">44.3.2.</span> <span class="toc-text">EMAN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-45-recognizing-actions-in-videos-from-unseen-viewpoints"><span class="toc-number">45.</span> <span class="toc-text">Paper 45: Recognizing Actions in Videos from Unseen Viewpoints</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-43"><span class="toc-number">45.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-42"><span class="toc-number">45.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-44"><span class="toc-number">45.3.</span> <span class="toc-text">How</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-46-shot-contrastive-self-supervised-learning-for-scene-boundary-detection"><span class="toc-number">46.</span> <span class="toc-text">Paper 46: Shot Contrastive Self-Supervised Learning for Scene Boundary Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-44"><span class="toc-number">46.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-43"><span class="toc-number">46.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-45"><span class="toc-number">46.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#shot-level-representation-learning"><span class="toc-number">46.3.1.</span> <span class="toc-text">Shot-level representation learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#supervised-learning"><span class="toc-number">46.3.2.</span> <span class="toc-text">Supervised learning</span></a></li></ol></li></ol></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>



  
  
  

  

  

  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2022 Mia Feng
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


</footer>

    </div>
    <script>
	var yiliaConfig = {
		mathjax: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">posts</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">papers</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">about</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">notes</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Sensor</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">GPR</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">summer school</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">mathematics</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">cnn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">compress</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">infrared</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">hyperspectral</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">blog</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">logistic</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">applications</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">tendency</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">ML</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">naive</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">da</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">data assimilation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">machine learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">book</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">gcn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">paper</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">math</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SSL</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">survey</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">anomaly</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">skeleton</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">cv</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">3d shape</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">fall</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">remote sensing images</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">interpolation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">compression</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">reconstruction</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">AI</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Data assimilation</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>wind interpolation</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">&lt;br&gt; &amp;#160; &amp;#160; &amp;#160; &amp;#160;Mia, &lt;/br&gt;&lt;br&gt; &amp;#160; &amp;#160; &amp;#160; &amp;#160;a master student in NUDT.&lt;/br&gt; &lt;br&gt;  &amp;#160; &amp;#160; &amp;#160; &amp;#160;I love reading, exercising and cooking. &lt;br /&gt; &lt;br&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;Science fictions, detective novels are my favorite.&lt;/br&gt;</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>