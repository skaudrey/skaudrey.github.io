<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <link rel="dns-prefetch" href="//cdn.bootcss.com" />
  <link rel="dns-prefetch" href="//cdn.mathjax.org" />
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>Mia&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper 1: Jigsaw Clustering for Unsupervised Visual Representation Learning Paper 2: Self-supervised Motion Learning from Static Images Paper 3: Self-supervised Video Representation Learning by Con">
<meta property="og:type" content="article">
<meta property="og:title" content="Mia&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/posts/notes/2021-08-27-notes-paper-cvpr2021-ssl-graph.html">
<meta property="og:site_name" content="Mia&#39;s Blog">
<meta property="og:description" content="Paper 1: Jigsaw Clustering for Unsupervised Visual Representation Learning Paper 2: Self-supervised Motion Learning from Static Images Paper 3: Self-supervised Video Representation Learning by Con">
<meta property="og:locale">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827135623377.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827151850175.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172717585.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172910767.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827174808696.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181344187.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181534056.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827225903818.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901091727352.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901093537848.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901130744146.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901141700865.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901142059270.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901152032964.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901225711236.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901230258180.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902113314940.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902120453628.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902133618545.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151200499.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151226323.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902160626221.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902164243961.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903184316148.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232718696.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232735446.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232837194.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903234025866.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210904162325031.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905202700652.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905223900318.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906193605112.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194329698.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194519932.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908111357002.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908112654724.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120140074.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120117673.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910122543720.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130142797.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130231035.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130246505.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131617208.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131811218.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910132656530.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160437825.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160719249.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913111201445.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913112252883.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913131247274.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913133851127.png">
<meta property="og:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913135843608.png">
<meta property="article:published_time" content="2021-08-27T15:16:00.000Z">
<meta property="article:modified_time" content="2021-09-13T18:33:05.680Z">
<meta property="article:author" content="Mia Feng">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="SSL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827135623377.png">
  
    <link rel="alternative" href="/atom.xml" title="Mia&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: #4d4d4d;
    }
  </style>
  

  

<meta name="generator" content="Hexo 5.3.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/assets/img/avatar.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/posts/uncategorized/2019-06-13-about.html">about</a></li>
	        
				<li><a href="/categories/notes">notes</a></li>
	        
				<li><a href="/categories/projects">projects</a></li>
	        
				<li><a href="/categories/talks">talks</a></li>
	        
				<li><a href="/categories/meetings">meetings</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">posts</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">papers</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">about</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/skaudrey" title="github"><i class="icon-github"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:skaudreymia@gmail.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/assets/img/avatar.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/skaudrey" title="github"><i class="icon-github"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:skaudreymia@gmail.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 20%"><a href="/posts/uncategorized/2019-06-13-about.html">about</a></li>
		        
					<li style="width: 20%"><a href="/categories/notes">notes</a></li>
		        
					<li style="width: 20%"><a href="/categories/projects">projects</a></li>
		        
					<li style="width: 20%"><a href="/categories/talks">talks</a></li>
		        
					<li style="width: 20%"><a href="/categories/meetings">meetings</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-notes-paper-cvpr2021-ssl-graph" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Paper 1: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00323.pdf">Jigsaw Clustering for Unsupervised Visual Representation Learning</a></li>
<li>Paper 2: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00240.pdf">Self-supervised Motion Learning from Static Images</a></li>
<li>Paper 3: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00862.pdf">Self-supervised Video Representation Learning by Context and Motion Decoupling</a></li>
<li>Paper 4: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.11487.pdf">Skip-convolutions for Efficient Video Processing</a></li>
<li>Paper 5: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09496.pdf">Temporal Query Networks for Fine-grained Video Understanding</a></li>
<li>Paper 6: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></li>
<li>Paper 7: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></li>
<li>Paper 8: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.09711.pdf">An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</a></li>
<li>Paper 9: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.03087">Cascade Graph Neural Networks for RGB-D Salient Object Detection</a></li>
<li>Paper 10: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01302">Coarse-Fine Networks for Temporal Activity Detection in Videos</a></li>
<li>Paper 11: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.03851">CoCoNets: Continuous Contrastive 3D Scene Representations</a></li>
<li>Paper 12: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.04015">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</a></li>
<li>Paper 13: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.03662">Discriminative Latent Semantic Graph for Video Captioning</a></li>
<li>Paper 14: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.02183?context=cs">Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</a></li>
<li>Paper 15: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.10566">Exploring simple siamese representation learning</a></li>
<li>Paper 16: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.05475">GiT: Graph Interactive Transformer for Vehicle Re-identification</a></li>
<li>Paper 17: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01730">Graph-Time Convolutional Neural Networks</a></li>
<li>Paper 18: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02370">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</a></li>
<li>Paper 19: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.09787.pdf">Group Contrastive Self-Supervised Learning on Graphs</a></li>
<li>Paper 20: <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s10618-021-00750-y">Homophily outlier detection in non-IID categorical data</a></li>
<li>Paper 21: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.02113">Hyperparameter-free and Explainable Whole Graph Embedding</a></li>
<li>Paper 22: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.01000">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</a></li>
<li>Paper 23: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.12609">Iterative graph self-distillation</a></li>
<li>Paper 24: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.17260">Learning by Aligning Videos in Time</a></li>
<li>Paper 25: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.13125">Learning graph representation by aggregating subgraphs via mutual information maximization</a></li>
<li>Paper 26: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.09612">Mile: A multi-level framework for scalable graph embedding</a></li>
<li>Paper 27: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.03400.pdf">Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</a></li>
<li>Paper 28: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.02639">Multi-Level Graph Contrastive Learning</a></li>
<li>Paper 29: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.09856">Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</a></li>
<li>Paper 30: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.04575">PiNet: Attention Pooling for Graph Classification</a></li>
<li>Paper 31: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.02039">Power Law Graph Transformer for Machine Translation and Representation Learning</a></li>
<li>Paper 32: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.04113">Self-supervised Graph-level Representation Learning with Local and Global Structure</a></li>
<li>Paper 33: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.09111">Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</a></li>
<li>Paper 34: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.01903">SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</a></li>
<li>Paper 35: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.14613">Space-time correspondence as a contrastive random walk</a></li>
<li>Paper 36: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.06122">Spatially consistent representation learning</a></li>
<li>Paper 37: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.03800">Spatiotemporal contrastive video representation learning</a></li>
<li>Paper 38: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.11452">SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</a></li>
<li>Paper 39: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.13033">SSAN: Separable Self-Attention Network for Video Representation Learning</a></li>
<li>Paper 40: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411953">tdgraphembed: Temporal dynamic graph-level embedding</a></li>
<li>Paper 41: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf">Videomoco: Contrastive video representation learning with temporally adversarial examples</a></li>
<li>Paper 42: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.01181">Visual Relationship Forecasting in Videos</a></li>
<li>Paper 43: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=AAes_3W-2z">Wasserstein embedding for graph learning</a></li>
</ul>
<a id="more"></a>
<h2 id="paper-1-jigsaw-clustering-for-unsupervised-visual-representation-learning">Paper 1: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00323.pdf">Jigsaw Clustering for Unsupervised Visual Representation Learning</a></h2>
<h3 id="previous-pretext-task">Previous pretext task</h3>
<ul>
<li>Intra-image tasks: including colorization and jigsaw puzzle, design a transform of one image and train the network to learn the transform.
<ul>
<li>Since only the training batch itself is forwarded each time, they name these methods as single -batch methods.</li>
<li>Can be achieved using only one image's information, limiting the learning ability of feature extractors.</li>
</ul></li>
<li>Inter-image tasks
<ul>
<li>require the network to discriminate among different images.</li>
<li>Try to reduce the distance between representations of positive pairs and enlarge the distance between representations of negative samples.</li>
<li>since each training batch and its augmented version are forwarded simultaneously, methods are named as dual-batches methods.</li>
</ul></li>
<li>The way to design an efficient single-batch based method with similar performance to dual-batches methods is still an open problem.</li>
</ul>
<h3 id="ideas">Ideas</h3>
<p>They propose a framework for efficient training of unsupervised models using Jigsaw clustering, which combines advantages of solving jigsaw puzzles and contrastive learning, and makes use of both intra- and inter-image information to guide feature extractor.</p>
<h4 id="jigsaw-clustering-task">Jigsaw Clustering task</h4>
<ul>
<li>Every image in a batch is split into different patches. They are randomly permuted and stitched to form a new batch for training.</li>
<li><strong>Goal</strong> : recover the disrupted parts back to the original images.</li>
<li>The patches are permuted in a batch</li>
<li>The network has to distinguish between different parts of one image and identifies their original positions to recover the original image from multiple montage input images.</li>
<li><strong><em>Why works?</em></strong>
<ul>
<li>Discriminating among different patches in one stitched image forces the model to <em>capture instance-level information inside an image</em>. This level of feature is missing in general in other contrastive learning methods.</li>
<li>Clustering different patches from multiple input images helps the model <em>learn image-level features across images.</em></li>
<li>arranging every patch to the correct location requires detailed location information, which was considered in single-batch methods.</li>
</ul></li>
</ul>
<h3 id="how">How?</h3>
<ul>
<li><p>Batches</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827135623377.png" alt="image-20210827135623377" style="zoom:80%;" /></p>
<p>Each batch will have <span class="math inline">\(n\)</span> images, and after splitting <strong>(patches split in images have a level of overlap)</strong>, there will be <span class="math inline">\(n\times m\times m\)</span> patches, the patches are latterly stitched into <span class="math inline">\(n\)</span> images. The cluster branch will cluster the <span class="math inline">\(n\times m\times m\)</span> patches into <span class="math inline">\(n\)</span> classes so as to define which original image that one patch comes from.</p>
<ul>
<li>Using montage images as input instead of every single patch is noteworthy, since directly using small patches as input leads to the solution with only global information.</li>
<li>the input images form only one batch with the same size as the original batch, which costs half of resource during training compared with recent methods.</li>
<li><strong><em>The choice of <span class="math inline">\(m\)</span> affects the difficulty of the task</em></strong>. They show that <span class="math inline">\(m=2\)</span> is good.</li>
</ul></li>
<li><p>Network</p>
<ul>
<li>The logits is in size <span class="math inline">\(n\times m \times m\)</span>. (in location branch)</li>
</ul></li>
<li><p>Loss function</p>
<ul>
<li><p>The target of clustering is pulling together objects from the same class and pushing away patches from different classes.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827151850175.png" alt="image-20210827151850175" /><figcaption aria-hidden="true">image-20210827151850175</figcaption>
</figure></li>
<li><p>The loss function of location branch is simply cross-entropy loss.</p></li>
<li><p>The final objective is the weighted summation of the two losses mentioned above. But in their experiments, when the two loss are simply summed, they get the best result.</p></li>
</ul></li>
</ul>
<h2 id="paper-2-self-supervised-motion-learning-from-static-images">Paper 2: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00240.pdf">Self-supervised Motion Learning from Static Images</a></h2>
<h3 id="why">Why</h3>
<ul>
<li>To well distinguish actions, correctly locating the prominent motion areas is of crucial importance.</li>
</ul>
<h3 id="previous-work">Previous work</h3>
<ul>
<li>Motion learning by architectures: two-stream networks and 3D convolutional networks. The two-stream networks extract motions representations explicitly from optical flows, while 3D structures apply convolutions on the temporal dimension or space-time cubics to extract motion cues implicitly.</li>
<li>Self-supervised image representation learning: patch-based approaches , image-level pretext tasks such as image inpainting, image colorization, motion segment prediction and predicting image rotations.</li>
<li>Self-supervised video representation learning: extend patch-based context prediction to spatial-temporal scenarios, e.g., spatio-temporal puzzles, video cloze procedure and frame/clip order prediction; learn representations by predicting future frames; generate supervision signals, such as speed up prediction and play back rate prediction.</li>
</ul>
<h3 id="how-1">How</h3>
<h4 id="idea">Idea</h4>
<ul>
<li>Learn <strong>M</strong>otion from <strong>S</strong>tatic <strong>I</strong>mages (MoSI), take images as our data source, and generate deterministic motion patterns.</li>
<li>Given the desired direction and the speed of the motions, MoSI generates pseudo motions from static images. By correctly classifying the direction and speed of the movement in the image sequence, models trained with MoSI is able to well encode motion patterns.</li>
<li>Furthermore, a static mask is applied to the pseudo motion sequences. This produces inconsistent motions between the masked area and the unmasked one, which guides the network to focus on the inconsistent local motions</li>
</ul>
<h4 id="motion-learning-from-static-images">Motion learning from static images</h4>
<h5 id="pseudo-motions">Pseudo motions</h5>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172717585.png" alt="image-20210827172717585" /><figcaption aria-hidden="true">image-20210827172717585</figcaption>
</figure>
<ul>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827172910767.png" alt="image-20210827172910767" /><figcaption aria-hidden="true">image-20210827172910767</figcaption>
</figure></li>
<li><p>Label pool:</p>
<ul>
<li>For each label, a non-zero speed only exists on one axis.</li>
</ul></li>
<li><p>Pseudo motion generation</p>
<ul>
<li><p>To generate the samples with different speeds, the moving distance from the start to the end of the Pseudo sequences are need.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827174808696.png" alt="image-20210827174808696" /><figcaption aria-hidden="true">image-20210827174808696</figcaption>
</figure></li>
<li><p>The start location is randomly sampled from a certain area which ensures the end location is located completely within the source image.</p></li>
<li><p>For label <span class="math inline">\((x, y) = (0, 0)\)</span>, where the sampled image sequence is static on both axis, the start location is selected from the whole image with uniform distribution.</p></li>
</ul></li>
<li><p>Classification</p>
<ul>
<li>each batch contains all transformed image sequences generated from the same source image</li>
<li>The model is trained by cross entropy loss.</li>
</ul></li>
</ul>
<h5 id="static-masks">Static masks</h5>
<ul>
<li>The static masks creates local motion patterns that are inconsistent with the background.</li>
<li>introduce static masks as the second core component of the proposed MoSI since the model may possibly focus on just several pixels.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181344187.png" title="fig:" alt="image-20210827181344187" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827181534056.png" title="fig:" alt="image-20210827181534056" /></li>
<li>the model is now required not only to recognize motion patterns, but also to spot where the motion is happening</li>
</ul>
<h5 id="implementation">Implementation</h5>
<ul>
<li>Data preparations: the source images need to be first sampled from the videos in the video datasets.</li>
<li>Augmentation: randomize the location and the size of the unmasked area. In addition, randomize the selection of the background frames in the MoSI.</li>
</ul>
<h2 id="paper-3-self-supervised-video-representation-learning-by-context-and-motion-decoupling">Paper 3: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.00862.pdf">Self-supervised Video Representation Learning by Context and Motion Decoupling</a></h2>
<h3 id="why-1">Why</h3>
<ul>
<li>What to learn ?
<ul>
<li>Context representation can be used to classify certain actions, but also leads to background bias.</li>
<li>Motion representation</li>
</ul></li>
<li>Problem
<ul>
<li>The source of supervision: video in compressed format (such as MPEG-4) roughly decouples the context and motion information in its I-frames and motion vectors.
<ul>
<li>I-frames can represent relatively static and coarse-grained context information, while motion vectors depict dynamic and fine-grained movements</li>
</ul></li>
</ul></li>
</ul>
<h3 id="previous-work-1">Previous work</h3>
<ul>
<li>SS video representation learning
<ul>
<li>video specific pretext tasks: estimating video playback rates, verifying temporal order of clips, predicting video rotations, solving space-time cubic puzzles, and dense predictive coding.</li>
<li>Contrastive learning
<ul>
<li>clips from the same video are pulled together while clips from different videos are pushed away.</li>
<li>employ adaptive cluster assignment, where the representation and embedding clusters are simultaneously learned.</li>
<li>But they may suffer from the context bias problem.</li>
</ul></li>
<li>mutual supervision across modalities</li>
<li>DSM: enhance the learned video representation by decoupling the scene and the motion. It simply changes the construction of positive and negative pairs in contrastive learning.</li>
</ul></li>
<li>Action recognition in compressed videos
<ul>
<li>Video compression techniques (e.g., H.264 and MPEG4) usually store only a few key frames completely, and reconstruct other frames using motion vectors and residual errors from the key frames.</li>
<li>Some methods directly build models on the compressed data.
<ul>
<li>One replace the optical flow stream in two-stream action recognition models with a motion vector stream.</li>
<li>CoViAR: use all modalities, including I-frames, motion vectors and residuals.</li>
</ul></li>
</ul></li>
<li>Motion prediction
<ul>
<li>deduce the states of an object in a near future.</li>
<li>Typical models: RNNs, Transformers, and GNNs.</li>
</ul></li>
</ul>
<h3 id="goal">Goal</h3>
<p>Design a self-supervised video representation learning method that jointly learns motion prediction and context matching.</p>
<ul>
<li>The context matching task aims to give the video network a rough grasp of the environment in which actions take place. It casts a NCE loss between global features of video clips and I-frames, where clips and I-frames from the same videos are pulled together, while those from different videos are pushed away.</li>
<li>The motion prediction task requires the model to predict pointwise motion dynamics in a near future based on visual information of the current clip.
<ul>
<li>They use pointwise contrastive learning to compare predicted and real motion features at every spatial and temporal location <span class="math inline">\((x,y,t)\)</span>, which will lead to more stable pretraining and better transferring performance.</li>
<li>It works as a strong regularization for video networks, and it can also be regarded as an auxiliary task clearly improves the performance of supervised action recognition.</li>
</ul></li>
</ul>
<h3 id="how-2">How</h3>
<ul>
<li>Data: compressed videos, to be exactly, MPEG-2 Part2, where every I-frames is followed by 11 consecutive P-frames.</li>
<li>Methods: context matching task for coarse-grained and relatively static context representation, and a motion prediction task for learning fine-grained and high-level motion representation.
<ul>
<li>context matching
<ul>
<li>where (video clip, I-frame) pairs from the same videos are pulled together, while pairs from different videos are pushed away</li>
<li></li>
</ul></li>
<li>Motion prediction
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210827225903818.png" title="fig:" alt="image-20210827225903818" /></li>
<li>Only feature points corresponding to the same video <span class="math inline">\(i\)</span> and at the same spatial and temporal position <span class="math inline">\((x, y, t)\)</span> are regarded as positive pairs, otherwise they are regarded as negative pairs.</li>
<li>The input and output for Transformer is considered as a 1-D sequence.</li>
<li>Some findings
<ul>
<li>Predicting future motion information leads to significantly better video retrieval performance compared with estimating current motion information;</li>
<li>Matching predicted and “groundtruth” motion features using the pointwise InfoNCE loss brings better results than directly estimating motion vector values;</li>
<li>Different encoder-decoder networks lead to similar results, while using Transformer performs slightly better.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h2 id="paper-4-skip-convolutions-for-efficient-video-processing">Paper 4: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.11487.pdf">Skip-convolutions for Efficient Video Processing</a></h2>
<h3 id="why-2">Why</h3>
<ul>
<li>Leverage the large amount of redundancies in video streams and save computations</li>
<li>The spiking nets is lack of efficient training algorithms</li>
<li>Residual frames provide a strong prior on the relevant regions, easing the design of effective gating functions</li>
</ul>
<h3 id="previous-work-2">Previous work</h3>
<ul>
<li>Efficient video models
<ul>
<li>feature propagation, which computes the expensive backbone features only on key-frames.</li>
<li>interleave deep and shallow backbones between consecutive frames: methods are mostly suitable for global prediction tasks where a single prediction is made for the whole clip.</li>
</ul></li>
<li>Efficient image models: The reduction of parameter redundancies
<ul>
<li>model compression: Skip-Conv leverages temporal redundancies in activations.</li>
<li>conditional computations in developing efficient models for images.</li>
</ul></li>
</ul>
<h3 id="goal-1">Goal</h3>
<p>To speed up any convolutional network for inference on video streams. Considering a video as a series of changes across frames and network activations, denotes as residual frames. They reformulate standard convolution to be efficiently computed over such residual frames by limiting the computation only to the regions with significant changes while skipping the others. The important residuals are learned by a gating function.</p>
<h3 id="how-3">How</h3>
<ul>
<li>The contributions
<ul>
<li>a simple reformulation of convolution, which computes features on highly sparse residuals instead of dense video frames</li>
<li>Two gating functions, Norm gate and Gumbel gate, to effectively decide whether to process or skip each location, where Gumbel gate is trainable.</li>
</ul></li>
</ul>
<h4 id="skip-convolutions">Skip Convolutions</h4>
<ul>
<li><p>Convolutions on residual frames</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901091727352.png" title="fig:" alt="image-20210901091727352" /></li>
<li>for every kernel support filled with zero values in <span class="math inline">\(\mathrm{r}_t\)</span>, the corresponding output will be trivially zero, and the convolution can be skipped by copying values from <span class="math inline">\(\mathrm{z}_{t−1}\)</span> to <span class="math inline">\(\mathrm{z}_{t}\)</span>.</li>
<li><em>Introduce a gating function for each convolutional layer to predict a binary mask indicating which locations should be processed, and taking only <span class="math inline">\(\mathrm{r}_t\)</span> as input.</em> <span class="math inline">\(\mathrm{r}_t\)</span> as input will provide a strong prior to the gating function.</li>
</ul></li>
<li><p>Gating functions</p>
<ul>
<li><p>Norm gate: decides to skip a residual if its magnitude (norm) is small enough, not learnable</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901093537848.png" title="fig:" alt="image-20210901093537848" /></li>
<li>indicate regions that change significantly across frames, but not all changes are equally important for the final prediction.</li>
</ul></li>
<li><p>Gumbel gate, trainable with the convolutional kernels.</p>
<ul>
<li><p>A higher efficiency can be gained by introducing a higher</p></li>
<li><p>pixel-wise Bernoulli distributions by applying a sigmoid function. During training, sample binary deisions from the Bernoulli distribution.</p></li>
<li><p>Employ the Gumbel reparametrization and a straight-through gradient estimator in order to backpropagate through the sampling procedure.</p></li>
<li><p>The Gating parameters are learned jointly with all model parameters by minimizing <span class="math inline">\(\mathcal{L}_{task}+\beta \mathcal{L}_{gate}\)</span>.</p></li>
<li><p>The gating loss is defined as the average multiply-accumulate (MAC) count needed to process <span class="math inline">\(T\)</span> consecutive frames as</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901130744146.png" alt="image-20210901130744146" style="zoom:80%;" /></p></li>
<li><p>train the model over a fixed-length of frames and do inference iteratively on an binary number of frames.</p></li>
<li><p>By simply adding a downsampling and an unsampling function on the predicted gates, the Skip-conv can be extended to generate structured sparsity. This structure will enable more efficient implementation with minimal effect on performance.</p></li>
</ul></li>
</ul></li>
<li><p>Generalization and future work</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901141700865.png" title="fig:" alt="image-20210901141700865" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901142059270.png" title="fig:" alt="image-20210901142059270" /></li>
</ul></li>
</ul>
<h2 id="paper-5-temporal-query-networks-for-fine-grained-video-understanding">Paper 5: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09496.pdf">Temporal Query Networks for Fine-grained Video Understanding</a></h2>
<p>http://www.robots.ox.ac.uk/~vgg/research/tqn/</p>
<h3 id="why-3">Why</h3>
<ul>
<li>For finer-grain classification which depends on subtle differences in pose, the specific sequence, duration and number of certain subactions, it requires reasoning about events at varying temporal scales and attention to fine details.</li>
<li>the constraints imposed by finite GPU memory. To overcome this, one way is to use pretrained features, but this relies on good initializations and ensures a small domain gap. Another solution focuses on extracting key frames from untrimmed videos.</li>
<li>VQA (visual question and answering)
<ul>
<li>Have queries which attends to relevant features for predicting the answers.</li>
<li>The problem in this paper is more interested in a common set of queries shared across the whole dataset.</li>
</ul></li>
</ul>
<h3 id="goal-2">Goal</h3>
<ul>
<li><p>Fine-grained classification of actions in untrimmed videos.</p></li>
<li><p>Propose a Transformer-based video network, namely the Temporal Query Network (TQN) for fine-grained action classification, which will take a video and a predefined set of queries as input and output responses for each query, where the response is query dependent.</p></li>
<li><p>The queries act as "experts" that are able to pick out from the video the temporal segments required for their response.</p>
<ul>
<li>Pick out relevant temporal segments and ignore irrelevant segments.</li>
<li>Since only relevant segments will help in classification, the excessive temporal aggregation may lose the signal in the noise.</li>
</ul></li>
<li><p>Introduce a stochastically updated feature bank to solve memory constraints.</p>
<ul>
<li>features from densely sampled contiguous temporal segments are cached over the course of training,</li>
<li>only a random subset of these features is computed online and backpropagated through in each training iteration</li>
</ul></li>
</ul>
<h3 id="how-4">How</h3>
<p>Train with weak supervision, meaning that at training time the temporal location information for the response is not proposed.</p>
<h4 id="tqn">TQN</h4>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901152032964.png" alt="image-20210901152032964" style="zoom:50%;" /></p>
<ul>
<li>Identifies rapidly occurring discriminative events in untrimmed videos and can be trained given only weak supervision.</li>
<li>Achieves by learning a set of permutation-invariant query vectors corresponding to predefined queries about events and their attributes, which are transformed into response vectors using Transformer decoder layers attending to visual features extracted from a 3DCNN backbone.</li>
<li>Given an untrimmed video, first visual features for <em>contiguous non-overlapping clips of 8 frames</em> are extracted using a 3D ConvNet.</li>
<li>multiple layers of a parallel non-autoregressive Transformer decoder</li>
<li>The training loss is a multi-task combination of individual classifier losses, which are Softmax cross-entropy, where the labels are the ground-truth attribute for the label query.</li>
</ul>
<h4 id="stochastically-updated-feature-bank">Stochastically updated feature bank</h4>
<ul>
<li>The memory bank caches the clip-level 3DCNN visual features.</li>
<li>In each training iteration, a fixed number <span class="math inline">\(n_{online}\)</span> of randomly samples consecutive clips are forwarded through the visual encoder, while the remaining <span class="math inline">\(t-n_{online}\)</span> clip features are retrieved from the memory bank.</li>
<li>The two sets mentioned above are then combined and input into the TQN decoder for final prediction and backpropagation.</li>
<li>During inference, all features are computed online without the memory bank.</li>
<li>Advantages: fixed number of clips to reduce the memory price. Also enables to extend temporal context and promotes diversity in each mini-batch as multiple different videos can be included instead of just a single long video.</li>
</ul>
<h4 id="factorizing-categories-into-attribute-queries">Factorizing categories into attribute queries</h4>
<ul>
<li>This factorization unpacks the monolithic category labels into their semantic constituents</li>
<li>The categories are factorized into multiple queries that with several attributes respectively.</li>
</ul>
<h4 id="implementation-1">Implementation</h4>
<ul>
<li>Use S3D as visual backbone, operating on non-overlapping contiguous video clips of 8 frames.</li>
<li>The decoder consists of 4 standard post-normalization Transformer decoder layers, each with 4 attention heads.</li>
<li>The visual encoder is pre-trained on Kinetics-400.</li>
</ul>
<h2 id="paper-6-unsupervised-disentanglement-of-linear-encoded-facial-semantics">Paper 6: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></h2>
<h3 id="why-4">Why</h3>
<ul>
<li>Sampling along the linear-encoded representation vector in latent space will change the associated facial semantics accordingly.</li>
<li>Current frameworks that maps a particular facial semantics to a latent representation vector relies on training offline classifiers with manually labeled datasets. Therefore they require artificially defined semantics and provide the associated labels for all facial images. If training with labeled facial semantics:
<ul>
<li>They demand extra effort on human annotations for each new attributes proposed</li>
<li>Each semantics is defined artificially</li>
<li>unable to give any insights on the connections among different semantics</li>
</ul></li>
<li>Previous work
<ul>
<li>Synthesizing faces by GAN, which changes the target attribute but keep other information ideally unchanged.
<ul>
<li>The comprehensive design of loss functions.</li>
<li>the involvement of additional attribute features</li>
<li>the architecture design</li>
</ul></li>
<li>To achieve meaningful representations, one should always introduce either supervision or inductive biases to the disentanglement method
<ul>
<li>Inductive bias: rise from the symmetry of natural objects and the 3D graphical information.</li>
<li>Reconstruct the face images by carefully remodeling the graphics of camera principal, which makes it possible to decompose the facial images into environmental semantics and other facial semantics.</li>
<li>It's unable to generate realistic faces and perform pixel-level face editing on it.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="goal-3">Goal</h3>
<ul>
<li>Photo-realistic images synthesizing, minimize the demand for human annotations</li>
<li>Capture linear-encoded facial semantics.</li>
</ul>
<h3 id="how-5">How</h3>
<p>With a given collection of coarsely aligned faces, a GAN is trained to mimic the overall distribution of the data. Then use the faces that the trained GAN generates as training data and trains a 3D deformable face reconstruction method. A mutual reconstruction strategy stabilizes the training significantly. Then they keep a record of the latent code from StyleGAN and apply linear regression to disentangle the target semantics in the latent space.</p>
<h4 id="decorrelating-latent-code-in-stylegan">Decorrelating latent code in StyleGAN</h4>
<ul>
<li><p>Enhance the disentangled representation by decorrelating latent codes.</p>
<ul>
<li><p>In order to maximizes the utilization of all dimensions, they use Pearson correlation coefficient to zero and variance of all dimension.</p></li>
<li><p>Introduce decorrelation regularization via a loss function</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901225711236.png" alt="image-20210901225711236" /><figcaption aria-hidden="true">image-20210901225711236</figcaption>
</figure></li>
<li><p>The mapping network is the only one to update with the new loss.</p></li>
</ul></li>
</ul>
<h4 id="stabilized-training-for-3d-face-reconstruction">Stabilized training for 3D face reconstruction</h4>
<ul>
<li><p>Use the decomposed semantics to reconstruct the original input image with the reconstruction loss</p>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210901230258180.png" title="fig:" alt="image-20210901230258180" /></li>
<li>The 3D face reconstruction algorithm struggles to estimate the pose of profile or near-profile faces.</li>
<li>The algorithm tries to use extreme values to estimate the texture and shape of each face independently, which deviate far away from the actual texture and shape of the face. To solve this, the mutual reconstruction strategy is proposed to prevent the model from using extreme values to fit individual reconstruction, and the model learns to reconstruct faces with a minimum variance of the shape and texture among all samples.</li>
</ul></li>
<li><p>During training, they swap the albedo and depth map between two images with a probability <span class="math inline">\(\epsilon\)</span> to perform the reconstruction with the alternative loss.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902113314940.png" alt="image-20210902113314940" /><figcaption aria-hidden="true">image-20210902113314940</figcaption>
</figure></li>
<li><p>simply concatenate the two images channel-wise as input to the confidence network</p></li>
</ul>
<h4 id="disentangle-semantics-with-linear-regression">Disentangle semantics with linear regression</h4>
<ul>
<li>The Ultimate goal of disentangling semantics is to find a vector in StyleGAN, such that it only takes control of the target semantics.</li>
<li>Semantic gradient estimation
<ul>
<li>It's observed that with StyleGAN, many semantics can be linear-encoded. Therefore, the gradient is now independent of the input latent code.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902120453628.png" title="fig:" alt="image-20210902120453628" /></li>
</ul></li>
<li>Semantic linear regression
<ul>
<li>In real world scenario, the gradient is hard to estimate directly because back-propagation only captures local gradient, making it less robust to noises.</li>
<li>Propose a linear regression model to capture global linearity for gradient estimation.</li>
</ul></li>
</ul>
<h4 id="image-manipulation-for-data-augmentation">Image manipulation for data augmentation</h4>
<ul>
<li>One application is to perform data augmentation.</li>
<li>By extrapolating along <span class="math inline">\(\mathrm{v}\)</span> beyond its standard deviation, we can get samples with more extreme values for the associated semantics.</li>
</ul>
<h4 id="localized-representation-learning">Localized representation learning</h4>
<ul>
<li>Find the manipulation vectors <span class="math inline">\(\hat{\mathrm{v}}\)</span> that capture interpretable combinations of pixel value variations.</li>
<li>Start by defining a Jacobian matrix, which is the concatenation of all canonical pixel-level <span class="math inline">\(\mathrm{v}\)</span>.</li>
<li>interpolation along <span class="math inline">\(\hat{\mathrm{v}}\)</span> should result in significant but localized (i.e. sparse) change across the image domain.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902133618545.png" title="fig:" alt="image-20210902133618545" /></li>
</ul>
<h2 id="paper-7-unsupervised-disentanglement-of-linear-encoded-facial-semantics">Paper 7: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.16605.pdf">Unsupervised disentanglement of linear-encoded facial semantics</a></h2>
<h3 id="why-5">Why</h3>
<ul>
<li>The current success of GNNs is attributed to an implicit assumption that the input of GNNs contains the entire attributed graph, which will collapse or the accuracy will decrease if the entire graph is too large.
<ul>
<li>One intuitive solution for the problem is sampling: neighbor sampling or graph sampling. The graph sampling will sample subgraphs and can avoid neighbor explosion. But not like neighbor sampling, it cannot guarantee that each node is sampled.
<ul>
<li>Neighbor sampling: GraphSAGE, VRGCN</li>
<li>Sampling subgraphs: Fast-GCN, ClusterGCN, DropEdge, DropConnection, GraphSAINT for edge sampling.</li>
</ul></li>
<li>Another solution is compressing the size of input graph data the the GNN model: such as pruning, shallow networks, designing compact layers and quantizing the parameters.</li>
</ul></li>
<li>The challenges of compressed GNN
<ul>
<li>The compression of the loaded data demands more attention</li>
<li>The original GNN is shallow and therefore the compression will be more difficult to be achieved.</li>
<li>Require the compressed GNNs to possess sufficient parameters for representations.</li>
</ul></li>
</ul>
<h3 id="goal-4">Goal</h3>
<ul>
<li>reduce the redundancies in the node representations while maintain the principle information.</li>
</ul>
<h3 id="how-6">How</h3>
<ul>
<li>Binarizes both the network parameters and input node features. The original matrices multiplications are revised to binary operations for accelerations. Design a new gradient approximation based back-propagation to train the proposed Bi-GCN.</li>
</ul>
<h4 id="gcn">GCN</h4>
<ul>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151200499.png" alt="image-20210902151200499" /><figcaption aria-hidden="true">image-20210902151200499</figcaption>
</figure>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902151226323.png" alt="image-20210902151226323" /><figcaption aria-hidden="true">image-20210902151226323</figcaption>
</figure></li>
<li><p>Use task-dependent loss function, e.g. the cross-entropy.</p></li>
</ul>
<h4 id="bi-gcn">Bi-GCN</h4>
<p>Only focus on binarizing the feature extraction step, because the aggregation step possesses no learnable parameters and it only requires a few calculations. To reduce the computational complexities and accelerates the inference process, the XNOR and bit count operations are utilized.</p>
<h5 id="binarization-of-the-feature-extraction-step">Binarization of the feature extraction step</h5>
<ul>
<li><p>Binarization of the parameters</p>
<p>Each column if the parameter matrix is splitted as a bucket, and <span class="math inline">\(\alpha\)</span> maintain the scalars for each bucket.</p></li>
<li><p>Binarization of the node features</p>
<p>Processed by the graph convolutional layers.</p>
<ul>
<li>Split the hidden state at layer <span class="math inline">\(l\)</span> into row buckets based on the constraints of the matrix multiplication.</li>
<li>Let <span class="math inline">\(F^{(l)}\)</span> be the binarized buckets, the binary approximation of <span class="math inline">\(H^{(l)}\)</span> can be obtained via <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902160626221.png" alt="image-20210902160626221" style="zoom:67%;" /></li>
<li><span class="math inline">\(\beta\)</span> can be considered as the node-weights for the features representations. Each element of <span class="math inline">\(F^{(l)}\)</span> and <span class="math inline">\(B^{(l)}\)</span> is either -1 or 1.</li>
<li>This Binarization also possesses the ability of activation, therefore the activation operations can be eliminated.</li>
</ul></li>
</ul>
<h5 id="binary-gradient-approximation-based-back-propagation">Binary gradient approximation based back propagation</h5>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210902164243961.png" alt="image-20210902164243961" /><figcaption aria-hidden="true">image-20210902164243961</figcaption>
</figure>
<h2 id="paper-8-an-attractor-guided-neural-networks-for-skeleton-based-human-motion-prediction">Paper 8: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.09711.pdf">An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</a></h2>
<h3 id="why-6">Why</h3>
<ul>
<li>Most existing methods tend to build the relations among joints, where local interactions between joint pairs are well learned. However, the global coordination of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously.</li>
<li>Most graphs are designed according to the kinematic structure of the human to extract motion features, but hardly do they learn the relations between spatial separated joint pairs directly.</li>
<li>Except for speed, other dynamic information like accelerated speed are not counted into previous work, which ignores important motion information.</li>
<li>Previous work
<ul>
<li>Human motion prediction
<ul>
<li>Many works suffer from discontinuities between the observed poses and the predicted future ones.</li>
<li>Consider global spatial and temporal features simultaneously, such as transform temporal space to trajectory space to take the global temporal information into account.</li>
</ul></li>
<li>Joint relation modeling
<ul>
<li>Focus on skeletal constraints to model correlation between joints.</li>
<li>adaptive graph: the existed works weak the global coordination of all joints since they are learned from parts.</li>
</ul></li>
<li>Dynamic representation of skeleton sequence
<ul>
<li>Many attempts proposed to extract enriching dynamic representation from raw data, but they only extract the dynamics from neighbor frames</li>
<li>Extract the dynamic features among frames through multiple timescale will extract more motion features.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="goal-5">Goal</h3>
<p>To characterize the global motion features and thus can learn both the local and global motion features simultaneously.</p>
<p>Generate predicted poses through proposed framework AGN and the historical 3D skeleton-based poses.</p>
<h3 id="how-7">How</h3>
<ul>
<li>Pipeline: A BA (balance attractor) is learned by calculating dynamic weighted aggregation of single joint feature. Then the difference between the BA and each joint feature is calculated. Later the resulting new joint features are used to calculate joints similarities to generate final joint relations.</li>
<li>Framework: Attractor-Guided Neural Network, which first learn an enriching dynamic representation from raw position information adaptively through MTDE (multi-timescale dynamics extractor). Then the AJRE (attractor-based joint relation extractor) is imported , including a LIE (local interaction extractor), a GCE (global coordination extractor) and an adaptive feature fusing module.
<ul>
<li>AJRE: a joint relation modeling = GCE+LIE. The GCE models the global coordination of all joints, while LIE mines the local interactions between joint pairs.</li>
<li>MTDE: extract enriching dynamic information from raw input data for effective prediction.</li>
</ul></li>
</ul>
<h4 id="mtde">MTDE</h4>
<ul>
<li><p>A combination of different time scales motion dynamics</p></li>
<li><p>Two stream, one path is the raw input poses, the other is the difference between adjacent frames in raw input. The dynamics of each joint separately is also modeled to avoid the interference of other joints.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903184316148.png" alt="image-20210903184316148" /><figcaption aria-hidden="true">image-20210903184316148</figcaption>
</figure></li>
<li><p>The MTDE uses three 1DCNN but in different kernel size (5，3，1) to extract the local (joint-level) dynamics.</p></li>
</ul>
<h4 id="ajre">AJRE</h4>
<ul>
<li>Consists of GCE and LIE to separately model global coordination of all joints and local interactions between joint pairs, and also AFFM which is used to fuse features according to channel-wise attention to improve the flexibility of joint relation modeling.</li>
<li>GCE and LIE work in parallel, and they are followed by AFFM.</li>
</ul>
<h5 id="gce">GCE</h5>
<p>Global coordination of all joints, so they learn a medium to build new joint relations indirectly.</p>
<ul>
<li>BA (balance attractor unit) unit calculates all joints' aggregation to characterize the global motion features. After transpose the input features, then BA unit applies <span class="math inline">\(1\times 1\)</span> conv to get a dynamic weighted feature aggregation of N joints features. And <span class="math inline">\(X_{new}\)</span> is the difference between the output features of BA and the original <span class="math inline">\(X\)</span>.</li>
<li>The new relations of all joints is built by the Cosine similarity unit, which measure between <span class="math inline">\(X_{new},X\)</span>. The cosine similarity between all row vector pairs to illustrate the correlation between joint pairs. The correlation matrix on each channel is calculated since each channel encodes specific spatiotemporal features and should focus on different correlations compared with other channels.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232718696.png" alt="image-20210903232718696" /><figcaption aria-hidden="true">image-20210903232718696</figcaption>
</figure>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232735446.png" alt="image-20210903232735446" /><figcaption aria-hidden="true">image-20210903232735446</figcaption>
</figure>
<h5 id="lie">LIE</h5>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903232837194.png" alt="image-20210903232837194" /><figcaption aria-hidden="true">image-20210903232837194</figcaption>
</figure>
<ul>
<li>It's used to learn local interactions between joint pairs, including adjacent and distant joints.</li>
<li>To learn the relations between adjacent joint pairs, a pure <span class="math inline">\(3\times 3\)</span> convolution is adopted. To learn the relations between distant joint pairs, the self-attention is used.</li>
</ul>
<h5 id="affm">AFFM</h5>
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210903234025866.png" title="fig:" alt="image-20210903234025866" /></li>
<li>Channel attention to fuse features adaptively and reform more reliable representation.</li>
<li>After the sigmoid in the AFFM unit, the importance ratio of each channel is obtained. Then the channel-wise multiplication between ratio and raw input is done to reform features.</li>
</ul>
<h2 id="paper-9-cascade-graph-neural-networks-for-rgb-d-salient-object-detection">Paper 9: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.03087">Cascade Graph Neural Networks for RGB-D Salient Object Detection</a></h2>
<p>Codes: https://github.com/LA30/Cas-Gnn</p>
<h3 id="why-7">Why</h3>
<ul>
<li>How to leverage the two complementary data sources: color and depth information</li>
<li>Current works either simply distill prior knowledge from the corresponding depth map for handling the RGB-image or blindly fuse color and geometric information to generate the coarse depth-aware representations, hindering the performance of RGB-D saliency detectors</li>
<li>Identify saliency objects of varying shape and appearance, show robustness towards heavy occlusion, various illumination and background.</li>
<li>Network cascade is an effective scheme for a variety of high-level vision applications. It will ensemble a set of models to handle challenging tasks in a coarse-to-fine or easy-to-hard manner.</li>
</ul>
<h3 id="goal-6">Goal</h3>
<p>Salient object detection for RGB-D images. To distill and reason the mutual benefits between the color and depth data sources through a set of cascade graphs.</p>
<p>Predict a saliency Map given an input image and its corresponding depth image.</p>
<h3 id="how-8">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210904162325031.png" alt="image-20210904162325031" /><figcaption aria-hidden="true">image-20210904162325031</figcaption>
</figure>
<ul>
<li>CGR (cascade graph reasoning ) module to learn dense features, from which the saliency map can be easily inferred. It explicitly reasons about the 2D appearance and 3D geometry information for RGBD SOD.</li>
<li>Each graph consists of two types of nodes, geometry nodes storing depth features and appearance nodes storing RGB-related features.</li>
<li>Multiple-level graphs sequentially chained by coarsening the preceding graph into two domain-specific guidance nodes for the following cascade graph.</li>
</ul>
<h4 id="cross-modality-reasoning-with-gnns">Cross-modality reasoning with GNNs</h4>
<ul>
<li>Build a directed graph, where the edges connect i) the nodes from the same modality but different scales and ii) the nodes of the same scale from different modalities.</li>
<li>The backbone is VGG-16 plus dilated network technique, which will extract 2D appearance representations and 3D geometry representations. They also propose a graph-based reasoning (GR) module to reason about the cross-modality, high-order relations between them.</li>
<li>GRU module
<ul>
<li>Input 2D features and 3D features.</li>
<li>gated recurrent unit for node state updating</li>
</ul></li>
</ul>
<h4 id="cascade-graph-neural-networks">Cascade Graph Neural networks</h4>
<ul>
<li><p>To overcome the drawbacks of independent multilevel (graph-based) reasoning, propose cascade GNNs.</p></li>
<li><p>coarsening the preceding graph into two domain-specific guidance nodes for the following cascade graph to perform the joint reasoning</p></li>
<li><p>The guidance nodes only deliver the guidance information, and will stay fixed during the message passing process.</p></li>
<li><p>The guidance node is built by firstly concatenation and then the fusion via <span class="math inline">\(3\times 3\)</span> convolution layer.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905202700652.png" alt="image-20210905202700652" /><figcaption aria-hidden="true">image-20210905202700652</figcaption>
</figure></li>
<li><p>Each guidance node propagates the guidance information to other nodes of the same domain in the graph through the attention mechanism.</p></li>
<li><p>Multi-level feature fusion: The merge function is either element-wise addition or channel-wise concatenation.</p></li>
</ul>
<h2 id="paper-10-coarse-fine-networks-for-temporal-activity-detection-in-videos">Paper 10: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01302">Coarse-Fine Networks for Temporal Activity Detection in Videos</a></h2>
<p>Code: https://github.com/kkahatapitiya/Coarse-Fine-Networks</p>
<h3 id="why-8">Why</h3>
<ul>
<li>One main challenge for video representation learning is capturing long-term motion from a continuous video.</li>
<li>Use of frame striding or temporal pooling has been a successful strategy to cover a larger time interval without increasing the number of parameters</li>
<li>Previous work
<ul>
<li>Action localization: temporal action localization task, which tends to annotate every frame with multiple ongoing activities. Use of sequential models such as LSTMs have been popular.</li>
<li>Dynamic sampling: selective processing of information, like spatially, temporally or spatio-temporally sampling.</li>
</ul></li>
<li>Two challenges of the network
<ul>
<li>how to abstract the information at a lower temporal resolution meaningfully, and</li>
<li>how to utilize the fine-grained context information effectively.</li>
</ul></li>
</ul>
<h3 id="goal-7">Goal</h3>
<p>learn better video representations for long-term motion, works in multiple temporal resolutions of the input and selects frames dynamically.</p>
<h3 id="how-9">How</h3>
<ul>
<li>Grid Pool, a learned temporal downsampling layer to extract coarse features, which adaptively samples the most informative frame locations with a differentiable process.</li>
<li>Multi-stage Fusion, a spatio-temporal attention mechanism to fuse a fine-grained context with the coarse features.</li>
</ul>
<h4 id="grid-pool">Grid pool</h4>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210905223900318.png" alt="image-20210905223900318" /><figcaption aria-hidden="true">image-20210905223900318</figcaption>
</figure>
<ul>
<li>Samples by interpolating on a non-uniform grid with learnable grid locations. The intuition comes from that sampling frames at a higher frame rate where the confidence is high and at a lower frame rate where it is low. The stride between the interpolated frame locations should be small and vice-versa.</li>
<li>The confidence value is modeled as a function of the input representation.</li>
<li>To get a set of <span class="math inline">\(\alpha T\)</span> (an integer) grid locations based on confidence values, the CDF is considered.</li>
<li>When a grid location is non-integer, the corresponding sampled frame is a temporal interpolation between the adjacent frames.</li>
<li>A grid unpool operation is coupled with the grid locations learned by the Grid pool layer, which simply performs the inverse operation of the Grid pool. In this way, one will resamples with a low frame-rate in the regions where one used a high frame-rate in Grid pool, and vice-versa.</li>
</ul>
<h4 id="multi-stage-fusion">Multi-stage fusion</h4>
<ul>
<li>Fuse the context from the fine stream and the coarse stream. Aims: filter out what fine-grained information should be passed down to the coarse stream, have a calibration step to align the coarse features and fine features, learn and benefit from multiple abstraction-levels of fine-grained context at each fuse-location in the coarse stream</li>
<li>filtering fine-grained information: self-attention mask by processing the fine feature through a lightweight head consists of point-wise convolutional layers followed by a sigmoid non-linearity.</li>
<li>Fine to coarse correspondence: use a set of temporal Gaussian distributions centered at each coarse frame location which abstract a location dependent weighted average of the fine feature.</li>
<li>Multiple abstraction-levels: allow each fusion connection to look at the features from all abstraction levels by concatenating them channel-wise. The scale and shift features at each fusion location is calculated to finally fuse the features from any abstraction-levels.</li>
</ul>
<h4 id="model-details">Model details</h4>
<ul>
<li>Backbone: X3D, which follows ResNet structure but designed for efficiency in video models.</li>
<li>The coarse stream takes in segmented clips of <span class="math inline">\(T=64\)</span> frames to follow the standard X3D architecture after the Grid pool later during training, while the fine stream always process the entire input clip.</li>
<li>The main difference between the coarse and the fine stream is the Grid pool layer and the corresponding grid unpool operation.</li>
<li>The grid pool later is placed after the 1st residual block.</li>
<li>The peak magnitude of each mask is normalized to 1.</li>
<li>The standard deviation <span class="math inline">\(\sigma\)</span> is set to be <span class="math inline">\(\frac{T&#39;}{8}\)</span>, empirically.</li>
</ul>
<h2 id="paper-11-coconets-continuous-contrastive-3d-scene-representations">Paper 11: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.03851">CoCoNets: Continuous Contrastive 3D Scene Representations</a></h2>
<p>https://mihirp1998.github.io/project_pages/coconets/</p>
<h3 id="why-9">Why</h3>
<ul>
<li>Combine 3D voxel grids and implicit functions and learn to predict 3D scene and object 3D occupancy from a single view with unlimited spatial resolution</li>
</ul>
<h3 id="goal-8">Goal</h3>
<p>SSL learning of amodal 3D feature representations from RGB and RGBD posed images and videos, and finally generate the representations to help object detection, object tracks or visual correspondence.</p>
<p>Trained for contrastive view prediction by rendering 3D feature clouds in queried viewpoints and matching against the 3D feature point cloud predicted frim the query view.</p>
<p>Finally, the model forms plausible 3D completions of the scene given a single RGB-D image as input.</p>
<h3 id="how-10">How</h3>
<ul>
<li>3d feature grids as a 3D-informed neural bottleneck for contrastive view prediction, and implicit functions for handling the resolution limitations of 3D grids.</li>
<li>Propose CoCoNets (continuous contrastive 3D networks) that learns to map RGB-D images to infinite-resolution 3D scene representations by contrastively predicting views. Specifically, the model is trained to lift 2.5D images to 3D feature function grids of the scene by optimizing for view-contrastive prediction</li>
<li>There are two branches in CoCoNet, one is to encode RGB-D images into a 3D feature map, and the other is to encode the RGB-D of the target viewpoint into a 3D feature cloud.</li>
<li>Two branches, one adopts top-down idea which encodes the input RGB-D image and orienting the feature map to the target viewpoint, and predict the features for the target 3D points in target domain by querying, and later output a feature cloud for the target domain. The other branch is the bottom-up one, which simply encodes the target RGB-D image and predict the features for the target 3D position in target domain, obtaining the feature cloud for target domains</li>
<li>The positive samples are from the target domain that works in bottom-up branch.</li>
</ul>
<h4 id="result">Result</h4>
<ul>
<li>The scene representations learnt by CoCoNets can detect objects in 3D across large frame gaps</li>
<li>Using the learnt 3D point features as initialization boosts the performance of the SOTA Deep Hough Voting detector.</li>
<li>The learnt 3D feature representations can infer 6DoF alignment between the same object in different viewpoints, and across different objects of the same category.</li>
<li>optimize a contrastive view prediction objective but uses a 3D girder of implicit functions as its latent bottleneck.</li>
</ul>
<h2 id="paper-12-cutpaste-self-supervised-learning-for-anomaly-detection-and-localization">Paper 12: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.04015">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</a></h2>
<h3 id="why-10">Why</h3>
<ul>
<li>Difficult to obtain a large amount of anomalous data, and the difference between normal and anomalous patterns are often fine-grained.</li>
<li>The anomaly score defined as an aggregation of pixel-wise reconstruction error or probability densities lacks to capture a high-level semantic information.</li>
<li>deep one-class classifier outperforms, but most existing work focus on detecting semantic outliers, which cannot generalize well in detecting fine-grained anomalous patterns as in defet detection.</li>
<li>Naively applying existed methods such as rotation prediction or contrastive learning, is sub-optimal for detecting local defects.</li>
<li>Rotation and translation ect. lacks of irregularity.</li>
</ul>
<h3 id="goal-9">Goal</h3>
<ul>
<li>detects unknown anomalous patterns of an image without anomalous data</li>
<li>Design a pretext task that can identify local irregularity.</li>
<li>The pretext task is also amenable to combine with existing methods, such as transfer learning from pretrained models for better performance or patch-based models for more accurate localization</li>
</ul>
<h3 id="how-11">How</h3>
<ul>
<li>Learn representations by classifying normal data from the CutPaste (data augmentation strategy that cuts an image patch at a random location of a large image). First learn SS deep representations and then build a generative one-class classifier.</li>
<li>transfer learning on pretrained representations on ImageNet.</li>
<li>designing a novel proxy classification task between normal training data and the ones augmented by the CutPaste. The CutPaste motivated to produce a spatial irregularity to serve as a coarse approximation of real defects.</li>
<li>A two-stage framework to build an anomaly detector, where in the first stage they learn deep representations from normal data and then construct an one-class classifier using learned representations.</li>
</ul>
<h4 id="ssl-with-cutpaste">SSL with CutPaste</h4>
<ul>
<li>CutPaste augmentation as follows:
<ul>
<li>Cut a small rectangular area of variable sizes and aspect ratios from a normal training image.</li>
<li>Optionally, we rotate or jitter pixel values in the patch.</li>
<li>Paste a patch back to an image at a random location</li>
</ul></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906193605112.png" title="fig:" alt="image-20210906193605112" /></li>
<li>In practice , data augmentation or color jitter, are applied before feeding x into g or CP.</li>
</ul>
<h4 id="cutpaste-variants">CutPaste variants</h4>
<ul>
<li>CutPaste scar: a long-thin rectangular box filled with an image patch</li>
<li>Multi-class classification: formulate a finer-grained 3-way classification task among normal, CutPaste and CutPaste-Scar by treating CutPaste variants as two separate classes.</li>
<li>Similarity between CutPaste and real defects: outliers exposure. CutPaste creates examples preserving more local structures of the normal examples, while is more challenging for the model to learn to find this irregularity.</li>
<li>CutPaste does look similar to some real defects.</li>
</ul>
<h4 id="computing-anomaly-score">Computing anomaly score</h4>
<ul>
<li><p>A simple Gaussian density estimator whose log-density is computed as follows</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194329698.png" alt="image-20210906194329698" style="zoom:50%;" /></p></li>
<li></li>
</ul>
<h4 id="localization-with-patch-representation">Localization with patch representation</h4>
<ul>
<li><p>CutPaste prediction is readily applicable to learn a patch representation – all we need to do at training is to crop a patch before applying CutPaste augmentation.</p>
<p><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210906194519932.png" alt="image-20210906194519932" style="zoom:50%;" /></p></li>
</ul>
<h2 id="paper-13-discriminative-latent-semantic-graph-for-video-captioning">Paper 13: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.03662">Discriminative Latent Semantic Graph for Video Captioning</a></h2>
<p>https://github.com/baiyang4/D-LSG-Video-Caption</p>
<h3 id="why-11">Why</h3>
<ul>
<li>Key challenge of the video captioning task: no explicit mapping between video frames and captions, and the output sentence should be natural</li>
<li>GNNs show particular advantages in modeling relationships between objects, but they don't jointly consider the frame-based spatial-temporal contexts in the entire video sequence</li>
<li>discriminative modeling for caption generation suffers from stability issues and requires pre-trained generators.</li>
<li>Traditional GNNs for video captioning cannot take adequate information into consideration, while the work of this paper (conditional graph) jointly consider objects, contexts and motion information at both region and frame levels.</li>
</ul>
<h3 id="goal-10">Goal</h3>
<ul>
<li>Video captioning</li>
<li>Encode-decoder frameworks cannot explicitly explore the object-level interactions and frame-level information from complex spatio-temporal data to generate semantic-rich captions.</li>
<li>Contributions on three key sub-tasks in video captioning
<ul>
<li>Enhanced object proposal: propose a novel conditional graph that can fuse spatio-temporally information into latent object proposal.</li>
<li>visual knowledge: latent proposal aggregation to dynamically extract visual words</li>
<li>sentence validation: a novel discriminative language validator</li>
</ul></li>
<li>Propose D-LSG, where the graph model for feature fusion from multiple base models, the latent semantic refers to the higher-level semantic knowledge that can be extracted from the enhanced object proposals. The discriminative module is designed as a plug-in language validator, which uses the Multimodal Low-rank Bi-linear (MLB) pooling as metrics.</li>
</ul>
<h3 id="how-12">How</h3>
<ul>
<li>a semantic relevance discriminative graph based on Wasserstein gradient penalty.</li>
<li>Modeled as a sequence to sequence process.</li>
</ul>
<h4 id="architecture-design">Architecture Design</h4>
<ul>
<li>Multiple feature extraction: use 2D CNNs for appearance features and 3D CNNs for motion features. Then these two features are concatenated and apply LSTM on them.</li>
<li>Enhanced object proposal: enhanced by their visual contexts of appearance and motion respectively, which result in enhanced appearance proposals and enhanced motion proposal, together these two form the enhanced object proposals.</li>
<li>Visual knowledge: latent semantic proposals as <span class="math inline">\(K\)</span> dynamic visual words, after introducing the dynamic graph built by LPA to summarize the enhanced appearance and motion features.</li>
<li>Language decoder: language generation decoder will take the visual knowledge extracted by the LPA to generate captions. it consists of an attention LSTM for weighting dynamic visual words and a language LSTM for caption generation.</li>
</ul>
<h4 id="latent-semantic-graph">Latent Semantic graph</h4>
<ul>
<li><p>conditional graph operation : model the complex object-level interactions and relationships, and learn informative object-level features that are in context of frame-based background information.</p>
<ul>
<li>To build the graph, each region feature is regarded as a node. During message passing, the enhanced appearance proposal and object-level region features are handled with a kernel function to encode relations between them. The kernel is defined by linear functions followed by Tanh activation function.</li>
</ul>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908111357002.png" alt="image-20210908111357002" /><figcaption aria-hidden="true">image-20210908111357002</figcaption>
</figure></li>
<li><p>LPA</p>
<ul>
<li>to further summarize the enhanced object proposals</li>
<li>propose a latent proposal aggregation method to generate visual words dynamically based on the enhanced features.</li>
<li>Introduce a set of object visual words, which means potential object candidates in the given video, and then they summarize the enhanced proposals into informative dynamic visual words.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908112654724.png" title="fig:" alt="image-20210908112654724" /></li>
</ul></li>
</ul>
<h4 id="discriminative-language-validation">Discriminative language validation</h4>
<ul>
<li>The module is designed to as a language validation process that encourages the generated captions to contain more informative Semantic concepts via reconstructing the visual words or knowledge based on the input sentences under the condition of corresponding true visual words encoded by LSG.</li>
<li>Use WGAN-GP</li>
<li>Extract sentence features from given captions by 1DCNN,</li>
<li>The output of the discriminative model is weighted since sentences have different proportions of object and motion concepts
<ul>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120140074.png" title="fig:" alt="image-20210908120140074" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210908120117673.png" title="fig:" alt="image-20210908120117673" /></li>
</ul></li>
</ul>
<h2 id="paper-14-enhancing-self-supervised-video-representation-learning-via-multi-level-feature-optimization">Paper 14: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.02183?context=cs">Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</a></h2>
<p>https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization</p>
<h3 id="why-12">Why</h3>
<ul>
<li>most recent works have mainly focused on high-level semantics and neglected lower-level representations and their temporal relationship</li>
<li>The requirement of developing unsupervised video representation learning without resorting to manual labeling</li>
<li>Drawbacks
<ul>
<li>previous works only explore either instance-wise or semantic-wise distribution, lacking a comprehensive perspective over both sides.</li>
<li>less effort has been placed on low-level features than high-level representations, while the former is proven critical for knowledge transfer</li>
<li>Third, directly performing temporal augmentations, e.g., shuffle and reverse, at input level instead of feature level could impair feature learning</li>
</ul></li>
<li>High-level features are more representative towards instances or semantics but less feasible towards cross-task transfer, while low-level features are transfer-friendly but lack structural information over samples.</li>
<li>a line of works expanded contrastive learning pipeline to video domain
<ul>
<li>InfoNCE loss for dense future prediction</li>
<li>the temporal information in videos is not well leveraged</li>
<li>require a simple yet effective operation to apply temporal augmentations on extracted multi-level features</li>
</ul></li>
<li></li>
</ul>
<h3 id="goal-11">Goal</h3>
<ul>
<li>proposes a multi-level feature optimization framework to improve the generalization and temporal modeling ability of learned video representations</li>
<li>avoids forcing the backbone model to adapt to unnatural sequences which corrupts spatiotemporal statistics.</li>
<li>Jointly consider the instance and semantic-wise similarity distribution to form a reliable SS signal.</li>
</ul>
<h3 id="how-13">How</h3>
<ul>
<li>high-level features obtained from naive and prototypical contrastive learning are utilized to build distribution graphs</li>
<li>devise a simple temporal modeling module from multi-level view features to enhance motion pattern learning.</li>
<li>For low-level representation, apply temporal augmentation on multi-level features to construct contrastive pairs that have different motion patterns with the objective designed to distinguish the augmented samples and original ones. And one retrieval task is proposed to match the features in short and long time spans based on their semantic consistency.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910122543720.png" title="fig:" alt="image-20210910122543720" /></li>
</ul>
<h4 id="beyond-instance-discrimination">Beyond instance discrimination</h4>
<ul>
<li>The one-hot labels in InfoNCE loss neglect the relationship between different samples. But there exist some negative samples that may share similar characteristics.</li>
<li>besides instance-wise discrimination, we explicitly develop another branch on the projected high-level feature vectors for inter-sample relationship modeling.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130142797.png" title="fig:" alt="image-20210910130142797" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130231035.png" title="fig:" alt="image-20210910130231035" /></li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910130246505.png" title="fig:" alt="image-20210910130246505" /></li>
<li>design a queue to store the semantic-wise distributions from previous batches to ensure equal partition into K prototypes, but using only those from the current batch for gradient back-propagation</li>
<li>Finally, we jointly leverage <span class="math inline">\(\mathcal{L}_{ins}\)</span> and <span class="math inline">\(\mathcal{L}_{sem}\)</span> to form the self-supervisory objective for high-level representations:</li>
</ul>
<h4 id="graph-constraint-for-multi-level-features">Graph constraint for multi-level features</h4>
<ul>
<li>It is the lower-level features that mainly transfer from the pretrained network to downstream tasks. One can infer instance- and semantic-wise distribution from high-level features.</li>
<li>Denote the instance-wise similarity distribution as a <strong><em>directed</em></strong> graph <span class="math inline">\(\mathcal{G}_{ins}\)</span>, and semantic-wise distribution as an <strong><em>undirected</em></strong> graph <span class="math inline">\(\mathcal{G}_{sem}\)</span>. Each graph contains N nodes representing N different samples within a batch, and edges indicating the relationship between each sample.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131617208.png" title="fig:" alt="image-20210910131617208" /></li>
<li><span class="math inline">\(\mathcal{E}_{ins}\)</span> indicates the inferred instance-wise similarity distribution, which respects inter-sample relationship and is more realistic data distribution than one-hot encoding. <span class="math inline">\(\mathcal{E}_{sem}\)</span> to truncate the edges between nodes of different pseudo categories.</li>
<li>Jointly leverage <span class="math inline">\(\mathcal{G}_{ins}\)</span> and <span class="math inline">\(\mathcal{G}_{sem}\)</span> to form the combined graph <span class="math inline">\(\mathcal{G}\)</span>, whose edge weights serve as the final soft targets: <img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910131811218.png" alt="image-20210910131811218" style="zoom:53%;" /></li>
<li>The cross-entropy between <span class="math inline">\(\mathcal{E}\)</span> and inferred similarity distribution to optimize lower-level features.</li>
</ul>
<h4 id="temporal-modeling">Temporal modeling</h4>
<ul>
<li><p>Use the temporal information at diverse time scales to enhance motion pattern modeling since the features at different layers possess different temporal characteristic.</p></li>
<li><p>A robust temporal model requires two aspects: semantic discrimination between different motion patters; semantic consistency under different temporal views.==&gt; Two learning objectives</p></li>
<li><p>perform temporal augmentation on multi-level features <span class="math inline">\(\mathrm{f}_r\)</span>, and then leverage a lightweight motion excitation module to extract motion enhanced feature representations</p></li>
<li><p>Temporal transformations that result in semantically inconsistent motion patterns can be regarded as a negative pair of the original sample</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910132656530.png" alt="image-20210910132656530" /><figcaption aria-hidden="true">image-20210910132656530</figcaption>
</figure></li>
<li><p>To boost the consistency, they propose to match feature of a specific timestamp from sequences of different lengths. For one short sequence <span class="math inline">\(v_s\)</span> that is contained in a long sequence <span class="math inline">\(v_l\)</span>, they retrieve the feature at each timestep of <span class="math inline">\(v_s\)</span> in the feature set of <span class="math inline">\(v_l\)</span>. The feature of corresponding timestamp in <span class="math inline">\(v_l\)</span> serves as the positive key, while others serve as negatives.</p></li>
</ul>
<h2 id="paper-15-exploring-simple-siamese-representation-learning">Paper 15: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.10566">Exploring simple siamese representation learning</a></h2>
<h3 id="why-13">Why</h3>
<ul>
<li>Siamese networks are natural tools for comparing (including but not limited to “contrasting”) entities</li>
<li>Recent methods define the inputs as two augmentations of one image, and maximize the similarity subject to different conditions</li>
<li>An undesired trivial solution to Siamese networks is all outputs “collapsing” to a constant.
<ul>
<li>Methods like Contrastive learning, e.g., SimCLR etc. work to fix this.</li>
<li>Clustering is another way of avoiding constant output. While these methods do not define negative exemplars, this cluster centers can play as negative prototypes.</li>
<li>BYOL relies only on positive pairs but it does not collapse in case a momentum encoder is used. The momentum encoder is important for BYOL to avoid collapsing, and it reports failure results if removing the momentum encoder</li>
</ul></li>
<li>the weight-sharing Siamese networks can model invariance w.r.t. more complicated transformations</li>
</ul>
<h3 id="goal-12">Goal</h3>
<ul>
<li>report that simple Siamese networks can work surprisingly well with none of the above strategies (contrastive learning, clustering or BYOL) for preventing collapsing</li>
<li>our method ( SimSiam) can be thought of as “<em>BYOL without the momentum encoder”</em>. Directly shares the weights between the two branches, so it can also be thought of as “SimCLR without negative pairs”, and “SwAV without online clustering”.</li>
<li>SimSiam is related to each method by removing one of its core components.</li>
<li>The importance of stop-gradient suggests that <em>there should be a different underlying optimization problem that is being solved</em>.</li>
</ul>
<h3 id="how-14">How</h3>
<ul>
<li><p>The proposed architecture takes as input two randomly augmented views <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> from an image <span class="math inline">\(x\)</span>. The two views are processed by an encoder network <span class="math inline">\(f\)</span> consisting of a backbone and a project MLP head. A predict MLP head is denoted as <span class="math inline">\(h\)</span>.</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160437825.png" alt="image-20210910160437825" /><figcaption aria-hidden="true">image-20210910160437825</figcaption>
</figure></li>
<li><p>The symmetrized loss is denoted as <span class="math inline">\(\mathcal{L}=\frac{1}{2}\mathcal{D}(p_1,p_2)+\frac{1}{2}\mathcal{D}(p_2,z_1)\)</span>. Its minimum possible value is −1.</p></li>
<li><figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210910160719249.png" alt="image-20210910160719249" /><figcaption aria-hidden="true">image-20210910160719249</figcaption>
</figure></li>
<li><p>the encoder on <span class="math inline">\(x_2\)</span> receives no gradient from <span class="math inline">\(z_2\)</span>, but gradients from <span class="math inline">\(p_2\)</span>.</p></li>
<li><p>Use SGD as optimizer, with a base <span class="math inline">\(lr=0.05\)</span>, the learning rate is <span class="math inline">\(lr\times \mathrm{BatchSize}/256\)</span>.</p></li>
<li><p>Use ResNet50 as the default backbone.</p></li>
<li><p>Unsupervised pretraining on the 1000-class ImageNet training set without using labels.</p></li>
</ul>
<h2 id="paper-16-git-graph-interactive-transformer-for-vehicle-re-identification">Paper 16: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.05475">GiT: Graph Interactive Transformer for Vehicle Re-identification</a></h2>
<h3 id="why-14">Why</h3>
<ul>
<li>Vehicle re-identification aiming to retrieve a target vehicle from non-overlapping cameras. But there are challenges
<ul>
<li>vehicle images of different identifications usually have similar global appearances and subtle differences in local regions</li>
</ul></li>
<li>The technologies of vehicle re-identification
<ul>
<li>Early methods: pure CNNs, fail to catch local information</li>
<li>Based on CNNs, cooperate part divisions (uniform spatial division suffer from partition misalignment, part detection requires a high cost of extra manual part annotations) to learn global features and local features.</li>
<li>CNNs cooperate GNNs to learn global and local features: the CNN's downsampling and convolution operations reduce the resolution of feature maps, the CNN and GNN branches are supervised with two independent loss functions and lack interaction.</li>
<li>This paper: couple global and local features via transformer and local correction graph modules.</li>
</ul></li>
<li>The advantages of transformer
<ul>
<li>The transformer can use multi-head attention module to capture global context information to establish long-distance dependence on global features of vehicles.</li>
<li>The multi-head attention module of transformer does not require convolution and down-sampling operations, which retain more detailed vehicle information.</li>
</ul></li>
</ul>
<h3 id="goal-13">Goal</h3>
<p>Propose a graph interactive transformer (GiT) for vehicle-reidentification. Each GiT block employs a novel local correlation graph (LCG) module to extract discriminative local features within patches.</p>
<p>LCG Modules and transformer layers are in a coupled status.</p>
<h3 id="how-15">How</h3>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913111201445.png" alt="image-20210913111201445" /><figcaption aria-hidden="true">image-20210913111201445</figcaption>
</figure>
<ul>
<li>The transformer later and LCG module interact each other by skip connection when these two components work in sequence.</li>
</ul>
<h4 id="lcg-module">LCG module</h4>
<ul>
<li><p>To aggregate and learn discriminative local features within every patch.</p></li>
<li><p>flatten <span class="math inline">\(n\)</span> local features <span class="math inline">\(d\)</span> dimensions and map to <span class="math inline">\(d&#39;\)</span> dimensions with a trainable linear projection in every patch</p></li>
<li><p>The spatial graph's edges are constructed as <span class="math inline">\(E_{v_{i,j}}=\frac{\exp (F_{cos}(v_i,v_j))}{\sum\limits_{k=1}^{n}\exp {(F_{cos}(v_i,v_k))}}\)</span>, where <span class="math inline">\(i,j\in[1，2,...,n]\)</span>. The score of the cosine distance is denoted as <span class="math inline">\(F_{cos}=\frac{v_i,v_j}{\|v_i\|\|v_j\|}\)</span>.</p></li>
<li><p>To aggregate and update nodes, the aggregation node <span class="math inline">\(U\)</span> of <span class="math inline">\(i\)</span>-th graph is updated according as follows</p>
<ul>
<li><p><span class="math inline">\(U=(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}X_i)\cdot W\)</span>,</p></li>
<li><p>Then <span class="math inline">\(U\)</span> is processed non-linearly as</p>
<p><span class="math inline">\(O=GELU(LN(U))\)</span>, where GELU represents the gaussian error linear units and LN denotes the layer normalization.</p></li>
</ul></li>
</ul>
<h4 id="transformer-layer">Transformer layer</h4>
<ul>
<li>Model the global features between the different patches.</li>
<li>Patches are the input for multi-head attention layer</li>
<li>Later, the output from the attention layer is normalized and then processed by MLP.</li>
</ul>
<h4 id="graph-interactive-transformer">Graph interactive Transformer</h4>
<ul>
<li>Each GiT block consists of a LCG module and a Transformer layer.</li>
</ul>
<h4 id="loss-function-design">Loss function design</h4>
<ul>
<li><p>The proposed GiT's total loss function is</p>
<p><span class="math inline">\(L_{total}=\alpha L_{CE}+\beta L_{Triplet}\)</span>, where <span class="math inline">\(L_{CE}\)</span> denote cross-entropy loss, and <span class="math inline">\(L_T\)</span> denotes triplet loss.</p></li>
<li><p>The <span class="math inline">\(L_{CE}\)</span> formulates the cross-entropy of each patch's label</p></li>
<li><p>The triplet loss is</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913112252883.png" alt="image-20210913112252883" /><figcaption aria-hidden="true">image-20210913112252883</figcaption>
</figure></li>
</ul>
<h2 id="paper-17-graph-time-convolutional-neural-networks">Paper 17: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.01730">Graph-Time Convolutional Neural Networks</a></h2>
<h3 id="why-15">Why</h3>
<ul>
<li>The key for learning on multivariate temporal data is to embed spatiotemporal relations into into its inner-working mechanism.</li>
<li>Spatiotemporal graph-base models
<ul>
<li>Hybrid: combine learning algorithms developed separately for the graph domain and the temporal domain
<ul>
<li>Such as a temporal RNN, CNN</li>
<li>their spatial and temporal blocks are modular and can be implemented efficiently</li>
<li>unclear how to best interleave these blocks for learning from spatiotemporal relationships</li>
</ul></li>
<li>Fused
<ul>
<li>force the graph structure into conventional spatiotemporal solutions and provide a single strategy to jointly capture the spatiotemporal relationships.</li>
<li>substitute the parameter matrices in these models with graph convolutional filters</li>
<li>fused models capture naturally these relationships as they have graph-time dependent inner-working mechanisms.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="goal-14">Goal</h3>
<ul>
<li>Represent spatiotemporal relations through product graphs and develop a first principle graph-time convolutional neural network (GTCNN).</li>
<li>For multivariate temporal data such as sensor or social networks</li>
</ul>
<h3 id="how-16">How</h3>
<ul>
<li>Each layer consists of a graph-time convolutional module, a graphtime pooling module, and a nonlinearity.</li>
<li>The product graph itself is parametric to learn the spatiotemporal coupling</li>
<li>The zero-pad pooling preserves the spatial graph while reducing the number of active noes and parameters</li>
</ul>
<h4 id="signals-over-product-graphs">Signals over product graphs</h4>
<ul>
<li><p>Two graphs:</p>
<ul>
<li>Spatial graph <span class="math inline">\(\mathcal{G}\)</span>, consider the original sensor network, each node has a time sequence</li>
<li>Temporal graph <span class="math inline">\(\mathcal{G}_T\)</span>: for each node, there is a line graph that take each timestep as a node.</li>
</ul></li>
<li><p>Given graphs <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}_T\)</span> , we can capture the spatiotemporal relations in <span class="math inline">\(\mathrm{X}\)</span> through the product graph <span class="math inline">\(\mathcal{G}_\diamond = \mathcal{G}_T\times \mathcal{G}=(\mathcal{V}_\diamond,\mathcal{E}_\diamond)\)</span> , where the vertex set <span class="math inline">\(\mathcal{V}_\diamond = \mathcal{V}_T \times \mathcal{V}\)</span> is the Kronecker product between <span class="math inline">\(\mathcal{V}_T\)</span> and <span class="math inline">\(\mathcal{V}\)</span>.</p></li>
<li><p>Product graphs</p>
<figure>
<img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913131247274.png" alt="image-20210913131247274" /><figcaption aria-hidden="true">image-20210913131247274</figcaption>
</figure>
<ul>
<li>The Kronecher product preserves the relations between different nodes along temporal dimension. The 1st node at the 1st timestep have influences for the 2nd node at the 2nd timestep. And the influence is bidirectional.</li>
<li>The cartesian product preserves the original spatial infromation and the temporal evolution on each node's temporal dimension.</li>
</ul></li>
<li><p>Goal: to learn spatiotemporal representations in a form akin to temporal or graph CNNs.</p></li>
</ul>
<h4 id="graph-time-cnns">Graph-time CNNs</h4>
<ul>
<li>A compositional architecture of <span class="math inline">\(L\)</span> layers each having a graph-time convolutional module, a graph-time pooling module and a nonlinearity.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913133851127.png" title="fig:" alt="image-20210913133851127" /></li>
<li>at convolutions allow for effective parameter sharing, inductive learning, and efficient implementation, while zero-pad pooling and pointwise nonlinearities make the architecture independent from graph-reduction techniques or other modules.</li>
<li>Graph-time convolutional filtering
<ul>
<li>The graph-time convolutional filter aggregates at the space-time location <span class="math inline">\((i, t)\)</span> information from space-time neighbors that are up to <span class="math inline">\(K\)</span> hops away over the product graph <span class="math inline">\(\mathcal{G}_\diamond\)</span>.</li>
<li><img src="https://miao-picbed-1305768714.cos.ap-shanghai.myqcloud.com/img/image-20210913135843608.png" title="fig:" alt="image-20210913135843608" /></li>
<li>Implement it recursively, and expand all polynomials of order <span class="math inline">\(k\)</span>. Then the computational cost is liner in the product graph dimensions.</li>
</ul></li>
<li>Graph-time pooling: The pooling approach has three steps: i) summarization; ii) slicing; iii) downsamplin
<ul>
<li>Summarization: up to <span class="math inline">\(\alpha_l\)</span> hops away for each node . Use mean or max function.
<ul>
<li>Summarization is an implicit low-pass operation and the type of product graph has an impact on its severit</li>
</ul></li>
<li>Slicing: reduces the dimensionality across the temporal dimension.</li>
<li>Downsampling: reduces the number of active nodes across the spatial dimension from <span class="math inline">\(N_{l-1}\)</span> to <span class="math inline">\(N_l\)</span> without modifying the underlying spatial graph.</li>
</ul></li>
</ul>
<h2 id="paper-18-graphzoom-a-multi-level-spectral-approach-for-accurate-and-scalable-graph-embedding">Paper 18: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02370">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</a></h2>
<h2 id="paper-19-group-contrastive-self-supervised-learning-on-graphs">Paper 19: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.09787.pdf">Group Contrastive Self-Supervised Learning on Graphs</a></h2>
<h2 id="paper-20-homophily-outlier-detection-in-non-iid-categorical-data">Paper 20: <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s10618-021-00750-y">Homophily outlier detection in non-IID categorical data</a></h2>
<h2 id="paper-21-hyperparameter-free-and-explainable-whole-graph-embedding">Paper 21: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.02113">Hyperparameter-free and Explainable Whole Graph Embedding</a></h2>
<h2 id="paper-22-infograph-unsupervised-and-semi-supervised-graph-level-representation-learning-via-mutual-information-maximization">Paper 22: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.01000">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</a></h2>
<h2 id="paper-23-iterative-graph-self-distillation">Paper 23: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.12609">Iterative graph self-distillation</a></h2>
<h2 id="paper-24-learning-by-aligning-videos-in-time">Paper 24: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.17260">Learning by Aligning Videos in Time</a></h2>
<h2 id="paper-25-learning-graph-representation-by-aggregating-subgraphs-via-mutual-information-maximization">Paper 25: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.13125">Learning graph representation by aggregating subgraphs via mutual information maximization</a></h2>
<h2 id="paper-26-mile-a-multi-level-framework-for-scalable-graph-embedding">Paper 26: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.09612">Mile: A multi-level framework for scalable graph embedding</a></h2>
<h2 id="paper-27-missing-data-estimation-in-temporal-multilayer-position-aware-graph-neural-network-tmp-gnn">Paper 27: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.03400.pdf">Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</a></h2>
<h2 id="paper-28-multi-level-graph-contrastive-learning">Paper 28: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.02639">Multi-Level Graph Contrastive Learning</a></h2>
<h2 id="paper-29-permutation-invariant-variational-autoencoder-for-graph-level-representation-learning">Paper 29: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.09856">Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</a></h2>
<h2 id="paper-30-pinet-attention-pooling-for-graph-classification">Paper 30: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.04575">PiNet: Attention Pooling for Graph Classification</a></h2>
<h2 id="paper-31-power-law-graph-transformer-for-machine-translation-and-representation-learning">Paper 31: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.02039">Power Law Graph Transformer for Machine Translation and Representation Learning</a></h2>
<h2 id="paper-32-self-supervised-graph-level-representation-learning-with-local-and-global-structure">Paper 32: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.04113">Self-supervised Graph-level Representation Learning with Local and Global Structure</a></h2>
<h2 id="paper-33-self-supervised-heterogeneous-graph-neural-network-with-co-contrastive-learning">Paper 33: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.09111">Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</a></h2>
<h2 id="paper-34-sm-sge-a-self-supervised-multi-scale-skeleton-graph-encoding-framework-for-person-re-identification">Paper 34: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.01903">SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</a></h2>
<h2 id="paper-35-space-time-correspondence-as-a-contrastive-random-walk">Paper 35: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.14613">Space-time correspondence as a contrastive random walk</a></h2>
<h2 id="paper-36-spatially-consistent-representation-learning">Paper 36: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.06122">Spatially consistent representation learning</a></h2>
<h2 id="paper-37-spatiotemporal-contrastive-video-representation-learning">Paper 37: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.03800">Spatiotemporal contrastive video representation learning</a></h2>
<h2 id="paper-38-sportscap-monocular-3d-human-motion-capture-and-fine-grained-understanding-in-challenging-sports-videos">Paper 38: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.11452">SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</a></h2>
<h2 id="paper-39-ssan-separable-self-attention-network-for-video-representation-learning">Paper 39: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.13033">SSAN: Separable Self-Attention Network for Video Representation Learning</a></h2>
<h2 id="paper-40-tdgraphembed-temporal-dynamic-graph-level-embedding">Paper 40: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411953">tdgraphembed: Temporal dynamic graph-level embedding</a></h2>
<h2 id="paper-41-videomoco-contrastive-video-representation-learning-with-temporally-adversarial-examples">Paper 41: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf">Videomoco: Contrastive video representation learning with temporally adversarial examples</a></h2>
<h2 id="paper-42-visual-relationship-forecasting-in-videos">Paper 42: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.01181">Visual Relationship Forecasting in Videos</a></h2>
<h2 id="paper-43-wasserstein-embedding-for-graph-learning">Paper 43: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=AAes_3W-2z">Wasserstein embedding for graph learning</a></h2>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">paper</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">SSL</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/notes//" class="article-tag-list-link color1">notes</a>
        		</li>
      		
		</ul>
	</div>


      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
  
    <a href="/posts/notes/2021-08-23-notes-paper-SSL-survey.html" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Paper--Fall Surveys</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-1-jigsaw-clustering-for-unsupervised-visual-representation-learning"><span class="toc-number">1.</span> <span class="toc-text">Paper 1: Jigsaw Clustering for Unsupervised Visual Representation Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-pretext-task"><span class="toc-number">1.1.</span> <span class="toc-text">Previous pretext task</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ideas"><span class="toc-number">1.2.</span> <span class="toc-text">Ideas</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#jigsaw-clustering-task"><span class="toc-number">1.2.1.</span> <span class="toc-text">Jigsaw Clustering task</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how"><span class="toc-number">1.3.</span> <span class="toc-text">How?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-2-self-supervised-motion-learning-from-static-images"><span class="toc-number">2.</span> <span class="toc-text">Paper 2: Self-supervised Motion Learning from Static Images</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why"><span class="toc-number">2.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-work"><span class="toc-number">2.2.</span> <span class="toc-text">Previous work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-1"><span class="toc-number">2.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#idea"><span class="toc-number">2.3.1.</span> <span class="toc-text">Idea</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#motion-learning-from-static-images"><span class="toc-number">2.3.2.</span> <span class="toc-text">Motion learning from static images</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#pseudo-motions"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">Pseudo motions</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#static-masks"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">Static masks</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#implementation"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">Implementation</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-3-self-supervised-video-representation-learning-by-context-and-motion-decoupling"><span class="toc-number">3.</span> <span class="toc-text">Paper 3: Self-supervised Video Representation Learning by Context and Motion Decoupling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-1"><span class="toc-number">3.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-work-1"><span class="toc-number">3.2.</span> <span class="toc-text">Previous work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal"><span class="toc-number">3.3.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-2"><span class="toc-number">3.4.</span> <span class="toc-text">How</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-4-skip-convolutions-for-efficient-video-processing"><span class="toc-number">4.</span> <span class="toc-text">Paper 4: Skip-convolutions for Efficient Video Processing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-2"><span class="toc-number">4.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#previous-work-2"><span class="toc-number">4.2.</span> <span class="toc-text">Previous work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-1"><span class="toc-number">4.3.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-3"><span class="toc-number">4.4.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#skip-convolutions"><span class="toc-number">4.4.1.</span> <span class="toc-text">Skip Convolutions</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-5-temporal-query-networks-for-fine-grained-video-understanding"><span class="toc-number">5.</span> <span class="toc-text">Paper 5: Temporal Query Networks for Fine-grained Video Understanding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-3"><span class="toc-number">5.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-2"><span class="toc-number">5.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-4"><span class="toc-number">5.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tqn"><span class="toc-number">5.3.1.</span> <span class="toc-text">TQN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#stochastically-updated-feature-bank"><span class="toc-number">5.3.2.</span> <span class="toc-text">Stochastically updated feature bank</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#factorizing-categories-into-attribute-queries"><span class="toc-number">5.3.3.</span> <span class="toc-text">Factorizing categories into attribute queries</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#implementation-1"><span class="toc-number">5.3.4.</span> <span class="toc-text">Implementation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-6-unsupervised-disentanglement-of-linear-encoded-facial-semantics"><span class="toc-number">6.</span> <span class="toc-text">Paper 6: Unsupervised disentanglement of linear-encoded facial semantics</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-4"><span class="toc-number">6.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-3"><span class="toc-number">6.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-5"><span class="toc-number">6.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#decorrelating-latent-code-in-stylegan"><span class="toc-number">6.3.1.</span> <span class="toc-text">Decorrelating latent code in StyleGAN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#stabilized-training-for-3d-face-reconstruction"><span class="toc-number">6.3.2.</span> <span class="toc-text">Stabilized training for 3D face reconstruction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#disentangle-semantics-with-linear-regression"><span class="toc-number">6.3.3.</span> <span class="toc-text">Disentangle semantics with linear regression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#image-manipulation-for-data-augmentation"><span class="toc-number">6.3.4.</span> <span class="toc-text">Image manipulation for data augmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#localized-representation-learning"><span class="toc-number">6.3.5.</span> <span class="toc-text">Localized representation learning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-7-unsupervised-disentanglement-of-linear-encoded-facial-semantics"><span class="toc-number">7.</span> <span class="toc-text">Paper 7: Unsupervised disentanglement of linear-encoded facial semantics</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-5"><span class="toc-number">7.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-4"><span class="toc-number">7.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-6"><span class="toc-number">7.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#gcn"><span class="toc-number">7.3.1.</span> <span class="toc-text">GCN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bi-gcn"><span class="toc-number">7.3.2.</span> <span class="toc-text">Bi-GCN</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#binarization-of-the-feature-extraction-step"><span class="toc-number">7.3.2.1.</span> <span class="toc-text">Binarization of the feature extraction step</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#binary-gradient-approximation-based-back-propagation"><span class="toc-number">7.3.2.2.</span> <span class="toc-text">Binary gradient approximation based back propagation</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-8-an-attractor-guided-neural-networks-for-skeleton-based-human-motion-prediction"><span class="toc-number">8.</span> <span class="toc-text">Paper 8: An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-6"><span class="toc-number">8.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-5"><span class="toc-number">8.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-7"><span class="toc-number">8.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mtde"><span class="toc-number">8.3.1.</span> <span class="toc-text">MTDE</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ajre"><span class="toc-number">8.3.2.</span> <span class="toc-text">AJRE</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#gce"><span class="toc-number">8.3.2.1.</span> <span class="toc-text">GCE</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#lie"><span class="toc-number">8.3.2.2.</span> <span class="toc-text">LIE</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#affm"><span class="toc-number">8.3.2.3.</span> <span class="toc-text">AFFM</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-9-cascade-graph-neural-networks-for-rgb-d-salient-object-detection"><span class="toc-number">9.</span> <span class="toc-text">Paper 9: Cascade Graph Neural Networks for RGB-D Salient Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-7"><span class="toc-number">9.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-6"><span class="toc-number">9.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-8"><span class="toc-number">9.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#cross-modality-reasoning-with-gnns"><span class="toc-number">9.3.1.</span> <span class="toc-text">Cross-modality reasoning with GNNs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cascade-graph-neural-networks"><span class="toc-number">9.3.2.</span> <span class="toc-text">Cascade Graph Neural networks</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-10-coarse-fine-networks-for-temporal-activity-detection-in-videos"><span class="toc-number">10.</span> <span class="toc-text">Paper 10: Coarse-Fine Networks for Temporal Activity Detection in Videos</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-8"><span class="toc-number">10.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-7"><span class="toc-number">10.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-9"><span class="toc-number">10.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#grid-pool"><span class="toc-number">10.3.1.</span> <span class="toc-text">Grid pool</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-stage-fusion"><span class="toc-number">10.3.2.</span> <span class="toc-text">Multi-stage fusion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#model-details"><span class="toc-number">10.3.3.</span> <span class="toc-text">Model details</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-11-coconets-continuous-contrastive-3d-scene-representations"><span class="toc-number">11.</span> <span class="toc-text">Paper 11: CoCoNets: Continuous Contrastive 3D Scene Representations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-9"><span class="toc-number">11.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-8"><span class="toc-number">11.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-10"><span class="toc-number">11.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#result"><span class="toc-number">11.3.1.</span> <span class="toc-text">Result</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-12-cutpaste-self-supervised-learning-for-anomaly-detection-and-localization"><span class="toc-number">12.</span> <span class="toc-text">Paper 12: CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-10"><span class="toc-number">12.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-9"><span class="toc-number">12.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-11"><span class="toc-number">12.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ssl-with-cutpaste"><span class="toc-number">12.3.1.</span> <span class="toc-text">SSL with CutPaste</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cutpaste-variants"><span class="toc-number">12.3.2.</span> <span class="toc-text">CutPaste variants</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#computing-anomaly-score"><span class="toc-number">12.3.3.</span> <span class="toc-text">Computing anomaly score</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#localization-with-patch-representation"><span class="toc-number">12.3.4.</span> <span class="toc-text">Localization with patch representation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-13-discriminative-latent-semantic-graph-for-video-captioning"><span class="toc-number">13.</span> <span class="toc-text">Paper 13: Discriminative Latent Semantic Graph for Video Captioning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-11"><span class="toc-number">13.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-10"><span class="toc-number">13.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-12"><span class="toc-number">13.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#architecture-design"><span class="toc-number">13.3.1.</span> <span class="toc-text">Architecture Design</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#latent-semantic-graph"><span class="toc-number">13.3.2.</span> <span class="toc-text">Latent Semantic graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#discriminative-language-validation"><span class="toc-number">13.3.3.</span> <span class="toc-text">Discriminative language validation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-14-enhancing-self-supervised-video-representation-learning-via-multi-level-feature-optimization"><span class="toc-number">14.</span> <span class="toc-text">Paper 14: Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-12"><span class="toc-number">14.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-11"><span class="toc-number">14.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-13"><span class="toc-number">14.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#beyond-instance-discrimination"><span class="toc-number">14.3.1.</span> <span class="toc-text">Beyond instance discrimination</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graph-constraint-for-multi-level-features"><span class="toc-number">14.3.2.</span> <span class="toc-text">Graph constraint for multi-level features</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#temporal-modeling"><span class="toc-number">14.3.3.</span> <span class="toc-text">Temporal modeling</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-15-exploring-simple-siamese-representation-learning"><span class="toc-number">15.</span> <span class="toc-text">Paper 15: Exploring simple siamese representation learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-13"><span class="toc-number">15.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-12"><span class="toc-number">15.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-14"><span class="toc-number">15.3.</span> <span class="toc-text">How</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-16-git-graph-interactive-transformer-for-vehicle-re-identification"><span class="toc-number">16.</span> <span class="toc-text">Paper 16: GiT: Graph Interactive Transformer for Vehicle Re-identification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-14"><span class="toc-number">16.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-13"><span class="toc-number">16.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-15"><span class="toc-number">16.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lcg-module"><span class="toc-number">16.3.1.</span> <span class="toc-text">LCG module</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformer-layer"><span class="toc-number">16.3.2.</span> <span class="toc-text">Transformer layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graph-interactive-transformer"><span class="toc-number">16.3.3.</span> <span class="toc-text">Graph interactive Transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#loss-function-design"><span class="toc-number">16.3.4.</span> <span class="toc-text">Loss function design</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-17-graph-time-convolutional-neural-networks"><span class="toc-number">17.</span> <span class="toc-text">Paper 17: Graph-Time Convolutional Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why-15"><span class="toc-number">17.1.</span> <span class="toc-text">Why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#goal-14"><span class="toc-number">17.2.</span> <span class="toc-text">Goal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-16"><span class="toc-number">17.3.</span> <span class="toc-text">How</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#signals-over-product-graphs"><span class="toc-number">17.3.1.</span> <span class="toc-text">Signals over product graphs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graph-time-cnns"><span class="toc-number">17.3.2.</span> <span class="toc-text">Graph-time CNNs</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-18-graphzoom-a-multi-level-spectral-approach-for-accurate-and-scalable-graph-embedding"><span class="toc-number">18.</span> <span class="toc-text">Paper 18: Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-19-group-contrastive-self-supervised-learning-on-graphs"><span class="toc-number">19.</span> <span class="toc-text">Paper 19: Group Contrastive Self-Supervised Learning on Graphs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-20-homophily-outlier-detection-in-non-iid-categorical-data"><span class="toc-number">20.</span> <span class="toc-text">Paper 20: Homophily outlier detection in non-IID categorical data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-21-hyperparameter-free-and-explainable-whole-graph-embedding"><span class="toc-number">21.</span> <span class="toc-text">Paper 21: Hyperparameter-free and Explainable Whole Graph Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-22-infograph-unsupervised-and-semi-supervised-graph-level-representation-learning-via-mutual-information-maximization"><span class="toc-number">22.</span> <span class="toc-text">Paper 22: Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-23-iterative-graph-self-distillation"><span class="toc-number">23.</span> <span class="toc-text">Paper 23: Iterative graph self-distillation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-24-learning-by-aligning-videos-in-time"><span class="toc-number">24.</span> <span class="toc-text">Paper 24: Learning by Aligning Videos in Time</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-25-learning-graph-representation-by-aggregating-subgraphs-via-mutual-information-maximization"><span class="toc-number">25.</span> <span class="toc-text">Paper 25: Learning graph representation by aggregating subgraphs via mutual information maximization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-26-mile-a-multi-level-framework-for-scalable-graph-embedding"><span class="toc-number">26.</span> <span class="toc-text">Paper 26: Mile: A multi-level framework for scalable graph embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-27-missing-data-estimation-in-temporal-multilayer-position-aware-graph-neural-network-tmp-gnn"><span class="toc-number">27.</span> <span class="toc-text">Paper 27: Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-28-multi-level-graph-contrastive-learning"><span class="toc-number">28.</span> <span class="toc-text">Paper 28: Multi-Level Graph Contrastive Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-29-permutation-invariant-variational-autoencoder-for-graph-level-representation-learning"><span class="toc-number">29.</span> <span class="toc-text">Paper 29: Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-30-pinet-attention-pooling-for-graph-classification"><span class="toc-number">30.</span> <span class="toc-text">Paper 30: PiNet: Attention Pooling for Graph Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-31-power-law-graph-transformer-for-machine-translation-and-representation-learning"><span class="toc-number">31.</span> <span class="toc-text">Paper 31: Power Law Graph Transformer for Machine Translation and Representation Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-32-self-supervised-graph-level-representation-learning-with-local-and-global-structure"><span class="toc-number">32.</span> <span class="toc-text">Paper 32: Self-supervised Graph-level Representation Learning with Local and Global Structure</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-33-self-supervised-heterogeneous-graph-neural-network-with-co-contrastive-learning"><span class="toc-number">33.</span> <span class="toc-text">Paper 33: Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-34-sm-sge-a-self-supervised-multi-scale-skeleton-graph-encoding-framework-for-person-re-identification"><span class="toc-number">34.</span> <span class="toc-text">Paper 34: SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-35-space-time-correspondence-as-a-contrastive-random-walk"><span class="toc-number">35.</span> <span class="toc-text">Paper 35: Space-time correspondence as a contrastive random walk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-36-spatially-consistent-representation-learning"><span class="toc-number">36.</span> <span class="toc-text">Paper 36: Spatially consistent representation learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-37-spatiotemporal-contrastive-video-representation-learning"><span class="toc-number">37.</span> <span class="toc-text">Paper 37: Spatiotemporal contrastive video representation learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-38-sportscap-monocular-3d-human-motion-capture-and-fine-grained-understanding-in-challenging-sports-videos"><span class="toc-number">38.</span> <span class="toc-text">Paper 38: SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-39-ssan-separable-self-attention-network-for-video-representation-learning"><span class="toc-number">39.</span> <span class="toc-text">Paper 39: SSAN: Separable Self-Attention Network for Video Representation Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-40-tdgraphembed-temporal-dynamic-graph-level-embedding"><span class="toc-number">40.</span> <span class="toc-text">Paper 40: tdgraphembed: Temporal dynamic graph-level embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-41-videomoco-contrastive-video-representation-learning-with-temporally-adversarial-examples"><span class="toc-number">41.</span> <span class="toc-text">Paper 41: Videomoco: Contrastive video representation learning with temporally adversarial examples</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-42-visual-relationship-forecasting-in-videos"><span class="toc-number">42.</span> <span class="toc-text">Paper 42: Visual Relationship Forecasting in Videos</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#paper-43-wasserstein-embedding-for-graph-learning"><span class="toc-number">43.</span> <span class="toc-text">Paper 43: Wasserstein embedding for graph learning</span></a></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>



  
  
  

  

  

  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2021 Mia Feng
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


</footer>

    </div>
    <script>
	var yiliaConfig = {
		mathjax: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">posts</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">papers</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">about</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">notes</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Sensor</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">GPR</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">summer school</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">mathematics</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">cnn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">compress</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">infrared</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">hyperspectral</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">blog</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">logistic</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">ML</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">naive</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">applications</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">tendency</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">da</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">data assimilation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">machine learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">paper</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">gcn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SSL</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">skeleton</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">remote sensing images</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">interpolation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">compression</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">reconstruction</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">AI</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Data assimilation</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">survey</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">math</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">anomaly</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">cv</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">3d shape</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">fall</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">book</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://skaudrey.github.io/posts/projects/2018-11-11-gpr.html" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>wind interpolation</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">&lt;br&gt; &amp;#160; &amp;#160; &amp;#160; &amp;#160;Mia, &lt;/br&gt;&lt;br&gt; &amp;#160; &amp;#160; &amp;#160; &amp;#160;a master student in NUDT.&lt;/br&gt; &lt;br&gt;  &amp;#160; &amp;#160; &amp;#160; &amp;#160;I love reading, exercising and cooking. &lt;br /&gt; &lt;br&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;Science fictions, detective novels are my favorite.&lt;/br&gt;</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>